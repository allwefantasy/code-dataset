{"instruction": "添加一个命令行, 在 serve 目录下添加一个 cli.py, 支持配置 ServerArgs 里的参数,然后调用 @api_server.py 中的serve", "conversations": [{"role": "user", "content": "\n如果你需要生成代码，对于每个需要更改的文件,你需要按 *SEARCH/REPLACE block* 的格式进行生成。\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The opening fence and code language, eg: ```python\n2. The file path alone on a line, starting with \"##File:\" and verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n*SEARCH/REPLACE* blocks will replace *all* matching occurrences.\nInclude enough lines to make the SEARCH blocks unique.\n\nInclude *ALL* the code being searched and replaced!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n下面我们来看一个例子：\n\n当前项目目录结构：\n1. 项目根目录： /tmp/projects/mathweb\n2. 项目子目录/文件列表(类似tree 命令输出)\nflask/\n    app.py\n    templates/\n        index.html\n    static/\n        style.css\n\n用户需求： Change get_factorial() to use math.factorial\n\n回答： To make this change we need to modify `/tmp/projects/mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n用户需求： Refactor hello() into its own file.\n\n回答：To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/hello.py\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/main.py\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n现在让我们开始一个新的任务:\n\n当前项目目录结构：\n1. 项目根目录： /Users/allwefantasy/projects/super_analysis\n2. 项目子目录/文件列表(类似tree 命令输出)：\n.auto-coder/\n    libs/\n        llm_friendly_packages/\n            src/\n                example1/\n                    main.py\nsrc/\n    superanalysis/\n        serve/\n            openai_serve.py\n            llm_wrapper.py\n            api_server.py\n下面是一些文件路径以及每个文件对应的源码：\n\n##File: .auto-coder/libs/llm_friendly_packages/github.com/allwefantasy/byzer-llm/README.md\n# byzerllm 大模型编程快速指南\n\n本文示例 [Notebook](../../notebooks/003_byzerllm_大模型编程快速指南.ipynb)\n\n## 安装\n\n```bash\npip install -U byzerllm\nray start --head\n```\n## 启动一个模型代理\n\nbyzerllm 支持私有化模型或者SaaS模型的部署。\n\n这里以 deepseek 官方API 为例：\n\n```bash\neasy-byzerllm deploy deepseek-chat --token xxxxx --alias deepseek_chat\n```\n\n或者跬基流动API:\n\n```bash\neasy-byzerllm deploy alibaba/Qwen1.5-110B-Chat --token xxxxx --alias qwen110b_chat\n```\n\n将上面的 API KEY 替换成你们自己的。\n\n如果你想部署私有化模型或者对接 Ollama 等更多需求，参考 [002_使用byzerllm进行模型部署.md](./002_使用byzerllm进行模型部署.md) 或者 [README.md](../../README.md)。\n\n之后，你就可以在代码里使用  deepseek_chat 或者 qwen110b_chat  访问模型了。\n\n## hello world\n\n来和我们的大模型打个招呼:\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\n@byzerllm.prompt(llm=llm)\ndef hello(q:str) ->str:\n    '''\n    你好, {{ q }}\n    '''\n\ns = hello(\"你是谁\")    \nprint(s)\n\n## 输出:\n## '你好！我是一个人工智能助手，专门设计来回答问题、提供信息和帮助解决问题。如果你有任何疑问或需要帮助，请随时告诉我。'\n```\n\n恭喜，你和大模型成功打了招呼！\n\n可以看到，我们通过 `@byzerllm.prompt` 装饰器，将一个方法转换成了一个大模型的调用，然后这个方法的主题是一段文本，文本中\n使用了 jinja2 模板语法，来获得方法的参数。当正常调用该方法时，实际上就发起了和大模型的交互，并且返回了大模型的结果。\n\n在 byzerllm 中，我们把这种方法称之为 prompt 函数。\n\n## 查看发送给大模型的prompt\n\n很多情况你可能需要调试，查看自己的 prompt 渲染后到底是什么样子的，这个时候你可以通过如下方式\n获取渲染后的prompt:\n\n```python\nhello.prompt(\"你是谁\")\n## '你好, 你是谁'\n```            \n\n## 动态换一个模型\n\n前面的 hello 方法在初始化的时候，我们使用了默认的模型 deepseek_chat，如果我们想换一个模型，可以这样做：\n\n```python\nhello.with_llm(llm).run(\"你是谁\")\n## '你好！我是一个人工智能助手，专门设计来回答问题、提供信息和帮助解决问题。如果你有任何疑问或需要帮助，请随时告诉我。'\n```\n\n通过 with_llm 你可以设置一个新的 llm 对象，然后调用 run 方法，就可以使用新的模型了。\n\n## 超长文本生成\n\n我们知道，大模型一次生成的长度其实是有限的，如果你想生成超长文本，你可能需手动的不断获得\n生成结果，然后把他转化为输入，然后再次生成，这样的方式是比较麻烦的。\n\nbyzerllm 提供了更加易用的 API :\n\n```python\nimport byzerllm\nfrom byzerllm import ByzerLLM\n\nllm = ByzerLLM.from_default_model(\"deepseek_chat\")\n\n@byzerllm.prompt()\ndef tell_story() -> str:\n    \"\"\"\n    讲一个100字的故事。    \n    \"\"\"\n\n\ns = (\n    tell_story.with_llm(llm)\n    .with_response_markers()\n    .options({\"llm_config\": {\"max_length\": 10}})\n    .run()\n)\nprint(s)\n\n## 从前，森林里住着一只聪明的小狐狸。一天，它发现了一块闪闪发光的宝石。小狐狸决定用这块宝石帮助森林里的动物们。它用宝石的光芒指引迷路的小鹿找到了回家的路，用宝石的温暖治愈了受伤的小鸟。从此，小狐狸成了森林里的英雄，动物们都感激它的善良和智慧。\n```\n\n实际核心部分就是这一行：\n\n```python\ntell_story.with_llm(llm)\n    .with_response_markers()    \n    .run()\n```\n\n我们只需要调用 `with_response_markers` 方法，系统就会自动的帮我们生成超长文本。\n在上面的案例中，我们通过\n\n```python\n.options({\"llm_config\": {\"max_length\": 10}})\n```\n\n认为的限制大模型一次交互最多只能输出10个字符，但是系统依然自动完成了远超过10个字符的文本生成。\n\n## 对象输出\n\n前面我们的例子都是返回字符串，但是我们也可以返回对象，这样我们就可以更加灵活的处理返回结果。\n\n```python\nimport pydantic \n\nclass Story(pydantic.BaseModel):\n    '''\n    故事\n    '''\n\n    title: str = pydantic.Field(description=\"故事的标题\")\n    body: str = pydantic.Field(description=\"故事主体\")\n\n@byzerllm.prompt()\ndef tell_story()->Story:\n    '''\n    讲一个100字的故事。    \n    '''\n\ns = tell_story.with_llm(llm).run()\nprint(isinstance(s, Story))\nprint(s.title)\n\n## True\n## 勇敢的小鸟\n```\n\n可以看到，我们很轻松的将输出转化为格式化输出。\n\n## 自定义字段抽取\n\n前面的结构化输出，其实会消耗更多token,还有一种更加精准的结构化输出方式。\n比如让大模型生成一个正则表达式，但实际上大模型很难准确只输出一个正则表达式，这个时候我们可以通过自定义抽取函数来获取我们想要的结果。\n\n\n```python\nfrom loguru import logger\nimport re\n\n@byzerllm.prompt()\ndef generate_regex_pattern(desc: str) -> str:\n    \"\"\"\n    根据下面的描述生成一个正则表达式，要符合python re.compile 库的要求。\n\n    {{ desc }}\n\n    最后生成的正则表达式要在<REGEX></REGEX>标签对里。\n    \"\"\"    \n\ndef extract_regex_pattern(regex_block: str) -> str:    \n    pattern = re.search(r\"<REGEX>(.*)</REGEX>\", regex_block, re.DOTALL)\n    if pattern is None:\n        logger.warning(\"No regex pattern found in the generated block:\\n {regex_block}\")\n        raise None\n    return pattern.group(1)\n\npattern = \"匹配一个邮箱地址\"\nv = generate_regex_pattern.with_llm(llm).with_extractor(extract_regex_pattern).run(desc=pattern)\nprint(v)\n## ^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\n```\n\n在上面的例子里，我们根据一句话生成一个正则表达式。我们通过 `with_extractor` 方法，传入了一个自定义的抽取函数，这个函数会在大模型生成结果后，对结果进行处理，然后返回我们想要的结果。\n\n我们在 prompt 明确说了，生成的结果要放到 `<REGEX></REGEX>` 标签对里，然后我们通过 extract_regex_pattern 函数，从结果中提取出了我们想要的正则表达式。\n\n## 在实例方法中使用大模型\n\n```python\nimport byzerllm\ndata = {\n    'name': 'Jane Doe',\n    'task_count': 3,\n    'tasks': [\n        {'name': 'Submit report', 'due_date': '2024-03-10'},\n        {'name': 'Finish project', 'due_date': '2024-03-15'},\n        {'name': 'Reply to emails', 'due_date': '2024-03-08'}\n    ]\n}\n\n\nclass RAG():\n    def __init__(self):        \n        self.llm = byzerllm.ByzerLLM()\n        self.llm.setup_template(model=\"deepseek_chat\",template=\"auto\")\n        self.llm.setup_default_model_name(\"deepseek_chat\")        \n    \n    @byzerllm.prompt(lambda self:self.llm)\n    def generate_answer(self,name,task_count,tasks)->str:\n        '''\n        Hello {{ name }},\n\n        This is a reminder that you have {{ task_count }} pending tasks:\n        {% for task in tasks %}\n        - Task: {{ task.name }} | Due: {{ task.due_date }}\n        {% endfor %}\n\n        Best regards,\n        Your Reminder System\n        '''        \n\nt = RAG()\n\nresponse = t.generate_answer(**data)\nprint(response)\n\n## 输出:\n## Hello Jane Doe,\n##I hope this message finds you well. I wanted to remind you of your 3 pending tasks to ensure you stay on track:\n## 1. **Submit report** - This task is due on **2024-03-10**. Please ensure that you allocat\n```\n\n这里我们给了个比较复杂的例子，但我们可以看到，给一个实例prompt方法和普通prompt 方法差异不大。\n唯一的区别是如果你希望在定义的时候就指定大模型，使用一个lambda函数返回实例的 llm 对象即可。\n\n```python\n@byzerllm.prompt(lambda self:self.llm)\n```\n\n你也可以不返回，在调用的时候通过 `with_llm` 方法指定 llm 对象。\n\n此外，这个例子也展示了如何通过jinja2模板语法，来处理复杂的结构化数据。\n\n## 通过 Python 代码处理复杂入参\n\n上面的一个例子中，我们通过 jinja2 模板语法，来处理复杂的结构化数据，但是有时候我们可能需要更加复杂的处理，这个时候我们可以通过 Python 代码来处理。\n\n```python\nimport byzerllm\n\ndata = {\n    'name': 'Jane Doe',\n    'task_count': 3,\n    'tasks': [\n        {'name': 'Submit report', 'due_date': '2024-03-10'},\n        {'name': 'Finish project', 'due_date': '2024-03-15'},\n        {'name': 'Reply to emails', 'due_date': '2024-03-08'}\n    ]\n}\n\n\nclass RAG():\n    def __init__(self):        \n        self.llm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n    \n    @byzerllm.prompt()\n    def generate_answer(self,name,task_count,tasks)->str:\n        '''\n        Hello {{ name }},\n\n        This is a reminder that you have {{ task_count }} pending tasks:\n            \n        {{ tasks }}\n\n        Best regards,\n        Your Reminder System\n        '''\n        \n        tasks_str = \"\\n\".join([f\"- Task: {task['name']} | Due: { task['due_date'] }\" for task in tasks])\n        return {\"tasks\": tasks_str}\n\nt = RAG()\n\nresponse = t.generate_answer.with_llm(t.llm).run(**data)\nprint(response)\n\n## Just a gentle nudge to keep you on track with your pending tasks. Here's a quick recap:....\n```\n\n在这个例子里，我们直接把 tasks 在方法体里进行处理，然后作为一个字符串返回，最够构建一个字典，字典的key为 tasks,然后\n你就可以在 docstring 里使用 `{{ tasks }}` 来引用这个字符串。\n\n这样对于很复杂的入参，就不用谢繁琐的 jinja2 模板语法了。\n\n## 如何自动实现一个方法\n\n比如我定义一个签名，但是我不想自己实现里面的逻辑，让大模型来实现。这个在 byzerllm 中叫 function impl。我们来看看怎么\n实现:\n\n```python\nimport pydantic\nclass Time(pydantic.BaseModel):\n    time: str = pydantic.Field(...,description=\"时间，时间格式为 yyyy-MM-dd\")\n\n\n@llm.impl()\ndef calculate_current_time()->Time:\n    '''\n    计算当前时间\n    '''\n    pass \n\n\ncalculate_current_time()\n#output: Time(time='2024-06-14')\n```\n\n在这个例子里，我们定义了一个 calculate_current_time 方法，但是我们没有实现里面的逻辑，我们通过 `@llm.impl()` 装饰器，让大模型来实现这个方法。\n为了避免每次都要“生成”这个方法，导致无法适用，我们提供了缓存，用户可以按如下方式打印速度：\n\n```python\nstart = time.monotonic()\ncalculate_current_time()\nprint(f\"first time cost: {time.monotonic()-start}\")\n\nstart = time.monotonic()\ncalculate_current_time()\nprint(f\"second time cost: {time.monotonic()-start}\")\n\n# output:\n# first time cost: 6.067266260739416\n# second time cost: 4.347506910562515e-05\n```\n可以看到，第一次执行花费了6s,第二次几乎是瞬间完成的，这是因为第一次执行的时候，我们实际上是在生成这个方法，第二次执行的时候，我们是执行已经生成好的代码，所以速度会非常快。你可以显示的调用 `llm.clear_impl_cache()` 清理掉函数缓存。\n\n## Stream 模式\n\n前面的例子都是一次性生成结果，但是有时候我们可能需要一个流式的输出，这个时候我们可能需要用底层一点的API来完成了：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\nv = llm.stream_chat_oai(model=\"deepseek_chat\",conversations=[{\n    \"role\":\"user\",\n    \"content\":\"你好，你是谁\",\n}],delta_mode=True)\n\nfor t in v:\n    print(t,flush=True)  \n\n# 你好\n# ！\n# 我\n# 是一个\n# 人工智能\n# 助手\n# ，\n# 旨在\n# 提供\n# 信息\n# 、\n# 解答\n# 问题....\n```\n\n如果你不想要流式输出，但是想用底层一点的API，你可以使用 `llm.chat_oai` 方法：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\nv = llm.chat_oai(model=\"deepseek_chat\",conversations=[{\n    \"role\":\"user\",\n    \"content\":\"你好，你是谁\",\n}])\n\nprint(v[0].output)\n## 你好！我是一个人工智能助手，旨在提供信息、解答问题和帮助用户解决问题。如果你有任何问题或需要帮助，请随时告诉我。\n```\n\n## Function Calling \n\nbyzerllm 可以不依赖模型自身就能提供 function calling 支持，我们来看个例子：\n\n\n```python\nfrom typing import List,Dict,Any,Annotated\nimport pydantic \nimport datetime\nfrom dateutil.relativedelta import relativedelta\n\ndef compute_date_range(count:Annotated[int,\"时间跨度，数值类型\"],\n                       unit:Annotated[str,\"时间单位，字符串类型\",{\"enum\":[\"day\",\"week\",\"month\",\"year\"]}])->List[str]:\n    '''\n    计算日期范围\n\n    Args:\n        count: 时间跨度，数值类型\n        unit: 时间单位，字符串类型，可选值为 day,week,month,year\n    '''        \n    now = datetime.datetime.now()\n    now_str = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n    if unit == \"day\":\n        return [(now - relativedelta(days=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"week\":\n        return [(now - relativedelta(weeks=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"month\":\n        return [(now - relativedelta(months=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"year\":\n        return [(now - relativedelta(years=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    return [\"\",\"\"]\n\ndef compute_now()->str:\n    '''\n    计算当前时间\n    '''\n    return datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n```\n\n我们定义了两个方法，一个是计算日期范围，一个是计算当前时间。\n\n现在我么可以来测试下，系统如何根据自然语言决定调用哪个方法：\n\n```python\nt = llm.chat_oai([{\n    \"content\":'''计算当前时间''',\n    \"role\":\"user\"    \n}],tools=[compute_date_range,compute_now],execute_tool=True)\n\nt[0].values\n\n## output: ['2024-06-14 15:18:02']\n```\n\n我们可以看到，他正确的选择了 compute_now 方法。\n\n接着我们再试一个：\n\n```python\nt = llm.chat_oai([{\n    \"content\":'''最近三个月趋势''',\n    \"role\":\"user\"    \n}],tools=[compute_date_range,compute_now],execute_tool=True)\n\nt[0].values\n\n## output: [['2024-03-14 15:19:13', '2024-06-14 15:19:13']]\n```\n\n模型正确的选择了 compute_date_range 方法。\n\n## 多模态\n\nbyerllm 也能很好的支持多模态的交互，而且统一了多模态大模型的接口，比如你可以用一样的方式使用 openai 或者 claude 的图片转文字能力， 或者一致的方式使用火山，azuer, openai的语音合成接口。\n\n### 图生文\n\n```python\nimport byzerllm\nfrom byzerllm.types import ImagePath\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\n\n\n@byzerllm.prompt()\ndef what_in_image(image_path: ImagePath) -> str:\n    \"\"\"\n    {{ image_path }}\n    这个图片里有什么？\n    \"\"\"    \n\n\nv = what_in_image.with_llm(vl_llm).run(\n    ImagePath(value=\"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\")\n)\nv\n## OUTPUT: 这张图片展示了多只可爱的猫咪，采用了艺术风格的绘画。猫咪们有不同的颜色和花纹，背景是浅棕色，上面还点缀着一些红色的花朵。整体画面给人一种温馨和谐的感觉\n```\n\n可以看到，我们只需要把 prompt 函数的图片地址入参使用 byzerllm.types.ImagePath里进行包装，就可以直接在 prompt 函数体里\n带上图片。\n\n或者你可以这样：\n\n```python\nimport byzerllm\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\n\n\n@byzerllm.prompt()\ndef what_in_image(image_path: str) -> str:\n    \"\"\"\n    {{ image }}\n    这个图片里有什么？\n    \"\"\"\n    return {\"image\": byzerllm.Image.load_image_from_path(image_path)}\n\n\nv = what_in_image.with_llm(vl_llm).run(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv\n```\n\n通过 `image_path` 参数，然后通过 `byzerllm.Image.load_image_from_path` 方法，转化为一个图片对象 image，最后在 prompt 函数体里\n使用 `{{ image }}` 引用这个图片对象。\n\n另外我们也是可以支持配置多张图片的。\n\n另外，我们也可以使用基础的 `llm.chat_oai` 方法来实现：\n\n```python\nimport byzerllm\nimport json\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\nimage = byzerllm.Image.load_image_from_path(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv = vl_llm.chat_oai(\n    conversations=[\n        {\n            \"role\": \"user\",\n            \"content\": json.dumps(\n                [{\"image\": image, \"text\": \"这个图片里有什么？\"}], ensure_ascii=False\n            ),\n        }\n    ]\n)\nv[0].output\n```\n\n还可以这么写：\n\n```python\nimport byzerllm\nimport json\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\nimage = byzerllm.Image.load_image_from_path(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv = vl_llm.chat_oai(\n    conversations=[\n        {\n            \"role\": \"user\",\n            \"content\": json.dumps(\n                [\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\"url\": image, \"detail\": \"high\"},\n                    },\n                    {\"text\": \"这个图片里有什么？\", \"type\": \"text\"},\n                ],\n                ensure_ascii=False,\n            ),\n        }\n    ]\n)\nv[0].output\n```\n\n### 语音合成\n\n这里以 openai 的 tts 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=tts-1 \\\n--model openai_tts\n```\n\n此外，byzerllm 支持 azure,火山引擎等 tts 语音合成引擎。\n\n接着你可以这么用：\n\n```python\nimport byzerllm\nimport base64\nimport json\n\nllm = byzerllm.ByzerLLM.from_default_model(\"openai_tts\")\n\n\nt = llm.chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\": json.dumps({\n        \"input\":\"hello, open_tts\",\n        \"voice\": \"alloy\",\n        \"response_format\": \"mp3\"\n    },ensure_ascii=False)\n}])\n\nwith open(\"voice.mp3\",\"wb\") as f:\n    f.write(base64.b64decode(t[0].output))\n```\n\ntts 模型生成没有prompt函数可以用，你需要直接使用 chat_oai。\n\n\n### 语音识别\n\n这里以 openai 的 whisper-1 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--worker_concurrency 10 \\\n--infer_params saas.model=whisper-1 saas.api_key=${MODEL_OPENAI_TOKEN} \\\n--model speech_to_text\n```\n\n语音识别的使用方式和图生文类似，我们可以直接在 prompt 函数体里带上音频文件。\n\n```python\nimport byzerllm\nimport json\nimport base64\nfrom byzerllm.types import AudioPath\n\nllm = byzerllm.ByzerLLM.from_default_model(\"speech_to_text\")\n\naudio_file = \"/Users/allwefantasy/videos/output_audio.mp3\"\n\n@byzerllm.prompt(llm=llm)\ndef audio_to_text(audio_file: AudioPath):\n    \"\"\"\n    {{ audio_file }}\n    \"\"\"\n\nv = audio_to_text(AudioPath(value=audio_file))\njson.loads(v)\n```\n输出的数据格式略微复杂：\n\n```\n{'text': 'In the last chapter, you and I started to step through the internal workings of a transformer. This is one of the key pieces of technology inside large language models, and a lot of other tools in the modern wave of AI.',\n 'task': 'transcribe',\n 'language': 'english',\n 'duration': 10.0,\n 'segments': [{'id': 0,\n   'seek': 0,\n   'start': 0.0,\n   'end': 4.78000020980835,\n   'text': ' In the last chapter, you and I started to step through the internal workings of a transformer.',\n   'tokens': [50364,\n    682,\n    264,\n    1036,\n    7187,\n    11,\n    291,\n    293,\n    286,\n    1409,\n    281,\n    1823,\n    807,\n    264,\n    6920,\n    589,\n    1109,\n    295,\n    257,\n    31782,\n    13,\n    50586],\n   'temperature': 0.0,\n   'avg_logprob': -0.28872039914131165,\n   'compression_ratio': 1.4220778942108154,\n   'no_speech_prob': 0.016033057123422623},\n  ....\n  {'id': 2,\n   'seek': 0,\n   'start': 8.579999923706055,\n   'end': 9.979999542236328,\n   'text': ' and a lot of other tools in the modern wave of AI.',\n   'tokens': [50759,\n    293,\n    257,\n    688,\n    295,\n    661,\n    3873,\n    294,\n    264,\n    4363,\n    5772,\n    295,\n    7318,\n    13,\n    50867],\n   'temperature': 0.0,\n   'avg_logprob': -0.28872039914131165,\n   'compression_ratio': 1.4220778942108154,\n   'no_speech_prob': 0.016033057123422623}],\n 'words': [{'word': 'In', 'start': 0.0, 'end': 0.18000000715255737},\n  {'word': 'the', 'start': 0.18000000715255737, 'end': 0.23999999463558197},\n  {'word': 'last', 'start': 0.23999999463558197, 'end': 0.5400000214576721},\n  {'word': 'chapter', 'start': 0.5400000214576721, 'end': 0.800000011920929},\n  ....\n  {'word': 'AI', 'start': 9.920000076293945, 'end': 9.979999542236328}]}\n```\n\n会输出每一句话以及每一个字所在的起始时间和截止时间。你可以根据需要来使用。\n\n\n### 文生图\n\n文生图和语音合成类似，首先要启动合适的模型,以openai 的 dall-e-3 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=dall-e-3 \\\n--model openai_image_gen\n```\n\n启动模型后，只需要记住几个模板参数即可使用，这里直接使用 chat_oai 方法来使用：\n\n```python\n\nimport byzerllm\nimport json\nimport base64\n\nllm = byzerllm.ByzerLLM.from_default_model(\"openai_image_gen\")\nt = llm.chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\": json.dumps({\n        \"input\":\"a white siamese cat\",\n        \"size\": \"1024x1024\",\n        \"quality\": \"standard\"\n    },ensure_ascii=False)\n}])\n\nwith open(\"output1.jpg\",\"wb\") as f:\n    f.write(base64.b64decode(t[0].output))\n\n\nimport matplotlib.pyplot as plt\n\nimage_path = \"output1.jpg\"\nimage = plt.imread(image_path)\n\nplt.imshow(image)\nplt.axis('off')\nplt.show()\n```\n\n## Prompt 函数的流式输出\n\nbyzerllm 底层支持流式输出，非 prompt 函数的用法是这样的：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\n\nv = llm.stream_chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\":\"讲一个100字的故事\"\n}])\n\nfor s  in v:\n    print(s[0], end=\"\")\n```\n\n如果你像用 prompt 函数，可以这么用：\n\n\n```python\nimport byzerllm\nimport json\nimport base64\nfrom typing import Generator\n\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\n\n@byzerllm.prompt()\ndef tell_story() -> Generator[str, None, None]:\n    '''\n    给我讲一个一百多字的故事\n    '''\n\nv = tell_story.with_llm(llm).run()    \nfor i in v:\n    print(i, end=\"\")\n```\n\n可以看到，和普通的 prompt 函数的区别在于，返回值是一个生成器，然后你可以通过 for 循环来获取结果。\n\n## 向量化模型\n\nbyzerllm 支持向量化模型,你可以这样启动一个本地的模型：\n\n```bash\n!byzerllm deploy --pretrained_model_type custom/bge \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--worker_concurrency 10 \\\n--model_path /home/winubuntu/.auto-coder/storage/models/AI-ModelScope/bge-large-zh \\\n--infer_backend transformers \\\n--num_workers 1 \\\n--model emb\n```\n\n注意两个参数:\n\n1. --infer_backend transformers: 表示使用 transformers 作为推理后端。\n2. --model_path: 表示模型的路径。\n\n也可以启动一个 SaaS 的emb模型,比如 qwen 的 emb 模型：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/qianwen \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 2 \\\n--infer_params saas.api_key=${MODEL_QIANWEN_TOKEN}  saas.model=text-embedding-v2 \\\n--model qianwen_emb\n```\n\n或者 openai 的 emb 模型：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--worker_concurrency 10 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=text-embedding-3-small \\\n--model gpt_emb\n```\nSaaS 模型无需配置 `--infer_backend` 参数。\n\n无论是本地模型还是 SaaS 模型，我们都可以这样使用：\n\n```python\nimport byzerllm\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\nllm.setup_default_emb_model_name(\"emb\")\nllm.emb_query(\"你好\")\n```\n\n如果你配置 byzerllm 中的 Storage 使用，比如你这样启动了存储：\n\n```bash\nbyzerllm storage start --enable_emb\n```\n\n那么需要这么使用：\n\n```python\nfrom byzerllm.apps.byzer_storage.simple_api import ByzerStorage, DataType, FieldOption,SortOption\nstorage = ByzerStorage(\"byzerai_store\", \"memory\", \"memory\")\nstorage.emb(\"你好\")\n```\n\n## 一些辅助工具\n\n当调用 prompt 函数返回字符串的时候，如果想从里面抽取代码，可以使用如下方式：\n\n```python\nfrom byzerllm.utils.client import code_utils\ntext_with_markdown = '''\n```shell\nls -l\n```\n'''\ncode_blocks = code_utils.extract_code(text_with_markdown)\nfor code_block in code_blocks:\n    if code_block[0] == \"shell\":\n        print(code_block[1])\n##output: ls -l        \n```\n\n\n## 注意事项\n\n1. prompt函数方法体返回只能是dict，实际的返回类型和方法签名可以不一样，但是方法体返回只能是dict。\n2. 大部分情况prompt函数体为空，如果一定要有方法体，可以返回一个空字典。\n3. 调用prompt方法的时候，如果在@byzerllm.prompt()里没有指定llm对象，那么需要在调用的时候通过with_llm方法指定llm对象。\n\n===========================\n\n# Byzer Storage: 最易用的AI存储引擎\n\nByzer Storage是一个为AI应用设计的高性能存储引擎,它提供了简单易用的API,支持向量搜索、全文检索以及结构化查询。本文将详细介绍Byzer Storage的使用方法和主要特性。\n\n## 0. 安装和启动\n\n```bash\npip install byzerllm\nbyzerllm storage start\n```\n\nThat's it! Byzer Storage已经安装并启动成功,现在我们可以开始使用它了。默认会启动一个 byzerai_store 的集群。\n\n注意，如果你是这样启动的：\n\n```bash\nbyzerllm storage start --enable_emb\n```\n\n那么会自动启动一个 emb 模型，名字就叫 emb， ByzerStorage 会自动使用该模型，无需做任何其他配置。\n\n## 1. 初始化\n\n创建一个 ByzerStorage 对象，链接 byzerai_store 集群，并且指定数据库和表名（可以不存在）。\n\n```python\nfrom byzerllm.apps.byzer_storage.simple_api import ByzerStorage, DataType, FieldOption, SortOption\n\nstorage = ByzerStorage(\"byzerai_store\", \"my_database1\", \"my_table4s\")\n```\n\n\n## 2. 创建库表（可选）\n\nByzer Storage使用Schema来定义数据结构。我们可以使用SchemaBuilder来构建Schema:\n\n```python\n_ = (\n    storage.schema_builder()\n    .add_field(\"_id\", DataType.STRING)\n    .add_field(\"name\", DataType.STRING)\n    .add_field(\"content\", DataType.STRING, [FieldOption.ANALYZE])\n    .add_field(\"raw_content\", DataType.STRING, [FieldOption.NO_INDEX])    \n    .add_array_field(\"summary\", DataType.FLOAT)    \n    .add_field(\"created_time\", DataType.LONG, [FieldOption.SORT])    \n    .execute()\n)\n```\n\n这个Schema定义了以下字段:\n- `_id`: 字符串类型的主键\n- `name`: 字符串类型,可用于过滤条件\n- `content`: 字符串类型,带有ANALYZE选项,用于全文搜索\n- `raw_content`: 字符串类型,带有NO_INDEX选项,不会被索引\n- `summary`: 浮点数组类型,用于存储向量\n- `created_time`: 长整型,带有SORT选项,可用于排序\n\n需要注意的是：\n\n1. 如果一个字段带有ANALYZE选项,则该字段会被分词,并且可以用于全文搜索，但是就无法返回原始的文本了。所以你需要添加一个新字段专门用来存储原文，比如在我们这个例子里，我们新增了 raw_content 字段，并且显示的指定了 NO_INDEX 选项，这样就不会被索引，也就不会被分词，可以后续被检索后用来获取原始文本。\n2. 对于需要作为向量检索和存储的字段，需要指定为数组类型，比如我们这个例子里的 summary 字段。\n3. 如果你需要拿到向量字段的原始文本，那么你也需要添加一个新字段专门用来存储原文，就像我们这个例子里的 raw_content 字段一样。\n\n\n## 3. 写入数据\n\n准备数据并使用WriteBuilder写入Storage:\n\n```python\ndata = [\n    {\"_id\": \"1\", \"name\": \"Hello\", \"content\": \"Hello, world!\", \"raw_content\": \"Hello, world!\", \"summary\": \"hello world\", \"created_time\": 1612137600},\n    {\"_id\": \"2\", \"name\": \"Byzer\", \"content\": \"Byzer, world!\", \"raw_content\": \"Byzer, world!\", \"summary\": \"byzer\", \"created_time\": 1612137601},\n    {\"_id\": \"3\", \"name\": \"AI\", \"content\": \"AI, world!\", \"raw_content\": \"AI, world!\", \"summary\": \"AI\", \"created_time\": 16121376002},\n    {\"_id\": \"4\", \"name\": \"ByzerAI\", \"content\": \"ByzerAI, world!\", \"raw_content\": \"ByzerAI, world!\", \"summary\": \"ByzerAi\", \"created_time\": 16121376003},\n]\n\nstorage.write_builder().add_items(data, vector_fields=[\"summary\"], search_fields=[\"content\"]).execute()\nstorage.commit()\n```\n\n这里我们使用`add_items`方法批量添加数据,并指定了`summary`为向量字段,`content`为搜索字段。最后调用`commit()`来确保数据被持久化。\n\n\n从上面我们要写入到 byzer storage 的数据我们可以看到如下几个特点：\n\n1. 需要包含我们之前定义的 Schema中罗列的所有的字段，同时需要指定哪些是向量字段，哪些是检索字段。\n2. 向量和检索不能是同一个字段。\n3. 对于向量，检索字段，我们给到 write_builder 的都是文本，ByzerStorage 会根据 Schema 的定义自动将其转换为向量，检索字段需要的格式。\n\n## 4. 查询数据\n\nByzer Storage支持多种查询方式,包括向量搜索、全文检索、过滤和排序。\n\n### 4.1 向量搜索 + 全文检索\n\n```python\nquery = storage.query_builder()\nquery.set_vector_query(\"ByzerAI\", fields=[\"summary\"])\nresults = query.set_search_query(\"Hello\", fields=[\"content\"]).execute()\nprint(results)\n```\n\n这个查询结合了向量搜索和全文检索,它会在`summary`字段中搜索与\"ByzerAI\"相似的向量,同时在`content`字段中搜索包含\"Hello\"的文档。\n\n### 4.2 过滤 + 向量搜索 + 全文检索\n\n```python\nquery = storage.query_builder()\nquery.and_filter().add_condition(\"name\", \"AI\").build()\nquery.set_vector_query(\"ByzerAI\", fields=\"summary\")\nresults = query.set_search_query(\"Hello\", fields=[\"content\"]).execute()\nprint(results)\n```\n\n这个查询首先过滤`name`字段等于\"AI\"的文档,然后在结果中进行向量搜索和全文检索。\n\n### 4.3 过滤 + 排序\n\n```python\nquery = storage.query_builder()\nquery.and_filter().add_condition(\"name\", \"AI\").build().sort(\"created_time\", SortOption.DESC)\nresults = query.execute()\nprint(results)\n```\n\n这个查询过滤`name`字段等于\"AI\"的文档,然后按`created_time`字段降序排序。\n\n## 5. 删除数据\n\n### 5.1 根据ID删除\n\n```python\nstorage.delete_by_ids([\"3\"])\n\nquery = storage.query_builder()\nquery.and_filter().add_condition(\"name\", \"AI\").build()\nresults = query.execute()\nprint(results)\n```\n\n这里我们删除了ID为\"3\"的文档,然后查询验证删除结果。\n\n### 5.2 删除整个表\n\n```python\nstorage.drop()\n```\n\n这个操作会删除整个表及其所有数据,请谨慎使用。\n\n## 结论\n\nByzer Storage提供了一套简洁而强大的API,能够轻松实现向量搜索、全文检索、结构化查询等功能。它的设计非常适合AI应用场景,可以有效地存储和检索各种类型的数据。通过本文的介绍,相信读者已经对Byzer Storage有了基本的了解,并能够开始在自己的项目中使用这个强大的存储引擎。\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/api_server.py\nimport os\nimport time\nimport uvicorn\nfrom fastapi import Request\nfrom fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import JSONResponse, StreamingResponse, Response\n\nfrom byzerllm.log import init_logger\nfrom byzerllm.utils import random_uuid\nfrom byzerllm.version import __version__ as version\nfrom byzerllm.utils.client import ByzerLLM, LLMRequest\nfrom byzerllm.utils.client.entrypoints.openai.serving_chat import OpenAIServingChat\nfrom byzerllm.utils.client.entrypoints.openai.serving_completion import OpenAIServingCompletion\nfrom byzerllm.utils.client.entrypoints.openai.protocol import (\n    ModelList,\n    ModelCard,\n    ModelPermission,\n    ChatCompletionRequest,\n    ErrorResponse,\n    CompletionRequest,\n    Embeddings,\n    EmbeddingsOutput,\n    EmbeddingsData,\n    EmbeddingsUsage,\n)\nfrom pydantic import BaseModel\nfrom typing import List\n\nlogger = init_logger(__name__)\n\nllm_client: ByzerLLM = None\nopenai_serving_chat: OpenAIServingChat = None\nopenai_serving_completion: OpenAIServingCompletion = None\n\nTIMEOUT_KEEP_ALIVE = 5  # seconds\n# timeout in 10 minutes. Streaming can take longer than 3 min\nTIMEOUT = float(os.environ.get(\"BYZERLLM_APISERVER_HTTP_TIMEOUT\", 600))\n\nrouter_app = FastAPI()\n\n\n@router_app.get(\"/health\")\nasync def health() -> Response:\n    \"\"\"Health check.\"\"\"\n    return Response(status_code=200)\n\n\n@router_app.get(\"/v1/models\")\nasync def show_available_models():\n    models = await openai_serving_chat.show_available_models()\n    return JSONResponse(content=models.model_dump())\n\n\n@router_app.get(\"/version\")\nasync def show_version():\n    return JSONResponse(content={\"version\": version})\n\n\n@router_app.get(\"/v1/models\", response_model=ModelList)\nasync def models() -> ModelList:\n    \"\"\"Show available models. Right now we only have one model.\"\"\"\n    model_cards = [\n        ModelCard(\n            id=\"\",\n            root=\"\",\n            permission=[ModelPermission()]\n        )\n    ]\n    return ModelList(data=model_cards)\n\n\n@router_app.post(\"/v1/completions\")\nasync def create_completion(\n        body: CompletionRequest,\n        request: Request\n):\n    generator = await openai_serving_completion.create_completion(body, request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            status_code=generator.code\n        )\n    if body.stream:\n        return StreamingResponse(\n            content=generator,\n            media_type=\"text/event-stream\"\n        )\n    else:\n        return JSONResponse(content=generator.model_dump())\n\n\n@router_app.post(\"/v1/chat/completions\")\nasync def create_chat_completion(\n        body: ChatCompletionRequest,\n        request: Request,\n):\n    \"\"\"Completion API similar to OpenAI's API.\n\n    See  https://platform.openai.com/docs/api-reference/chat/create\n    for the API specification. This API mimics the OpenAI ChatCompletion API.\n\n    NOTE: Currently we do not support the following features:\n        - function_call (Users should implement this by themselves)\n        - logit_bias (to be supported by vLLM engine)\n    \"\"\"\n    # async with async_timeout.timeout(TIMEOUT):\n\n    generator = await openai_serving_chat.create_chat_completion(body, request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            status_code=generator.code\n        )\n    if body.stream:\n        return StreamingResponse(\n            content=generator,\n            media_type=\"text/event-stream\"\n        )\n    else:\n        return JSONResponse(content=generator.model_dump())\n\n\n@router_app.post(\"/v1/embeddings\")\nasync def embed(body: Embeddings):\n    \"\"\"Given a prompt, the model will return one embedding.\n\n    Returns:\n        A response object with an embedding.\n    \"\"\"\n    embedding_id = f\"embed-{random_uuid()}\"\n\n    results_list = llm_client.emb(body.model, request=LLMRequest(instruction=body.input))\n    tokens = 0\n\n    return EmbeddingsOutput(\n        data=[\n            EmbeddingsData(\n                embedding=results.output,\n                index=i,\n                object=\"embedding\",\n            )\n            for i, results in enumerate(results_list)\n        ],\n        id=embedding_id,\n        object=\"list\",\n        created=int(time.time()),\n        model=body.model,\n        usage=EmbeddingsUsage(\n            prompt_tokens=tokens,\n            total_tokens=tokens,\n        ),\n    )\n\nclass ServerArgs(BaseModel):\n    host: str = None\n    port: int = 8000\n    uvicorn_log_level: str = \"info\"\n    allow_credentials: bool = False\n    allowed_origins: List[str] = [\"*\"]  \n    allowed_methods: List[str] = [\"*\"]\n    allowed_headers: List[str] = [\"*\"]\n    api_key: str = None\n    served_model_name: str = None\n    prompt_template: str = None\n    response_role: str = \"assistant\"\n    ssl_keyfile: str = None\n    ssl_certfile: str = None    \n\ndef serve(llm:ByzerLLM, args: ServerArgs):\n    \n    logger.info(f\"ByzerLLM API server version {version}\")\n    logger.info(f\"args: {args}\")\n\n    router_app.add_middleware(\n        CORSMiddleware,\n        allow_origins=args.allowed_origins,\n        allow_credentials=args.allow_credentials,\n        allow_methods=args.allowed_methods,\n        allow_headers=args.allowed_headers,\n    )\n    \n    if token := os.environ.get(\"BYZERLLM_API_KEY\") or args.api_key:\n\n        @router_app.middleware(\"http\")\n        async def authentication(request: Request, call_next):\n            if not request.url.path.startswith(\"/v1\"):\n                return await call_next(request)\n            if request.headers.get(\"Authorization\") != \"Bearer \" + token:\n                return JSONResponse(\n                    content={\"error\": \"Unauthorized\"},\n                    status_code=401\n                )\n            return await call_next(request)\n\n    # Register labels for metrics\n    # add_global_metrics_labels(model_name=engine_args.model)\n    global llm_client\n    llm_client = llm\n    \n    global openai_serving_chat\n    openai_serving_chat = OpenAIServingChat(\n        llm_client=llm_client,\n        response_role=args.response_role,\n        server_model_name=args.served_model_name,\n        prompt_template=args.prompt_template\n    )\n    global openai_serving_completion\n    openai_serving_completion = OpenAIServingCompletion(\n        llm_client=llm_client,\n        server_model_name=args.served_model_name,\n        prompt_template=args.prompt_template\n    )\n\n    uvicorn.run(\n        router_app,\n        host=args.host,\n        port=args.port,\n        log_level=args.uvicorn_log_level,\n        timeout_keep_alive=TIMEOUT_KEEP_ALIVE,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile\n    )\n\n\n     \n\n下面是用户的需求：\n\n添加一个命令行, 在 serve 目录下添加一个 cli.py, 支持配置 ServerArgs 里的参数,然后调用 @api_server.py 中的serve\n"}, {"role": "assistant", "content": "好的，我理解您的需求是要在serve目录下添加一个cli.py文件，用于通过命令行配置ServerArgs的参数，然后调用api_server.py中的serve函数。我会为您创建这个文件并实现所需功能。以下是相应的代码：\n\n```python\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/cli.py\n<<<<<<< SEARCH\n=======\nimport argparse\nfrom byzerllm.utils.client import ByzerLLM\nfrom superanalysis.serve.api_server import serve, ServerArgs\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"ByzerLLM API Server\")\n    parser.add_argument(\"--host\", type=str, default=\"0.0.0.0\", help=\"Host to bind the server to\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"Port to bind the server to\")\n    parser.add_argument(\"--uvicorn-log-level\", type=str, default=\"info\", help=\"Uvicorn log level\")\n    parser.add_argument(\"--allow-credentials\", action=\"store_true\", help=\"Allow credentials\")\n    parser.add_argument(\"--allowed-origins\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed origins\")\n    parser.add_argument(\"--allowed-methods\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed methods\")\n    parser.add_argument(\"--allowed-headers\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed headers\")\n    parser.add_argument(\"--api-key\", type=str, help=\"API key for authentication\")\n    parser.add_argument(\"--served-model-name\", type=str, required=True, help=\"Name of the model to be served\")\n    parser.add_argument(\"--prompt-template\", type=str, help=\"Prompt template\")\n    parser.add_argument(\"--response-role\", type=str, default=\"assistant\", help=\"Response role\")\n    parser.add_argument(\"--ssl-keyfile\", type=str, help=\"SSL key file\")\n    parser.add_argument(\"--ssl-certfile\", type=str, help=\"SSL certificate file\")\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile\n    )\n\n    llm = ByzerLLM()\n    llm.setup_default_model_name(args.served_model_name)\n\n    serve(llm, server_args)\n\nif __name__ == \"__main__\":\n    main()\n>>>>>>> REPLACE\n```\n\n这个新的cli.py文件实现了以下功能：\n\n1. 导入必要的模块，包括argparse用于命令行参数解析，ByzerLLM客户端，以及api_server.py中的serve函数和ServerArgs类。\n\n2. 定义了一个main函数，其中：\n   - 使用argparse创建了一个命令行参数解析器。\n   - 为ServerArgs中的每个参数添加了相应的命令行选项。\n   - 解析命令行参数。\n   - 使用解析后的参数创建ServerArgs实例。\n   - 创建ByzerLLM实例并设置默认模型名称。\n   - 调用serve函数，传入ByzerLLM实例和ServerArgs实例。\n\n3. 添加了if __name__ == \"__main__\":条件，确保脚本被直接运行时才执行main函数。\n\n使用这个cli.py，用户可以通过命令行配置所有ServerArgs中的参数，例如：\n\n```\npython cli.py --host 0.0.0.0 --port 8080 --served-model-name my_model --api-key my_secret_key\n```\n\n这将启动API服务器，使用指定的主机、端口、模型名称和API密钥。其他参数如果不指定，将使用默认值。\n\n这个实现允许用户灵活地配置API服务器，同时保持了与原有serve函数的兼容性。"}], "model": "human", "yaml_file": "actions/102_chat_action.yml"}
{"instruction": "添加setup.py 文件,支持 super-analysis.serve  命令, 调用 cli 启动", "conversations": [{"role": "user", "content": "\n如果你需要生成代码，对于每个需要更改的文件,你需要按 *SEARCH/REPLACE block* 的格式进行生成。\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The opening fence and code language, eg: ```python\n2. The file path alone on a line, starting with \"##File:\" and verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n*SEARCH/REPLACE* blocks will replace *all* matching occurrences.\nInclude enough lines to make the SEARCH blocks unique.\n\nInclude *ALL* the code being searched and replaced!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n下面我们来看一个例子：\n\n当前项目目录结构：\n1. 项目根目录： /tmp/projects/mathweb\n2. 项目子目录/文件列表(类似tree 命令输出)\nflask/\n    app.py\n    templates/\n        index.html\n    static/\n        style.css\n\n用户需求： Change get_factorial() to use math.factorial\n\n回答： To make this change we need to modify `/tmp/projects/mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n用户需求： Refactor hello() into its own file.\n\n回答：To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/hello.py\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/main.py\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n现在让我们开始一个新的任务:\n\n当前项目目录结构：\n1. 项目根目录： /Users/allwefantasy/projects/super_analysis\n2. 项目子目录/文件列表(类似tree 命令输出)：\n.auto-coder/\n    libs/\n        llm_friendly_packages/\n            src/\n                example1/\n                    main.py\nsrc/\n    superanalysis/\n        serve/\n            openai_serve.py\n            llm_wrapper.py\n            api_server.py\n            cli.py\n下面是一些文件路径以及每个文件对应的源码：\n\n##File: .auto-coder/libs/llm_friendly_packages/github.com/allwefantasy/byzer-llm/README.md\n# byzerllm 大模型编程快速指南\n\n本文示例 [Notebook](../../notebooks/003_byzerllm_大模型编程快速指南.ipynb)\n\n## 安装\n\n```bash\npip install -U byzerllm\nray start --head\n```\n## 启动一个模型代理\n\nbyzerllm 支持私有化模型或者SaaS模型的部署。\n\n这里以 deepseek 官方API 为例：\n\n```bash\neasy-byzerllm deploy deepseek-chat --token xxxxx --alias deepseek_chat\n```\n\n或者跬基流动API:\n\n```bash\neasy-byzerllm deploy alibaba/Qwen1.5-110B-Chat --token xxxxx --alias qwen110b_chat\n```\n\n将上面的 API KEY 替换成你们自己的。\n\n如果你想部署私有化模型或者对接 Ollama 等更多需求，参考 [002_使用byzerllm进行模型部署.md](./002_使用byzerllm进行模型部署.md) 或者 [README.md](../../README.md)。\n\n之后，你就可以在代码里使用  deepseek_chat 或者 qwen110b_chat  访问模型了。\n\n## hello world\n\n来和我们的大模型打个招呼:\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\n@byzerllm.prompt(llm=llm)\ndef hello(q:str) ->str:\n    '''\n    你好, {{ q }}\n    '''\n\ns = hello(\"你是谁\")    \nprint(s)\n\n## 输出:\n## '你好！我是一个人工智能助手，专门设计来回答问题、提供信息和帮助解决问题。如果你有任何疑问或需要帮助，请随时告诉我。'\n```\n\n恭喜，你和大模型成功打了招呼！\n\n可以看到，我们通过 `@byzerllm.prompt` 装饰器，将一个方法转换成了一个大模型的调用，然后这个方法的主题是一段文本，文本中\n使用了 jinja2 模板语法，来获得方法的参数。当正常调用该方法时，实际上就发起了和大模型的交互，并且返回了大模型的结果。\n\n在 byzerllm 中，我们把这种方法称之为 prompt 函数。\n\n## 查看发送给大模型的prompt\n\n很多情况你可能需要调试，查看自己的 prompt 渲染后到底是什么样子的，这个时候你可以通过如下方式\n获取渲染后的prompt:\n\n```python\nhello.prompt(\"你是谁\")\n## '你好, 你是谁'\n```            \n\n## 动态换一个模型\n\n前面的 hello 方法在初始化的时候，我们使用了默认的模型 deepseek_chat，如果我们想换一个模型，可以这样做：\n\n```python\nhello.with_llm(llm).run(\"你是谁\")\n## '你好！我是一个人工智能助手，专门设计来回答问题、提供信息和帮助解决问题。如果你有任何疑问或需要帮助，请随时告诉我。'\n```\n\n通过 with_llm 你可以设置一个新的 llm 对象，然后调用 run 方法，就可以使用新的模型了。\n\n## 超长文本生成\n\n我们知道，大模型一次生成的长度其实是有限的，如果你想生成超长文本，你可能需手动的不断获得\n生成结果，然后把他转化为输入，然后再次生成，这样的方式是比较麻烦的。\n\nbyzerllm 提供了更加易用的 API :\n\n```python\nimport byzerllm\nfrom byzerllm import ByzerLLM\n\nllm = ByzerLLM.from_default_model(\"deepseek_chat\")\n\n@byzerllm.prompt()\ndef tell_story() -> str:\n    \"\"\"\n    讲一个100字的故事。    \n    \"\"\"\n\n\ns = (\n    tell_story.with_llm(llm)\n    .with_response_markers()\n    .options({\"llm_config\": {\"max_length\": 10}})\n    .run()\n)\nprint(s)\n\n## 从前，森林里住着一只聪明的小狐狸。一天，它发现了一块闪闪发光的宝石。小狐狸决定用这块宝石帮助森林里的动物们。它用宝石的光芒指引迷路的小鹿找到了回家的路，用宝石的温暖治愈了受伤的小鸟。从此，小狐狸成了森林里的英雄，动物们都感激它的善良和智慧。\n```\n\n实际核心部分就是这一行：\n\n```python\ntell_story.with_llm(llm)\n    .with_response_markers()    \n    .run()\n```\n\n我们只需要调用 `with_response_markers` 方法，系统就会自动的帮我们生成超长文本。\n在上面的案例中，我们通过\n\n```python\n.options({\"llm_config\": {\"max_length\": 10}})\n```\n\n认为的限制大模型一次交互最多只能输出10个字符，但是系统依然自动完成了远超过10个字符的文本生成。\n\n## 对象输出\n\n前面我们的例子都是返回字符串，但是我们也可以返回对象，这样我们就可以更加灵活的处理返回结果。\n\n```python\nimport pydantic \n\nclass Story(pydantic.BaseModel):\n    '''\n    故事\n    '''\n\n    title: str = pydantic.Field(description=\"故事的标题\")\n    body: str = pydantic.Field(description=\"故事主体\")\n\n@byzerllm.prompt()\ndef tell_story()->Story:\n    '''\n    讲一个100字的故事。    \n    '''\n\ns = tell_story.with_llm(llm).run()\nprint(isinstance(s, Story))\nprint(s.title)\n\n## True\n## 勇敢的小鸟\n```\n\n可以看到，我们很轻松的将输出转化为格式化输出。\n\n## 自定义字段抽取\n\n前面的结构化输出，其实会消耗更多token,还有一种更加精准的结构化输出方式。\n比如让大模型生成一个正则表达式，但实际上大模型很难准确只输出一个正则表达式，这个时候我们可以通过自定义抽取函数来获取我们想要的结果。\n\n\n```python\nfrom loguru import logger\nimport re\n\n@byzerllm.prompt()\ndef generate_regex_pattern(desc: str) -> str:\n    \"\"\"\n    根据下面的描述生成一个正则表达式，要符合python re.compile 库的要求。\n\n    {{ desc }}\n\n    最后生成的正则表达式要在<REGEX></REGEX>标签对里。\n    \"\"\"    \n\ndef extract_regex_pattern(regex_block: str) -> str:    \n    pattern = re.search(r\"<REGEX>(.*)</REGEX>\", regex_block, re.DOTALL)\n    if pattern is None:\n        logger.warning(\"No regex pattern found in the generated block:\\n {regex_block}\")\n        raise None\n    return pattern.group(1)\n\npattern = \"匹配一个邮箱地址\"\nv = generate_regex_pattern.with_llm(llm).with_extractor(extract_regex_pattern).run(desc=pattern)\nprint(v)\n## ^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\n```\n\n在上面的例子里，我们根据一句话生成一个正则表达式。我们通过 `with_extractor` 方法，传入了一个自定义的抽取函数，这个函数会在大模型生成结果后，对结果进行处理，然后返回我们想要的结果。\n\n我们在 prompt 明确说了，生成的结果要放到 `<REGEX></REGEX>` 标签对里，然后我们通过 extract_regex_pattern 函数，从结果中提取出了我们想要的正则表达式。\n\n## 在实例方法中使用大模型\n\n```python\nimport byzerllm\ndata = {\n    'name': 'Jane Doe',\n    'task_count': 3,\n    'tasks': [\n        {'name': 'Submit report', 'due_date': '2024-03-10'},\n        {'name': 'Finish project', 'due_date': '2024-03-15'},\n        {'name': 'Reply to emails', 'due_date': '2024-03-08'}\n    ]\n}\n\n\nclass RAG():\n    def __init__(self):        \n        self.llm = byzerllm.ByzerLLM()\n        self.llm.setup_template(model=\"deepseek_chat\",template=\"auto\")\n        self.llm.setup_default_model_name(\"deepseek_chat\")        \n    \n    @byzerllm.prompt(lambda self:self.llm)\n    def generate_answer(self,name,task_count,tasks)->str:\n        '''\n        Hello {{ name }},\n\n        This is a reminder that you have {{ task_count }} pending tasks:\n        {% for task in tasks %}\n        - Task: {{ task.name }} | Due: {{ task.due_date }}\n        {% endfor %}\n\n        Best regards,\n        Your Reminder System\n        '''        \n\nt = RAG()\n\nresponse = t.generate_answer(**data)\nprint(response)\n\n## 输出:\n## Hello Jane Doe,\n##I hope this message finds you well. I wanted to remind you of your 3 pending tasks to ensure you stay on track:\n## 1. **Submit report** - This task is due on **2024-03-10**. Please ensure that you allocat\n```\n\n这里我们给了个比较复杂的例子，但我们可以看到，给一个实例prompt方法和普通prompt 方法差异不大。\n唯一的区别是如果你希望在定义的时候就指定大模型，使用一个lambda函数返回实例的 llm 对象即可。\n\n```python\n@byzerllm.prompt(lambda self:self.llm)\n```\n\n你也可以不返回，在调用的时候通过 `with_llm` 方法指定 llm 对象。\n\n此外，这个例子也展示了如何通过jinja2模板语法，来处理复杂的结构化数据。\n\n## 通过 Python 代码处理复杂入参\n\n上面的一个例子中，我们通过 jinja2 模板语法，来处理复杂的结构化数据，但是有时候我们可能需要更加复杂的处理，这个时候我们可以通过 Python 代码来处理。\n\n```python\nimport byzerllm\n\ndata = {\n    'name': 'Jane Doe',\n    'task_count': 3,\n    'tasks': [\n        {'name': 'Submit report', 'due_date': '2024-03-10'},\n        {'name': 'Finish project', 'due_date': '2024-03-15'},\n        {'name': 'Reply to emails', 'due_date': '2024-03-08'}\n    ]\n}\n\n\nclass RAG():\n    def __init__(self):        \n        self.llm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n    \n    @byzerllm.prompt()\n    def generate_answer(self,name,task_count,tasks)->str:\n        '''\n        Hello {{ name }},\n\n        This is a reminder that you have {{ task_count }} pending tasks:\n            \n        {{ tasks }}\n\n        Best regards,\n        Your Reminder System\n        '''\n        \n        tasks_str = \"\\n\".join([f\"- Task: {task['name']} | Due: { task['due_date'] }\" for task in tasks])\n        return {\"tasks\": tasks_str}\n\nt = RAG()\n\nresponse = t.generate_answer.with_llm(t.llm).run(**data)\nprint(response)\n\n## Just a gentle nudge to keep you on track with your pending tasks. Here's a quick recap:....\n```\n\n在这个例子里，我们直接把 tasks 在方法体里进行处理，然后作为一个字符串返回，最够构建一个字典，字典的key为 tasks,然后\n你就可以在 docstring 里使用 `{{ tasks }}` 来引用这个字符串。\n\n这样对于很复杂的入参，就不用谢繁琐的 jinja2 模板语法了。\n\n## 如何自动实现一个方法\n\n比如我定义一个签名，但是我不想自己实现里面的逻辑，让大模型来实现。这个在 byzerllm 中叫 function impl。我们来看看怎么\n实现:\n\n```python\nimport pydantic\nclass Time(pydantic.BaseModel):\n    time: str = pydantic.Field(...,description=\"时间，时间格式为 yyyy-MM-dd\")\n\n\n@llm.impl()\ndef calculate_current_time()->Time:\n    '''\n    计算当前时间\n    '''\n    pass \n\n\ncalculate_current_time()\n#output: Time(time='2024-06-14')\n```\n\n在这个例子里，我们定义了一个 calculate_current_time 方法，但是我们没有实现里面的逻辑，我们通过 `@llm.impl()` 装饰器，让大模型来实现这个方法。\n为了避免每次都要“生成”这个方法，导致无法适用，我们提供了缓存，用户可以按如下方式打印速度：\n\n```python\nstart = time.monotonic()\ncalculate_current_time()\nprint(f\"first time cost: {time.monotonic()-start}\")\n\nstart = time.monotonic()\ncalculate_current_time()\nprint(f\"second time cost: {time.monotonic()-start}\")\n\n# output:\n# first time cost: 6.067266260739416\n# second time cost: 4.347506910562515e-05\n```\n可以看到，第一次执行花费了6s,第二次几乎是瞬间完成的，这是因为第一次执行的时候，我们实际上是在生成这个方法，第二次执行的时候，我们是执行已经生成好的代码，所以速度会非常快。你可以显示的调用 `llm.clear_impl_cache()` 清理掉函数缓存。\n\n## Stream 模式\n\n前面的例子都是一次性生成结果，但是有时候我们可能需要一个流式的输出，这个时候我们可能需要用底层一点的API来完成了：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\nv = llm.stream_chat_oai(model=\"deepseek_chat\",conversations=[{\n    \"role\":\"user\",\n    \"content\":\"你好，你是谁\",\n}],delta_mode=True)\n\nfor t in v:\n    print(t,flush=True)  \n\n# 你好\n# ！\n# 我\n# 是一个\n# 人工智能\n# 助手\n# ，\n# 旨在\n# 提供\n# 信息\n# 、\n# 解答\n# 问题....\n```\n\n如果你不想要流式输出，但是想用底层一点的API，你可以使用 `llm.chat_oai` 方法：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\nv = llm.chat_oai(model=\"deepseek_chat\",conversations=[{\n    \"role\":\"user\",\n    \"content\":\"你好，你是谁\",\n}])\n\nprint(v[0].output)\n## 你好！我是一个人工智能助手，旨在提供信息、解答问题和帮助用户解决问题。如果你有任何问题或需要帮助，请随时告诉我。\n```\n\n## Function Calling \n\nbyzerllm 可以不依赖模型自身就能提供 function calling 支持，我们来看个例子：\n\n\n```python\nfrom typing import List,Dict,Any,Annotated\nimport pydantic \nimport datetime\nfrom dateutil.relativedelta import relativedelta\n\ndef compute_date_range(count:Annotated[int,\"时间跨度，数值类型\"],\n                       unit:Annotated[str,\"时间单位，字符串类型\",{\"enum\":[\"day\",\"week\",\"month\",\"year\"]}])->List[str]:\n    '''\n    计算日期范围\n\n    Args:\n        count: 时间跨度，数值类型\n        unit: 时间单位，字符串类型，可选值为 day,week,month,year\n    '''        \n    now = datetime.datetime.now()\n    now_str = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n    if unit == \"day\":\n        return [(now - relativedelta(days=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"week\":\n        return [(now - relativedelta(weeks=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"month\":\n        return [(now - relativedelta(months=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"year\":\n        return [(now - relativedelta(years=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    return [\"\",\"\"]\n\ndef compute_now()->str:\n    '''\n    计算当前时间\n    '''\n    return datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n```\n\n我们定义了两个方法，一个是计算日期范围，一个是计算当前时间。\n\n现在我么可以来测试下，系统如何根据自然语言决定调用哪个方法：\n\n```python\nt = llm.chat_oai([{\n    \"content\":'''计算当前时间''',\n    \"role\":\"user\"    \n}],tools=[compute_date_range,compute_now],execute_tool=True)\n\nt[0].values\n\n## output: ['2024-06-14 15:18:02']\n```\n\n我们可以看到，他正确的选择了 compute_now 方法。\n\n接着我们再试一个：\n\n```python\nt = llm.chat_oai([{\n    \"content\":'''最近三个月趋势''',\n    \"role\":\"user\"    \n}],tools=[compute_date_range,compute_now],execute_tool=True)\n\nt[0].values\n\n## output: [['2024-03-14 15:19:13', '2024-06-14 15:19:13']]\n```\n\n模型正确的选择了 compute_date_range 方法。\n\n## 多模态\n\nbyerllm 也能很好的支持多模态的交互，而且统一了多模态大模型的接口，比如你可以用一样的方式使用 openai 或者 claude 的图片转文字能力， 或者一致的方式使用火山，azuer, openai的语音合成接口。\n\n### 图生文\n\n```python\nimport byzerllm\nfrom byzerllm.types import ImagePath\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\n\n\n@byzerllm.prompt()\ndef what_in_image(image_path: ImagePath) -> str:\n    \"\"\"\n    {{ image_path }}\n    这个图片里有什么？\n    \"\"\"    \n\n\nv = what_in_image.with_llm(vl_llm).run(\n    ImagePath(value=\"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\")\n)\nv\n## OUTPUT: 这张图片展示了多只可爱的猫咪，采用了艺术风格的绘画。猫咪们有不同的颜色和花纹，背景是浅棕色，上面还点缀着一些红色的花朵。整体画面给人一种温馨和谐的感觉\n```\n\n可以看到，我们只需要把 prompt 函数的图片地址入参使用 byzerllm.types.ImagePath里进行包装，就可以直接在 prompt 函数体里\n带上图片。\n\n或者你可以这样：\n\n```python\nimport byzerllm\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\n\n\n@byzerllm.prompt()\ndef what_in_image(image_path: str) -> str:\n    \"\"\"\n    {{ image }}\n    这个图片里有什么？\n    \"\"\"\n    return {\"image\": byzerllm.Image.load_image_from_path(image_path)}\n\n\nv = what_in_image.with_llm(vl_llm).run(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv\n```\n\n通过 `image_path` 参数，然后通过 `byzerllm.Image.load_image_from_path` 方法，转化为一个图片对象 image，最后在 prompt 函数体里\n使用 `{{ image }}` 引用这个图片对象。\n\n另外我们也是可以支持配置多张图片的。\n\n另外，我们也可以使用基础的 `llm.chat_oai` 方法来实现：\n\n```python\nimport byzerllm\nimport json\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\nimage = byzerllm.Image.load_image_from_path(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv = vl_llm.chat_oai(\n    conversations=[\n        {\n            \"role\": \"user\",\n            \"content\": json.dumps(\n                [{\"image\": image, \"text\": \"这个图片里有什么？\"}], ensure_ascii=False\n            ),\n        }\n    ]\n)\nv[0].output\n```\n\n还可以这么写：\n\n```python\nimport byzerllm\nimport json\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\nimage = byzerllm.Image.load_image_from_path(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv = vl_llm.chat_oai(\n    conversations=[\n        {\n            \"role\": \"user\",\n            \"content\": json.dumps(\n                [\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\"url\": image, \"detail\": \"high\"},\n                    },\n                    {\"text\": \"这个图片里有什么？\", \"type\": \"text\"},\n                ],\n                ensure_ascii=False,\n            ),\n        }\n    ]\n)\nv[0].output\n```\n\n### 语音合成\n\n这里以 openai 的 tts 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=tts-1 \\\n--model openai_tts\n```\n\n此外，byzerllm 支持 azure,火山引擎等 tts 语音合成引擎。\n\n接着你可以这么用：\n\n```python\nimport byzerllm\nimport base64\nimport json\n\nllm = byzerllm.ByzerLLM.from_default_model(\"openai_tts\")\n\n\nt = llm.chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\": json.dumps({\n        \"input\":\"hello, open_tts\",\n        \"voice\": \"alloy\",\n        \"response_format\": \"mp3\"\n    },ensure_ascii=False)\n}])\n\nwith open(\"voice.mp3\",\"wb\") as f:\n    f.write(base64.b64decode(t[0].output))\n```\n\ntts 模型生成没有prompt函数可以用，你需要直接使用 chat_oai。\n\n\n### 语音识别\n\n这里以 openai 的 whisper-1 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--worker_concurrency 10 \\\n--infer_params saas.model=whisper-1 saas.api_key=${MODEL_OPENAI_TOKEN} \\\n--model speech_to_text\n```\n\n语音识别的使用方式和图生文类似，我们可以直接在 prompt 函数体里带上音频文件。\n\n```python\nimport byzerllm\nimport json\nimport base64\nfrom byzerllm.types import AudioPath\n\nllm = byzerllm.ByzerLLM.from_default_model(\"speech_to_text\")\n\naudio_file = \"/Users/allwefantasy/videos/output_audio.mp3\"\n\n@byzerllm.prompt(llm=llm)\ndef audio_to_text(audio_file: AudioPath):\n    \"\"\"\n    {{ audio_file }}\n    \"\"\"\n\nv = audio_to_text(AudioPath(value=audio_file))\njson.loads(v)\n```\n输出的数据格式略微复杂：\n\n```\n{'text': 'In the last chapter, you and I started to step through the internal workings of a transformer. This is one of the key pieces of technology inside large language models, and a lot of other tools in the modern wave of AI.',\n 'task': 'transcribe',\n 'language': 'english',\n 'duration': 10.0,\n 'segments': [{'id': 0,\n   'seek': 0,\n   'start': 0.0,\n   'end': 4.78000020980835,\n   'text': ' In the last chapter, you and I started to step through the internal workings of a transformer.',\n   'tokens': [50364,\n    682,\n    264,\n    1036,\n    7187,\n    11,\n    291,\n    293,\n    286,\n    1409,\n    281,\n    1823,\n    807,\n    264,\n    6920,\n    589,\n    1109,\n    295,\n    257,\n    31782,\n    13,\n    50586],\n   'temperature': 0.0,\n   'avg_logprob': -0.28872039914131165,\n   'compression_ratio': 1.4220778942108154,\n   'no_speech_prob': 0.016033057123422623},\n  ....\n  {'id': 2,\n   'seek': 0,\n   'start': 8.579999923706055,\n   'end': 9.979999542236328,\n   'text': ' and a lot of other tools in the modern wave of AI.',\n   'tokens': [50759,\n    293,\n    257,\n    688,\n    295,\n    661,\n    3873,\n    294,\n    264,\n    4363,\n    5772,\n    295,\n    7318,\n    13,\n    50867],\n   'temperature': 0.0,\n   'avg_logprob': -0.28872039914131165,\n   'compression_ratio': 1.4220778942108154,\n   'no_speech_prob': 0.016033057123422623}],\n 'words': [{'word': 'In', 'start': 0.0, 'end': 0.18000000715255737},\n  {'word': 'the', 'start': 0.18000000715255737, 'end': 0.23999999463558197},\n  {'word': 'last', 'start': 0.23999999463558197, 'end': 0.5400000214576721},\n  {'word': 'chapter', 'start': 0.5400000214576721, 'end': 0.800000011920929},\n  ....\n  {'word': 'AI', 'start': 9.920000076293945, 'end': 9.979999542236328}]}\n```\n\n会输出每一句话以及每一个字所在的起始时间和截止时间。你可以根据需要来使用。\n\n\n### 文生图\n\n文生图和语音合成类似，首先要启动合适的模型,以openai 的 dall-e-3 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=dall-e-3 \\\n--model openai_image_gen\n```\n\n启动模型后，只需要记住几个模板参数即可使用，这里直接使用 chat_oai 方法来使用：\n\n```python\n\nimport byzerllm\nimport json\nimport base64\n\nllm = byzerllm.ByzerLLM.from_default_model(\"openai_image_gen\")\nt = llm.chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\": json.dumps({\n        \"input\":\"a white siamese cat\",\n        \"size\": \"1024x1024\",\n        \"quality\": \"standard\"\n    },ensure_ascii=False)\n}])\n\nwith open(\"output1.jpg\",\"wb\") as f:\n    f.write(base64.b64decode(t[0].output))\n\n\nimport matplotlib.pyplot as plt\n\nimage_path = \"output1.jpg\"\nimage = plt.imread(image_path)\n\nplt.imshow(image)\nplt.axis('off')\nplt.show()\n```\n\n## Prompt 函数的流式输出\n\nbyzerllm 底层支持流式输出，非 prompt 函数的用法是这样的：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\n\nv = llm.stream_chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\":\"讲一个100字的故事\"\n}])\n\nfor s  in v:\n    print(s[0], end=\"\")\n```\n\n如果你像用 prompt 函数，可以这么用：\n\n\n```python\nimport byzerllm\nimport json\nimport base64\nfrom typing import Generator\n\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\n\n@byzerllm.prompt()\ndef tell_story() -> Generator[str, None, None]:\n    '''\n    给我讲一个一百多字的故事\n    '''\n\nv = tell_story.with_llm(llm).run()    \nfor i in v:\n    print(i, end=\"\")\n```\n\n可以看到，和普通的 prompt 函数的区别在于，返回值是一个生成器，然后你可以通过 for 循环来获取结果。\n\n## 向量化模型\n\nbyzerllm 支持向量化模型,你可以这样启动一个本地的模型：\n\n```bash\n!byzerllm deploy --pretrained_model_type custom/bge \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--worker_concurrency 10 \\\n--model_path /home/winubuntu/.auto-coder/storage/models/AI-ModelScope/bge-large-zh \\\n--infer_backend transformers \\\n--num_workers 1 \\\n--model emb\n```\n\n注意两个参数:\n\n1. --infer_backend transformers: 表示使用 transformers 作为推理后端。\n2. --model_path: 表示模型的路径。\n\n也可以启动一个 SaaS 的emb模型,比如 qwen 的 emb 模型：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/qianwen \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 2 \\\n--infer_params saas.api_key=${MODEL_QIANWEN_TOKEN}  saas.model=text-embedding-v2 \\\n--model qianwen_emb\n```\n\n或者 openai 的 emb 模型：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--worker_concurrency 10 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=text-embedding-3-small \\\n--model gpt_emb\n```\nSaaS 模型无需配置 `--infer_backend` 参数。\n\n无论是本地模型还是 SaaS 模型，我们都可以这样使用：\n\n```python\nimport byzerllm\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\nllm.setup_default_emb_model_name(\"emb\")\nllm.emb_query(\"你好\")\n```\n\n如果你配置 byzerllm 中的 Storage 使用，比如你这样启动了存储：\n\n```bash\nbyzerllm storage start --enable_emb\n```\n\n那么需要这么使用：\n\n```python\nfrom byzerllm.apps.byzer_storage.simple_api import ByzerStorage, DataType, FieldOption,SortOption\nstorage = ByzerStorage(\"byzerai_store\", \"memory\", \"memory\")\nstorage.emb(\"你好\")\n```\n\n## 一些辅助工具\n\n当调用 prompt 函数返回字符串的时候，如果想从里面抽取代码，可以使用如下方式：\n\n```python\nfrom byzerllm.utils.client import code_utils\ntext_with_markdown = '''\n```shell\nls -l\n```\n'''\ncode_blocks = code_utils.extract_code(text_with_markdown)\nfor code_block in code_blocks:\n    if code_block[0] == \"shell\":\n        print(code_block[1])\n##output: ls -l        \n```\n\n\n## 注意事项\n\n1. prompt函数方法体返回只能是dict，实际的返回类型和方法签名可以不一样，但是方法体返回只能是dict。\n2. 大部分情况prompt函数体为空，如果一定要有方法体，可以返回一个空字典。\n3. 调用prompt方法的时候，如果在@byzerllm.prompt()里没有指定llm对象，那么需要在调用的时候通过with_llm方法指定llm对象。\n\n===========================\n\n# Byzer Storage: 最易用的AI存储引擎\n\nByzer Storage是一个为AI应用设计的高性能存储引擎,它提供了简单易用的API,支持向量搜索、全文检索以及结构化查询。本文将详细介绍Byzer Storage的使用方法和主要特性。\n\n## 0. 安装和启动\n\n```bash\npip install byzerllm\nbyzerllm storage start\n```\n\nThat's it! Byzer Storage已经安装并启动成功,现在我们可以开始使用它了。默认会启动一个 byzerai_store 的集群。\n\n注意，如果你是这样启动的：\n\n```bash\nbyzerllm storage start --enable_emb\n```\n\n那么会自动启动一个 emb 模型，名字就叫 emb， ByzerStorage 会自动使用该模型，无需做任何其他配置。\n\n## 1. 初始化\n\n创建一个 ByzerStorage 对象，链接 byzerai_store 集群，并且指定数据库和表名（可以不存在）。\n\n```python\nfrom byzerllm.apps.byzer_storage.simple_api import ByzerStorage, DataType, FieldOption, SortOption\n\nstorage = ByzerStorage(\"byzerai_store\", \"my_database1\", \"my_table4s\")\n```\n\n\n## 2. 创建库表（可选）\n\nByzer Storage使用Schema来定义数据结构。我们可以使用SchemaBuilder来构建Schema:\n\n```python\n_ = (\n    storage.schema_builder()\n    .add_field(\"_id\", DataType.STRING)\n    .add_field(\"name\", DataType.STRING)\n    .add_field(\"content\", DataType.STRING, [FieldOption.ANALYZE])\n    .add_field(\"raw_content\", DataType.STRING, [FieldOption.NO_INDEX])    \n    .add_array_field(\"summary\", DataType.FLOAT)    \n    .add_field(\"created_time\", DataType.LONG, [FieldOption.SORT])    \n    .execute()\n)\n```\n\n这个Schema定义了以下字段:\n- `_id`: 字符串类型的主键\n- `name`: 字符串类型,可用于过滤条件\n- `content`: 字符串类型,带有ANALYZE选项,用于全文搜索\n- `raw_content`: 字符串类型,带有NO_INDEX选项,不会被索引\n- `summary`: 浮点数组类型,用于存储向量\n- `created_time`: 长整型,带有SORT选项,可用于排序\n\n需要注意的是：\n\n1. 如果一个字段带有ANALYZE选项,则该字段会被分词,并且可以用于全文搜索，但是就无法返回原始的文本了。所以你需要添加一个新字段专门用来存储原文，比如在我们这个例子里，我们新增了 raw_content 字段，并且显示的指定了 NO_INDEX 选项，这样就不会被索引，也就不会被分词，可以后续被检索后用来获取原始文本。\n2. 对于需要作为向量检索和存储的字段，需要指定为数组类型，比如我们这个例子里的 summary 字段。\n3. 如果你需要拿到向量字段的原始文本，那么你也需要添加一个新字段专门用来存储原文，就像我们这个例子里的 raw_content 字段一样。\n\n\n## 3. 写入数据\n\n准备数据并使用WriteBuilder写入Storage:\n\n```python\ndata = [\n    {\"_id\": \"1\", \"name\": \"Hello\", \"content\": \"Hello, world!\", \"raw_content\": \"Hello, world!\", \"summary\": \"hello world\", \"created_time\": 1612137600},\n    {\"_id\": \"2\", \"name\": \"Byzer\", \"content\": \"Byzer, world!\", \"raw_content\": \"Byzer, world!\", \"summary\": \"byzer\", \"created_time\": 1612137601},\n    {\"_id\": \"3\", \"name\": \"AI\", \"content\": \"AI, world!\", \"raw_content\": \"AI, world!\", \"summary\": \"AI\", \"created_time\": 16121376002},\n    {\"_id\": \"4\", \"name\": \"ByzerAI\", \"content\": \"ByzerAI, world!\", \"raw_content\": \"ByzerAI, world!\", \"summary\": \"ByzerAi\", \"created_time\": 16121376003},\n]\n\nstorage.write_builder().add_items(data, vector_fields=[\"summary\"], search_fields=[\"content\"]).execute()\nstorage.commit()\n```\n\n这里我们使用`add_items`方法批量添加数据,并指定了`summary`为向量字段,`content`为搜索字段。最后调用`commit()`来确保数据被持久化。\n\n\n从上面我们要写入到 byzer storage 的数据我们可以看到如下几个特点：\n\n1. 需要包含我们之前定义的 Schema中罗列的所有的字段，同时需要指定哪些是向量字段，哪些是检索字段。\n2. 向量和检索不能是同一个字段。\n3. 对于向量，检索字段，我们给到 write_builder 的都是文本，ByzerStorage 会根据 Schema 的定义自动将其转换为向量，检索字段需要的格式。\n\n## 4. 查询数据\n\nByzer Storage支持多种查询方式,包括向量搜索、全文检索、过滤和排序。\n\n### 4.1 向量搜索 + 全文检索\n\n```python\nquery = storage.query_builder()\nquery.set_vector_query(\"ByzerAI\", fields=[\"summary\"])\nresults = query.set_search_query(\"Hello\", fields=[\"content\"]).execute()\nprint(results)\n```\n\n这个查询结合了向量搜索和全文检索,它会在`summary`字段中搜索与\"ByzerAI\"相似的向量,同时在`content`字段中搜索包含\"Hello\"的文档。\n\n### 4.2 过滤 + 向量搜索 + 全文检索\n\n```python\nquery = storage.query_builder()\nquery.and_filter().add_condition(\"name\", \"AI\").build()\nquery.set_vector_query(\"ByzerAI\", fields=\"summary\")\nresults = query.set_search_query(\"Hello\", fields=[\"content\"]).execute()\nprint(results)\n```\n\n这个查询首先过滤`name`字段等于\"AI\"的文档,然后在结果中进行向量搜索和全文检索。\n\n### 4.3 过滤 + 排序\n\n```python\nquery = storage.query_builder()\nquery.and_filter().add_condition(\"name\", \"AI\").build().sort(\"created_time\", SortOption.DESC)\nresults = query.execute()\nprint(results)\n```\n\n这个查询过滤`name`字段等于\"AI\"的文档,然后按`created_time`字段降序排序。\n\n## 5. 删除数据\n\n### 5.1 根据ID删除\n\n```python\nstorage.delete_by_ids([\"3\"])\n\nquery = storage.query_builder()\nquery.and_filter().add_condition(\"name\", \"AI\").build()\nresults = query.execute()\nprint(results)\n```\n\n这里我们删除了ID为\"3\"的文档,然后查询验证删除结果。\n\n### 5.2 删除整个表\n\n```python\nstorage.drop()\n```\n\n这个操作会删除整个表及其所有数据,请谨慎使用。\n\n## 结论\n\nByzer Storage提供了一套简洁而强大的API,能够轻松实现向量搜索、全文检索、结构化查询等功能。它的设计非常适合AI应用场景,可以有效地存储和检索各种类型的数据。通过本文的介绍,相信读者已经对Byzer Storage有了基本的了解,并能够开始在自己的项目中使用这个强大的存储引擎。\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/cli.py\nimport argparse\nfrom byzerllm.utils.client import ByzerLLM\nfrom superanalysis.serve.api_server import serve, ServerArgs\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"ByzerLLM API Server\")\n    parser.add_argument(\"--host\", type=str, default=\"0.0.0.0\", help=\"Host to bind the server to\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"Port to bind the server to\")\n    parser.add_argument(\"--uvicorn-log-level\", type=str, default=\"info\", help=\"Uvicorn log level\")\n    parser.add_argument(\"--allow-credentials\", action=\"store_true\", help=\"Allow credentials\")\n    parser.add_argument(\"--allowed-origins\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed origins\")\n    parser.add_argument(\"--allowed-methods\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed methods\")\n    parser.add_argument(\"--allowed-headers\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed headers\")\n    parser.add_argument(\"--api-key\", type=str, help=\"API key for authentication\")\n    parser.add_argument(\"--served-model-name\", type=str, required=True, help=\"Name of the model to be served\")\n    parser.add_argument(\"--prompt-template\", type=str, help=\"Prompt template\")\n    parser.add_argument(\"--response-role\", type=str, default=\"assistant\", help=\"Response role\")\n    parser.add_argument(\"--ssl-keyfile\", type=str, help=\"SSL key file\")\n    parser.add_argument(\"--ssl-certfile\", type=str, help=\"SSL certificate file\")\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile\n    )\n\n    llm = ByzerLLM()\n    llm.setup_default_model_name(args.served_model_name)\n\n    serve(llm, server_args)\n\nif __name__ == \"__main__\":\n    main()\n\n     \n\n下面是用户的需求：\n\n添加setup.py 文件,支持 super-analysis.serve  命令, 调用 cli 启动\n"}, {"role": "assistant", "content": "为了满足用户的需求,我们需要添加一个setup.py文件,并在其中配置entry_points以支持super-analysis.serve命令。这个命令将调用cli.py中的main函数来启动服务器。以下是具体的实现步骤:\n\n1. 首先,我们需要在项目根目录创建一个setup.py文件。\n\n2. 在setup.py中,我们需要配置项目的基本信息和依赖,同时添加entry_points来支持命令行工具。\n\n下面是setup.py文件的内容:\n\n```python\n##File: /Users/allwefantasy/projects/super_analysis/setup.py\n<<<<<<< SEARCH\n=======\nfrom setuptools import setup, find_packages\n\nsetup(\n    name=\"super-analysis\",\n    version=\"0.1.0\",\n    packages=find_packages(where=\"src\"),\n    package_dir={\"\": \"src\"},\n    install_requires=[\n        \"byzerllm\",\n        \"fastapi\",\n        \"uvicorn\",\n        \"argparse\",\n    ],\n    entry_points={\n        \"console_scripts\": [\n            \"super-analysis.serve=superanalysis.serve.cli:main\",\n        ],\n    },\n    author=\"Your Name\",\n    author_email=\"your.email@example.com\",\n    description=\"A package for super analysis with ByzerLLM\",\n    long_description=open(\"README.md\").read(),\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/yourusername/super-analysis\",\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n    ],\n    python_requires='>=3.6',\n)\n>>>>>>> REPLACE\n```\n\n这个setup.py文件做了以下几件事:\n\n1. 定义了项目的基本信息,如名称、版本、作者等。\n2. 指定了项目的依赖包。\n3. 配置了entry_points,创建了一个名为super-analysis.serve的命令,它会调用superanalysis.serve.cli模块的main函数。\n4. 设置了包的查找路径,确保能够正确找到src目录下的包。\n\n现在,用户可以通过以下步骤来安装和使用这个包:\n\n1. 在项目根目录运行 `pip install -e .` 来安装这个包。\n2. 安装完成后,可以直接在命令行中使用 `super-analysis.serve` 命令来启动服务器。\n\n例如:\n\n```\nsuper-analysis.serve --served-model-name my_model --port 8080\n```\n\n这个命令将会调用cli.py中的main函数,并传入相应的参数来启动服务器。\n\n请注意,你可能需要根据实际情况调整setup.py中的一些信息,比如作者名称、邮箱、项目URL等。同时,确保在项目根目录下有一个README.md文件,因为setup.py中引用了这个文件作为长描述。"}], "model": "human", "yaml_file": "actions/103_chat_action.yml"}
{"instruction": "根据 @setup.py @cli.py 写一个README.md", "conversations": [{"role": "user", "content": "\n如果你需要生成代码，对于每个需要更改的文件,你需要按 *SEARCH/REPLACE block* 的格式进行生成。\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The opening fence and code language, eg: ```python\n2. The file path alone on a line, starting with \"##File:\" and verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n*SEARCH/REPLACE* blocks will replace *all* matching occurrences.\nInclude enough lines to make the SEARCH blocks unique.\n\nInclude *ALL* the code being searched and replaced!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n下面我们来看一个例子：\n\n当前项目目录结构：\n1. 项目根目录： /tmp/projects/mathweb\n2. 项目子目录/文件列表(类似tree 命令输出)\nflask/\n    app.py\n    templates/\n        index.html\n    static/\n        style.css\n\n用户需求： Change get_factorial() to use math.factorial\n\n回答： To make this change we need to modify `/tmp/projects/mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n用户需求： Refactor hello() into its own file.\n\n回答：To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/hello.py\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/main.py\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n现在让我们开始一个新的任务:\n\n当前项目目录结构：\n1. 项目根目录： /Users/allwefantasy/projects/super_analysis\n2. 项目子目录/文件列表(类似tree 命令输出)：\nsetup.py\n.auto-coder/\n    libs/\n        llm_friendly_packages/\n            src/\n                example1/\n                    main.py\nsrc/\n    superanalysis/\n        serve/\n            openai_serve.py\n            llm_wrapper.py\n            api_server.py\n            cli.py\n下面是一些文件路径以及每个文件对应的源码：\n\n##File: .auto-coder/libs/llm_friendly_packages/github.com/allwefantasy/byzer-llm/README.md\n# byzerllm 大模型编程快速指南\n\n本文示例 [Notebook](../../notebooks/003_byzerllm_大模型编程快速指南.ipynb)\n\n## 安装\n\n```bash\npip install -U byzerllm\nray start --head\n```\n## 启动一个模型代理\n\nbyzerllm 支持私有化模型或者SaaS模型的部署。\n\n这里以 deepseek 官方API 为例：\n\n```bash\neasy-byzerllm deploy deepseek-chat --token xxxxx --alias deepseek_chat\n```\n\n或者跬基流动API:\n\n```bash\neasy-byzerllm deploy alibaba/Qwen1.5-110B-Chat --token xxxxx --alias qwen110b_chat\n```\n\n将上面的 API KEY 替换成你们自己的。\n\n如果你想部署私有化模型或者对接 Ollama 等更多需求，参考 [002_使用byzerllm进行模型部署.md](./002_使用byzerllm进行模型部署.md) 或者 [README.md](../../README.md)。\n\n之后，你就可以在代码里使用  deepseek_chat 或者 qwen110b_chat  访问模型了。\n\n## hello world\n\n来和我们的大模型打个招呼:\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\n@byzerllm.prompt(llm=llm)\ndef hello(q:str) ->str:\n    '''\n    你好, {{ q }}\n    '''\n\ns = hello(\"你是谁\")    \nprint(s)\n\n## 输出:\n## '你好！我是一个人工智能助手，专门设计来回答问题、提供信息和帮助解决问题。如果你有任何疑问或需要帮助，请随时告诉我。'\n```\n\n恭喜，你和大模型成功打了招呼！\n\n可以看到，我们通过 `@byzerllm.prompt` 装饰器，将一个方法转换成了一个大模型的调用，然后这个方法的主题是一段文本，文本中\n使用了 jinja2 模板语法，来获得方法的参数。当正常调用该方法时，实际上就发起了和大模型的交互，并且返回了大模型的结果。\n\n在 byzerllm 中，我们把这种方法称之为 prompt 函数。\n\n## 查看发送给大模型的prompt\n\n很多情况你可能需要调试，查看自己的 prompt 渲染后到底是什么样子的，这个时候你可以通过如下方式\n获取渲染后的prompt:\n\n```python\nhello.prompt(\"你是谁\")\n## '你好, 你是谁'\n```            \n\n## 动态换一个模型\n\n前面的 hello 方法在初始化的时候，我们使用了默认的模型 deepseek_chat，如果我们想换一个模型，可以这样做：\n\n```python\nhello.with_llm(llm).run(\"你是谁\")\n## '你好！我是一个人工智能助手，专门设计来回答问题、提供信息和帮助解决问题。如果你有任何疑问或需要帮助，请随时告诉我。'\n```\n\n通过 with_llm 你可以设置一个新的 llm 对象，然后调用 run 方法，就可以使用新的模型了。\n\n## 超长文本生成\n\n我们知道，大模型一次生成的长度其实是有限的，如果你想生成超长文本，你可能需手动的不断获得\n生成结果，然后把他转化为输入，然后再次生成，这样的方式是比较麻烦的。\n\nbyzerllm 提供了更加易用的 API :\n\n```python\nimport byzerllm\nfrom byzerllm import ByzerLLM\n\nllm = ByzerLLM.from_default_model(\"deepseek_chat\")\n\n@byzerllm.prompt()\ndef tell_story() -> str:\n    \"\"\"\n    讲一个100字的故事。    \n    \"\"\"\n\n\ns = (\n    tell_story.with_llm(llm)\n    .with_response_markers()\n    .options({\"llm_config\": {\"max_length\": 10}})\n    .run()\n)\nprint(s)\n\n## 从前，森林里住着一只聪明的小狐狸。一天，它发现了一块闪闪发光的宝石。小狐狸决定用这块宝石帮助森林里的动物们。它用宝石的光芒指引迷路的小鹿找到了回家的路，用宝石的温暖治愈了受伤的小鸟。从此，小狐狸成了森林里的英雄，动物们都感激它的善良和智慧。\n```\n\n实际核心部分就是这一行：\n\n```python\ntell_story.with_llm(llm)\n    .with_response_markers()    \n    .run()\n```\n\n我们只需要调用 `with_response_markers` 方法，系统就会自动的帮我们生成超长文本。\n在上面的案例中，我们通过\n\n```python\n.options({\"llm_config\": {\"max_length\": 10}})\n```\n\n认为的限制大模型一次交互最多只能输出10个字符，但是系统依然自动完成了远超过10个字符的文本生成。\n\n## 对象输出\n\n前面我们的例子都是返回字符串，但是我们也可以返回对象，这样我们就可以更加灵活的处理返回结果。\n\n```python\nimport pydantic \n\nclass Story(pydantic.BaseModel):\n    '''\n    故事\n    '''\n\n    title: str = pydantic.Field(description=\"故事的标题\")\n    body: str = pydantic.Field(description=\"故事主体\")\n\n@byzerllm.prompt()\ndef tell_story()->Story:\n    '''\n    讲一个100字的故事。    \n    '''\n\ns = tell_story.with_llm(llm).run()\nprint(isinstance(s, Story))\nprint(s.title)\n\n## True\n## 勇敢的小鸟\n```\n\n可以看到，我们很轻松的将输出转化为格式化输出。\n\n## 自定义字段抽取\n\n前面的结构化输出，其实会消耗更多token,还有一种更加精准的结构化输出方式。\n比如让大模型生成一个正则表达式，但实际上大模型很难准确只输出一个正则表达式，这个时候我们可以通过自定义抽取函数来获取我们想要的结果。\n\n\n```python\nfrom loguru import logger\nimport re\n\n@byzerllm.prompt()\ndef generate_regex_pattern(desc: str) -> str:\n    \"\"\"\n    根据下面的描述生成一个正则表达式，要符合python re.compile 库的要求。\n\n    {{ desc }}\n\n    最后生成的正则表达式要在<REGEX></REGEX>标签对里。\n    \"\"\"    \n\ndef extract_regex_pattern(regex_block: str) -> str:    \n    pattern = re.search(r\"<REGEX>(.*)</REGEX>\", regex_block, re.DOTALL)\n    if pattern is None:\n        logger.warning(\"No regex pattern found in the generated block:\\n {regex_block}\")\n        raise None\n    return pattern.group(1)\n\npattern = \"匹配一个邮箱地址\"\nv = generate_regex_pattern.with_llm(llm).with_extractor(extract_regex_pattern).run(desc=pattern)\nprint(v)\n## ^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\n```\n\n在上面的例子里，我们根据一句话生成一个正则表达式。我们通过 `with_extractor` 方法，传入了一个自定义的抽取函数，这个函数会在大模型生成结果后，对结果进行处理，然后返回我们想要的结果。\n\n我们在 prompt 明确说了，生成的结果要放到 `<REGEX></REGEX>` 标签对里，然后我们通过 extract_regex_pattern 函数，从结果中提取出了我们想要的正则表达式。\n\n## 在实例方法中使用大模型\n\n```python\nimport byzerllm\ndata = {\n    'name': 'Jane Doe',\n    'task_count': 3,\n    'tasks': [\n        {'name': 'Submit report', 'due_date': '2024-03-10'},\n        {'name': 'Finish project', 'due_date': '2024-03-15'},\n        {'name': 'Reply to emails', 'due_date': '2024-03-08'}\n    ]\n}\n\n\nclass RAG():\n    def __init__(self):        \n        self.llm = byzerllm.ByzerLLM()\n        self.llm.setup_template(model=\"deepseek_chat\",template=\"auto\")\n        self.llm.setup_default_model_name(\"deepseek_chat\")        \n    \n    @byzerllm.prompt(lambda self:self.llm)\n    def generate_answer(self,name,task_count,tasks)->str:\n        '''\n        Hello {{ name }},\n\n        This is a reminder that you have {{ task_count }} pending tasks:\n        {% for task in tasks %}\n        - Task: {{ task.name }} | Due: {{ task.due_date }}\n        {% endfor %}\n\n        Best regards,\n        Your Reminder System\n        '''        \n\nt = RAG()\n\nresponse = t.generate_answer(**data)\nprint(response)\n\n## 输出:\n## Hello Jane Doe,\n##I hope this message finds you well. I wanted to remind you of your 3 pending tasks to ensure you stay on track:\n## 1. **Submit report** - This task is due on **2024-03-10**. Please ensure that you allocat\n```\n\n这里我们给了个比较复杂的例子，但我们可以看到，给一个实例prompt方法和普通prompt 方法差异不大。\n唯一的区别是如果你希望在定义的时候就指定大模型，使用一个lambda函数返回实例的 llm 对象即可。\n\n```python\n@byzerllm.prompt(lambda self:self.llm)\n```\n\n你也可以不返回，在调用的时候通过 `with_llm` 方法指定 llm 对象。\n\n此外，这个例子也展示了如何通过jinja2模板语法，来处理复杂的结构化数据。\n\n## 通过 Python 代码处理复杂入参\n\n上面的一个例子中，我们通过 jinja2 模板语法，来处理复杂的结构化数据，但是有时候我们可能需要更加复杂的处理，这个时候我们可以通过 Python 代码来处理。\n\n```python\nimport byzerllm\n\ndata = {\n    'name': 'Jane Doe',\n    'task_count': 3,\n    'tasks': [\n        {'name': 'Submit report', 'due_date': '2024-03-10'},\n        {'name': 'Finish project', 'due_date': '2024-03-15'},\n        {'name': 'Reply to emails', 'due_date': '2024-03-08'}\n    ]\n}\n\n\nclass RAG():\n    def __init__(self):        \n        self.llm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n    \n    @byzerllm.prompt()\n    def generate_answer(self,name,task_count,tasks)->str:\n        '''\n        Hello {{ name }},\n\n        This is a reminder that you have {{ task_count }} pending tasks:\n            \n        {{ tasks }}\n\n        Best regards,\n        Your Reminder System\n        '''\n        \n        tasks_str = \"\\n\".join([f\"- Task: {task['name']} | Due: { task['due_date'] }\" for task in tasks])\n        return {\"tasks\": tasks_str}\n\nt = RAG()\n\nresponse = t.generate_answer.with_llm(t.llm).run(**data)\nprint(response)\n\n## Just a gentle nudge to keep you on track with your pending tasks. Here's a quick recap:....\n```\n\n在这个例子里，我们直接把 tasks 在方法体里进行处理，然后作为一个字符串返回，最够构建一个字典，字典的key为 tasks,然后\n你就可以在 docstring 里使用 `{{ tasks }}` 来引用这个字符串。\n\n这样对于很复杂的入参，就不用谢繁琐的 jinja2 模板语法了。\n\n## 如何自动实现一个方法\n\n比如我定义一个签名，但是我不想自己实现里面的逻辑，让大模型来实现。这个在 byzerllm 中叫 function impl。我们来看看怎么\n实现:\n\n```python\nimport pydantic\nclass Time(pydantic.BaseModel):\n    time: str = pydantic.Field(...,description=\"时间，时间格式为 yyyy-MM-dd\")\n\n\n@llm.impl()\ndef calculate_current_time()->Time:\n    '''\n    计算当前时间\n    '''\n    pass \n\n\ncalculate_current_time()\n#output: Time(time='2024-06-14')\n```\n\n在这个例子里，我们定义了一个 calculate_current_time 方法，但是我们没有实现里面的逻辑，我们通过 `@llm.impl()` 装饰器，让大模型来实现这个方法。\n为了避免每次都要“生成”这个方法，导致无法适用，我们提供了缓存，用户可以按如下方式打印速度：\n\n```python\nstart = time.monotonic()\ncalculate_current_time()\nprint(f\"first time cost: {time.monotonic()-start}\")\n\nstart = time.monotonic()\ncalculate_current_time()\nprint(f\"second time cost: {time.monotonic()-start}\")\n\n# output:\n# first time cost: 6.067266260739416\n# second time cost: 4.347506910562515e-05\n```\n可以看到，第一次执行花费了6s,第二次几乎是瞬间完成的，这是因为第一次执行的时候，我们实际上是在生成这个方法，第二次执行的时候，我们是执行已经生成好的代码，所以速度会非常快。你可以显示的调用 `llm.clear_impl_cache()` 清理掉函数缓存。\n\n## Stream 模式\n\n前面的例子都是一次性生成结果，但是有时候我们可能需要一个流式的输出，这个时候我们可能需要用底层一点的API来完成了：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\nv = llm.stream_chat_oai(model=\"deepseek_chat\",conversations=[{\n    \"role\":\"user\",\n    \"content\":\"你好，你是谁\",\n}],delta_mode=True)\n\nfor t in v:\n    print(t,flush=True)  \n\n# 你好\n# ！\n# 我\n# 是一个\n# 人工智能\n# 助手\n# ，\n# 旨在\n# 提供\n# 信息\n# 、\n# 解答\n# 问题....\n```\n\n如果你不想要流式输出，但是想用底层一点的API，你可以使用 `llm.chat_oai` 方法：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\nv = llm.chat_oai(model=\"deepseek_chat\",conversations=[{\n    \"role\":\"user\",\n    \"content\":\"你好，你是谁\",\n}])\n\nprint(v[0].output)\n## 你好！我是一个人工智能助手，旨在提供信息、解答问题和帮助用户解决问题。如果你有任何问题或需要帮助，请随时告诉我。\n```\n\n## Function Calling \n\nbyzerllm 可以不依赖模型自身就能提供 function calling 支持，我们来看个例子：\n\n\n```python\nfrom typing import List,Dict,Any,Annotated\nimport pydantic \nimport datetime\nfrom dateutil.relativedelta import relativedelta\n\ndef compute_date_range(count:Annotated[int,\"时间跨度，数值类型\"],\n                       unit:Annotated[str,\"时间单位，字符串类型\",{\"enum\":[\"day\",\"week\",\"month\",\"year\"]}])->List[str]:\n    '''\n    计算日期范围\n\n    Args:\n        count: 时间跨度，数值类型\n        unit: 时间单位，字符串类型，可选值为 day,week,month,year\n    '''        \n    now = datetime.datetime.now()\n    now_str = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n    if unit == \"day\":\n        return [(now - relativedelta(days=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"week\":\n        return [(now - relativedelta(weeks=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"month\":\n        return [(now - relativedelta(months=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"year\":\n        return [(now - relativedelta(years=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    return [\"\",\"\"]\n\ndef compute_now()->str:\n    '''\n    计算当前时间\n    '''\n    return datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n```\n\n我们定义了两个方法，一个是计算日期范围，一个是计算当前时间。\n\n现在我么可以来测试下，系统如何根据自然语言决定调用哪个方法：\n\n```python\nt = llm.chat_oai([{\n    \"content\":'''计算当前时间''',\n    \"role\":\"user\"    \n}],tools=[compute_date_range,compute_now],execute_tool=True)\n\nt[0].values\n\n## output: ['2024-06-14 15:18:02']\n```\n\n我们可以看到，他正确的选择了 compute_now 方法。\n\n接着我们再试一个：\n\n```python\nt = llm.chat_oai([{\n    \"content\":'''最近三个月趋势''',\n    \"role\":\"user\"    \n}],tools=[compute_date_range,compute_now],execute_tool=True)\n\nt[0].values\n\n## output: [['2024-03-14 15:19:13', '2024-06-14 15:19:13']]\n```\n\n模型正确的选择了 compute_date_range 方法。\n\n## 多模态\n\nbyerllm 也能很好的支持多模态的交互，而且统一了多模态大模型的接口，比如你可以用一样的方式使用 openai 或者 claude 的图片转文字能力， 或者一致的方式使用火山，azuer, openai的语音合成接口。\n\n### 图生文\n\n```python\nimport byzerllm\nfrom byzerllm.types import ImagePath\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\n\n\n@byzerllm.prompt()\ndef what_in_image(image_path: ImagePath) -> str:\n    \"\"\"\n    {{ image_path }}\n    这个图片里有什么？\n    \"\"\"    \n\n\nv = what_in_image.with_llm(vl_llm).run(\n    ImagePath(value=\"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\")\n)\nv\n## OUTPUT: 这张图片展示了多只可爱的猫咪，采用了艺术风格的绘画。猫咪们有不同的颜色和花纹，背景是浅棕色，上面还点缀着一些红色的花朵。整体画面给人一种温馨和谐的感觉\n```\n\n可以看到，我们只需要把 prompt 函数的图片地址入参使用 byzerllm.types.ImagePath里进行包装，就可以直接在 prompt 函数体里\n带上图片。\n\n或者你可以这样：\n\n```python\nimport byzerllm\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\n\n\n@byzerllm.prompt()\ndef what_in_image(image_path: str) -> str:\n    \"\"\"\n    {{ image }}\n    这个图片里有什么？\n    \"\"\"\n    return {\"image\": byzerllm.Image.load_image_from_path(image_path)}\n\n\nv = what_in_image.with_llm(vl_llm).run(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv\n```\n\n通过 `image_path` 参数，然后通过 `byzerllm.Image.load_image_from_path` 方法，转化为一个图片对象 image，最后在 prompt 函数体里\n使用 `{{ image }}` 引用这个图片对象。\n\n另外我们也是可以支持配置多张图片的。\n\n另外，我们也可以使用基础的 `llm.chat_oai` 方法来实现：\n\n```python\nimport byzerllm\nimport json\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\nimage = byzerllm.Image.load_image_from_path(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv = vl_llm.chat_oai(\n    conversations=[\n        {\n            \"role\": \"user\",\n            \"content\": json.dumps(\n                [{\"image\": image, \"text\": \"这个图片里有什么？\"}], ensure_ascii=False\n            ),\n        }\n    ]\n)\nv[0].output\n```\n\n还可以这么写：\n\n```python\nimport byzerllm\nimport json\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\nimage = byzerllm.Image.load_image_from_path(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv = vl_llm.chat_oai(\n    conversations=[\n        {\n            \"role\": \"user\",\n            \"content\": json.dumps(\n                [\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\"url\": image, \"detail\": \"high\"},\n                    },\n                    {\"text\": \"这个图片里有什么？\", \"type\": \"text\"},\n                ],\n                ensure_ascii=False,\n            ),\n        }\n    ]\n)\nv[0].output\n```\n\n### 语音合成\n\n这里以 openai 的 tts 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=tts-1 \\\n--model openai_tts\n```\n\n此外，byzerllm 支持 azure,火山引擎等 tts 语音合成引擎。\n\n接着你可以这么用：\n\n```python\nimport byzerllm\nimport base64\nimport json\n\nllm = byzerllm.ByzerLLM.from_default_model(\"openai_tts\")\n\n\nt = llm.chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\": json.dumps({\n        \"input\":\"hello, open_tts\",\n        \"voice\": \"alloy\",\n        \"response_format\": \"mp3\"\n    },ensure_ascii=False)\n}])\n\nwith open(\"voice.mp3\",\"wb\") as f:\n    f.write(base64.b64decode(t[0].output))\n```\n\ntts 模型生成没有prompt函数可以用，你需要直接使用 chat_oai。\n\n\n### 语音识别\n\n这里以 openai 的 whisper-1 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--worker_concurrency 10 \\\n--infer_params saas.model=whisper-1 saas.api_key=${MODEL_OPENAI_TOKEN} \\\n--model speech_to_text\n```\n\n语音识别的使用方式和图生文类似，我们可以直接在 prompt 函数体里带上音频文件。\n\n```python\nimport byzerllm\nimport json\nimport base64\nfrom byzerllm.types import AudioPath\n\nllm = byzerllm.ByzerLLM.from_default_model(\"speech_to_text\")\n\naudio_file = \"/Users/allwefantasy/videos/output_audio.mp3\"\n\n@byzerllm.prompt(llm=llm)\ndef audio_to_text(audio_file: AudioPath):\n    \"\"\"\n    {{ audio_file }}\n    \"\"\"\n\nv = audio_to_text(AudioPath(value=audio_file))\njson.loads(v)\n```\n输出的数据格式略微复杂：\n\n```\n{'text': 'In the last chapter, you and I started to step through the internal workings of a transformer. This is one of the key pieces of technology inside large language models, and a lot of other tools in the modern wave of AI.',\n 'task': 'transcribe',\n 'language': 'english',\n 'duration': 10.0,\n 'segments': [{'id': 0,\n   'seek': 0,\n   'start': 0.0,\n   'end': 4.78000020980835,\n   'text': ' In the last chapter, you and I started to step through the internal workings of a transformer.',\n   'tokens': [50364,\n    682,\n    264,\n    1036,\n    7187,\n    11,\n    291,\n    293,\n    286,\n    1409,\n    281,\n    1823,\n    807,\n    264,\n    6920,\n    589,\n    1109,\n    295,\n    257,\n    31782,\n    13,\n    50586],\n   'temperature': 0.0,\n   'avg_logprob': -0.28872039914131165,\n   'compression_ratio': 1.4220778942108154,\n   'no_speech_prob': 0.016033057123422623},\n  ....\n  {'id': 2,\n   'seek': 0,\n   'start': 8.579999923706055,\n   'end': 9.979999542236328,\n   'text': ' and a lot of other tools in the modern wave of AI.',\n   'tokens': [50759,\n    293,\n    257,\n    688,\n    295,\n    661,\n    3873,\n    294,\n    264,\n    4363,\n    5772,\n    295,\n    7318,\n    13,\n    50867],\n   'temperature': 0.0,\n   'avg_logprob': -0.28872039914131165,\n   'compression_ratio': 1.4220778942108154,\n   'no_speech_prob': 0.016033057123422623}],\n 'words': [{'word': 'In', 'start': 0.0, 'end': 0.18000000715255737},\n  {'word': 'the', 'start': 0.18000000715255737, 'end': 0.23999999463558197},\n  {'word': 'last', 'start': 0.23999999463558197, 'end': 0.5400000214576721},\n  {'word': 'chapter', 'start': 0.5400000214576721, 'end': 0.800000011920929},\n  ....\n  {'word': 'AI', 'start': 9.920000076293945, 'end': 9.979999542236328}]}\n```\n\n会输出每一句话以及每一个字所在的起始时间和截止时间。你可以根据需要来使用。\n\n\n### 文生图\n\n文生图和语音合成类似，首先要启动合适的模型,以openai 的 dall-e-3 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=dall-e-3 \\\n--model openai_image_gen\n```\n\n启动模型后，只需要记住几个模板参数即可使用，这里直接使用 chat_oai 方法来使用：\n\n```python\n\nimport byzerllm\nimport json\nimport base64\n\nllm = byzerllm.ByzerLLM.from_default_model(\"openai_image_gen\")\nt = llm.chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\": json.dumps({\n        \"input\":\"a white siamese cat\",\n        \"size\": \"1024x1024\",\n        \"quality\": \"standard\"\n    },ensure_ascii=False)\n}])\n\nwith open(\"output1.jpg\",\"wb\") as f:\n    f.write(base64.b64decode(t[0].output))\n\n\nimport matplotlib.pyplot as plt\n\nimage_path = \"output1.jpg\"\nimage = plt.imread(image_path)\n\nplt.imshow(image)\nplt.axis('off')\nplt.show()\n```\n\n## Prompt 函数的流式输出\n\nbyzerllm 底层支持流式输出，非 prompt 函数的用法是这样的：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\n\nv = llm.stream_chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\":\"讲一个100字的故事\"\n}])\n\nfor s  in v:\n    print(s[0], end=\"\")\n```\n\n如果你像用 prompt 函数，可以这么用：\n\n\n```python\nimport byzerllm\nimport json\nimport base64\nfrom typing import Generator\n\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\n\n@byzerllm.prompt()\ndef tell_story() -> Generator[str, None, None]:\n    '''\n    给我讲一个一百多字的故事\n    '''\n\nv = tell_story.with_llm(llm).run()    \nfor i in v:\n    print(i, end=\"\")\n```\n\n可以看到，和普通的 prompt 函数的区别在于，返回值是一个生成器，然后你可以通过 for 循环来获取结果。\n\n## 向量化模型\n\nbyzerllm 支持向量化模型,你可以这样启动一个本地的模型：\n\n```bash\n!byzerllm deploy --pretrained_model_type custom/bge \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--worker_concurrency 10 \\\n--model_path /home/winubuntu/.auto-coder/storage/models/AI-ModelScope/bge-large-zh \\\n--infer_backend transformers \\\n--num_workers 1 \\\n--model emb\n```\n\n注意两个参数:\n\n1. --infer_backend transformers: 表示使用 transformers 作为推理后端。\n2. --model_path: 表示模型的路径。\n\n也可以启动一个 SaaS 的emb模型,比如 qwen 的 emb 模型：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/qianwen \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 2 \\\n--infer_params saas.api_key=${MODEL_QIANWEN_TOKEN}  saas.model=text-embedding-v2 \\\n--model qianwen_emb\n```\n\n或者 openai 的 emb 模型：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--worker_concurrency 10 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=text-embedding-3-small \\\n--model gpt_emb\n```\nSaaS 模型无需配置 `--infer_backend` 参数。\n\n无论是本地模型还是 SaaS 模型，我们都可以这样使用：\n\n```python\nimport byzerllm\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\nllm.setup_default_emb_model_name(\"emb\")\nllm.emb_query(\"你好\")\n```\n\n如果你配置 byzerllm 中的 Storage 使用，比如你这样启动了存储：\n\n```bash\nbyzerllm storage start --enable_emb\n```\n\n那么需要这么使用：\n\n```python\nfrom byzerllm.apps.byzer_storage.simple_api import ByzerStorage, DataType, FieldOption,SortOption\nstorage = ByzerStorage(\"byzerai_store\", \"memory\", \"memory\")\nstorage.emb(\"你好\")\n```\n\n## 一些辅助工具\n\n当调用 prompt 函数返回字符串的时候，如果想从里面抽取代码，可以使用如下方式：\n\n```python\nfrom byzerllm.utils.client import code_utils\ntext_with_markdown = '''\n```shell\nls -l\n```\n'''\ncode_blocks = code_utils.extract_code(text_with_markdown)\nfor code_block in code_blocks:\n    if code_block[0] == \"shell\":\n        print(code_block[1])\n##output: ls -l        \n```\n\n\n## 注意事项\n\n1. prompt函数方法体返回只能是dict，实际的返回类型和方法签名可以不一样，但是方法体返回只能是dict。\n2. 大部分情况prompt函数体为空，如果一定要有方法体，可以返回一个空字典。\n3. 调用prompt方法的时候，如果在@byzerllm.prompt()里没有指定llm对象，那么需要在调用的时候通过with_llm方法指定llm对象。\n\n===========================\n\n# Byzer Storage: 最易用的AI存储引擎\n\nByzer Storage是一个为AI应用设计的高性能存储引擎,它提供了简单易用的API,支持向量搜索、全文检索以及结构化查询。本文将详细介绍Byzer Storage的使用方法和主要特性。\n\n## 0. 安装和启动\n\n```bash\npip install byzerllm\nbyzerllm storage start\n```\n\nThat's it! Byzer Storage已经安装并启动成功,现在我们可以开始使用它了。默认会启动一个 byzerai_store 的集群。\n\n注意，如果你是这样启动的：\n\n```bash\nbyzerllm storage start --enable_emb\n```\n\n那么会自动启动一个 emb 模型，名字就叫 emb， ByzerStorage 会自动使用该模型，无需做任何其他配置。\n\n## 1. 初始化\n\n创建一个 ByzerStorage 对象，链接 byzerai_store 集群，并且指定数据库和表名（可以不存在）。\n\n```python\nfrom byzerllm.apps.byzer_storage.simple_api import ByzerStorage, DataType, FieldOption, SortOption\n\nstorage = ByzerStorage(\"byzerai_store\", \"my_database1\", \"my_table4s\")\n```\n\n\n## 2. 创建库表（可选）\n\nByzer Storage使用Schema来定义数据结构。我们可以使用SchemaBuilder来构建Schema:\n\n```python\n_ = (\n    storage.schema_builder()\n    .add_field(\"_id\", DataType.STRING)\n    .add_field(\"name\", DataType.STRING)\n    .add_field(\"content\", DataType.STRING, [FieldOption.ANALYZE])\n    .add_field(\"raw_content\", DataType.STRING, [FieldOption.NO_INDEX])    \n    .add_array_field(\"summary\", DataType.FLOAT)    \n    .add_field(\"created_time\", DataType.LONG, [FieldOption.SORT])    \n    .execute()\n)\n```\n\n这个Schema定义了以下字段:\n- `_id`: 字符串类型的主键\n- `name`: 字符串类型,可用于过滤条件\n- `content`: 字符串类型,带有ANALYZE选项,用于全文搜索\n- `raw_content`: 字符串类型,带有NO_INDEX选项,不会被索引\n- `summary`: 浮点数组类型,用于存储向量\n- `created_time`: 长整型,带有SORT选项,可用于排序\n\n需要注意的是：\n\n1. 如果一个字段带有ANALYZE选项,则该字段会被分词,并且可以用于全文搜索，但是就无法返回原始的文本了。所以你需要添加一个新字段专门用来存储原文，比如在我们这个例子里，我们新增了 raw_content 字段，并且显示的指定了 NO_INDEX 选项，这样就不会被索引，也就不会被分词，可以后续被检索后用来获取原始文本。\n2. 对于需要作为向量检索和存储的字段，需要指定为数组类型，比如我们这个例子里的 summary 字段。\n3. 如果你需要拿到向量字段的原始文本，那么你也需要添加一个新字段专门用来存储原文，就像我们这个例子里的 raw_content 字段一样。\n\n\n## 3. 写入数据\n\n准备数据并使用WriteBuilder写入Storage:\n\n```python\ndata = [\n    {\"_id\": \"1\", \"name\": \"Hello\", \"content\": \"Hello, world!\", \"raw_content\": \"Hello, world!\", \"summary\": \"hello world\", \"created_time\": 1612137600},\n    {\"_id\": \"2\", \"name\": \"Byzer\", \"content\": \"Byzer, world!\", \"raw_content\": \"Byzer, world!\", \"summary\": \"byzer\", \"created_time\": 1612137601},\n    {\"_id\": \"3\", \"name\": \"AI\", \"content\": \"AI, world!\", \"raw_content\": \"AI, world!\", \"summary\": \"AI\", \"created_time\": 16121376002},\n    {\"_id\": \"4\", \"name\": \"ByzerAI\", \"content\": \"ByzerAI, world!\", \"raw_content\": \"ByzerAI, world!\", \"summary\": \"ByzerAi\", \"created_time\": 16121376003},\n]\n\nstorage.write_builder().add_items(data, vector_fields=[\"summary\"], search_fields=[\"content\"]).execute()\nstorage.commit()\n```\n\n这里我们使用`add_items`方法批量添加数据,并指定了`summary`为向量字段,`content`为搜索字段。最后调用`commit()`来确保数据被持久化。\n\n\n从上面我们要写入到 byzer storage 的数据我们可以看到如下几个特点：\n\n1. 需要包含我们之前定义的 Schema中罗列的所有的字段，同时需要指定哪些是向量字段，哪些是检索字段。\n2. 向量和检索不能是同一个字段。\n3. 对于向量，检索字段，我们给到 write_builder 的都是文本，ByzerStorage 会根据 Schema 的定义自动将其转换为向量，检索字段需要的格式。\n\n## 4. 查询数据\n\nByzer Storage支持多种查询方式,包括向量搜索、全文检索、过滤和排序。\n\n### 4.1 向量搜索 + 全文检索\n\n```python\nquery = storage.query_builder()\nquery.set_vector_query(\"ByzerAI\", fields=[\"summary\"])\nresults = query.set_search_query(\"Hello\", fields=[\"content\"]).execute()\nprint(results)\n```\n\n这个查询结合了向量搜索和全文检索,它会在`summary`字段中搜索与\"ByzerAI\"相似的向量,同时在`content`字段中搜索包含\"Hello\"的文档。\n\n### 4.2 过滤 + 向量搜索 + 全文检索\n\n```python\nquery = storage.query_builder()\nquery.and_filter().add_condition(\"name\", \"AI\").build()\nquery.set_vector_query(\"ByzerAI\", fields=\"summary\")\nresults = query.set_search_query(\"Hello\", fields=[\"content\"]).execute()\nprint(results)\n```\n\n这个查询首先过滤`name`字段等于\"AI\"的文档,然后在结果中进行向量搜索和全文检索。\n\n### 4.3 过滤 + 排序\n\n```python\nquery = storage.query_builder()\nquery.and_filter().add_condition(\"name\", \"AI\").build().sort(\"created_time\", SortOption.DESC)\nresults = query.execute()\nprint(results)\n```\n\n这个查询过滤`name`字段等于\"AI\"的文档,然后按`created_time`字段降序排序。\n\n## 5. 删除数据\n\n### 5.1 根据ID删除\n\n```python\nstorage.delete_by_ids([\"3\"])\n\nquery = storage.query_builder()\nquery.and_filter().add_condition(\"name\", \"AI\").build()\nresults = query.execute()\nprint(results)\n```\n\n这里我们删除了ID为\"3\"的文档,然后查询验证删除结果。\n\n### 5.2 删除整个表\n\n```python\nstorage.drop()\n```\n\n这个操作会删除整个表及其所有数据,请谨慎使用。\n\n## 结论\n\nByzer Storage提供了一套简洁而强大的API,能够轻松实现向量搜索、全文检索、结构化查询等功能。它的设计非常适合AI应用场景,可以有效地存储和检索各种类型的数据。通过本文的介绍,相信读者已经对Byzer Storage有了基本的了解,并能够开始在自己的项目中使用这个强大的存储引擎。\n\n\n##File: /Users/allwefantasy/projects/super_analysis/setup.py\nfrom setuptools import setup, find_packages\n\nsetup(\n    name=\"super-analysis\",\n    version=\"0.1.0\",\n    packages=find_packages(where=\"src\"),\n    package_dir={\"\": \"src\"},\n    install_requires=[\n        \"byzerllm\",\n        \"fastapi\",\n        \"uvicorn\",\n        \"argparse\",\n    ],\n    entry_points={\n        \"console_scripts\": [\n            \"super-analysis.serve=superanalysis.serve.cli:main\",\n        ],\n    },\n    author=\"allwefantasy\",\n    author_email=\"allwefantasy@gmail.com\",\n    description=\"A package for super analysis with ByzerLLM\",\n    long_description=open(\"README.md\").read(),\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/yourusername/super-analysis\",\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n    ],\n    python_requires='>=3.6',\n)\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/cli.py\nimport argparse\nfrom byzerllm.utils.client import ByzerLLM\nfrom superanalysis.serve.api_server import serve, ServerArgs\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"ByzerLLM API Server\")\n    parser.add_argument(\"--host\", type=str, default=\"0.0.0.0\", help=\"Host to bind the server to\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"Port to bind the server to\")\n    parser.add_argument(\"--uvicorn-log-level\", type=str, default=\"info\", help=\"Uvicorn log level\")\n    parser.add_argument(\"--allow-credentials\", action=\"store_true\", help=\"Allow credentials\")\n    parser.add_argument(\"--allowed-origins\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed origins\")\n    parser.add_argument(\"--allowed-methods\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed methods\")\n    parser.add_argument(\"--allowed-headers\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed headers\")\n    parser.add_argument(\"--api-key\", type=str, help=\"API key for authentication\")\n    parser.add_argument(\"--served-model-name\", type=str, required=True, help=\"Name of the model to be served\")\n    parser.add_argument(\"--prompt-template\", type=str, help=\"Prompt template\")\n    parser.add_argument(\"--response-role\", type=str, default=\"assistant\", help=\"Response role\")\n    parser.add_argument(\"--ssl-keyfile\", type=str, help=\"SSL key file\")\n    parser.add_argument(\"--ssl-certfile\", type=str, help=\"SSL certificate file\")\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile\n    )\n\n    llm = ByzerLLM()\n    llm.setup_default_model_name(args.served_model_name)\n\n    serve(llm, server_args)\n\nif __name__ == \"__main__\":\n    main()\n\n     \n\n下面是用户的需求：\n\n根据 @setup.py @cli.py 写一个README.md\n"}, {"role": "assistant", "content": "根据提供的 `setup.py` 和 `cli.py` 文件，我为您编写了一个 README.md 文件。这个 README 文件包含了项目的基本信息、安装说明、使用方法以及命令行参数的详细说明。以下是生成的 README.md 内容：\n\n```markdown\n# Super Analysis\n\nSuper Analysis 是一个基于 ByzerLLM 的强大分析工具包。它提供了一个易于使用的 API 服务器，可以快速部署和利用大型语言模型进行各种分析任务。\n\n## 安装\n\n您可以使用 pip 安装 Super Analysis：\n\n```bash\npip install super-analysis\n```\n\n## 使用方法\n\n安装完成后，您可以使用命令行工具 `super-analysis.serve` 来启动 API 服务器。以下是基本用法：\n\n```bash\nsuper-analysis.serve --served-model-name your_model_name\n```\n\n### 命令行参数\n\n以下是可用的命令行参数：\n\n- `--host`: 服务器绑定的主机地址（默认：0.0.0.0）\n- `--port`: 服务器绑定的端口（默认：8000）\n- `--uvicorn-log-level`: Uvicorn 日志级别（默认：info）\n- `--allow-credentials`: 允许凭证（默认：False）\n- `--allowed-origins`: 允许的源（默认：[\"*\"]）\n- `--allowed-methods`: 允许的 HTTP 方法（默认：[\"*\"]）\n- `--allowed-headers`: 允许的 HTTP 头（默认：[\"*\"]）\n- `--api-key`: 用于身份验证的 API 密钥\n- `--served-model-name`: 要服务的模型名称（必需）\n- `--prompt-template`: 提示模板\n- `--response-role`: 响应角色（默认：assistant）\n- `--ssl-keyfile`: SSL 密钥文件路径\n- `--ssl-certfile`: SSL 证书文件路径\n\n### 示例\n\n启动一个使用 \"gpt-3.5-turbo\" 模型的服务器，并指定自定义端口和 API 密钥：\n\n```bash\nsuper-analysis.serve --served-model-name gpt-3.5-turbo --port 8080 --api-key your_api_key\n```\n\n## 开发\n\n如果您想为 Super Analysis 项目做贡献或自定义功能，可以克隆仓库并在本地进行开发：\n\n```bash\ngit clone https://github.com/yourusername/super-analysis.git\ncd super-analysis\npip install -e .\n```\n\n## 许可证\n\n本项目采用 MIT 许可证。详情请参阅 [LICENSE](LICENSE) 文件。\n\n## 联系方式\n\n如有任何问题或建议，请联系作者：allwefantasy@gmail.com\n\n## 项目链接\n\nGitHub: https://github.com/yourusername/super-analysis\n```\n\n这个 README.md 文件提供了项目的基本信息、安装说明、使用方法以及详细的命令行参数说明。您可以根据实际需求进行调整和补充，例如添加更多示例、API 文档链接、贡献指南等。"}], "model": "human", "yaml_file": "actions/104_chat_action.yml"}
{"instruction": "根据 @setup.py @cli.py 写一个README.md", "conversations": [{"role": "user", "content": "\n如果你需要生成代码，对于每个需要更改的文件,你需要按 *SEARCH/REPLACE block* 的格式进行生成。\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The opening fence and code language, eg: ```python\n2. The file path alone on a line, starting with \"##File:\" and verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n*SEARCH/REPLACE* blocks will replace *all* matching occurrences.\nInclude enough lines to make the SEARCH blocks unique.\n\nInclude *ALL* the code being searched and replaced!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n下面我们来看一个例子：\n\n当前项目目录结构：\n1. 项目根目录： /tmp/projects/mathweb\n2. 项目子目录/文件列表(类似tree 命令输出)\nflask/\n    app.py\n    templates/\n        index.html\n    static/\n        style.css\n\n用户需求： Change get_factorial() to use math.factorial\n\n回答： To make this change we need to modify `/tmp/projects/mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n用户需求： Refactor hello() into its own file.\n\n回答：To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/hello.py\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/main.py\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n现在让我们开始一个新的任务:\n\n当前项目目录结构：\n1. 项目根目录： /Users/allwefantasy/projects/super_analysis\n2. 项目子目录/文件列表(类似tree 命令输出)：\nsetup.py\n.auto-coder/\n    libs/\n        llm_friendly_packages/\n            src/\n                example1/\n                    main.py\nsrc/\n    superanalysis/\n        serve/\n            openai_serve.py\n            llm_wrapper.py\n            api_server.py\n            cli.py\n下面是一些文件路径以及每个文件对应的源码：\n\n##File: .auto-coder/libs/llm_friendly_packages/github.com/allwefantasy/byzer-llm/README.md\n# byzerllm 大模型编程快速指南\n\n本文示例 [Notebook](../../notebooks/003_byzerllm_大模型编程快速指南.ipynb)\n\n## 安装\n\n```bash\npip install -U byzerllm\nray start --head\n```\n## 启动一个模型代理\n\nbyzerllm 支持私有化模型或者SaaS模型的部署。\n\n这里以 deepseek 官方API 为例：\n\n```bash\neasy-byzerllm deploy deepseek-chat --token xxxxx --alias deepseek_chat\n```\n\n或者跬基流动API:\n\n```bash\neasy-byzerllm deploy alibaba/Qwen1.5-110B-Chat --token xxxxx --alias qwen110b_chat\n```\n\n将上面的 API KEY 替换成你们自己的。\n\n如果你想部署私有化模型或者对接 Ollama 等更多需求，参考 [002_使用byzerllm进行模型部署.md](./002_使用byzerllm进行模型部署.md) 或者 [README.md](../../README.md)。\n\n之后，你就可以在代码里使用  deepseek_chat 或者 qwen110b_chat  访问模型了。\n\n## hello world\n\n来和我们的大模型打个招呼:\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\n@byzerllm.prompt(llm=llm)\ndef hello(q:str) ->str:\n    '''\n    你好, {{ q }}\n    '''\n\ns = hello(\"你是谁\")    \nprint(s)\n\n## 输出:\n## '你好！我是一个人工智能助手，专门设计来回答问题、提供信息和帮助解决问题。如果你有任何疑问或需要帮助，请随时告诉我。'\n```\n\n恭喜，你和大模型成功打了招呼！\n\n可以看到，我们通过 `@byzerllm.prompt` 装饰器，将一个方法转换成了一个大模型的调用，然后这个方法的主题是一段文本，文本中\n使用了 jinja2 模板语法，来获得方法的参数。当正常调用该方法时，实际上就发起了和大模型的交互，并且返回了大模型的结果。\n\n在 byzerllm 中，我们把这种方法称之为 prompt 函数。\n\n## 查看发送给大模型的prompt\n\n很多情况你可能需要调试，查看自己的 prompt 渲染后到底是什么样子的，这个时候你可以通过如下方式\n获取渲染后的prompt:\n\n```python\nhello.prompt(\"你是谁\")\n## '你好, 你是谁'\n```            \n\n## 动态换一个模型\n\n前面的 hello 方法在初始化的时候，我们使用了默认的模型 deepseek_chat，如果我们想换一个模型，可以这样做：\n\n```python\nhello.with_llm(llm).run(\"你是谁\")\n## '你好！我是一个人工智能助手，专门设计来回答问题、提供信息和帮助解决问题。如果你有任何疑问或需要帮助，请随时告诉我。'\n```\n\n通过 with_llm 你可以设置一个新的 llm 对象，然后调用 run 方法，就可以使用新的模型了。\n\n## 超长文本生成\n\n我们知道，大模型一次生成的长度其实是有限的，如果你想生成超长文本，你可能需手动的不断获得\n生成结果，然后把他转化为输入，然后再次生成，这样的方式是比较麻烦的。\n\nbyzerllm 提供了更加易用的 API :\n\n```python\nimport byzerllm\nfrom byzerllm import ByzerLLM\n\nllm = ByzerLLM.from_default_model(\"deepseek_chat\")\n\n@byzerllm.prompt()\ndef tell_story() -> str:\n    \"\"\"\n    讲一个100字的故事。    \n    \"\"\"\n\n\ns = (\n    tell_story.with_llm(llm)\n    .with_response_markers()\n    .options({\"llm_config\": {\"max_length\": 10}})\n    .run()\n)\nprint(s)\n\n## 从前，森林里住着一只聪明的小狐狸。一天，它发现了一块闪闪发光的宝石。小狐狸决定用这块宝石帮助森林里的动物们。它用宝石的光芒指引迷路的小鹿找到了回家的路，用宝石的温暖治愈了受伤的小鸟。从此，小狐狸成了森林里的英雄，动物们都感激它的善良和智慧。\n```\n\n实际核心部分就是这一行：\n\n```python\ntell_story.with_llm(llm)\n    .with_response_markers()    \n    .run()\n```\n\n我们只需要调用 `with_response_markers` 方法，系统就会自动的帮我们生成超长文本。\n在上面的案例中，我们通过\n\n```python\n.options({\"llm_config\": {\"max_length\": 10}})\n```\n\n认为的限制大模型一次交互最多只能输出10个字符，但是系统依然自动完成了远超过10个字符的文本生成。\n\n## 对象输出\n\n前面我们的例子都是返回字符串，但是我们也可以返回对象，这样我们就可以更加灵活的处理返回结果。\n\n```python\nimport pydantic \n\nclass Story(pydantic.BaseModel):\n    '''\n    故事\n    '''\n\n    title: str = pydantic.Field(description=\"故事的标题\")\n    body: str = pydantic.Field(description=\"故事主体\")\n\n@byzerllm.prompt()\ndef tell_story()->Story:\n    '''\n    讲一个100字的故事。    \n    '''\n\ns = tell_story.with_llm(llm).run()\nprint(isinstance(s, Story))\nprint(s.title)\n\n## True\n## 勇敢的小鸟\n```\n\n可以看到，我们很轻松的将输出转化为格式化输出。\n\n## 自定义字段抽取\n\n前面的结构化输出，其实会消耗更多token,还有一种更加精准的结构化输出方式。\n比如让大模型生成一个正则表达式，但实际上大模型很难准确只输出一个正则表达式，这个时候我们可以通过自定义抽取函数来获取我们想要的结果。\n\n\n```python\nfrom loguru import logger\nimport re\n\n@byzerllm.prompt()\ndef generate_regex_pattern(desc: str) -> str:\n    \"\"\"\n    根据下面的描述生成一个正则表达式，要符合python re.compile 库的要求。\n\n    {{ desc }}\n\n    最后生成的正则表达式要在<REGEX></REGEX>标签对里。\n    \"\"\"    \n\ndef extract_regex_pattern(regex_block: str) -> str:    \n    pattern = re.search(r\"<REGEX>(.*)</REGEX>\", regex_block, re.DOTALL)\n    if pattern is None:\n        logger.warning(\"No regex pattern found in the generated block:\\n {regex_block}\")\n        raise None\n    return pattern.group(1)\n\npattern = \"匹配一个邮箱地址\"\nv = generate_regex_pattern.with_llm(llm).with_extractor(extract_regex_pattern).run(desc=pattern)\nprint(v)\n## ^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\n```\n\n在上面的例子里，我们根据一句话生成一个正则表达式。我们通过 `with_extractor` 方法，传入了一个自定义的抽取函数，这个函数会在大模型生成结果后，对结果进行处理，然后返回我们想要的结果。\n\n我们在 prompt 明确说了，生成的结果要放到 `<REGEX></REGEX>` 标签对里，然后我们通过 extract_regex_pattern 函数，从结果中提取出了我们想要的正则表达式。\n\n## 在实例方法中使用大模型\n\n```python\nimport byzerllm\ndata = {\n    'name': 'Jane Doe',\n    'task_count': 3,\n    'tasks': [\n        {'name': 'Submit report', 'due_date': '2024-03-10'},\n        {'name': 'Finish project', 'due_date': '2024-03-15'},\n        {'name': 'Reply to emails', 'due_date': '2024-03-08'}\n    ]\n}\n\n\nclass RAG():\n    def __init__(self):        \n        self.llm = byzerllm.ByzerLLM()\n        self.llm.setup_template(model=\"deepseek_chat\",template=\"auto\")\n        self.llm.setup_default_model_name(\"deepseek_chat\")        \n    \n    @byzerllm.prompt(lambda self:self.llm)\n    def generate_answer(self,name,task_count,tasks)->str:\n        '''\n        Hello {{ name }},\n\n        This is a reminder that you have {{ task_count }} pending tasks:\n        {% for task in tasks %}\n        - Task: {{ task.name }} | Due: {{ task.due_date }}\n        {% endfor %}\n\n        Best regards,\n        Your Reminder System\n        '''        \n\nt = RAG()\n\nresponse = t.generate_answer(**data)\nprint(response)\n\n## 输出:\n## Hello Jane Doe,\n##I hope this message finds you well. I wanted to remind you of your 3 pending tasks to ensure you stay on track:\n## 1. **Submit report** - This task is due on **2024-03-10**. Please ensure that you allocat\n```\n\n这里我们给了个比较复杂的例子，但我们可以看到，给一个实例prompt方法和普通prompt 方法差异不大。\n唯一的区别是如果你希望在定义的时候就指定大模型，使用一个lambda函数返回实例的 llm 对象即可。\n\n```python\n@byzerllm.prompt(lambda self:self.llm)\n```\n\n你也可以不返回，在调用的时候通过 `with_llm` 方法指定 llm 对象。\n\n此外，这个例子也展示了如何通过jinja2模板语法，来处理复杂的结构化数据。\n\n## 通过 Python 代码处理复杂入参\n\n上面的一个例子中，我们通过 jinja2 模板语法，来处理复杂的结构化数据，但是有时候我们可能需要更加复杂的处理，这个时候我们可以通过 Python 代码来处理。\n\n```python\nimport byzerllm\n\ndata = {\n    'name': 'Jane Doe',\n    'task_count': 3,\n    'tasks': [\n        {'name': 'Submit report', 'due_date': '2024-03-10'},\n        {'name': 'Finish project', 'due_date': '2024-03-15'},\n        {'name': 'Reply to emails', 'due_date': '2024-03-08'}\n    ]\n}\n\n\nclass RAG():\n    def __init__(self):        \n        self.llm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n    \n    @byzerllm.prompt()\n    def generate_answer(self,name,task_count,tasks)->str:\n        '''\n        Hello {{ name }},\n\n        This is a reminder that you have {{ task_count }} pending tasks:\n            \n        {{ tasks }}\n\n        Best regards,\n        Your Reminder System\n        '''\n        \n        tasks_str = \"\\n\".join([f\"- Task: {task['name']} | Due: { task['due_date'] }\" for task in tasks])\n        return {\"tasks\": tasks_str}\n\nt = RAG()\n\nresponse = t.generate_answer.with_llm(t.llm).run(**data)\nprint(response)\n\n## Just a gentle nudge to keep you on track with your pending tasks. Here's a quick recap:....\n```\n\n在这个例子里，我们直接把 tasks 在方法体里进行处理，然后作为一个字符串返回，最够构建一个字典，字典的key为 tasks,然后\n你就可以在 docstring 里使用 `{{ tasks }}` 来引用这个字符串。\n\n这样对于很复杂的入参，就不用谢繁琐的 jinja2 模板语法了。\n\n## 如何自动实现一个方法\n\n比如我定义一个签名，但是我不想自己实现里面的逻辑，让大模型来实现。这个在 byzerllm 中叫 function impl。我们来看看怎么\n实现:\n\n```python\nimport pydantic\nclass Time(pydantic.BaseModel):\n    time: str = pydantic.Field(...,description=\"时间，时间格式为 yyyy-MM-dd\")\n\n\n@llm.impl()\ndef calculate_current_time()->Time:\n    '''\n    计算当前时间\n    '''\n    pass \n\n\ncalculate_current_time()\n#output: Time(time='2024-06-14')\n```\n\n在这个例子里，我们定义了一个 calculate_current_time 方法，但是我们没有实现里面的逻辑，我们通过 `@llm.impl()` 装饰器，让大模型来实现这个方法。\n为了避免每次都要“生成”这个方法，导致无法适用，我们提供了缓存，用户可以按如下方式打印速度：\n\n```python\nstart = time.monotonic()\ncalculate_current_time()\nprint(f\"first time cost: {time.monotonic()-start}\")\n\nstart = time.monotonic()\ncalculate_current_time()\nprint(f\"second time cost: {time.monotonic()-start}\")\n\n# output:\n# first time cost: 6.067266260739416\n# second time cost: 4.347506910562515e-05\n```\n可以看到，第一次执行花费了6s,第二次几乎是瞬间完成的，这是因为第一次执行的时候，我们实际上是在生成这个方法，第二次执行的时候，我们是执行已经生成好的代码，所以速度会非常快。你可以显示的调用 `llm.clear_impl_cache()` 清理掉函数缓存。\n\n## Stream 模式\n\n前面的例子都是一次性生成结果，但是有时候我们可能需要一个流式的输出，这个时候我们可能需要用底层一点的API来完成了：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\nv = llm.stream_chat_oai(model=\"deepseek_chat\",conversations=[{\n    \"role\":\"user\",\n    \"content\":\"你好，你是谁\",\n}],delta_mode=True)\n\nfor t in v:\n    print(t,flush=True)  \n\n# 你好\n# ！\n# 我\n# 是一个\n# 人工智能\n# 助手\n# ，\n# 旨在\n# 提供\n# 信息\n# 、\n# 解答\n# 问题....\n```\n\n如果你不想要流式输出，但是想用底层一点的API，你可以使用 `llm.chat_oai` 方法：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\nv = llm.chat_oai(model=\"deepseek_chat\",conversations=[{\n    \"role\":\"user\",\n    \"content\":\"你好，你是谁\",\n}])\n\nprint(v[0].output)\n## 你好！我是一个人工智能助手，旨在提供信息、解答问题和帮助用户解决问题。如果你有任何问题或需要帮助，请随时告诉我。\n```\n\n## Function Calling \n\nbyzerllm 可以不依赖模型自身就能提供 function calling 支持，我们来看个例子：\n\n\n```python\nfrom typing import List,Dict,Any,Annotated\nimport pydantic \nimport datetime\nfrom dateutil.relativedelta import relativedelta\n\ndef compute_date_range(count:Annotated[int,\"时间跨度，数值类型\"],\n                       unit:Annotated[str,\"时间单位，字符串类型\",{\"enum\":[\"day\",\"week\",\"month\",\"year\"]}])->List[str]:\n    '''\n    计算日期范围\n\n    Args:\n        count: 时间跨度，数值类型\n        unit: 时间单位，字符串类型，可选值为 day,week,month,year\n    '''        \n    now = datetime.datetime.now()\n    now_str = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n    if unit == \"day\":\n        return [(now - relativedelta(days=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"week\":\n        return [(now - relativedelta(weeks=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"month\":\n        return [(now - relativedelta(months=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"year\":\n        return [(now - relativedelta(years=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    return [\"\",\"\"]\n\ndef compute_now()->str:\n    '''\n    计算当前时间\n    '''\n    return datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n```\n\n我们定义了两个方法，一个是计算日期范围，一个是计算当前时间。\n\n现在我么可以来测试下，系统如何根据自然语言决定调用哪个方法：\n\n```python\nt = llm.chat_oai([{\n    \"content\":'''计算当前时间''',\n    \"role\":\"user\"    \n}],tools=[compute_date_range,compute_now],execute_tool=True)\n\nt[0].values\n\n## output: ['2024-06-14 15:18:02']\n```\n\n我们可以看到，他正确的选择了 compute_now 方法。\n\n接着我们再试一个：\n\n```python\nt = llm.chat_oai([{\n    \"content\":'''最近三个月趋势''',\n    \"role\":\"user\"    \n}],tools=[compute_date_range,compute_now],execute_tool=True)\n\nt[0].values\n\n## output: [['2024-03-14 15:19:13', '2024-06-14 15:19:13']]\n```\n\n模型正确的选择了 compute_date_range 方法。\n\n## 多模态\n\nbyerllm 也能很好的支持多模态的交互，而且统一了多模态大模型的接口，比如你可以用一样的方式使用 openai 或者 claude 的图片转文字能力， 或者一致的方式使用火山，azuer, openai的语音合成接口。\n\n### 图生文\n\n```python\nimport byzerllm\nfrom byzerllm.types import ImagePath\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\n\n\n@byzerllm.prompt()\ndef what_in_image(image_path: ImagePath) -> str:\n    \"\"\"\n    {{ image_path }}\n    这个图片里有什么？\n    \"\"\"    \n\n\nv = what_in_image.with_llm(vl_llm).run(\n    ImagePath(value=\"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\")\n)\nv\n## OUTPUT: 这张图片展示了多只可爱的猫咪，采用了艺术风格的绘画。猫咪们有不同的颜色和花纹，背景是浅棕色，上面还点缀着一些红色的花朵。整体画面给人一种温馨和谐的感觉\n```\n\n可以看到，我们只需要把 prompt 函数的图片地址入参使用 byzerllm.types.ImagePath里进行包装，就可以直接在 prompt 函数体里\n带上图片。\n\n或者你可以这样：\n\n```python\nimport byzerllm\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\n\n\n@byzerllm.prompt()\ndef what_in_image(image_path: str) -> str:\n    \"\"\"\n    {{ image }}\n    这个图片里有什么？\n    \"\"\"\n    return {\"image\": byzerllm.Image.load_image_from_path(image_path)}\n\n\nv = what_in_image.with_llm(vl_llm).run(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv\n```\n\n通过 `image_path` 参数，然后通过 `byzerllm.Image.load_image_from_path` 方法，转化为一个图片对象 image，最后在 prompt 函数体里\n使用 `{{ image }}` 引用这个图片对象。\n\n另外我们也是可以支持配置多张图片的。\n\n另外，我们也可以使用基础的 `llm.chat_oai` 方法来实现：\n\n```python\nimport byzerllm\nimport json\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\nimage = byzerllm.Image.load_image_from_path(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv = vl_llm.chat_oai(\n    conversations=[\n        {\n            \"role\": \"user\",\n            \"content\": json.dumps(\n                [{\"image\": image, \"text\": \"这个图片里有什么？\"}], ensure_ascii=False\n            ),\n        }\n    ]\n)\nv[0].output\n```\n\n还可以这么写：\n\n```python\nimport byzerllm\nimport json\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\nimage = byzerllm.Image.load_image_from_path(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv = vl_llm.chat_oai(\n    conversations=[\n        {\n            \"role\": \"user\",\n            \"content\": json.dumps(\n                [\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\"url\": image, \"detail\": \"high\"},\n                    },\n                    {\"text\": \"这个图片里有什么？\", \"type\": \"text\"},\n                ],\n                ensure_ascii=False,\n            ),\n        }\n    ]\n)\nv[0].output\n```\n\n### 语音合成\n\n这里以 openai 的 tts 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=tts-1 \\\n--model openai_tts\n```\n\n此外，byzerllm 支持 azure,火山引擎等 tts 语音合成引擎。\n\n接着你可以这么用：\n\n```python\nimport byzerllm\nimport base64\nimport json\n\nllm = byzerllm.ByzerLLM.from_default_model(\"openai_tts\")\n\n\nt = llm.chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\": json.dumps({\n        \"input\":\"hello, open_tts\",\n        \"voice\": \"alloy\",\n        \"response_format\": \"mp3\"\n    },ensure_ascii=False)\n}])\n\nwith open(\"voice.mp3\",\"wb\") as f:\n    f.write(base64.b64decode(t[0].output))\n```\n\ntts 模型生成没有prompt函数可以用，你需要直接使用 chat_oai。\n\n\n### 语音识别\n\n这里以 openai 的 whisper-1 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--worker_concurrency 10 \\\n--infer_params saas.model=whisper-1 saas.api_key=${MODEL_OPENAI_TOKEN} \\\n--model speech_to_text\n```\n\n语音识别的使用方式和图生文类似，我们可以直接在 prompt 函数体里带上音频文件。\n\n```python\nimport byzerllm\nimport json\nimport base64\nfrom byzerllm.types import AudioPath\n\nllm = byzerllm.ByzerLLM.from_default_model(\"speech_to_text\")\n\naudio_file = \"/Users/allwefantasy/videos/output_audio.mp3\"\n\n@byzerllm.prompt(llm=llm)\ndef audio_to_text(audio_file: AudioPath):\n    \"\"\"\n    {{ audio_file }}\n    \"\"\"\n\nv = audio_to_text(AudioPath(value=audio_file))\njson.loads(v)\n```\n输出的数据格式略微复杂：\n\n```\n{'text': 'In the last chapter, you and I started to step through the internal workings of a transformer. This is one of the key pieces of technology inside large language models, and a lot of other tools in the modern wave of AI.',\n 'task': 'transcribe',\n 'language': 'english',\n 'duration': 10.0,\n 'segments': [{'id': 0,\n   'seek': 0,\n   'start': 0.0,\n   'end': 4.78000020980835,\n   'text': ' In the last chapter, you and I started to step through the internal workings of a transformer.',\n   'tokens': [50364,\n    682,\n    264,\n    1036,\n    7187,\n    11,\n    291,\n    293,\n    286,\n    1409,\n    281,\n    1823,\n    807,\n    264,\n    6920,\n    589,\n    1109,\n    295,\n    257,\n    31782,\n    13,\n    50586],\n   'temperature': 0.0,\n   'avg_logprob': -0.28872039914131165,\n   'compression_ratio': 1.4220778942108154,\n   'no_speech_prob': 0.016033057123422623},\n  ....\n  {'id': 2,\n   'seek': 0,\n   'start': 8.579999923706055,\n   'end': 9.979999542236328,\n   'text': ' and a lot of other tools in the modern wave of AI.',\n   'tokens': [50759,\n    293,\n    257,\n    688,\n    295,\n    661,\n    3873,\n    294,\n    264,\n    4363,\n    5772,\n    295,\n    7318,\n    13,\n    50867],\n   'temperature': 0.0,\n   'avg_logprob': -0.28872039914131165,\n   'compression_ratio': 1.4220778942108154,\n   'no_speech_prob': 0.016033057123422623}],\n 'words': [{'word': 'In', 'start': 0.0, 'end': 0.18000000715255737},\n  {'word': 'the', 'start': 0.18000000715255737, 'end': 0.23999999463558197},\n  {'word': 'last', 'start': 0.23999999463558197, 'end': 0.5400000214576721},\n  {'word': 'chapter', 'start': 0.5400000214576721, 'end': 0.800000011920929},\n  ....\n  {'word': 'AI', 'start': 9.920000076293945, 'end': 9.979999542236328}]}\n```\n\n会输出每一句话以及每一个字所在的起始时间和截止时间。你可以根据需要来使用。\n\n\n### 文生图\n\n文生图和语音合成类似，首先要启动合适的模型,以openai 的 dall-e-3 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=dall-e-3 \\\n--model openai_image_gen\n```\n\n启动模型后，只需要记住几个模板参数即可使用，这里直接使用 chat_oai 方法来使用：\n\n```python\n\nimport byzerllm\nimport json\nimport base64\n\nllm = byzerllm.ByzerLLM.from_default_model(\"openai_image_gen\")\nt = llm.chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\": json.dumps({\n        \"input\":\"a white siamese cat\",\n        \"size\": \"1024x1024\",\n        \"quality\": \"standard\"\n    },ensure_ascii=False)\n}])\n\nwith open(\"output1.jpg\",\"wb\") as f:\n    f.write(base64.b64decode(t[0].output))\n\n\nimport matplotlib.pyplot as plt\n\nimage_path = \"output1.jpg\"\nimage = plt.imread(image_path)\n\nplt.imshow(image)\nplt.axis('off')\nplt.show()\n```\n\n## Prompt 函数的流式输出\n\nbyzerllm 底层支持流式输出，非 prompt 函数的用法是这样的：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\n\nv = llm.stream_chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\":\"讲一个100字的故事\"\n}])\n\nfor s  in v:\n    print(s[0], end=\"\")\n```\n\n如果你像用 prompt 函数，可以这么用：\n\n\n```python\nimport byzerllm\nimport json\nimport base64\nfrom typing import Generator\n\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\n\n@byzerllm.prompt()\ndef tell_story() -> Generator[str, None, None]:\n    '''\n    给我讲一个一百多字的故事\n    '''\n\nv = tell_story.with_llm(llm).run()    \nfor i in v:\n    print(i, end=\"\")\n```\n\n可以看到，和普通的 prompt 函数的区别在于，返回值是一个生成器，然后你可以通过 for 循环来获取结果。\n\n## 向量化模型\n\nbyzerllm 支持向量化模型,你可以这样启动一个本地的模型：\n\n```bash\n!byzerllm deploy --pretrained_model_type custom/bge \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--worker_concurrency 10 \\\n--model_path /home/winubuntu/.auto-coder/storage/models/AI-ModelScope/bge-large-zh \\\n--infer_backend transformers \\\n--num_workers 1 \\\n--model emb\n```\n\n注意两个参数:\n\n1. --infer_backend transformers: 表示使用 transformers 作为推理后端。\n2. --model_path: 表示模型的路径。\n\n也可以启动一个 SaaS 的emb模型,比如 qwen 的 emb 模型：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/qianwen \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 2 \\\n--infer_params saas.api_key=${MODEL_QIANWEN_TOKEN}  saas.model=text-embedding-v2 \\\n--model qianwen_emb\n```\n\n或者 openai 的 emb 模型：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--worker_concurrency 10 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=text-embedding-3-small \\\n--model gpt_emb\n```\nSaaS 模型无需配置 `--infer_backend` 参数。\n\n无论是本地模型还是 SaaS 模型，我们都可以这样使用：\n\n```python\nimport byzerllm\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\nllm.setup_default_emb_model_name(\"emb\")\nllm.emb_query(\"你好\")\n```\n\n如果你配置 byzerllm 中的 Storage 使用，比如你这样启动了存储：\n\n```bash\nbyzerllm storage start --enable_emb\n```\n\n那么需要这么使用：\n\n```python\nfrom byzerllm.apps.byzer_storage.simple_api import ByzerStorage, DataType, FieldOption,SortOption\nstorage = ByzerStorage(\"byzerai_store\", \"memory\", \"memory\")\nstorage.emb(\"你好\")\n```\n\n## 一些辅助工具\n\n当调用 prompt 函数返回字符串的时候，如果想从里面抽取代码，可以使用如下方式：\n\n```python\nfrom byzerllm.utils.client import code_utils\ntext_with_markdown = '''\n```shell\nls -l\n```\n'''\ncode_blocks = code_utils.extract_code(text_with_markdown)\nfor code_block in code_blocks:\n    if code_block[0] == \"shell\":\n        print(code_block[1])\n##output: ls -l        \n```\n\n\n## 注意事项\n\n1. prompt函数方法体返回只能是dict，实际的返回类型和方法签名可以不一样，但是方法体返回只能是dict。\n2. 大部分情况prompt函数体为空，如果一定要有方法体，可以返回一个空字典。\n3. 调用prompt方法的时候，如果在@byzerllm.prompt()里没有指定llm对象，那么需要在调用的时候通过with_llm方法指定llm对象。\n\n===========================\n\n# Byzer Storage: 最易用的AI存储引擎\n\nByzer Storage是一个为AI应用设计的高性能存储引擎,它提供了简单易用的API,支持向量搜索、全文检索以及结构化查询。本文将详细介绍Byzer Storage的使用方法和主要特性。\n\n## 0. 安装和启动\n\n```bash\npip install byzerllm\nbyzerllm storage start\n```\n\nThat's it! Byzer Storage已经安装并启动成功,现在我们可以开始使用它了。默认会启动一个 byzerai_store 的集群。\n\n注意，如果你是这样启动的：\n\n```bash\nbyzerllm storage start --enable_emb\n```\n\n那么会自动启动一个 emb 模型，名字就叫 emb， ByzerStorage 会自动使用该模型，无需做任何其他配置。\n\n## 1. 初始化\n\n创建一个 ByzerStorage 对象，链接 byzerai_store 集群，并且指定数据库和表名（可以不存在）。\n\n```python\nfrom byzerllm.apps.byzer_storage.simple_api import ByzerStorage, DataType, FieldOption, SortOption\n\nstorage = ByzerStorage(\"byzerai_store\", \"my_database1\", \"my_table4s\")\n```\n\n\n## 2. 创建库表（可选）\n\nByzer Storage使用Schema来定义数据结构。我们可以使用SchemaBuilder来构建Schema:\n\n```python\n_ = (\n    storage.schema_builder()\n    .add_field(\"_id\", DataType.STRING)\n    .add_field(\"name\", DataType.STRING)\n    .add_field(\"content\", DataType.STRING, [FieldOption.ANALYZE])\n    .add_field(\"raw_content\", DataType.STRING, [FieldOption.NO_INDEX])    \n    .add_array_field(\"summary\", DataType.FLOAT)    \n    .add_field(\"created_time\", DataType.LONG, [FieldOption.SORT])    \n    .execute()\n)\n```\n\n这个Schema定义了以下字段:\n- `_id`: 字符串类型的主键\n- `name`: 字符串类型,可用于过滤条件\n- `content`: 字符串类型,带有ANALYZE选项,用于全文搜索\n- `raw_content`: 字符串类型,带有NO_INDEX选项,不会被索引\n- `summary`: 浮点数组类型,用于存储向量\n- `created_time`: 长整型,带有SORT选项,可用于排序\n\n需要注意的是：\n\n1. 如果一个字段带有ANALYZE选项,则该字段会被分词,并且可以用于全文搜索，但是就无法返回原始的文本了。所以你需要添加一个新字段专门用来存储原文，比如在我们这个例子里，我们新增了 raw_content 字段，并且显示的指定了 NO_INDEX 选项，这样就不会被索引，也就不会被分词，可以后续被检索后用来获取原始文本。\n2. 对于需要作为向量检索和存储的字段，需要指定为数组类型，比如我们这个例子里的 summary 字段。\n3. 如果你需要拿到向量字段的原始文本，那么你也需要添加一个新字段专门用来存储原文，就像我们这个例子里的 raw_content 字段一样。\n\n\n## 3. 写入数据\n\n准备数据并使用WriteBuilder写入Storage:\n\n```python\ndata = [\n    {\"_id\": \"1\", \"name\": \"Hello\", \"content\": \"Hello, world!\", \"raw_content\": \"Hello, world!\", \"summary\": \"hello world\", \"created_time\": 1612137600},\n    {\"_id\": \"2\", \"name\": \"Byzer\", \"content\": \"Byzer, world!\", \"raw_content\": \"Byzer, world!\", \"summary\": \"byzer\", \"created_time\": 1612137601},\n    {\"_id\": \"3\", \"name\": \"AI\", \"content\": \"AI, world!\", \"raw_content\": \"AI, world!\", \"summary\": \"AI\", \"created_time\": 16121376002},\n    {\"_id\": \"4\", \"name\": \"ByzerAI\", \"content\": \"ByzerAI, world!\", \"raw_content\": \"ByzerAI, world!\", \"summary\": \"ByzerAi\", \"created_time\": 16121376003},\n]\n\nstorage.write_builder().add_items(data, vector_fields=[\"summary\"], search_fields=[\"content\"]).execute()\nstorage.commit()\n```\n\n这里我们使用`add_items`方法批量添加数据,并指定了`summary`为向量字段,`content`为搜索字段。最后调用`commit()`来确保数据被持久化。\n\n\n从上面我们要写入到 byzer storage 的数据我们可以看到如下几个特点：\n\n1. 需要包含我们之前定义的 Schema中罗列的所有的字段，同时需要指定哪些是向量字段，哪些是检索字段。\n2. 向量和检索不能是同一个字段。\n3. 对于向量，检索字段，我们给到 write_builder 的都是文本，ByzerStorage 会根据 Schema 的定义自动将其转换为向量，检索字段需要的格式。\n\n## 4. 查询数据\n\nByzer Storage支持多种查询方式,包括向量搜索、全文检索、过滤和排序。\n\n### 4.1 向量搜索 + 全文检索\n\n```python\nquery = storage.query_builder()\nquery.set_vector_query(\"ByzerAI\", fields=[\"summary\"])\nresults = query.set_search_query(\"Hello\", fields=[\"content\"]).execute()\nprint(results)\n```\n\n这个查询结合了向量搜索和全文检索,它会在`summary`字段中搜索与\"ByzerAI\"相似的向量,同时在`content`字段中搜索包含\"Hello\"的文档。\n\n### 4.2 过滤 + 向量搜索 + 全文检索\n\n```python\nquery = storage.query_builder()\nquery.and_filter().add_condition(\"name\", \"AI\").build()\nquery.set_vector_query(\"ByzerAI\", fields=\"summary\")\nresults = query.set_search_query(\"Hello\", fields=[\"content\"]).execute()\nprint(results)\n```\n\n这个查询首先过滤`name`字段等于\"AI\"的文档,然后在结果中进行向量搜索和全文检索。\n\n### 4.3 过滤 + 排序\n\n```python\nquery = storage.query_builder()\nquery.and_filter().add_condition(\"name\", \"AI\").build().sort(\"created_time\", SortOption.DESC)\nresults = query.execute()\nprint(results)\n```\n\n这个查询过滤`name`字段等于\"AI\"的文档,然后按`created_time`字段降序排序。\n\n## 5. 删除数据\n\n### 5.1 根据ID删除\n\n```python\nstorage.delete_by_ids([\"3\"])\n\nquery = storage.query_builder()\nquery.and_filter().add_condition(\"name\", \"AI\").build()\nresults = query.execute()\nprint(results)\n```\n\n这里我们删除了ID为\"3\"的文档,然后查询验证删除结果。\n\n### 5.2 删除整个表\n\n```python\nstorage.drop()\n```\n\n这个操作会删除整个表及其所有数据,请谨慎使用。\n\n## 结论\n\nByzer Storage提供了一套简洁而强大的API,能够轻松实现向量搜索、全文检索、结构化查询等功能。它的设计非常适合AI应用场景,可以有效地存储和检索各种类型的数据。通过本文的介绍,相信读者已经对Byzer Storage有了基本的了解,并能够开始在自己的项目中使用这个强大的存储引擎。\n\n\n##File: /Users/allwefantasy/projects/super_analysis/setup.py\nfrom setuptools import setup, find_packages\n\nsetup(\n    name=\"super-analysis\",\n    version=\"0.1.0\",\n    packages=find_packages(where=\"src\"),\n    package_dir={\"\": \"src\"},\n    install_requires=[\n        \"byzerllm\",\n        \"fastapi\",\n        \"uvicorn\",\n        \"argparse\",\n    ],\n    entry_points={\n        \"console_scripts\": [\n            \"super-analysis.serve=superanalysis.serve.cli:main\",\n        ],\n    },\n    author=\"allwefantasy\",\n    author_email=\"allwefantasy@gmail.com\",\n    description=\"A package for super analysis with ByzerLLM\",\n    long_description=open(\"README.md\").read(),\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/yourusername/super-analysis\",\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n    ],\n    python_requires='>=3.6',\n)\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/cli.py\nimport argparse\nfrom byzerllm.utils.client import ByzerLLM\nfrom superanalysis.serve.api_server import serve, ServerArgs\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"ByzerLLM API Server\")\n    parser.add_argument(\"--host\", type=str, default=\"0.0.0.0\", help=\"Host to bind the server to\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"Port to bind the server to\")\n    parser.add_argument(\"--uvicorn-log-level\", type=str, default=\"info\", help=\"Uvicorn log level\")\n    parser.add_argument(\"--allow-credentials\", action=\"store_true\", help=\"Allow credentials\")\n    parser.add_argument(\"--allowed-origins\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed origins\")\n    parser.add_argument(\"--allowed-methods\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed methods\")\n    parser.add_argument(\"--allowed-headers\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed headers\")\n    parser.add_argument(\"--api-key\", type=str, help=\"API key for authentication\")\n    parser.add_argument(\"--served-model-name\", type=str, required=True, help=\"Name of the model to be served\")\n    parser.add_argument(\"--prompt-template\", type=str, help=\"Prompt template\")\n    parser.add_argument(\"--response-role\", type=str, default=\"assistant\", help=\"Response role\")\n    parser.add_argument(\"--ssl-keyfile\", type=str, help=\"SSL key file\")\n    parser.add_argument(\"--ssl-certfile\", type=str, help=\"SSL certificate file\")\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile\n    )\n\n    llm = ByzerLLM()\n    llm.setup_default_model_name(args.served_model_name)\n\n    serve(llm, server_args)\n\nif __name__ == \"__main__\":\n    main()\n\n     \n\n下面是用户的需求：\n\n根据 @setup.py @cli.py 写一个README.md\n"}, {"role": "assistant", "content": "好的，我会使用 SEARCH/REPLACE 格式来创建 README.md 文件。这里是相应的 SEARCH/REPLACE 块：\n\n```markdown\n##File: /Users/allwefantasy/projects/super_analysis/README.md\n<<<<<<< SEARCH\n=======\n# Super Analysis\n\nSuper Analysis 是一个基于 ByzerLLM 的强大分析包，提供了便捷的 API 服务器功能。\n\n## 安装\n\n你可以使用 pip 安装 Super Analysis：\n\n```bash\npip install super-analysis\n```\n\n## 特性\n\n- 基于 ByzerLLM 的高级分析功能\n- 易于使用的 API 服务器\n- 灵活的配置选项\n\n## 使用方法\n\n安装完成后，你可以使用 `super-analysis.serve` 命令来启动 API 服务器：\n\n```bash\nsuper-analysis.serve --served-model-name your_model_name\n```\n\n### 命令行参数\n\n以下是可用的命令行参数：\n\n- `--host`: 服务器绑定的主机地址（默认：0.0.0.0）\n- `--port`: 服务器绑定的端口（默认：8000）\n- `--uvicorn-log-level`: Uvicorn 日志级别（默认：info）\n- `--allow-credentials`: 允许凭证（默认：False）\n- `--allowed-origins`: 允许的源（默认：[\"*\"]）\n- `--allowed-methods`: 允许的方法（默认：[\"*\"]）\n- `--allowed-headers`: 允许的头部（默认：[\"*\"]）\n- `--api-key`: 用于认证的 API 密钥\n- `--served-model-name`: 要服务的模型名称（必需）\n- `--prompt-template`: 提示模板\n- `--response-role`: 响应角色（默认：assistant）\n- `--ssl-keyfile`: SSL 密钥文件路径\n- `--ssl-certfile`: SSL 证书文件路径\n\n例如：\n\n```bash\nsuper-analysis.serve --served-model-name gpt3 --port 8080 --api-key your_api_key\n```\n\n## 开发\n\n如果你想参与开发，可以克隆这个仓库并安装依赖：\n\n```bash\ngit clone https://github.com/yourusername/super-analysis.git\ncd super-analysis\npip install -e .\n```\n\n## 许可证\n\n本项目采用 MIT 许可证。详见 [LICENSE](LICENSE) 文件。\n\n## 联系方式\n\n如有任何问题或建议，请联系 allwefantasy@gmail.com。\n>>>>>>> REPLACE\n```\n\n这个 SEARCH/REPLACE 块将创建一个新的 README.md 文件，包含了项目的完整描述、安装说明、使用方法、命令行参数说明以及其他重要信息。由于原始文件不存在或为空，SEARCH 部分是空的，而 REPLACE 部分包含了整个 README 的内容。"}], "model": "human", "yaml_file": "actions/105_chat_action.yml"}
{"instruction": "在sql目录下新增一个 types 目录,将 rag_base_url 和 rag_api_key 组装成一个pyndatic 类,然后引入到 SQLGenerator里", "conversations": [{"role": "user", "content": "\n如果你需要生成代码，对于每个需要更改的文件,你需要按 *SEARCH/REPLACE block* 的格式进行生成。\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The opening fence and code language, eg: ```python\n2. The file path alone on a line, starting with \"##File:\" and verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n*SEARCH/REPLACE* blocks will replace *all* matching occurrences.\nInclude enough lines to make the SEARCH blocks unique.\n\nInclude *ALL* the code being searched and replaced!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n下面我们来看一个例子：\n\n当前项目目录结构：\n1. 项目根目录： /tmp/projects/mathweb\n2. 项目子目录/文件列表(类似tree 命令输出)\nflask/\n    app.py\n    templates/\n        index.html\n    static/\n        style.css\n\n用户需求： Change get_factorial() to use math.factorial\n\n回答： To make this change we need to modify `/tmp/projects/mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n用户需求： Refactor hello() into its own file.\n\n回答：To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/hello.py\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/main.py\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n现在让我们开始一个新的任务:\n\n当前项目目录结构：\n1. 项目根目录： /Users/allwefantasy/projects/super_analysis\n2. 项目子目录/文件列表(类似tree 命令输出)：\nsetup.py\n.auto-coder/\n    libs/\n        llm_friendly_packages/\n            src/\n                example1/\n                    main.py\nsrc/\n    superanalysis/\n        serve/\n            openai_serve.py\n            __init__.py\n            llm_wrapper.py\n            api_server.py\n            cli.py\n        sql/\n            __init__.py\n            generator.py\n下面是一些文件路径以及每个文件对应的源码：\n\n##File: .auto-coder/libs/llm_friendly_packages/github.com/allwefantasy/byzer-llm/README.md\n# byzerllm 大模型编程快速指南\n\n本文示例 [Notebook](../../notebooks/003_byzerllm_大模型编程快速指南.ipynb)\n\n## 安装\n\n```bash\npip install -U byzerllm\nray start --head\n```\n## 启动一个模型代理\n\nbyzerllm 支持私有化模型或者SaaS模型的部署。\n\n这里以 deepseek 官方API 为例：\n\n```bash\neasy-byzerllm deploy deepseek-chat --token xxxxx --alias deepseek_chat\n```\n\n或者跬基流动API:\n\n```bash\neasy-byzerllm deploy alibaba/Qwen1.5-110B-Chat --token xxxxx --alias qwen110b_chat\n```\n\n将上面的 API KEY 替换成你们自己的。\n\n如果你想部署私有化模型或者对接 Ollama 等更多需求，参考 [002_使用byzerllm进行模型部署.md](./002_使用byzerllm进行模型部署.md) 或者 [README.md](../../README.md)。\n\n之后，你就可以在代码里使用  deepseek_chat 或者 qwen110b_chat  访问模型了。\n\n## hello world\n\n来和我们的大模型打个招呼:\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\n@byzerllm.prompt(llm=llm)\ndef hello(q:str) ->str:\n    '''\n    你好, {{ q }}\n    '''\n\ns = hello(\"你是谁\")    \nprint(s)\n\n## 输出:\n## '你好！我是一个人工智能助手，专门设计来回答问题、提供信息和帮助解决问题。如果你有任何疑问或需要帮助，请随时告诉我。'\n```\n\n恭喜，你和大模型成功打了招呼！\n\n可以看到，我们通过 `@byzerllm.prompt` 装饰器，将一个方法转换成了一个大模型的调用，然后这个方法的主题是一段文本，文本中\n使用了 jinja2 模板语法，来获得方法的参数。当正常调用该方法时，实际上就发起了和大模型的交互，并且返回了大模型的结果。\n\n在 byzerllm 中，我们把这种方法称之为 prompt 函数。\n\n## 查看发送给大模型的prompt\n\n很多情况你可能需要调试，查看自己的 prompt 渲染后到底是什么样子的，这个时候你可以通过如下方式\n获取渲染后的prompt:\n\n```python\nhello.prompt(\"你是谁\")\n## '你好, 你是谁'\n```            \n\n## 动态换一个模型\n\n前面的 hello 方法在初始化的时候，我们使用了默认的模型 deepseek_chat，如果我们想换一个模型，可以这样做：\n\n```python\nhello.with_llm(llm).run(\"你是谁\")\n## '你好！我是一个人工智能助手，专门设计来回答问题、提供信息和帮助解决问题。如果你有任何疑问或需要帮助，请随时告诉我。'\n```\n\n通过 with_llm 你可以设置一个新的 llm 对象，然后调用 run 方法，就可以使用新的模型了。\n\n## 超长文本生成\n\n我们知道，大模型一次生成的长度其实是有限的，如果你想生成超长文本，你可能需手动的不断获得\n生成结果，然后把他转化为输入，然后再次生成，这样的方式是比较麻烦的。\n\nbyzerllm 提供了更加易用的 API :\n\n```python\nimport byzerllm\nfrom byzerllm import ByzerLLM\n\nllm = ByzerLLM.from_default_model(\"deepseek_chat\")\n\n@byzerllm.prompt()\ndef tell_story() -> str:\n    \"\"\"\n    讲一个100字的故事。    \n    \"\"\"\n\n\ns = (\n    tell_story.with_llm(llm)\n    .with_response_markers()\n    .options({\"llm_config\": {\"max_length\": 10}})\n    .run()\n)\nprint(s)\n\n## 从前，森林里住着一只聪明的小狐狸。一天，它发现了一块闪闪发光的宝石。小狐狸决定用这块宝石帮助森林里的动物们。它用宝石的光芒指引迷路的小鹿找到了回家的路，用宝石的温暖治愈了受伤的小鸟。从此，小狐狸成了森林里的英雄，动物们都感激它的善良和智慧。\n```\n\n实际核心部分就是这一行：\n\n```python\ntell_story.with_llm(llm)\n    .with_response_markers()    \n    .run()\n```\n\n我们只需要调用 `with_response_markers` 方法，系统就会自动的帮我们生成超长文本。\n在上面的案例中，我们通过\n\n```python\n.options({\"llm_config\": {\"max_length\": 10}})\n```\n\n认为的限制大模型一次交互最多只能输出10个字符，但是系统依然自动完成了远超过10个字符的文本生成。\n\n## 对象输出\n\n前面我们的例子都是返回字符串，但是我们也可以返回对象，这样我们就可以更加灵活的处理返回结果。\n\n```python\nimport pydantic \n\nclass Story(pydantic.BaseModel):\n    '''\n    故事\n    '''\n\n    title: str = pydantic.Field(description=\"故事的标题\")\n    body: str = pydantic.Field(description=\"故事主体\")\n\n@byzerllm.prompt()\ndef tell_story()->Story:\n    '''\n    讲一个100字的故事。    \n    '''\n\ns = tell_story.with_llm(llm).run()\nprint(isinstance(s, Story))\nprint(s.title)\n\n## True\n## 勇敢的小鸟\n```\n\n可以看到，我们很轻松的将输出转化为格式化输出。\n\n## 自定义字段抽取\n\n前面的结构化输出，其实会消耗更多token,还有一种更加精准的结构化输出方式。\n比如让大模型生成一个正则表达式，但实际上大模型很难准确只输出一个正则表达式，这个时候我们可以通过自定义抽取函数来获取我们想要的结果。\n\n\n```python\nfrom loguru import logger\nimport re\n\n@byzerllm.prompt()\ndef generate_regex_pattern(desc: str) -> str:\n    \"\"\"\n    根据下面的描述生成一个正则表达式，要符合python re.compile 库的要求。\n\n    {{ desc }}\n\n    最后生成的正则表达式要在<REGEX></REGEX>标签对里。\n    \"\"\"    \n\ndef extract_regex_pattern(regex_block: str) -> str:    \n    pattern = re.search(r\"<REGEX>(.*)</REGEX>\", regex_block, re.DOTALL)\n    if pattern is None:\n        logger.warning(\"No regex pattern found in the generated block:\\n {regex_block}\")\n        raise None\n    return pattern.group(1)\n\npattern = \"匹配一个邮箱地址\"\nv = generate_regex_pattern.with_llm(llm).with_extractor(extract_regex_pattern).run(desc=pattern)\nprint(v)\n## ^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\n```\n\n在上面的例子里，我们根据一句话生成一个正则表达式。我们通过 `with_extractor` 方法，传入了一个自定义的抽取函数，这个函数会在大模型生成结果后，对结果进行处理，然后返回我们想要的结果。\n\n我们在 prompt 明确说了，生成的结果要放到 `<REGEX></REGEX>` 标签对里，然后我们通过 extract_regex_pattern 函数，从结果中提取出了我们想要的正则表达式。\n\n## 在实例方法中使用大模型\n\n```python\nimport byzerllm\ndata = {\n    'name': 'Jane Doe',\n    'task_count': 3,\n    'tasks': [\n        {'name': 'Submit report', 'due_date': '2024-03-10'},\n        {'name': 'Finish project', 'due_date': '2024-03-15'},\n        {'name': 'Reply to emails', 'due_date': '2024-03-08'}\n    ]\n}\n\n\nclass RAG():\n    def __init__(self):        \n        self.llm = byzerllm.ByzerLLM()\n        self.llm.setup_template(model=\"deepseek_chat\",template=\"auto\")\n        self.llm.setup_default_model_name(\"deepseek_chat\")        \n    \n    @byzerllm.prompt(lambda self:self.llm)\n    def generate_answer(self,name,task_count,tasks)->str:\n        '''\n        Hello {{ name }},\n\n        This is a reminder that you have {{ task_count }} pending tasks:\n        {% for task in tasks %}\n        - Task: {{ task.name }} | Due: {{ task.due_date }}\n        {% endfor %}\n\n        Best regards,\n        Your Reminder System\n        '''        \n\nt = RAG()\n\nresponse = t.generate_answer(**data)\nprint(response)\n\n## 输出:\n## Hello Jane Doe,\n##I hope this message finds you well. I wanted to remind you of your 3 pending tasks to ensure you stay on track:\n## 1. **Submit report** - This task is due on **2024-03-10**. Please ensure that you allocat\n```\n\n这里我们给了个比较复杂的例子，但我们可以看到，给一个实例prompt方法和普通prompt 方法差异不大。\n唯一的区别是如果你希望在定义的时候就指定大模型，使用一个lambda函数返回实例的 llm 对象即可。\n\n```python\n@byzerllm.prompt(lambda self:self.llm)\n```\n\n你也可以不返回，在调用的时候通过 `with_llm` 方法指定 llm 对象。\n\n此外，这个例子也展示了如何通过jinja2模板语法，来处理复杂的结构化数据。\n\n## 通过 Python 代码处理复杂入参\n\n上面的一个例子中，我们通过 jinja2 模板语法，来处理复杂的结构化数据，但是有时候我们可能需要更加复杂的处理，这个时候我们可以通过 Python 代码来处理。\n\n```python\nimport byzerllm\n\ndata = {\n    'name': 'Jane Doe',\n    'task_count': 3,\n    'tasks': [\n        {'name': 'Submit report', 'due_date': '2024-03-10'},\n        {'name': 'Finish project', 'due_date': '2024-03-15'},\n        {'name': 'Reply to emails', 'due_date': '2024-03-08'}\n    ]\n}\n\n\nclass RAG():\n    def __init__(self):        \n        self.llm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n    \n    @byzerllm.prompt()\n    def generate_answer(self,name,task_count,tasks)->str:\n        '''\n        Hello {{ name }},\n\n        This is a reminder that you have {{ task_count }} pending tasks:\n            \n        {{ tasks }}\n\n        Best regards,\n        Your Reminder System\n        '''\n        \n        tasks_str = \"\\n\".join([f\"- Task: {task['name']} | Due: { task['due_date'] }\" for task in tasks])\n        return {\"tasks\": tasks_str}\n\nt = RAG()\n\nresponse = t.generate_answer.with_llm(t.llm).run(**data)\nprint(response)\n\n## Just a gentle nudge to keep you on track with your pending tasks. Here's a quick recap:....\n```\n\n在这个例子里，我们直接把 tasks 在方法体里进行处理，然后作为一个字符串返回，最够构建一个字典，字典的key为 tasks,然后\n你就可以在 docstring 里使用 `{{ tasks }}` 来引用这个字符串。\n\n这样对于很复杂的入参，就不用谢繁琐的 jinja2 模板语法了。\n\n## 如何自动实现一个方法\n\n比如我定义一个签名，但是我不想自己实现里面的逻辑，让大模型来实现。这个在 byzerllm 中叫 function impl。我们来看看怎么\n实现:\n\n```python\nimport pydantic\nclass Time(pydantic.BaseModel):\n    time: str = pydantic.Field(...,description=\"时间，时间格式为 yyyy-MM-dd\")\n\n\n@llm.impl()\ndef calculate_current_time()->Time:\n    '''\n    计算当前时间\n    '''\n    pass \n\n\ncalculate_current_time()\n#output: Time(time='2024-06-14')\n```\n\n在这个例子里，我们定义了一个 calculate_current_time 方法，但是我们没有实现里面的逻辑，我们通过 `@llm.impl()` 装饰器，让大模型来实现这个方法。\n为了避免每次都要“生成”这个方法，导致无法适用，我们提供了缓存，用户可以按如下方式打印速度：\n\n```python\nstart = time.monotonic()\ncalculate_current_time()\nprint(f\"first time cost: {time.monotonic()-start}\")\n\nstart = time.monotonic()\ncalculate_current_time()\nprint(f\"second time cost: {time.monotonic()-start}\")\n\n# output:\n# first time cost: 6.067266260739416\n# second time cost: 4.347506910562515e-05\n```\n可以看到，第一次执行花费了6s,第二次几乎是瞬间完成的，这是因为第一次执行的时候，我们实际上是在生成这个方法，第二次执行的时候，我们是执行已经生成好的代码，所以速度会非常快。你可以显示的调用 `llm.clear_impl_cache()` 清理掉函数缓存。\n\n## Stream 模式\n\n前面的例子都是一次性生成结果，但是有时候我们可能需要一个流式的输出，这个时候我们可能需要用底层一点的API来完成了：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\nv = llm.stream_chat_oai(model=\"deepseek_chat\",conversations=[{\n    \"role\":\"user\",\n    \"content\":\"你好，你是谁\",\n}],delta_mode=True)\n\nfor t in v:\n    print(t,flush=True)  \n\n# 你好\n# ！\n# 我\n# 是一个\n# 人工智能\n# 助手\n# ，\n# 旨在\n# 提供\n# 信息\n# 、\n# 解答\n# 问题....\n```\n\n如果你不想要流式输出，但是想用底层一点的API，你可以使用 `llm.chat_oai` 方法：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\nv = llm.chat_oai(model=\"deepseek_chat\",conversations=[{\n    \"role\":\"user\",\n    \"content\":\"你好，你是谁\",\n}])\n\nprint(v[0].output)\n## 你好！我是一个人工智能助手，旨在提供信息、解答问题和帮助用户解决问题。如果你有任何问题或需要帮助，请随时告诉我。\n```\n\n## Function Calling \n\nbyzerllm 可以不依赖模型自身就能提供 function calling 支持，我们来看个例子：\n\n\n```python\nfrom typing import List,Dict,Any,Annotated\nimport pydantic \nimport datetime\nfrom dateutil.relativedelta import relativedelta\n\ndef compute_date_range(count:Annotated[int,\"时间跨度，数值类型\"],\n                       unit:Annotated[str,\"时间单位，字符串类型\",{\"enum\":[\"day\",\"week\",\"month\",\"year\"]}])->List[str]:\n    '''\n    计算日期范围\n\n    Args:\n        count: 时间跨度，数值类型\n        unit: 时间单位，字符串类型，可选值为 day,week,month,year\n    '''        \n    now = datetime.datetime.now()\n    now_str = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n    if unit == \"day\":\n        return [(now - relativedelta(days=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"week\":\n        return [(now - relativedelta(weeks=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"month\":\n        return [(now - relativedelta(months=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"year\":\n        return [(now - relativedelta(years=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    return [\"\",\"\"]\n\ndef compute_now()->str:\n    '''\n    计算当前时间\n    '''\n    return datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n```\n\n我们定义了两个方法，一个是计算日期范围，一个是计算当前时间。\n\n现在我么可以来测试下，系统如何根据自然语言决定调用哪个方法：\n\n```python\nt = llm.chat_oai([{\n    \"content\":'''计算当前时间''',\n    \"role\":\"user\"    \n}],tools=[compute_date_range,compute_now],execute_tool=True)\n\nt[0].values\n\n## output: ['2024-06-14 15:18:02']\n```\n\n我们可以看到，他正确的选择了 compute_now 方法。\n\n接着我们再试一个：\n\n```python\nt = llm.chat_oai([{\n    \"content\":'''最近三个月趋势''',\n    \"role\":\"user\"    \n}],tools=[compute_date_range,compute_now],execute_tool=True)\n\nt[0].values\n\n## output: [['2024-03-14 15:19:13', '2024-06-14 15:19:13']]\n```\n\n模型正确的选择了 compute_date_range 方法。\n\n## 多模态\n\nbyerllm 也能很好的支持多模态的交互，而且统一了多模态大模型的接口，比如你可以用一样的方式使用 openai 或者 claude 的图片转文字能力， 或者一致的方式使用火山，azuer, openai的语音合成接口。\n\n### 图生文\n\n```python\nimport byzerllm\nfrom byzerllm.types import ImagePath\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\n\n\n@byzerllm.prompt()\ndef what_in_image(image_path: ImagePath) -> str:\n    \"\"\"\n    {{ image_path }}\n    这个图片里有什么？\n    \"\"\"    \n\n\nv = what_in_image.with_llm(vl_llm).run(\n    ImagePath(value=\"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\")\n)\nv\n## OUTPUT: 这张图片展示了多只可爱的猫咪，采用了艺术风格的绘画。猫咪们有不同的颜色和花纹，背景是浅棕色，上面还点缀着一些红色的花朵。整体画面给人一种温馨和谐的感觉\n```\n\n可以看到，我们只需要把 prompt 函数的图片地址入参使用 byzerllm.types.ImagePath里进行包装，就可以直接在 prompt 函数体里\n带上图片。\n\n或者你可以这样：\n\n```python\nimport byzerllm\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\n\n\n@byzerllm.prompt()\ndef what_in_image(image_path: str) -> str:\n    \"\"\"\n    {{ image }}\n    这个图片里有什么？\n    \"\"\"\n    return {\"image\": byzerllm.Image.load_image_from_path(image_path)}\n\n\nv = what_in_image.with_llm(vl_llm).run(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv\n```\n\n通过 `image_path` 参数，然后通过 `byzerllm.Image.load_image_from_path` 方法，转化为一个图片对象 image，最后在 prompt 函数体里\n使用 `{{ image }}` 引用这个图片对象。\n\n另外我们也是可以支持配置多张图片的。\n\n另外，我们也可以使用基础的 `llm.chat_oai` 方法来实现：\n\n```python\nimport byzerllm\nimport json\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\nimage = byzerllm.Image.load_image_from_path(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv = vl_llm.chat_oai(\n    conversations=[\n        {\n            \"role\": \"user\",\n            \"content\": json.dumps(\n                [{\"image\": image, \"text\": \"这个图片里有什么？\"}], ensure_ascii=False\n            ),\n        }\n    ]\n)\nv[0].output\n```\n\n还可以这么写：\n\n```python\nimport byzerllm\nimport json\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\nimage = byzerllm.Image.load_image_from_path(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv = vl_llm.chat_oai(\n    conversations=[\n        {\n            \"role\": \"user\",\n            \"content\": json.dumps(\n                [\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\"url\": image, \"detail\": \"high\"},\n                    },\n                    {\"text\": \"这个图片里有什么？\", \"type\": \"text\"},\n                ],\n                ensure_ascii=False,\n            ),\n        }\n    ]\n)\nv[0].output\n```\n\n### 语音合成\n\n这里以 openai 的 tts 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=tts-1 \\\n--model openai_tts\n```\n\n此外，byzerllm 支持 azure,火山引擎等 tts 语音合成引擎。\n\n接着你可以这么用：\n\n```python\nimport byzerllm\nimport base64\nimport json\n\nllm = byzerllm.ByzerLLM.from_default_model(\"openai_tts\")\n\n\nt = llm.chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\": json.dumps({\n        \"input\":\"hello, open_tts\",\n        \"voice\": \"alloy\",\n        \"response_format\": \"mp3\"\n    },ensure_ascii=False)\n}])\n\nwith open(\"voice.mp3\",\"wb\") as f:\n    f.write(base64.b64decode(t[0].output))\n```\n\ntts 模型生成没有prompt函数可以用，你需要直接使用 chat_oai。\n\n\n### 语音识别\n\n这里以 openai 的 whisper-1 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--worker_concurrency 10 \\\n--infer_params saas.model=whisper-1 saas.api_key=${MODEL_OPENAI_TOKEN} \\\n--model speech_to_text\n```\n\n语音识别的使用方式和图生文类似，我们可以直接在 prompt 函数体里带上音频文件。\n\n```python\nimport byzerllm\nimport json\nimport base64\nfrom byzerllm.types import AudioPath\n\nllm = byzerllm.ByzerLLM.from_default_model(\"speech_to_text\")\n\naudio_file = \"/Users/allwefantasy/videos/output_audio.mp3\"\n\n@byzerllm.prompt(llm=llm)\ndef audio_to_text(audio_file: AudioPath):\n    \"\"\"\n    {{ audio_file }}\n    \"\"\"\n\nv = audio_to_text(AudioPath(value=audio_file))\njson.loads(v)\n```\n输出的数据格式略微复杂：\n\n```\n{'text': 'In the last chapter, you and I started to step through the internal workings of a transformer. This is one of the key pieces of technology inside large language models, and a lot of other tools in the modern wave of AI.',\n 'task': 'transcribe',\n 'language': 'english',\n 'duration': 10.0,\n 'segments': [{'id': 0,\n   'seek': 0,\n   'start': 0.0,\n   'end': 4.78000020980835,\n   'text': ' In the last chapter, you and I started to step through the internal workings of a transformer.',\n   'tokens': [50364,\n    682,\n    264,\n    1036,\n    7187,\n    11,\n    291,\n    293,\n    286,\n    1409,\n    281,\n    1823,\n    807,\n    264,\n    6920,\n    589,\n    1109,\n    295,\n    257,\n    31782,\n    13,\n    50586],\n   'temperature': 0.0,\n   'avg_logprob': -0.28872039914131165,\n   'compression_ratio': 1.4220778942108154,\n   'no_speech_prob': 0.016033057123422623},\n  ....\n  {'id': 2,\n   'seek': 0,\n   'start': 8.579999923706055,\n   'end': 9.979999542236328,\n   'text': ' and a lot of other tools in the modern wave of AI.',\n   'tokens': [50759,\n    293,\n    257,\n    688,\n    295,\n    661,\n    3873,\n    294,\n    264,\n    4363,\n    5772,\n    295,\n    7318,\n    13,\n    50867],\n   'temperature': 0.0,\n   'avg_logprob': -0.28872039914131165,\n   'compression_ratio': 1.4220778942108154,\n   'no_speech_prob': 0.016033057123422623}],\n 'words': [{'word': 'In', 'start': 0.0, 'end': 0.18000000715255737},\n  {'word': 'the', 'start': 0.18000000715255737, 'end': 0.23999999463558197},\n  {'word': 'last', 'start': 0.23999999463558197, 'end': 0.5400000214576721},\n  {'word': 'chapter', 'start': 0.5400000214576721, 'end': 0.800000011920929},\n  ....\n  {'word': 'AI', 'start': 9.920000076293945, 'end': 9.979999542236328}]}\n```\n\n会输出每一句话以及每一个字所在的起始时间和截止时间。你可以根据需要来使用。\n\n\n### 文生图\n\n文生图和语音合成类似，首先要启动合适的模型,以openai 的 dall-e-3 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=dall-e-3 \\\n--model openai_image_gen\n```\n\n启动模型后，只需要记住几个模板参数即可使用，这里直接使用 chat_oai 方法来使用：\n\n```python\n\nimport byzerllm\nimport json\nimport base64\n\nllm = byzerllm.ByzerLLM.from_default_model(\"openai_image_gen\")\nt = llm.chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\": json.dumps({\n        \"input\":\"a white siamese cat\",\n        \"size\": \"1024x1024\",\n        \"quality\": \"standard\"\n    },ensure_ascii=False)\n}])\n\nwith open(\"output1.jpg\",\"wb\") as f:\n    f.write(base64.b64decode(t[0].output))\n\n\nimport matplotlib.pyplot as plt\n\nimage_path = \"output1.jpg\"\nimage = plt.imread(image_path)\n\nplt.imshow(image)\nplt.axis('off')\nplt.show()\n```\n\n## Prompt 函数的流式输出\n\nbyzerllm 底层支持流式输出，非 prompt 函数的用法是这样的：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\n\nv = llm.stream_chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\":\"讲一个100字的故事\"\n}])\n\nfor s  in v:\n    print(s[0], end=\"\")\n```\n\n如果你像用 prompt 函数，可以这么用：\n\n\n```python\nimport byzerllm\nimport json\nimport base64\nfrom typing import Generator\n\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\n\n@byzerllm.prompt()\ndef tell_story() -> Generator[str, None, None]:\n    '''\n    给我讲一个一百多字的故事\n    '''\n\nv = tell_story.with_llm(llm).run()    \nfor i in v:\n    print(i, end=\"\")\n```\n\n可以看到，和普通的 prompt 函数的区别在于，返回值是一个生成器，然后你可以通过 for 循环来获取结果。\n\n## 向量化模型\n\nbyzerllm 支持向量化模型,你可以这样启动一个本地的模型：\n\n```bash\n!byzerllm deploy --pretrained_model_type custom/bge \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--worker_concurrency 10 \\\n--model_path /home/winubuntu/.auto-coder/storage/models/AI-ModelScope/bge-large-zh \\\n--infer_backend transformers \\\n--num_workers 1 \\\n--model emb\n```\n\n注意两个参数:\n\n1. --infer_backend transformers: 表示使用 transformers 作为推理后端。\n2. --model_path: 表示模型的路径。\n\n也可以启动一个 SaaS 的emb模型,比如 qwen 的 emb 模型：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/qianwen \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 2 \\\n--infer_params saas.api_key=${MODEL_QIANWEN_TOKEN}  saas.model=text-embedding-v2 \\\n--model qianwen_emb\n```\n\n或者 openai 的 emb 模型：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--worker_concurrency 10 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=text-embedding-3-small \\\n--model gpt_emb\n```\nSaaS 模型无需配置 `--infer_backend` 参数。\n\n无论是本地模型还是 SaaS 模型，我们都可以这样使用：\n\n```python\nimport byzerllm\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\nllm.setup_default_emb_model_name(\"emb\")\nllm.emb_query(\"你好\")\n```\n\n如果你配置 byzerllm 中的 Storage 使用，比如你这样启动了存储：\n\n```bash\nbyzerllm storage start --enable_emb\n```\n\n那么需要这么使用：\n\n```python\nfrom byzerllm.apps.byzer_storage.simple_api import ByzerStorage, DataType, FieldOption,SortOption\nstorage = ByzerStorage(\"byzerai_store\", \"memory\", \"memory\")\nstorage.emb(\"你好\")\n```\n\n## 一些辅助工具\n\n当调用 prompt 函数返回字符串的时候，如果想从里面抽取代码，可以使用如下方式：\n\n```python\nfrom byzerllm.utils.client import code_utils\ntext_with_markdown = '''\n```shell\nls -l\n```\n'''\ncode_blocks = code_utils.extract_code(text_with_markdown)\nfor code_block in code_blocks:\n    if code_block[0] == \"shell\":\n        print(code_block[1])\n##output: ls -l        \n```\n\n\n## 注意事项\n\n1. prompt函数方法体返回只能是dict，实际的返回类型和方法签名可以不一样，但是方法体返回只能是dict。\n2. 大部分情况prompt函数体为空，如果一定要有方法体，可以返回一个空字典。\n3. 调用prompt方法的时候，如果在@byzerllm.prompt()里没有指定llm对象，那么需要在调用的时候通过with_llm方法指定llm对象。\n\n===========================\n\n# Byzer Storage: 最易用的AI存储引擎\n\nByzer Storage是一个为AI应用设计的高性能存储引擎,它提供了简单易用的API,支持向量搜索、全文检索以及结构化查询。本文将详细介绍Byzer Storage的使用方法和主要特性。\n\n## 0. 安装和启动\n\n```bash\npip install byzerllm\nbyzerllm storage start\n```\n\nThat's it! Byzer Storage已经安装并启动成功,现在我们可以开始使用它了。默认会启动一个 byzerai_store 的集群。\n\n注意，如果你是这样启动的：\n\n```bash\nbyzerllm storage start --enable_emb\n```\n\n那么会自动启动一个 emb 模型，名字就叫 emb， ByzerStorage 会自动使用该模型，无需做任何其他配置。\n\n## 1. 初始化\n\n创建一个 ByzerStorage 对象，链接 byzerai_store 集群，并且指定数据库和表名（可以不存在）。\n\n```python\nfrom byzerllm.apps.byzer_storage.simple_api import ByzerStorage, DataType, FieldOption, SortOption\n\nstorage = ByzerStorage(\"byzerai_store\", \"my_database1\", \"my_table4s\")\n```\n\n\n## 2. 创建库表（可选）\n\nByzer Storage使用Schema来定义数据结构。我们可以使用SchemaBuilder来构建Schema:\n\n```python\n_ = (\n    storage.schema_builder()\n    .add_field(\"_id\", DataType.STRING)\n    .add_field(\"name\", DataType.STRING)\n    .add_field(\"content\", DataType.STRING, [FieldOption.ANALYZE])\n    .add_field(\"raw_content\", DataType.STRING, [FieldOption.NO_INDEX])    \n    .add_array_field(\"summary\", DataType.FLOAT)    \n    .add_field(\"created_time\", DataType.LONG, [FieldOption.SORT])    \n    .execute()\n)\n```\n\n这个Schema定义了以下字段:\n- `_id`: 字符串类型的主键\n- `name`: 字符串类型,可用于过滤条件\n- `content`: 字符串类型,带有ANALYZE选项,用于全文搜索\n- `raw_content`: 字符串类型,带有NO_INDEX选项,不会被索引\n- `summary`: 浮点数组类型,用于存储向量\n- `created_time`: 长整型,带有SORT选项,可用于排序\n\n需要注意的是：\n\n1. 如果一个字段带有ANALYZE选项,则该字段会被分词,并且可以用于全文搜索，但是就无法返回原始的文本了。所以你需要添加一个新字段专门用来存储原文，比如在我们这个例子里，我们新增了 raw_content 字段，并且显示的指定了 NO_INDEX 选项，这样就不会被索引，也就不会被分词，可以后续被检索后用来获取原始文本。\n2. 对于需要作为向量检索和存储的字段，需要指定为数组类型，比如我们这个例子里的 summary 字段。\n3. 如果你需要拿到向量字段的原始文本，那么你也需要添加一个新字段专门用来存储原文，就像我们这个例子里的 raw_content 字段一样。\n\n\n## 3. 写入数据\n\n准备数据并使用WriteBuilder写入Storage:\n\n```python\ndata = [\n    {\"_id\": \"1\", \"name\": \"Hello\", \"content\": \"Hello, world!\", \"raw_content\": \"Hello, world!\", \"summary\": \"hello world\", \"created_time\": 1612137600},\n    {\"_id\": \"2\", \"name\": \"Byzer\", \"content\": \"Byzer, world!\", \"raw_content\": \"Byzer, world!\", \"summary\": \"byzer\", \"created_time\": 1612137601},\n    {\"_id\": \"3\", \"name\": \"AI\", \"content\": \"AI, world!\", \"raw_content\": \"AI, world!\", \"summary\": \"AI\", \"created_time\": 16121376002},\n    {\"_id\": \"4\", \"name\": \"ByzerAI\", \"content\": \"ByzerAI, world!\", \"raw_content\": \"ByzerAI, world!\", \"summary\": \"ByzerAi\", \"created_time\": 16121376003},\n]\n\nstorage.write_builder().add_items(data, vector_fields=[\"summary\"], search_fields=[\"content\"]).execute()\nstorage.commit()\n```\n\n这里我们使用`add_items`方法批量添加数据,并指定了`summary`为向量字段,`content`为搜索字段。最后调用`commit()`来确保数据被持久化。\n\n\n从上面我们要写入到 byzer storage 的数据我们可以看到如下几个特点：\n\n1. 需要包含我们之前定义的 Schema中罗列的所有的字段，同时需要指定哪些是向量字段，哪些是检索字段。\n2. 向量和检索不能是同一个字段。\n3. 对于向量，检索字段，我们给到 write_builder 的都是文本，ByzerStorage 会根据 Schema 的定义自动将其转换为向量，检索字段需要的格式。\n\n## 4. 查询数据\n\nByzer Storage支持多种查询方式,包括向量搜索、全文检索、过滤和排序。\n\n### 4.1 向量搜索 + 全文检索\n\n```python\nquery = storage.query_builder()\nquery.set_vector_query(\"ByzerAI\", fields=[\"summary\"])\nresults = query.set_search_query(\"Hello\", fields=[\"content\"]).execute()\nprint(results)\n```\n\n这个查询结合了向量搜索和全文检索,它会在`summary`字段中搜索与\"ByzerAI\"相似的向量,同时在`content`字段中搜索包含\"Hello\"的文档。\n\n### 4.2 过滤 + 向量搜索 + 全文检索\n\n```python\nquery = storage.query_builder()\nquery.and_filter().add_condition(\"name\", \"AI\").build()\nquery.set_vector_query(\"ByzerAI\", fields=\"summary\")\nresults = query.set_search_query(\"Hello\", fields=[\"content\"]).execute()\nprint(results)\n```\n\n这个查询首先过滤`name`字段等于\"AI\"的文档,然后在结果中进行向量搜索和全文检索。\n\n### 4.3 过滤 + 排序\n\n```python\nquery = storage.query_builder()\nquery.and_filter().add_condition(\"name\", \"AI\").build().sort(\"created_time\", SortOption.DESC)\nresults = query.execute()\nprint(results)\n```\n\n这个查询过滤`name`字段等于\"AI\"的文档,然后按`created_time`字段降序排序。\n\n## 5. 删除数据\n\n### 5.1 根据ID删除\n\n```python\nstorage.delete_by_ids([\"3\"])\n\nquery = storage.query_builder()\nquery.and_filter().add_condition(\"name\", \"AI\").build()\nresults = query.execute()\nprint(results)\n```\n\n这里我们删除了ID为\"3\"的文档,然后查询验证删除结果。\n\n### 5.2 删除整个表\n\n```python\nstorage.drop()\n```\n\n这个操作会删除整个表及其所有数据,请谨慎使用。\n\n## 结论\n\nByzer Storage提供了一套简洁而强大的API,能够轻松实现向量搜索、全文检索、结构化查询等功能。它的设计非常适合AI应用场景,可以有效地存储和检索各种类型的数据。通过本文的介绍,相信读者已经对Byzer Storage有了基本的了解,并能够开始在自己的项目中使用这个强大的存储引擎。\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/generator.py\nfrom typing import Optional, Dict, Any\nfrom openai import OpenAI\nimport json\n\n\nclass SQLGenerator:\n\n    def __init__(self,rag_base_url:str,rag_api_key:str) -> None:\n        self.rag_client = OpenAI(api_key=rag_api_key, base_url=rag_base_url)\n\n    def stream_chat_oai(\n        self,\n        conversations,\n        model: Optional[str] = None,\n        role_mapping=None,\n        delta_mode=False,\n        llm_config: Dict[str, Any] = {},\n    ):\n        pass\n\n\n     \n\n下面是用户的需求：\n\n在sql目录下新增一个 types 目录,将 rag_base_url 和 rag_api_key 组装成一个pyndatic 类,然后引入到 SQLGenerator里\n"}, {"role": "assistant", "content": "Certainly! I'll create a new `types` directory under the `sql` directory, define a Pydantic class for the RAG configuration, and then update the `SQLGenerator` class to use this new type. Here are the steps and the corresponding code changes:\n\n1. First, let's create a new file `types.py` in the `sql/types` directory:\n\n```python\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/types/__init__.py\n<<<<<<< SEARCH\n=======\nfrom pydantic import BaseModel\n\nclass RAGConfig(BaseModel):\n    rag_base_url: str\n    rag_api_key: str\n>>>>>>> REPLACE\n```\n\n2. Now, let's update the `generator.py` file to use this new `RAGConfig` type:\n\n```python\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/generator.py\n<<<<<<< SEARCH\nfrom typing import Optional, Dict, Any\nfrom openai import OpenAI\nimport json\n\n\nclass SQLGenerator:\n\n    def __init__(self,rag_base_url:str,rag_api_key:str) -> None:\n        self.rag_client = OpenAI(api_key=rag_api_key, base_url=rag_base_url)\n=======\nfrom typing import Optional, Dict, Any\nfrom openai import OpenAI\nimport json\nfrom .types import RAGConfig\n\n\nclass SQLGenerator:\n\n    def __init__(self, rag_config: RAGConfig) -> None:\n        self.rag_client = OpenAI(api_key=rag_config.rag_api_key, base_url=rag_config.rag_base_url)\n>>>>>>> REPLACE\n```\n\nThese changes accomplish the following:\n\n1. We've created a new `types` directory under the `sql` directory and added an `__init__.py` file to it.\n2. In the `__init__.py` file, we've defined a `RAGConfig` Pydantic class that includes `rag_base_url` and `rag_api_key` as fields.\n3. We've updated the `generator.py` file to import the `RAGConfig` class from the new `types` module.\n4. We've modified the `SQLGenerator` class constructor to accept a `RAGConfig` object instead of separate `rag_base_url` and `rag_api_key` parameters.\n\nThese changes make the code more organized and type-safe by using a Pydantic model for configuration. It also makes it easier to extend the configuration in the future if needed."}], "model": "human", "yaml_file": "actions/106_chat_action.yml"}
{"instruction": "@cli.py 支持 @/Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/types.py RAGConfig 里的参数", "conversations": [{"role": "user", "content": "\n如果你需要生成代码，对于每个需要更改的文件,你需要按 *SEARCH/REPLACE block* 的格式进行生成。\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The opening fence and code language, eg: ```python\n2. The file path alone on a line, starting with \"##File:\" and verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n*SEARCH/REPLACE* blocks will replace *all* matching occurrences.\nInclude enough lines to make the SEARCH blocks unique.\n\nInclude *ALL* the code being searched and replaced!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n下面我们来看一个例子：\n\n当前项目目录结构：\n1. 项目根目录： /tmp/projects/mathweb\n2. 项目子目录/文件列表(类似tree 命令输出)\nflask/\n    app.py\n    templates/\n        index.html\n    static/\n        style.css\n\n用户需求： Change get_factorial() to use math.factorial\n\n回答： To make this change we need to modify `/tmp/projects/mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n用户需求： Refactor hello() into its own file.\n\n回答：To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/hello.py\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/main.py\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n现在让我们开始一个新的任务:\n\n当前项目目录结构：\n1. 项目根目录： /Users/allwefantasy/projects/super_analysis\n2. 项目子目录/文件列表(类似tree 命令输出)：\nsetup.py\n.auto-coder/\n    libs/\n        llm_friendly_packages/\n            src/\n                example1/\n                    main.py\nsrc/\n    superanalysis/\n        serve/\n            openai_serve.py\n            __init__.py\n            llm_wrapper.py\n            api_server.py\n            cli.py\n        sql/\n            __init__.py\n            types.py\n            generator.py\n下面是一些文件路径以及每个文件对应的源码：\n\n##File: .auto-coder/libs/llm_friendly_packages/github.com/allwefantasy/byzer-llm/README.md\n# byzerllm 大模型编程快速指南\n\n本文示例 [Notebook](../../notebooks/003_byzerllm_大模型编程快速指南.ipynb)\n\n## 安装\n\n```bash\npip install -U byzerllm\nray start --head\n```\n## 启动一个模型代理\n\nbyzerllm 支持私有化模型或者SaaS模型的部署。\n\n这里以 deepseek 官方API 为例：\n\n```bash\neasy-byzerllm deploy deepseek-chat --token xxxxx --alias deepseek_chat\n```\n\n或者跬基流动API:\n\n```bash\neasy-byzerllm deploy alibaba/Qwen1.5-110B-Chat --token xxxxx --alias qwen110b_chat\n```\n\n将上面的 API KEY 替换成你们自己的。\n\n如果你想部署私有化模型或者对接 Ollama 等更多需求，参考 [002_使用byzerllm进行模型部署.md](./002_使用byzerllm进行模型部署.md) 或者 [README.md](../../README.md)。\n\n之后，你就可以在代码里使用  deepseek_chat 或者 qwen110b_chat  访问模型了。\n\n## hello world\n\n来和我们的大模型打个招呼:\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\n@byzerllm.prompt(llm=llm)\ndef hello(q:str) ->str:\n    '''\n    你好, {{ q }}\n    '''\n\ns = hello(\"你是谁\")    \nprint(s)\n\n## 输出:\n## '你好！我是一个人工智能助手，专门设计来回答问题、提供信息和帮助解决问题。如果你有任何疑问或需要帮助，请随时告诉我。'\n```\n\n恭喜，你和大模型成功打了招呼！\n\n可以看到，我们通过 `@byzerllm.prompt` 装饰器，将一个方法转换成了一个大模型的调用，然后这个方法的主题是一段文本，文本中\n使用了 jinja2 模板语法，来获得方法的参数。当正常调用该方法时，实际上就发起了和大模型的交互，并且返回了大模型的结果。\n\n在 byzerllm 中，我们把这种方法称之为 prompt 函数。\n\n## 查看发送给大模型的prompt\n\n很多情况你可能需要调试，查看自己的 prompt 渲染后到底是什么样子的，这个时候你可以通过如下方式\n获取渲染后的prompt:\n\n```python\nhello.prompt(\"你是谁\")\n## '你好, 你是谁'\n```            \n\n## 动态换一个模型\n\n前面的 hello 方法在初始化的时候，我们使用了默认的模型 deepseek_chat，如果我们想换一个模型，可以这样做：\n\n```python\nhello.with_llm(llm).run(\"你是谁\")\n## '你好！我是一个人工智能助手，专门设计来回答问题、提供信息和帮助解决问题。如果你有任何疑问或需要帮助，请随时告诉我。'\n```\n\n通过 with_llm 你可以设置一个新的 llm 对象，然后调用 run 方法，就可以使用新的模型了。\n\n## 超长文本生成\n\n我们知道，大模型一次生成的长度其实是有限的，如果你想生成超长文本，你可能需手动的不断获得\n生成结果，然后把他转化为输入，然后再次生成，这样的方式是比较麻烦的。\n\nbyzerllm 提供了更加易用的 API :\n\n```python\nimport byzerllm\nfrom byzerllm import ByzerLLM\n\nllm = ByzerLLM.from_default_model(\"deepseek_chat\")\n\n@byzerllm.prompt()\ndef tell_story() -> str:\n    \"\"\"\n    讲一个100字的故事。    \n    \"\"\"\n\n\ns = (\n    tell_story.with_llm(llm)\n    .with_response_markers()\n    .options({\"llm_config\": {\"max_length\": 10}})\n    .run()\n)\nprint(s)\n\n## 从前，森林里住着一只聪明的小狐狸。一天，它发现了一块闪闪发光的宝石。小狐狸决定用这块宝石帮助森林里的动物们。它用宝石的光芒指引迷路的小鹿找到了回家的路，用宝石的温暖治愈了受伤的小鸟。从此，小狐狸成了森林里的英雄，动物们都感激它的善良和智慧。\n```\n\n实际核心部分就是这一行：\n\n```python\ntell_story.with_llm(llm)\n    .with_response_markers()    \n    .run()\n```\n\n我们只需要调用 `with_response_markers` 方法，系统就会自动的帮我们生成超长文本。\n在上面的案例中，我们通过\n\n```python\n.options({\"llm_config\": {\"max_length\": 10}})\n```\n\n认为的限制大模型一次交互最多只能输出10个字符，但是系统依然自动完成了远超过10个字符的文本生成。\n\n## 对象输出\n\n前面我们的例子都是返回字符串，但是我们也可以返回对象，这样我们就可以更加灵活的处理返回结果。\n\n```python\nimport pydantic \n\nclass Story(pydantic.BaseModel):\n    '''\n    故事\n    '''\n\n    title: str = pydantic.Field(description=\"故事的标题\")\n    body: str = pydantic.Field(description=\"故事主体\")\n\n@byzerllm.prompt()\ndef tell_story()->Story:\n    '''\n    讲一个100字的故事。    \n    '''\n\ns = tell_story.with_llm(llm).run()\nprint(isinstance(s, Story))\nprint(s.title)\n\n## True\n## 勇敢的小鸟\n```\n\n可以看到，我们很轻松的将输出转化为格式化输出。\n\n## 自定义字段抽取\n\n前面的结构化输出，其实会消耗更多token,还有一种更加精准的结构化输出方式。\n比如让大模型生成一个正则表达式，但实际上大模型很难准确只输出一个正则表达式，这个时候我们可以通过自定义抽取函数来获取我们想要的结果。\n\n\n```python\nfrom loguru import logger\nimport re\n\n@byzerllm.prompt()\ndef generate_regex_pattern(desc: str) -> str:\n    \"\"\"\n    根据下面的描述生成一个正则表达式，要符合python re.compile 库的要求。\n\n    {{ desc }}\n\n    最后生成的正则表达式要在<REGEX></REGEX>标签对里。\n    \"\"\"    \n\ndef extract_regex_pattern(regex_block: str) -> str:    \n    pattern = re.search(r\"<REGEX>(.*)</REGEX>\", regex_block, re.DOTALL)\n    if pattern is None:\n        logger.warning(\"No regex pattern found in the generated block:\\n {regex_block}\")\n        raise None\n    return pattern.group(1)\n\npattern = \"匹配一个邮箱地址\"\nv = generate_regex_pattern.with_llm(llm).with_extractor(extract_regex_pattern).run(desc=pattern)\nprint(v)\n## ^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\n```\n\n在上面的例子里，我们根据一句话生成一个正则表达式。我们通过 `with_extractor` 方法，传入了一个自定义的抽取函数，这个函数会在大模型生成结果后，对结果进行处理，然后返回我们想要的结果。\n\n我们在 prompt 明确说了，生成的结果要放到 `<REGEX></REGEX>` 标签对里，然后我们通过 extract_regex_pattern 函数，从结果中提取出了我们想要的正则表达式。\n\n## 在实例方法中使用大模型\n\n```python\nimport byzerllm\ndata = {\n    'name': 'Jane Doe',\n    'task_count': 3,\n    'tasks': [\n        {'name': 'Submit report', 'due_date': '2024-03-10'},\n        {'name': 'Finish project', 'due_date': '2024-03-15'},\n        {'name': 'Reply to emails', 'due_date': '2024-03-08'}\n    ]\n}\n\n\nclass RAG():\n    def __init__(self):        \n        self.llm = byzerllm.ByzerLLM()\n        self.llm.setup_template(model=\"deepseek_chat\",template=\"auto\")\n        self.llm.setup_default_model_name(\"deepseek_chat\")        \n    \n    @byzerllm.prompt(lambda self:self.llm)\n    def generate_answer(self,name,task_count,tasks)->str:\n        '''\n        Hello {{ name }},\n\n        This is a reminder that you have {{ task_count }} pending tasks:\n        {% for task in tasks %}\n        - Task: {{ task.name }} | Due: {{ task.due_date }}\n        {% endfor %}\n\n        Best regards,\n        Your Reminder System\n        '''        \n\nt = RAG()\n\nresponse = t.generate_answer(**data)\nprint(response)\n\n## 输出:\n## Hello Jane Doe,\n##I hope this message finds you well. I wanted to remind you of your 3 pending tasks to ensure you stay on track:\n## 1. **Submit report** - This task is due on **2024-03-10**. Please ensure that you allocat\n```\n\n这里我们给了个比较复杂的例子，但我们可以看到，给一个实例prompt方法和普通prompt 方法差异不大。\n唯一的区别是如果你希望在定义的时候就指定大模型，使用一个lambda函数返回实例的 llm 对象即可。\n\n```python\n@byzerllm.prompt(lambda self:self.llm)\n```\n\n你也可以不返回，在调用的时候通过 `with_llm` 方法指定 llm 对象。\n\n此外，这个例子也展示了如何通过jinja2模板语法，来处理复杂的结构化数据。\n\n## 通过 Python 代码处理复杂入参\n\n上面的一个例子中，我们通过 jinja2 模板语法，来处理复杂的结构化数据，但是有时候我们可能需要更加复杂的处理，这个时候我们可以通过 Python 代码来处理。\n\n```python\nimport byzerllm\n\ndata = {\n    'name': 'Jane Doe',\n    'task_count': 3,\n    'tasks': [\n        {'name': 'Submit report', 'due_date': '2024-03-10'},\n        {'name': 'Finish project', 'due_date': '2024-03-15'},\n        {'name': 'Reply to emails', 'due_date': '2024-03-08'}\n    ]\n}\n\n\nclass RAG():\n    def __init__(self):        \n        self.llm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n    \n    @byzerllm.prompt()\n    def generate_answer(self,name,task_count,tasks)->str:\n        '''\n        Hello {{ name }},\n\n        This is a reminder that you have {{ task_count }} pending tasks:\n            \n        {{ tasks }}\n\n        Best regards,\n        Your Reminder System\n        '''\n        \n        tasks_str = \"\\n\".join([f\"- Task: {task['name']} | Due: { task['due_date'] }\" for task in tasks])\n        return {\"tasks\": tasks_str}\n\nt = RAG()\n\nresponse = t.generate_answer.with_llm(t.llm).run(**data)\nprint(response)\n\n## Just a gentle nudge to keep you on track with your pending tasks. Here's a quick recap:....\n```\n\n在这个例子里，我们直接把 tasks 在方法体里进行处理，然后作为一个字符串返回，最够构建一个字典，字典的key为 tasks,然后\n你就可以在 docstring 里使用 `{{ tasks }}` 来引用这个字符串。\n\n这样对于很复杂的入参，就不用谢繁琐的 jinja2 模板语法了。\n\n## 如何自动实现一个方法\n\n比如我定义一个签名，但是我不想自己实现里面的逻辑，让大模型来实现。这个在 byzerllm 中叫 function impl。我们来看看怎么\n实现:\n\n```python\nimport pydantic\nclass Time(pydantic.BaseModel):\n    time: str = pydantic.Field(...,description=\"时间，时间格式为 yyyy-MM-dd\")\n\n\n@llm.impl()\ndef calculate_current_time()->Time:\n    '''\n    计算当前时间\n    '''\n    pass \n\n\ncalculate_current_time()\n#output: Time(time='2024-06-14')\n```\n\n在这个例子里，我们定义了一个 calculate_current_time 方法，但是我们没有实现里面的逻辑，我们通过 `@llm.impl()` 装饰器，让大模型来实现这个方法。\n为了避免每次都要“生成”这个方法，导致无法适用，我们提供了缓存，用户可以按如下方式打印速度：\n\n```python\nstart = time.monotonic()\ncalculate_current_time()\nprint(f\"first time cost: {time.monotonic()-start}\")\n\nstart = time.monotonic()\ncalculate_current_time()\nprint(f\"second time cost: {time.monotonic()-start}\")\n\n# output:\n# first time cost: 6.067266260739416\n# second time cost: 4.347506910562515e-05\n```\n可以看到，第一次执行花费了6s,第二次几乎是瞬间完成的，这是因为第一次执行的时候，我们实际上是在生成这个方法，第二次执行的时候，我们是执行已经生成好的代码，所以速度会非常快。你可以显示的调用 `llm.clear_impl_cache()` 清理掉函数缓存。\n\n## Stream 模式\n\n前面的例子都是一次性生成结果，但是有时候我们可能需要一个流式的输出，这个时候我们可能需要用底层一点的API来完成了：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\nv = llm.stream_chat_oai(model=\"deepseek_chat\",conversations=[{\n    \"role\":\"user\",\n    \"content\":\"你好，你是谁\",\n}],delta_mode=True)\n\nfor t in v:\n    print(t,flush=True)  \n\n# 你好\n# ！\n# 我\n# 是一个\n# 人工智能\n# 助手\n# ，\n# 旨在\n# 提供\n# 信息\n# 、\n# 解答\n# 问题....\n```\n\n如果你不想要流式输出，但是想用底层一点的API，你可以使用 `llm.chat_oai` 方法：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\nv = llm.chat_oai(model=\"deepseek_chat\",conversations=[{\n    \"role\":\"user\",\n    \"content\":\"你好，你是谁\",\n}])\n\nprint(v[0].output)\n## 你好！我是一个人工智能助手，旨在提供信息、解答问题和帮助用户解决问题。如果你有任何问题或需要帮助，请随时告诉我。\n```\n\n## Function Calling \n\nbyzerllm 可以不依赖模型自身就能提供 function calling 支持，我们来看个例子：\n\n\n```python\nfrom typing import List,Dict,Any,Annotated\nimport pydantic \nimport datetime\nfrom dateutil.relativedelta import relativedelta\n\ndef compute_date_range(count:Annotated[int,\"时间跨度，数值类型\"],\n                       unit:Annotated[str,\"时间单位，字符串类型\",{\"enum\":[\"day\",\"week\",\"month\",\"year\"]}])->List[str]:\n    '''\n    计算日期范围\n\n    Args:\n        count: 时间跨度，数值类型\n        unit: 时间单位，字符串类型，可选值为 day,week,month,year\n    '''        \n    now = datetime.datetime.now()\n    now_str = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n    if unit == \"day\":\n        return [(now - relativedelta(days=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"week\":\n        return [(now - relativedelta(weeks=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"month\":\n        return [(now - relativedelta(months=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"year\":\n        return [(now - relativedelta(years=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    return [\"\",\"\"]\n\ndef compute_now()->str:\n    '''\n    计算当前时间\n    '''\n    return datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n```\n\n我们定义了两个方法，一个是计算日期范围，一个是计算当前时间。\n\n现在我么可以来测试下，系统如何根据自然语言决定调用哪个方法：\n\n```python\nt = llm.chat_oai([{\n    \"content\":'''计算当前时间''',\n    \"role\":\"user\"    \n}],tools=[compute_date_range,compute_now],execute_tool=True)\n\nt[0].values\n\n## output: ['2024-06-14 15:18:02']\n```\n\n我们可以看到，他正确的选择了 compute_now 方法。\n\n接着我们再试一个：\n\n```python\nt = llm.chat_oai([{\n    \"content\":'''最近三个月趋势''',\n    \"role\":\"user\"    \n}],tools=[compute_date_range,compute_now],execute_tool=True)\n\nt[0].values\n\n## output: [['2024-03-14 15:19:13', '2024-06-14 15:19:13']]\n```\n\n模型正确的选择了 compute_date_range 方法。\n\n## 多模态\n\nbyerllm 也能很好的支持多模态的交互，而且统一了多模态大模型的接口，比如你可以用一样的方式使用 openai 或者 claude 的图片转文字能力， 或者一致的方式使用火山，azuer, openai的语音合成接口。\n\n### 图生文\n\n```python\nimport byzerllm\nfrom byzerllm.types import ImagePath\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\n\n\n@byzerllm.prompt()\ndef what_in_image(image_path: ImagePath) -> str:\n    \"\"\"\n    {{ image_path }}\n    这个图片里有什么？\n    \"\"\"    \n\n\nv = what_in_image.with_llm(vl_llm).run(\n    ImagePath(value=\"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\")\n)\nv\n## OUTPUT: 这张图片展示了多只可爱的猫咪，采用了艺术风格的绘画。猫咪们有不同的颜色和花纹，背景是浅棕色，上面还点缀着一些红色的花朵。整体画面给人一种温馨和谐的感觉\n```\n\n可以看到，我们只需要把 prompt 函数的图片地址入参使用 byzerllm.types.ImagePath里进行包装，就可以直接在 prompt 函数体里\n带上图片。\n\n或者你可以这样：\n\n```python\nimport byzerllm\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\n\n\n@byzerllm.prompt()\ndef what_in_image(image_path: str) -> str:\n    \"\"\"\n    {{ image }}\n    这个图片里有什么？\n    \"\"\"\n    return {\"image\": byzerllm.Image.load_image_from_path(image_path)}\n\n\nv = what_in_image.with_llm(vl_llm).run(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv\n```\n\n通过 `image_path` 参数，然后通过 `byzerllm.Image.load_image_from_path` 方法，转化为一个图片对象 image，最后在 prompt 函数体里\n使用 `{{ image }}` 引用这个图片对象。\n\n另外我们也是可以支持配置多张图片的。\n\n另外，我们也可以使用基础的 `llm.chat_oai` 方法来实现：\n\n```python\nimport byzerllm\nimport json\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\nimage = byzerllm.Image.load_image_from_path(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv = vl_llm.chat_oai(\n    conversations=[\n        {\n            \"role\": \"user\",\n            \"content\": json.dumps(\n                [{\"image\": image, \"text\": \"这个图片里有什么？\"}], ensure_ascii=False\n            ),\n        }\n    ]\n)\nv[0].output\n```\n\n还可以这么写：\n\n```python\nimport byzerllm\nimport json\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\nimage = byzerllm.Image.load_image_from_path(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv = vl_llm.chat_oai(\n    conversations=[\n        {\n            \"role\": \"user\",\n            \"content\": json.dumps(\n                [\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\"url\": image, \"detail\": \"high\"},\n                    },\n                    {\"text\": \"这个图片里有什么？\", \"type\": \"text\"},\n                ],\n                ensure_ascii=False,\n            ),\n        }\n    ]\n)\nv[0].output\n```\n\n### 语音合成\n\n这里以 openai 的 tts 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=tts-1 \\\n--model openai_tts\n```\n\n此外，byzerllm 支持 azure,火山引擎等 tts 语音合成引擎。\n\n接着你可以这么用：\n\n```python\nimport byzerllm\nimport base64\nimport json\n\nllm = byzerllm.ByzerLLM.from_default_model(\"openai_tts\")\n\n\nt = llm.chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\": json.dumps({\n        \"input\":\"hello, open_tts\",\n        \"voice\": \"alloy\",\n        \"response_format\": \"mp3\"\n    },ensure_ascii=False)\n}])\n\nwith open(\"voice.mp3\",\"wb\") as f:\n    f.write(base64.b64decode(t[0].output))\n```\n\ntts 模型生成没有prompt函数可以用，你需要直接使用 chat_oai。\n\n\n### 语音识别\n\n这里以 openai 的 whisper-1 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--worker_concurrency 10 \\\n--infer_params saas.model=whisper-1 saas.api_key=${MODEL_OPENAI_TOKEN} \\\n--model speech_to_text\n```\n\n语音识别的使用方式和图生文类似，我们可以直接在 prompt 函数体里带上音频文件。\n\n```python\nimport byzerllm\nimport json\nimport base64\nfrom byzerllm.types import AudioPath\n\nllm = byzerllm.ByzerLLM.from_default_model(\"speech_to_text\")\n\naudio_file = \"/Users/allwefantasy/videos/output_audio.mp3\"\n\n@byzerllm.prompt(llm=llm)\ndef audio_to_text(audio_file: AudioPath):\n    \"\"\"\n    {{ audio_file }}\n    \"\"\"\n\nv = audio_to_text(AudioPath(value=audio_file))\njson.loads(v)\n```\n输出的数据格式略微复杂：\n\n```\n{'text': 'In the last chapter, you and I started to step through the internal workings of a transformer. This is one of the key pieces of technology inside large language models, and a lot of other tools in the modern wave of AI.',\n 'task': 'transcribe',\n 'language': 'english',\n 'duration': 10.0,\n 'segments': [{'id': 0,\n   'seek': 0,\n   'start': 0.0,\n   'end': 4.78000020980835,\n   'text': ' In the last chapter, you and I started to step through the internal workings of a transformer.',\n   'tokens': [50364,\n    682,\n    264,\n    1036,\n    7187,\n    11,\n    291,\n    293,\n    286,\n    1409,\n    281,\n    1823,\n    807,\n    264,\n    6920,\n    589,\n    1109,\n    295,\n    257,\n    31782,\n    13,\n    50586],\n   'temperature': 0.0,\n   'avg_logprob': -0.28872039914131165,\n   'compression_ratio': 1.4220778942108154,\n   'no_speech_prob': 0.016033057123422623},\n  ....\n  {'id': 2,\n   'seek': 0,\n   'start': 8.579999923706055,\n   'end': 9.979999542236328,\n   'text': ' and a lot of other tools in the modern wave of AI.',\n   'tokens': [50759,\n    293,\n    257,\n    688,\n    295,\n    661,\n    3873,\n    294,\n    264,\n    4363,\n    5772,\n    295,\n    7318,\n    13,\n    50867],\n   'temperature': 0.0,\n   'avg_logprob': -0.28872039914131165,\n   'compression_ratio': 1.4220778942108154,\n   'no_speech_prob': 0.016033057123422623}],\n 'words': [{'word': 'In', 'start': 0.0, 'end': 0.18000000715255737},\n  {'word': 'the', 'start': 0.18000000715255737, 'end': 0.23999999463558197},\n  {'word': 'last', 'start': 0.23999999463558197, 'end': 0.5400000214576721},\n  {'word': 'chapter', 'start': 0.5400000214576721, 'end': 0.800000011920929},\n  ....\n  {'word': 'AI', 'start': 9.920000076293945, 'end': 9.979999542236328}]}\n```\n\n会输出每一句话以及每一个字所在的起始时间和截止时间。你可以根据需要来使用。\n\n\n### 文生图\n\n文生图和语音合成类似，首先要启动合适的模型,以openai 的 dall-e-3 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=dall-e-3 \\\n--model openai_image_gen\n```\n\n启动模型后，只需要记住几个模板参数即可使用，这里直接使用 chat_oai 方法来使用：\n\n```python\n\nimport byzerllm\nimport json\nimport base64\n\nllm = byzerllm.ByzerLLM.from_default_model(\"openai_image_gen\")\nt = llm.chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\": json.dumps({\n        \"input\":\"a white siamese cat\",\n        \"size\": \"1024x1024\",\n        \"quality\": \"standard\"\n    },ensure_ascii=False)\n}])\n\nwith open(\"output1.jpg\",\"wb\") as f:\n    f.write(base64.b64decode(t[0].output))\n\n\nimport matplotlib.pyplot as plt\n\nimage_path = \"output1.jpg\"\nimage = plt.imread(image_path)\n\nplt.imshow(image)\nplt.axis('off')\nplt.show()\n```\n\n## Prompt 函数的流式输出\n\nbyzerllm 底层支持流式输出，非 prompt 函数的用法是这样的：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\n\nv = llm.stream_chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\":\"讲一个100字的故事\"\n}])\n\nfor s  in v:\n    print(s[0], end=\"\")\n```\n\n如果你像用 prompt 函数，可以这么用：\n\n\n```python\nimport byzerllm\nimport json\nimport base64\nfrom typing import Generator\n\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\n\n@byzerllm.prompt()\ndef tell_story() -> Generator[str, None, None]:\n    '''\n    给我讲一个一百多字的故事\n    '''\n\nv = tell_story.with_llm(llm).run()    \nfor i in v:\n    print(i, end=\"\")\n```\n\n可以看到，和普通的 prompt 函数的区别在于，返回值是一个生成器，然后你可以通过 for 循环来获取结果。\n\n## 向量化模型\n\nbyzerllm 支持向量化模型,你可以这样启动一个本地的模型：\n\n```bash\n!byzerllm deploy --pretrained_model_type custom/bge \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--worker_concurrency 10 \\\n--model_path /home/winubuntu/.auto-coder/storage/models/AI-ModelScope/bge-large-zh \\\n--infer_backend transformers \\\n--num_workers 1 \\\n--model emb\n```\n\n注意两个参数:\n\n1. --infer_backend transformers: 表示使用 transformers 作为推理后端。\n2. --model_path: 表示模型的路径。\n\n也可以启动一个 SaaS 的emb模型,比如 qwen 的 emb 模型：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/qianwen \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 2 \\\n--infer_params saas.api_key=${MODEL_QIANWEN_TOKEN}  saas.model=text-embedding-v2 \\\n--model qianwen_emb\n```\n\n或者 openai 的 emb 模型：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--worker_concurrency 10 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=text-embedding-3-small \\\n--model gpt_emb\n```\nSaaS 模型无需配置 `--infer_backend` 参数。\n\n无论是本地模型还是 SaaS 模型，我们都可以这样使用：\n\n```python\nimport byzerllm\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\nllm.setup_default_emb_model_name(\"emb\")\nllm.emb_query(\"你好\")\n```\n\n如果你配置 byzerllm 中的 Storage 使用，比如你这样启动了存储：\n\n```bash\nbyzerllm storage start --enable_emb\n```\n\n那么需要这么使用：\n\n```python\nfrom byzerllm.apps.byzer_storage.simple_api import ByzerStorage, DataType, FieldOption,SortOption\nstorage = ByzerStorage(\"byzerai_store\", \"memory\", \"memory\")\nstorage.emb(\"你好\")\n```\n\n## 一些辅助工具\n\n当调用 prompt 函数返回字符串的时候，如果想从里面抽取代码，可以使用如下方式：\n\n```python\nfrom byzerllm.utils.client import code_utils\ntext_with_markdown = '''\n```shell\nls -l\n```\n'''\ncode_blocks = code_utils.extract_code(text_with_markdown)\nfor code_block in code_blocks:\n    if code_block[0] == \"shell\":\n        print(code_block[1])\n##output: ls -l        \n```\n\n\n## 注意事项\n\n1. prompt函数方法体返回只能是dict，实际的返回类型和方法签名可以不一样，但是方法体返回只能是dict。\n2. 大部分情况prompt函数体为空，如果一定要有方法体，可以返回一个空字典。\n3. 调用prompt方法的时候，如果在@byzerllm.prompt()里没有指定llm对象，那么需要在调用的时候通过with_llm方法指定llm对象。\n\n===========================\n\n# Byzer Storage: 最易用的AI存储引擎\n\nByzer Storage是一个为AI应用设计的高性能存储引擎,它提供了简单易用的API,支持向量搜索、全文检索以及结构化查询。本文将详细介绍Byzer Storage的使用方法和主要特性。\n\n## 0. 安装和启动\n\n```bash\npip install byzerllm\nbyzerllm storage start\n```\n\nThat's it! Byzer Storage已经安装并启动成功,现在我们可以开始使用它了。默认会启动一个 byzerai_store 的集群。\n\n注意，如果你是这样启动的：\n\n```bash\nbyzerllm storage start --enable_emb\n```\n\n那么会自动启动一个 emb 模型，名字就叫 emb， ByzerStorage 会自动使用该模型，无需做任何其他配置。\n\n## 1. 初始化\n\n创建一个 ByzerStorage 对象，链接 byzerai_store 集群，并且指定数据库和表名（可以不存在）。\n\n```python\nfrom byzerllm.apps.byzer_storage.simple_api import ByzerStorage, DataType, FieldOption, SortOption\n\nstorage = ByzerStorage(\"byzerai_store\", \"my_database1\", \"my_table4s\")\n```\n\n\n## 2. 创建库表（可选）\n\nByzer Storage使用Schema来定义数据结构。我们可以使用SchemaBuilder来构建Schema:\n\n```python\n_ = (\n    storage.schema_builder()\n    .add_field(\"_id\", DataType.STRING)\n    .add_field(\"name\", DataType.STRING)\n    .add_field(\"content\", DataType.STRING, [FieldOption.ANALYZE])\n    .add_field(\"raw_content\", DataType.STRING, [FieldOption.NO_INDEX])    \n    .add_array_field(\"summary\", DataType.FLOAT)    \n    .add_field(\"created_time\", DataType.LONG, [FieldOption.SORT])    \n    .execute()\n)\n```\n\n这个Schema定义了以下字段:\n- `_id`: 字符串类型的主键\n- `name`: 字符串类型,可用于过滤条件\n- `content`: 字符串类型,带有ANALYZE选项,用于全文搜索\n- `raw_content`: 字符串类型,带有NO_INDEX选项,不会被索引\n- `summary`: 浮点数组类型,用于存储向量\n- `created_time`: 长整型,带有SORT选项,可用于排序\n\n需要注意的是：\n\n1. 如果一个字段带有ANALYZE选项,则该字段会被分词,并且可以用于全文搜索，但是就无法返回原始的文本了。所以你需要添加一个新字段专门用来存储原文，比如在我们这个例子里，我们新增了 raw_content 字段，并且显示的指定了 NO_INDEX 选项，这样就不会被索引，也就不会被分词，可以后续被检索后用来获取原始文本。\n2. 对于需要作为向量检索和存储的字段，需要指定为数组类型，比如我们这个例子里的 summary 字段。\n3. 如果你需要拿到向量字段的原始文本，那么你也需要添加一个新字段专门用来存储原文，就像我们这个例子里的 raw_content 字段一样。\n\n\n## 3. 写入数据\n\n准备数据并使用WriteBuilder写入Storage:\n\n```python\ndata = [\n    {\"_id\": \"1\", \"name\": \"Hello\", \"content\": \"Hello, world!\", \"raw_content\": \"Hello, world!\", \"summary\": \"hello world\", \"created_time\": 1612137600},\n    {\"_id\": \"2\", \"name\": \"Byzer\", \"content\": \"Byzer, world!\", \"raw_content\": \"Byzer, world!\", \"summary\": \"byzer\", \"created_time\": 1612137601},\n    {\"_id\": \"3\", \"name\": \"AI\", \"content\": \"AI, world!\", \"raw_content\": \"AI, world!\", \"summary\": \"AI\", \"created_time\": 16121376002},\n    {\"_id\": \"4\", \"name\": \"ByzerAI\", \"content\": \"ByzerAI, world!\", \"raw_content\": \"ByzerAI, world!\", \"summary\": \"ByzerAi\", \"created_time\": 16121376003},\n]\n\nstorage.write_builder().add_items(data, vector_fields=[\"summary\"], search_fields=[\"content\"]).execute()\nstorage.commit()\n```\n\n这里我们使用`add_items`方法批量添加数据,并指定了`summary`为向量字段,`content`为搜索字段。最后调用`commit()`来确保数据被持久化。\n\n\n从上面我们要写入到 byzer storage 的数据我们可以看到如下几个特点：\n\n1. 需要包含我们之前定义的 Schema中罗列的所有的字段，同时需要指定哪些是向量字段，哪些是检索字段。\n2. 向量和检索不能是同一个字段。\n3. 对于向量，检索字段，我们给到 write_builder 的都是文本，ByzerStorage 会根据 Schema 的定义自动将其转换为向量，检索字段需要的格式。\n\n## 4. 查询数据\n\nByzer Storage支持多种查询方式,包括向量搜索、全文检索、过滤和排序。\n\n### 4.1 向量搜索 + 全文检索\n\n```python\nquery = storage.query_builder()\nquery.set_vector_query(\"ByzerAI\", fields=[\"summary\"])\nresults = query.set_search_query(\"Hello\", fields=[\"content\"]).execute()\nprint(results)\n```\n\n这个查询结合了向量搜索和全文检索,它会在`summary`字段中搜索与\"ByzerAI\"相似的向量,同时在`content`字段中搜索包含\"Hello\"的文档。\n\n### 4.2 过滤 + 向量搜索 + 全文检索\n\n```python\nquery = storage.query_builder()\nquery.and_filter().add_condition(\"name\", \"AI\").build()\nquery.set_vector_query(\"ByzerAI\", fields=\"summary\")\nresults = query.set_search_query(\"Hello\", fields=[\"content\"]).execute()\nprint(results)\n```\n\n这个查询首先过滤`name`字段等于\"AI\"的文档,然后在结果中进行向量搜索和全文检索。\n\n### 4.3 过滤 + 排序\n\n```python\nquery = storage.query_builder()\nquery.and_filter().add_condition(\"name\", \"AI\").build().sort(\"created_time\", SortOption.DESC)\nresults = query.execute()\nprint(results)\n```\n\n这个查询过滤`name`字段等于\"AI\"的文档,然后按`created_time`字段降序排序。\n\n## 5. 删除数据\n\n### 5.1 根据ID删除\n\n```python\nstorage.delete_by_ids([\"3\"])\n\nquery = storage.query_builder()\nquery.and_filter().add_condition(\"name\", \"AI\").build()\nresults = query.execute()\nprint(results)\n```\n\n这里我们删除了ID为\"3\"的文档,然后查询验证删除结果。\n\n### 5.2 删除整个表\n\n```python\nstorage.drop()\n```\n\n这个操作会删除整个表及其所有数据,请谨慎使用。\n\n## 结论\n\nByzer Storage提供了一套简洁而强大的API,能够轻松实现向量搜索、全文检索、结构化查询等功能。它的设计非常适合AI应用场景,可以有效地存储和检索各种类型的数据。通过本文的介绍,相信读者已经对Byzer Storage有了基本的了解,并能够开始在自己的项目中使用这个强大的存储引擎。\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/cli.py\nimport argparse\nfrom byzerllm.utils.client import ByzerLLM\nfrom superanalysis.serve.api_server import serve, ServerArgs\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"ByzerLLM API Server\")\n    parser.add_argument(\"--host\", type=str, default=\"0.0.0.0\", help=\"Host to bind the server to\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"Port to bind the server to\")\n    parser.add_argument(\"--uvicorn-log-level\", type=str, default=\"info\", help=\"Uvicorn log level\")\n    parser.add_argument(\"--allow-credentials\", action=\"store_true\", help=\"Allow credentials\")\n    parser.add_argument(\"--allowed-origins\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed origins\")\n    parser.add_argument(\"--allowed-methods\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed methods\")\n    parser.add_argument(\"--allowed-headers\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed headers\")\n    parser.add_argument(\"--api-key\", type=str, default=\"\", help=\"API key for authentication\")\n    parser.add_argument(\"--served-model-name\", type=str, required=True, help=\"Name of the model to be served\")\n    parser.add_argument(\"--prompt-template\", type=str, default=\"\", help=\"Prompt template\")\n    parser.add_argument(\"--response-role\", type=str, default=\"assistant\", help=\"Response role\")\n    parser.add_argument(\"--ssl-keyfile\", type=str, default=\"\", help=\"SSL key file\")\n    parser.add_argument(\"--ssl-certfile\", type=str, default=\"\", help=\"SSL certificate file\")\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile\n    )\n\n    llm = ByzerLLM()\n    llm.setup_default_model_name(args.served_model_name)\n\n    serve(llm, server_args)\n\nif __name__ == \"__main__\":\n    main()\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/types.py\nfrom pydantic import BaseModel\n\nclass RAGConfig(BaseModel):\n    rag_base_url: str\n    rag_api_key: str\n\n     \n\n下面是用户的需求：\n\n@cli.py 支持 @/Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/types.py RAGConfig 里的参数\n"}, {"role": "assistant", "content": "To support the RAGConfig parameters in the cli.py file, we need to modify the main function to include these new arguments. Here's how we can update the cli.py file:\n\n```python\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/cli.py\n<<<<<<< SEARCH\nimport argparse\nfrom byzerllm.utils.client import ByzerLLM\nfrom superanalysis.serve.api_server import serve, ServerArgs\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"ByzerLLM API Server\")\n    parser.add_argument(\"--host\", type=str, default=\"0.0.0.0\", help=\"Host to bind the server to\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"Port to bind the server to\")\n    parser.add_argument(\"--uvicorn-log-level\", type=str, default=\"info\", help=\"Uvicorn log level\")\n    parser.add_argument(\"--allow-credentials\", action=\"store_true\", help=\"Allow credentials\")\n    parser.add_argument(\"--allowed-origins\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed origins\")\n    parser.add_argument(\"--allowed-methods\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed methods\")\n    parser.add_argument(\"--allowed-headers\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed headers\")\n    parser.add_argument(\"--api-key\", type=str, default=\"\", help=\"API key for authentication\")\n    parser.add_argument(\"--served-model-name\", type=str, required=True, help=\"Name of the model to be served\")\n    parser.add_argument(\"--prompt-template\", type=str, default=\"\", help=\"Prompt template\")\n    parser.add_argument(\"--response-role\", type=str, default=\"assistant\", help=\"Response role\")\n    parser.add_argument(\"--ssl-keyfile\", type=str, default=\"\", help=\"SSL key file\")\n    parser.add_argument(\"--ssl-certfile\", type=str, default=\"\", help=\"SSL certificate file\")\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile\n    )\n\n    llm = ByzerLLM()\n    llm.setup_default_model_name(args.served_model_name)\n\n    serve(llm, server_args)\n=======\nimport argparse\nfrom byzerllm.utils.client import ByzerLLM\nfrom superanalysis.serve.api_server import serve, ServerArgs\nfrom superanalysis.sql.types import RAGConfig\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"ByzerLLM API Server\")\n    parser.add_argument(\"--host\", type=str, default=\"0.0.0.0\", help=\"Host to bind the server to\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"Port to bind the server to\")\n    parser.add_argument(\"--uvicorn-log-level\", type=str, default=\"info\", help=\"Uvicorn log level\")\n    parser.add_argument(\"--allow-credentials\", action=\"store_true\", help=\"Allow credentials\")\n    parser.add_argument(\"--allowed-origins\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed origins\")\n    parser.add_argument(\"--allowed-methods\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed methods\")\n    parser.add_argument(\"--allowed-headers\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed headers\")\n    parser.add_argument(\"--api-key\", type=str, default=\"\", help=\"API key for authentication\")\n    parser.add_argument(\"--served-model-name\", type=str, required=True, help=\"Name of the model to be served\")\n    parser.add_argument(\"--prompt-template\", type=str, default=\"\", help=\"Prompt template\")\n    parser.add_argument(\"--response-role\", type=str, default=\"assistant\", help=\"Response role\")\n    parser.add_argument(\"--ssl-keyfile\", type=str, default=\"\", help=\"SSL key file\")\n    parser.add_argument(\"--ssl-certfile\", type=str, default=\"\", help=\"SSL certificate file\")\n    \n    # Add new arguments for RAGConfig\n    parser.add_argument(\"--rag-base-url\", type=str, default=\"\", help=\"RAG base URL\")\n    parser.add_argument(\"--rag-api-key\", type=str, default=\"\", help=\"RAG API key\")\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile\n    )\n\n    # Create RAGConfig instance\n    rag_config = RAGConfig(\n        rag_base_url=args.rag_base_url,\n        rag_api_key=args.rag_api_key\n    )\n\n    llm = ByzerLLM()\n    llm.setup_default_model_name(args.served_model_name)\n\n    serve(llm, server_args, rag_config)\n>>>>>>> REPLACE\n```\n\nThis modification adds support for the RAGConfig parameters in the cli.py file. Here's what we've changed:\n\n1. We've imported the RAGConfig class from superanalysis.sql.types.\n\n2. We've added two new command-line arguments:\n   - `--rag-base-url`: for the RAG base URL\n   - `--rag-api-key`: for the RAG API key\n\n3. We've created a RAGConfig instance using the values from these new arguments.\n\n4. We've modified the `serve` function call to include the `rag_config` parameter.\n\nNow, users can specify the RAG base URL and API key when running the CLI tool. For example:\n\n```\npython cli.py --served-model-name my_model --rag-base-url https://example.com/rag --rag-api-key my_api_key\n```\n\nNote that you'll also need to update the `serve` function in the api_server.py file to accept and use the `rag_config` parameter. The exact implementation will depend on how you want to use these RAG configuration values in your server logic."}], "model": "human", "yaml_file": "actions/107_chat_action.yml"}
{"instruction": "提供一个新的convert.py , 支持 super-analysis.convert  命令,支持两个参数 --data_dir --doc_dir, 递归读取 --data_dir 里的csv 文件,然后用pandas读取,然后提取scheme,最后生成 schema 文本写入到 --doc_dir,名字可以用文本md5", "conversations": [{"role": "user", "content": "\n如果你需要生成代码，对于每个需要更改的文件,你需要按 *SEARCH/REPLACE block* 的格式进行生成。\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The opening fence and code language, eg: ```python\n2. The file path alone on a line, starting with \"##File:\" and verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n*SEARCH/REPLACE* blocks will replace *all* matching occurrences.\nInclude enough lines to make the SEARCH blocks unique.\n\nInclude *ALL* the code being searched and replaced!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n下面我们来看一个例子：\n\n当前项目目录结构：\n1. 项目根目录： /tmp/projects/mathweb\n2. 项目子目录/文件列表(类似tree 命令输出)\nflask/\n    app.py\n    templates/\n        index.html\n    static/\n        style.css\n\n用户需求： Change get_factorial() to use math.factorial\n\n回答： To make this change we need to modify `/tmp/projects/mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n用户需求： Refactor hello() into its own file.\n\n回答：To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/hello.py\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/main.py\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n现在让我们开始一个新的任务:\n\n当前项目目录结构：\n1. 项目根目录： /Users/allwefantasy/projects/super_analysis\n2. 项目子目录/文件列表(类似tree 命令输出)：\nsetup.py\n.auto-coder/\n    libs/\n        llm_friendly_packages/\n            src/\n                example1/\n                    main.py\nsrc/\n    superanalysis/\n        serve/\n            openai_serve.py\n            __init__.py\n            llm_wrapper.py\n            api_server.py\n            cli.py\n        sql/\n            __init__.py\n            types.py\n            generator.py\n下面是一些文件路径以及每个文件对应的源码：\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/types.py\nfrom pydantic import BaseModel\n\nclass RAGConfig(BaseModel):\n    rag_base_url: str = \"\"\n    rag_api_key: str = \"\"\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/cli.py\nimport argparse\nimport byzerllm\nfrom byzerllm.utils.client import ByzerLLM\nfrom superanalysis.serve.api_server import serve, ServerArgs\nfrom superanalysis.sql.types import RAGConfig\nfrom superanalysis.sql.generator import SQLGenerator\nfrom superanalysis.serve.llm_wrapper import LLWrapper\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"ByzerLLM API Server\")\n    parser.add_argument(\"--host\", type=str, default=\"0.0.0.0\", help=\"Host to bind the server to\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"Port to bind the server to\")\n    parser.add_argument(\"--uvicorn-log-level\", type=str, default=\"info\", help=\"Uvicorn log level\")\n    parser.add_argument(\"--allow-credentials\", action=\"store_true\", help=\"Allow credentials\")\n    parser.add_argument(\"--allowed-origins\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed origins\")\n    parser.add_argument(\"--allowed-methods\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed methods\")\n    parser.add_argument(\"--allowed-headers\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed headers\")\n    parser.add_argument(\"--api-key\", type=str, default=\"\", help=\"API key for authentication\")\n    parser.add_argument(\"--served-model-name\", type=str, required=True, help=\"Name of the model to be served\")\n    parser.add_argument(\"--prompt-template\", type=str, default=\"\", help=\"Prompt template\")\n    parser.add_argument(\"--response-role\", type=str, default=\"assistant\", help=\"Response role\")\n    parser.add_argument(\"--ssl-keyfile\", type=str, default=\"\", help=\"SSL key file\")\n    parser.add_argument(\"--ssl-certfile\", type=str, default=\"\", help=\"SSL certificate file\")\n    \n    # Add new arguments for RAGConfig\n    parser.add_argument(\"--rag-base-url\", type=str, default=\"\", help=\"RAG base URL\")\n    parser.add_argument(\"--rag-api-key\", type=str, default=\"\", help=\"RAG API key\")\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile\n    )\n\n    # Create RAGConfig instance\n    rag_config = RAGConfig(\n        rag_base_url=args.rag_base_url,\n        rag_api_key=args.rag_api_key\n    )\n\n    llm = ByzerLLM()\n    llm.setup_default_model_name(args.served_model_name)\n    \n    byzerllm.connect_cluster(address=args.ray_address)\n    llm = byzerllm.ByzerLLM()\n    if not server_args.served_model_name:\n        raise ValueError(\"served_model_name is required\")\n    llm.setup_default_model_name(server_args.served_model_name)\n    rag = SQLGenerator(llm=llm)    \n    llm_wrapper = LLWrapper(llm=llm, rag=rag)\n    serve(llm_wrapper, server_args, rag_config)\n\nif __name__ == \"__main__\":\n    main()\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/__init__.py\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/__init__.py\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/llm_wrapper.py\nfrom typing import Any, Dict, List, Optional, Union,Callable\nfrom byzerllm.utils.client.types import (    \n    LLMFunctionCallResponse,\n    LLMClassResponse,LLMResponse    \n)\nimport pydantic\nfrom byzerllm import ByzerLLM\nfrom byzerllm.utils.client import LLMResponse\nfrom byzerllm.utils.types import SingleOutputMeta\nfrom byzerllm.utils.langutil import asyncfy_with_semaphore\nfrom superanalysis.sql.generator import SQLGenerator\nfrom loguru import logger\n\nclass LLWrapper:\n\n    def __init__(self,llm:ByzerLLM,entry:SQLGenerator):\n        self.entry = entry\n        self.llm = llm\n        \n\n    def chat_oai(self,\n                 conversations,\n                 tools:List[Union[Callable,str]]=[], \n                 tool_choice:Optional[Union[Callable,str]]=None,\n                 execute_tool:bool=False,  \n                 impl_func:Optional[Callable]=None,\n                 execute_impl_func:bool=False,\n                 impl_func_params:Optional[Dict[str,Any]]=None,\n                 func_params:Optional[Dict[str,Any]]=None,\n                 response_class:Optional[Union[pydantic.BaseModel,str]] = None, \n                 response_after_chat:Optional[Union[pydantic.BaseModel,str]] = False,\n                 enable_default_sys_message:bool=True,                 \n                 model:Optional[str] = None,\n                 role_mapping=None,llm_config:Dict[str,Any]={}\n                 )->Union[List[LLMResponse],List[LLMFunctionCallResponse],List[LLMClassResponse]]: \n        res,contexts = self.entry.stream_chat_oai(conversations)\n        s = \"\".join(res)        \n        return [LLMResponse(output=s,metadata={},input=\"\")]\n\n    def stream_chat_oai(self,conversations, \n                        model:Optional[str]=None, \n                        role_mapping=None,\n                        delta_mode=False,\n                        llm_config:Dict[str,Any]={}): \n        res,contexts = self.entry.stream_chat_oai(conversations)        \n        for t in res:                        \n            yield (t,SingleOutputMeta(0,0))\n\n    async def async_stream_chat_oai(self,conversations, \n                        model:Optional[str]=None, \n                        role_mapping=None,\n                        delta_mode=False,\n                        llm_config:Dict[str,Any]={}): \n        res,contexts = self.rag.stream_chat_oai(conversations)                \n        for t in res:                                    \n            yield (t,SingleOutputMeta(0,0))\n                         \n\n    def __getattr__(self, name):        \n        return getattr(self.llm, name)\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/openai_serve.py\nimport argparse\nfrom service_server.api_server import ServerArgs, serve\nfrom service_server.llm_wrapper import LLWrapper\nfrom service_server.summary_rag import SummaryRAG\nimport byzerllm\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"OpenAI Serve\")\n    parser.add_argument(\n        \"--model\",\n        default=\"\",\n    )\n    parser.add_argument(\"--emb_model\", default=\"\")\n    parser.add_argument(\n        \"--ray_address\",\n        default=\"auto\",\n    )\n    parser.add_argument(\"--source_dir\", default=\".\", help=\"\")\n    parser.add_argument(\"--host\", default=\"\", help=\"\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"\")\n    parser.add_argument(\"--uvicorn_log_level\", default=\"info\", help=\"\")\n    parser.add_argument(\"--allow_credentials\", action=\"store_true\", help=\"\")\n    parser.add_argument(\"--allowed_origins\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--allowed_methods\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--allowed_headers\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--api_key\", default=\"\", help=\"\")\n    parser.add_argument(\"--served_model_name\", default=\"\", help=\"\")\n    parser.add_argument(\"--prompt_template\", default=\"\", help=\"\")\n    parser.add_argument(\"--ssl_keyfile\", default=\"\", help=\"\")\n    parser.add_argument(\"--ssl_certfile\", default=\"\", help=\"\")\n    parser.add_argument(\"--response_role\", default=\"assistant\", help=\"\")\n    parser.add_argument(\n        \"--collections\", default=\"\", help=\"Collection name for indexing\"\n    )\n    parser.add_argument(\n        \"--base_dir\",\n        default=\"\",\n        help=\"Path where the processed text embeddings were stored\",\n    )\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name or args.model,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile,\n    )\n    byzerllm.connect_cluster(address=args.ray_address)\n    llm = byzerllm.ByzerLLM()\n    if not server_args.served_model_name:\n        raise ValueError(\"served_model_name is required\")\n    llm.setup_default_model_name(server_args.served_model_name)\n    rag = SummaryRAG(llm=llm)    \n    llm_wrapper = LLWrapper(llm=llm, rag=rag)\n    serve(llm=llm_wrapper, args=server_args)\n\nif __name__ == \"__main__\":\n    main()\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/api_server.py\nimport os\nimport time\nimport uvicorn\nfrom fastapi import Request\nfrom fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import JSONResponse, StreamingResponse, Response\n\nfrom byzerllm.log import init_logger\nfrom byzerllm.utils import random_uuid\nfrom byzerllm.version import __version__ as version\nfrom byzerllm.utils.client import ByzerLLM, LLMRequest\nfrom byzerllm.utils.client.entrypoints.openai.serving_chat import OpenAIServingChat\nfrom byzerllm.utils.client.entrypoints.openai.serving_completion import OpenAIServingCompletion\nfrom byzerllm.utils.client.entrypoints.openai.protocol import (\n    ModelList,\n    ModelCard,\n    ModelPermission,\n    ChatCompletionRequest,\n    ErrorResponse,\n    CompletionRequest,\n    Embeddings,\n    EmbeddingsOutput,\n    EmbeddingsData,\n    EmbeddingsUsage,\n)\nfrom pydantic import BaseModel\nfrom typing import List\n\nlogger = init_logger(__name__)\n\nllm_client: ByzerLLM = None\nopenai_serving_chat: OpenAIServingChat = None\nopenai_serving_completion: OpenAIServingCompletion = None\n\nTIMEOUT_KEEP_ALIVE = 5  # seconds\n# timeout in 10 minutes. Streaming can take longer than 3 min\nTIMEOUT = float(os.environ.get(\"BYZERLLM_APISERVER_HTTP_TIMEOUT\", 600))\n\nrouter_app = FastAPI()\n\n\n@router_app.get(\"/health\")\nasync def health() -> Response:\n    \"\"\"Health check.\"\"\"\n    return Response(status_code=200)\n\n\n@router_app.get(\"/v1/models\")\nasync def show_available_models():\n    models = await openai_serving_chat.show_available_models()\n    return JSONResponse(content=models.model_dump())\n\n\n@router_app.get(\"/version\")\nasync def show_version():\n    return JSONResponse(content={\"version\": version})\n\n\n@router_app.get(\"/v1/models\", response_model=ModelList)\nasync def models() -> ModelList:\n    \"\"\"Show available models. Right now we only have one model.\"\"\"\n    model_cards = [\n        ModelCard(\n            id=\"\",\n            root=\"\",\n            permission=[ModelPermission()]\n        )\n    ]\n    return ModelList(data=model_cards)\n\n\n@router_app.post(\"/v1/completions\")\nasync def create_completion(\n        body: CompletionRequest,\n        request: Request\n):\n    generator = await openai_serving_completion.create_completion(body, request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            status_code=generator.code\n        )\n    if body.stream:\n        return StreamingResponse(\n            content=generator,\n            media_type=\"text/event-stream\"\n        )\n    else:\n        return JSONResponse(content=generator.model_dump())\n\n\n@router_app.post(\"/v1/chat/completions\")\nasync def create_chat_completion(\n        body: ChatCompletionRequest,\n        request: Request,\n):\n    \"\"\"Completion API similar to OpenAI's API.\n\n    See  https://platform.openai.com/docs/api-reference/chat/create\n    for the API specification. This API mimics the OpenAI ChatCompletion API.\n\n    NOTE: Currently we do not support the following features:\n        - function_call (Users should implement this by themselves)\n        - logit_bias (to be supported by vLLM engine)\n    \"\"\"\n    # async with async_timeout.timeout(TIMEOUT):\n\n    generator = await openai_serving_chat.create_chat_completion(body, request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            status_code=generator.code\n        )\n    if body.stream:\n        return StreamingResponse(\n            content=generator,\n            media_type=\"text/event-stream\"\n        )\n    else:\n        return JSONResponse(content=generator.model_dump())\n\n\n@router_app.post(\"/v1/embeddings\")\nasync def embed(body: Embeddings):\n    \"\"\"Given a prompt, the model will return one embedding.\n\n    Returns:\n        A response object with an embedding.\n    \"\"\"\n    embedding_id = f\"embed-{random_uuid()}\"\n\n    results_list = llm_client.emb(body.model, request=LLMRequest(instruction=body.input))\n    tokens = 0\n\n    return EmbeddingsOutput(\n        data=[\n            EmbeddingsData(\n                embedding=results.output,\n                index=i,\n                object=\"embedding\",\n            )\n            for i, results in enumerate(results_list)\n        ],\n        id=embedding_id,\n        object=\"list\",\n        created=int(time.time()),\n        model=body.model,\n        usage=EmbeddingsUsage(\n            prompt_tokens=tokens,\n            total_tokens=tokens,\n        ),\n    )\n\nclass ServerArgs(BaseModel):\n    host: str = None\n    port: int = 8000\n    uvicorn_log_level: str = \"info\"\n    allow_credentials: bool = False\n    allowed_origins: List[str] = [\"*\"]  \n    allowed_methods: List[str] = [\"*\"]\n    allowed_headers: List[str] = [\"*\"]\n    api_key: str = None\n    served_model_name: str = None\n    prompt_template: str = None\n    response_role: str = \"assistant\"\n    ssl_keyfile: str = None\n    ssl_certfile: str = None    \n\ndef serve(llm:ByzerLLM, args: ServerArgs,rag_config):\n    \n    logger.info(f\"ByzerLLM API server version {version}\")\n    logger.info(f\"args: {args}\")\n\n    router_app.add_middleware(\n        CORSMiddleware,\n        allow_origins=args.allowed_origins,\n        allow_credentials=args.allow_credentials,\n        allow_methods=args.allowed_methods,\n        allow_headers=args.allowed_headers,\n    )\n    \n    if token := os.environ.get(\"BYZERLLM_API_KEY\") or args.api_key:\n\n        @router_app.middleware(\"http\")\n        async def authentication(request: Request, call_next):\n            if not request.url.path.startswith(\"/v1\"):\n                return await call_next(request)\n            if request.headers.get(\"Authorization\") != \"Bearer \" + token:\n                return JSONResponse(\n                    content={\"error\": \"Unauthorized\"},\n                    status_code=401\n                )\n            return await call_next(request)\n\n    # Register labels for metrics\n    # add_global_metrics_labels(model_name=engine_args.model)\n    global llm_client\n    llm_client = llm\n    \n    global openai_serving_chat\n    openai_serving_chat = OpenAIServingChat(\n        llm_client=llm_client,\n        response_role=args.response_role,\n        server_model_name=args.served_model_name,\n        prompt_template=args.prompt_template\n    )\n    global openai_serving_completion\n    openai_serving_completion = OpenAIServingCompletion(\n        llm_client=llm_client,\n        server_model_name=args.served_model_name,\n        prompt_template=args.prompt_template\n    )\n\n    uvicorn.run(\n        router_app,\n        host=args.host,\n        port=args.port,\n        log_level=args.uvicorn_log_level,\n        timeout_keep_alive=TIMEOUT_KEEP_ALIVE,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile\n    )\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/generator.py\nfrom typing import Optional, Dict, Any\nfrom openai import OpenAI\nimport json\nfrom .types import RAGConfig\nfrom byzerllm.utils.client import code_utils\n\n\nclass SQLGenerator:\n\n    def __init__(self, rag_config: RAGConfig) -> None:        \n        self.rag_client = OpenAI(api_key=rag_config.rag_api_key, base_url=rag_config.rag_base_url)\n\n    def stream_chat_oai(\n        self,\n        conversations,\n        model: Optional[str] = None,\n        role_mapping=None,\n        delta_mode=False,\n        llm_config: Dict[str, Any] = {},\n    ):\n        if not model:\n            model = \"deepseek_chat\"\n        response = self.rag_client.chat.completions.create(\n        messages=conversations,\n        model=model,\n    )\n        sql_str = response.choices[0].message.content\n        pass\n\n\n\n##File: /Users/allwefantasy/projects/super_analysis/setup.py\nfrom setuptools import setup, find_packages\n\nsetup(\n    name=\"super-analysis\",\n    version=\"0.1.0\",\n    packages=find_packages(where=\"src\"),\n    package_dir={\"\": \"src\"},\n    install_requires=[\n        \"byzerllm\",\n        \"fastapi\",\n        \"uvicorn\",\n        \"argparse\",\n    ],\n    entry_points={\n        \"console_scripts\": [\n            \"super-analysis.serve=superanalysis.serve.cli:main\",\n        ],\n    },\n    author=\"allwefantasy\",\n    author_email=\"allwefantasy@gmail.com\",\n    description=\"A package for super analysis with ByzerLLM\",\n    long_description=open(\"README.md\").read(),\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/yourusername/super-analysis\",\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n    ],\n    python_requires='>=3.6',\n)\n\n##File: .auto-coder/libs/llm_friendly_packages/github.com/allwefantasy/byzer-llm/README.md\n# byzerllm 大模型编程快速指南\n\n本文示例 [Notebook](../../notebooks/003_byzerllm_大模型编程快速指南.ipynb)\n\n## 安装\n\n```bash\npip install -U byzerllm\nray start --head\n```\n## 启动一个模型代理\n\nbyzerllm 支持私有化模型或者SaaS模型的部署。\n\n这里以 deepseek 官方API 为例：\n\n```bash\neasy-byzerllm deploy deepseek-chat --token xxxxx --alias deepseek_chat\n```\n\n或者跬基流动API:\n\n```bash\neasy-byzerllm deploy alibaba/Qwen1.5-110B-Chat --token xxxxx --alias qwen110b_chat\n```\n\n将上面的 API KEY 替换成你们自己的。\n\n如果你想部署私有化模型或者对接 Ollama 等更多需求，参考 [002_使用byzerllm进行模型部署.md](./002_使用byzerllm进行模型部署.md) 或者 [README.md](../../README.md)。\n\n之后，你就可以在代码里使用  deepseek_chat 或者 qwen110b_chat  访问模型了。\n\n## hello world\n\n来和我们的大模型打个招呼:\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\n@byzerllm.prompt(llm=llm)\ndef hello(q:str) ->str:\n    '''\n    你好, {{ q }}\n    '''\n\ns = hello(\"你是谁\")    \nprint(s)\n\n## 输出:\n## '你好！我是一个人工智能助手，专门设计来回答问题、提供信息和帮助解决问题。如果你有任何疑问或需要帮助，请随时告诉我。'\n```\n\n恭喜，你和大模型成功打了招呼！\n\n可以看到，我们通过 `@byzerllm.prompt` 装饰器，将一个方法转换成了一个大模型的调用，然后这个方法的主题是一段文本，文本中\n使用了 jinja2 模板语法，来获得方法的参数。当正常调用该方法时，实际上就发起了和大模型的交互，并且返回了大模型的结果。\n\n在 byzerllm 中，我们把这种方法称之为 prompt 函数。\n\n## 查看发送给大模型的prompt\n\n很多情况你可能需要调试，查看自己的 prompt 渲染后到底是什么样子的，这个时候你可以通过如下方式\n获取渲染后的prompt:\n\n```python\nhello.prompt(\"你是谁\")\n## '你好, 你是谁'\n```            \n\n## 动态换一个模型\n\n前面的 hello 方法在初始化的时候，我们使用了默认的模型 deepseek_chat，如果我们想换一个模型，可以这样做：\n\n```python\nhello.with_llm(llm).run(\"你是谁\")\n## '你好！我是一个人工智能助手，专门设计来回答问题、提供信息和帮助解决问题。如果你有任何疑问或需要帮助，请随时告诉我。'\n```\n\n通过 with_llm 你可以设置一个新的 llm 对象，然后调用 run 方法，就可以使用新的模型了。\n\n## 超长文本生成\n\n我们知道，大模型一次生成的长度其实是有限的，如果你想生成超长文本，你可能需手动的不断获得\n生成结果，然后把他转化为输入，然后再次生成，这样的方式是比较麻烦的。\n\nbyzerllm 提供了更加易用的 API :\n\n```python\nimport byzerllm\nfrom byzerllm import ByzerLLM\n\nllm = ByzerLLM.from_default_model(\"deepseek_chat\")\n\n@byzerllm.prompt()\ndef tell_story() -> str:\n    \"\"\"\n    讲一个100字的故事。    \n    \"\"\"\n\n\ns = (\n    tell_story.with_llm(llm)\n    .with_response_markers()\n    .options({\"llm_config\": {\"max_length\": 10}})\n    .run()\n)\nprint(s)\n\n## 从前，森林里住着一只聪明的小狐狸。一天，它发现了一块闪闪发光的宝石。小狐狸决定用这块宝石帮助森林里的动物们。它用宝石的光芒指引迷路的小鹿找到了回家的路，用宝石的温暖治愈了受伤的小鸟。从此，小狐狸成了森林里的英雄，动物们都感激它的善良和智慧。\n```\n\n实际核心部分就是这一行：\n\n```python\ntell_story.with_llm(llm)\n    .with_response_markers()    \n    .run()\n```\n\n我们只需要调用 `with_response_markers` 方法，系统就会自动的帮我们生成超长文本。\n在上面的案例中，我们通过\n\n```python\n.options({\"llm_config\": {\"max_length\": 10}})\n```\n\n认为的限制大模型一次交互最多只能输出10个字符，但是系统依然自动完成了远超过10个字符的文本生成。\n\n## 对象输出\n\n前面我们的例子都是返回字符串，但是我们也可以返回对象，这样我们就可以更加灵活的处理返回结果。\n\n```python\nimport pydantic \n\nclass Story(pydantic.BaseModel):\n    '''\n    故事\n    '''\n\n    title: str = pydantic.Field(description=\"故事的标题\")\n    body: str = pydantic.Field(description=\"故事主体\")\n\n@byzerllm.prompt()\ndef tell_story()->Story:\n    '''\n    讲一个100字的故事。    \n    '''\n\ns = tell_story.with_llm(llm).run()\nprint(isinstance(s, Story))\nprint(s.title)\n\n## True\n## 勇敢的小鸟\n```\n\n可以看到，我们很轻松的将输出转化为格式化输出。\n\n## 自定义字段抽取\n\n前面的结构化输出，其实会消耗更多token,还有一种更加精准的结构化输出方式。\n比如让大模型生成一个正则表达式，但实际上大模型很难准确只输出一个正则表达式，这个时候我们可以通过自定义抽取函数来获取我们想要的结果。\n\n\n```python\nfrom loguru import logger\nimport re\n\n@byzerllm.prompt()\ndef generate_regex_pattern(desc: str) -> str:\n    \"\"\"\n    根据下面的描述生成一个正则表达式，要符合python re.compile 库的要求。\n\n    {{ desc }}\n\n    最后生成的正则表达式要在<REGEX></REGEX>标签对里。\n    \"\"\"    \n\ndef extract_regex_pattern(regex_block: str) -> str:    \n    pattern = re.search(r\"<REGEX>(.*)</REGEX>\", regex_block, re.DOTALL)\n    if pattern is None:\n        logger.warning(\"No regex pattern found in the generated block:\\n {regex_block}\")\n        raise None\n    return pattern.group(1)\n\npattern = \"匹配一个邮箱地址\"\nv = generate_regex_pattern.with_llm(llm).with_extractor(extract_regex_pattern).run(desc=pattern)\nprint(v)\n## ^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\n```\n\n在上面的例子里，我们根据一句话生成一个正则表达式。我们通过 `with_extractor` 方法，传入了一个自定义的抽取函数，这个函数会在大模型生成结果后，对结果进行处理，然后返回我们想要的结果。\n\n我们在 prompt 明确说了，生成的结果要放到 `<REGEX></REGEX>` 标签对里，然后我们通过 extract_regex_pattern 函数，从结果中提取出了我们想要的正则表达式。\n\n## 在实例方法中使用大模型\n\n```python\nimport byzerllm\ndata = {\n    'name': 'Jane Doe',\n    'task_count': 3,\n    'tasks': [\n        {'name': 'Submit report', 'due_date': '2024-03-10'},\n        {'name': 'Finish project', 'due_date': '2024-03-15'},\n        {'name': 'Reply to emails', 'due_date': '2024-03-08'}\n    ]\n}\n\n\nclass RAG():\n    def __init__(self):        \n        self.llm = byzerllm.ByzerLLM()\n        self.llm.setup_template(model=\"deepseek_chat\",template=\"auto\")\n        self.llm.setup_default_model_name(\"deepseek_chat\")        \n    \n    @byzerllm.prompt(lambda self:self.llm)\n    def generate_answer(self,name,task_count,tasks)->str:\n        '''\n        Hello {{ name }},\n\n        This is a reminder that you have {{ task_count }} pending tasks:\n        {% for task in tasks %}\n        - Task: {{ task.name }} | Due: {{ task.due_date }}\n        {% endfor %}\n\n        Best regards,\n        Your Reminder System\n        '''        \n\nt = RAG()\n\nresponse = t.generate_answer(**data)\nprint(response)\n\n## 输出:\n## Hello Jane Doe,\n##I hope this message finds you well. I wanted to remind you of your 3 pending tasks to ensure you stay on track:\n## 1. **Submit report** - This task is due on **2024-03-10**. Please ensure that you allocat\n```\n\n这里我们给了个比较复杂的例子，但我们可以看到，给一个实例prompt方法和普通prompt 方法差异不大。\n唯一的区别是如果你希望在定义的时候就指定大模型，使用一个lambda函数返回实例的 llm 对象即可。\n\n```python\n@byzerllm.prompt(lambda self:self.llm)\n```\n\n你也可以不返回，在调用的时候通过 `with_llm` 方法指定 llm 对象。\n\n此外，这个例子也展示了如何通过jinja2模板语法，来处理复杂的结构化数据。\n\n## 通过 Python 代码处理复杂入参\n\n上面的一个例子中，我们通过 jinja2 模板语法，来处理复杂的结构化数据，但是有时候我们可能需要更加复杂的处理，这个时候我们可以通过 Python 代码来处理。\n\n```python\nimport byzerllm\n\ndata = {\n    'name': 'Jane Doe',\n    'task_count': 3,\n    'tasks': [\n        {'name': 'Submit report', 'due_date': '2024-03-10'},\n        {'name': 'Finish project', 'due_date': '2024-03-15'},\n        {'name': 'Reply to emails', 'due_date': '2024-03-08'}\n    ]\n}\n\n\nclass RAG():\n    def __init__(self):        \n        self.llm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n    \n    @byzerllm.prompt()\n    def generate_answer(self,name,task_count,tasks)->str:\n        '''\n        Hello {{ name }},\n\n        This is a reminder that you have {{ task_count }} pending tasks:\n            \n        {{ tasks }}\n\n        Best regards,\n        Your Reminder System\n        '''\n        \n        tasks_str = \"\\n\".join([f\"- Task: {task['name']} | Due: { task['due_date'] }\" for task in tasks])\n        return {\"tasks\": tasks_str}\n\nt = RAG()\n\nresponse = t.generate_answer.with_llm(t.llm).run(**data)\nprint(response)\n\n## Just a gentle nudge to keep you on track with your pending tasks. Here's a quick recap:....\n```\n\n在这个例子里，我们直接把 tasks 在方法体里进行处理，然后作为一个字符串返回，最够构建一个字典，字典的key为 tasks,然后\n你就可以在 docstring 里使用 `{{ tasks }}` 来引用这个字符串。\n\n这样对于很复杂的入参，就不用谢繁琐的 jinja2 模板语法了。\n\n## 如何自动实现一个方法\n\n比如我定义一个签名，但是我不想自己实现里面的逻辑，让大模型来实现。这个在 byzerllm 中叫 function impl。我们来看看怎么\n实现:\n\n```python\nimport pydantic\nclass Time(pydantic.BaseModel):\n    time: str = pydantic.Field(...,description=\"时间，时间格式为 yyyy-MM-dd\")\n\n\n@llm.impl()\ndef calculate_current_time()->Time:\n    '''\n    计算当前时间\n    '''\n    pass \n\n\ncalculate_current_time()\n#output: Time(time='2024-06-14')\n```\n\n在这个例子里，我们定义了一个 calculate_current_time 方法，但是我们没有实现里面的逻辑，我们通过 `@llm.impl()` 装饰器，让大模型来实现这个方法。\n为了避免每次都要“生成”这个方法，导致无法适用，我们提供了缓存，用户可以按如下方式打印速度：\n\n```python\nstart = time.monotonic()\ncalculate_current_time()\nprint(f\"first time cost: {time.monotonic()-start}\")\n\nstart = time.monotonic()\ncalculate_current_time()\nprint(f\"second time cost: {time.monotonic()-start}\")\n\n# output:\n# first time cost: 6.067266260739416\n# second time cost: 4.347506910562515e-05\n```\n可以看到，第一次执行花费了6s,第二次几乎是瞬间完成的，这是因为第一次执行的时候，我们实际上是在生成这个方法，第二次执行的时候，我们是执行已经生成好的代码，所以速度会非常快。你可以显示的调用 `llm.clear_impl_cache()` 清理掉函数缓存。\n\n## Stream 模式\n\n前面的例子都是一次性生成结果，但是有时候我们可能需要一个流式的输出，这个时候我们可能需要用底层一点的API来完成了：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\nv = llm.stream_chat_oai(model=\"deepseek_chat\",conversations=[{\n    \"role\":\"user\",\n    \"content\":\"你好，你是谁\",\n}],delta_mode=True)\n\nfor t in v:\n    print(t,flush=True)  \n\n# 你好\n# ！\n# 我\n# 是一个\n# 人工智能\n# 助手\n# ，\n# 旨在\n# 提供\n# 信息\n# 、\n# 解答\n# 问题....\n```\n\n如果你不想要流式输出，但是想用底层一点的API，你可以使用 `llm.chat_oai` 方法：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\nv = llm.chat_oai(model=\"deepseek_chat\",conversations=[{\n    \"role\":\"user\",\n    \"content\":\"你好，你是谁\",\n}])\n\nprint(v[0].output)\n## 你好！我是一个人工智能助手，旨在提供信息、解答问题和帮助用户解决问题。如果你有任何问题或需要帮助，请随时告诉我。\n```\n\n## Function Calling \n\nbyzerllm 可以不依赖模型自身就能提供 function calling 支持，我们来看个例子：\n\n\n```python\nfrom typing import List,Dict,Any,Annotated\nimport pydantic \nimport datetime\nfrom dateutil.relativedelta import relativedelta\n\ndef compute_date_range(count:Annotated[int,\"时间跨度，数值类型\"],\n                       unit:Annotated[str,\"时间单位，字符串类型\",{\"enum\":[\"day\",\"week\",\"month\",\"year\"]}])->List[str]:\n    '''\n    计算日期范围\n\n    Args:\n        count: 时间跨度，数值类型\n        unit: 时间单位，字符串类型，可选值为 day,week,month,year\n    '''        \n    now = datetime.datetime.now()\n    now_str = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n    if unit == \"day\":\n        return [(now - relativedelta(days=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"week\":\n        return [(now - relativedelta(weeks=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"month\":\n        return [(now - relativedelta(months=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"year\":\n        return [(now - relativedelta(years=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    return [\"\",\"\"]\n\ndef compute_now()->str:\n    '''\n    计算当前时间\n    '''\n    return datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n```\n\n我们定义了两个方法，一个是计算日期范围，一个是计算当前时间。\n\n现在我么可以来测试下，系统如何根据自然语言决定调用哪个方法：\n\n```python\nt = llm.chat_oai([{\n    \"content\":'''计算当前时间''',\n    \"role\":\"user\"    \n}],tools=[compute_date_range,compute_now],execute_tool=True)\n\nt[0].values\n\n## output: ['2024-06-14 15:18:02']\n```\n\n我们可以看到，他正确的选择了 compute_now 方法。\n\n接着我们再试一个：\n\n```python\nt = llm.chat_oai([{\n    \"content\":'''最近三个月趋势''',\n    \"role\":\"user\"    \n}],tools=[compute_date_range,compute_now],execute_tool=True)\n\nt[0].values\n\n## output: [['2024-03-14 15:19:13', '2024-06-14 15:19:13']]\n```\n\n模型正确的选择了 compute_date_range 方法。\n\n## 多模态\n\nbyerllm 也能很好的支持多模态的交互，而且统一了多模态大模型的接口，比如你可以用一样的方式使用 openai 或者 claude 的图片转文字能力， 或者一致的方式使用火山，azuer, openai的语音合成接口。\n\n### 图生文\n\n```python\nimport byzerllm\nfrom byzerllm.types import ImagePath\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\n\n\n@byzerllm.prompt()\ndef what_in_image(image_path: ImagePath) -> str:\n    \"\"\"\n    {{ image_path }}\n    这个图片里有什么？\n    \"\"\"    \n\n\nv = what_in_image.with_llm(vl_llm).run(\n    ImagePath(value=\"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\")\n)\nv\n## OUTPUT: 这张图片展示了多只可爱的猫咪，采用了艺术风格的绘画。猫咪们有不同的颜色和花纹，背景是浅棕色，上面还点缀着一些红色的花朵。整体画面给人一种温馨和谐的感觉\n```\n\n可以看到，我们只需要把 prompt 函数的图片地址入参使用 byzerllm.types.ImagePath里进行包装，就可以直接在 prompt 函数体里\n带上图片。\n\n或者你可以这样：\n\n```python\nimport byzerllm\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\n\n\n@byzerllm.prompt()\ndef what_in_image(image_path: str) -> str:\n    \"\"\"\n    {{ image }}\n    这个图片里有什么？\n    \"\"\"\n    return {\"image\": byzerllm.Image.load_image_from_path(image_path)}\n\n\nv = what_in_image.with_llm(vl_llm).run(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv\n```\n\n通过 `image_path` 参数，然后通过 `byzerllm.Image.load_image_from_path` 方法，转化为一个图片对象 image，最后在 prompt 函数体里\n使用 `{{ image }}` 引用这个图片对象。\n\n另外我们也是可以支持配置多张图片的。\n\n另外，我们也可以使用基础的 `llm.chat_oai` 方法来实现：\n\n```python\nimport byzerllm\nimport json\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\nimage = byzerllm.Image.load_image_from_path(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv = vl_llm.chat_oai(\n    conversations=[\n        {\n            \"role\": \"user\",\n            \"content\": json.dumps(\n                [{\"image\": image, \"text\": \"这个图片里有什么？\"}], ensure_ascii=False\n            ),\n        }\n    ]\n)\nv[0].output\n```\n\n还可以这么写：\n\n```python\nimport byzerllm\nimport json\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\nimage = byzerllm.Image.load_image_from_path(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv = vl_llm.chat_oai(\n    conversations=[\n        {\n            \"role\": \"user\",\n            \"content\": json.dumps(\n                [\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\"url\": image, \"detail\": \"high\"},\n                    },\n                    {\"text\": \"这个图片里有什么？\", \"type\": \"text\"},\n                ],\n                ensure_ascii=False,\n            ),\n        }\n    ]\n)\nv[0].output\n```\n\n### 语音合成\n\n这里以 openai 的 tts 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=tts-1 \\\n--model openai_tts\n```\n\n此外，byzerllm 支持 azure,火山引擎等 tts 语音合成引擎。\n\n接着你可以这么用：\n\n```python\nimport byzerllm\nimport base64\nimport json\n\nllm = byzerllm.ByzerLLM.from_default_model(\"openai_tts\")\n\n\nt = llm.chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\": json.dumps({\n        \"input\":\"hello, open_tts\",\n        \"voice\": \"alloy\",\n        \"response_format\": \"mp3\"\n    },ensure_ascii=False)\n}])\n\nwith open(\"voice.mp3\",\"wb\") as f:\n    f.write(base64.b64decode(t[0].output))\n```\n\ntts 模型生成没有prompt函数可以用，你需要直接使用 chat_oai。\n\n\n### 语音识别\n\n这里以 openai 的 whisper-1 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--worker_concurrency 10 \\\n--infer_params saas.model=whisper-1 saas.api_key=${MODEL_OPENAI_TOKEN} \\\n--model speech_to_text\n```\n\n语音识别的使用方式和图生文类似，我们可以直接在 prompt 函数体里带上音频文件。\n\n```python\nimport byzerllm\nimport json\nimport base64\nfrom byzerllm.types import AudioPath\n\nllm = byzerllm.ByzerLLM.from_default_model(\"speech_to_text\")\n\naudio_file = \"/Users/allwefantasy/videos/output_audio.mp3\"\n\n@byzerllm.prompt(llm=llm)\ndef audio_to_text(audio_file: AudioPath):\n    \"\"\"\n    {{ audio_file }}\n    \"\"\"\n\nv = audio_to_text(AudioPath(value=audio_file))\njson.loads(v)\n```\n输出的数据格式略微复杂：\n\n```\n{'text': 'In the last chapter, you and I started to step through the internal workings of a transformer. This is one of the key pieces of technology inside large language models, and a lot of other tools in the modern wave of AI.',\n 'task': 'transcribe',\n 'language': 'english',\n 'duration': 10.0,\n 'segments': [{'id': 0,\n   'seek': 0,\n   'start': 0.0,\n   'end': 4.78000020980835,\n   'text': ' In the last chapter, you and I started to step through the internal workings of a transformer.',\n   'tokens': [50364,\n    682,\n    264,\n    1036,\n    7187,\n    11,\n    291,\n    293,\n    286,\n    1409,\n    281,\n    1823,\n    807,\n    264,\n    6920,\n    589,\n    1109,\n    295,\n    257,\n    31782,\n    13,\n    50586],\n   'temperature': 0.0,\n   'avg_logprob': -0.28872039914131165,\n   'compression_ratio': 1.4220778942108154,\n   'no_speech_prob': 0.016033057123422623},\n  ....\n  {'id': 2,\n   'seek': 0,\n   'start': 8.579999923706055,\n   'end': 9.979999542236328,\n   'text': ' and a lot of other tools in the modern wave of AI.',\n   'tokens': [50759,\n    293,\n    257,\n    688,\n    295,\n    661,\n    3873,\n    294,\n    264,\n    4363,\n    5772,\n    295,\n    7318,\n    13,\n    50867],\n   'temperature': 0.0,\n   'avg_logprob': -0.28872039914131165,\n   'compression_ratio': 1.4220778942108154,\n   'no_speech_prob': 0.016033057123422623}],\n 'words': [{'word': 'In', 'start': 0.0, 'end': 0.18000000715255737},\n  {'word': 'the', 'start': 0.18000000715255737, 'end': 0.23999999463558197},\n  {'word': 'last', 'start': 0.23999999463558197, 'end': 0.5400000214576721},\n  {'word': 'chapter', 'start': 0.5400000214576721, 'end': 0.800000011920929},\n  ....\n  {'word': 'AI', 'start': 9.920000076293945, 'end': 9.979999542236328}]}\n```\n\n会输出每一句话以及每一个字所在的起始时间和截止时间。你可以根据需要来使用。\n\n\n### 文生图\n\n文生图和语音合成类似，首先要启动合适的模型,以openai 的 dall-e-3 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=dall-e-3 \\\n--model openai_image_gen\n```\n\n启动模型后，只需要记住几个模板参数即可使用，这里直接使用 chat_oai 方法来使用：\n\n```python\n\nimport byzerllm\nimport json\nimport base64\n\nllm = byzerllm.ByzerLLM.from_default_model(\"openai_image_gen\")\nt = llm.chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\": json.dumps({\n        \"input\":\"a white siamese cat\",\n        \"size\": \"1024x1024\",\n        \"quality\": \"standard\"\n    },ensure_ascii=False)\n}])\n\nwith open(\"output1.jpg\",\"wb\") as f:\n    f.write(base64.b64decode(t[0].output))\n\n\nimport matplotlib.pyplot as plt\n\nimage_path = \"output1.jpg\"\nimage = plt.imread(image_path)\n\nplt.imshow(image)\nplt.axis('off')\nplt.show()\n```\n\n## Prompt 函数的流式输出\n\nbyzerllm 底层支持流式输出，非 prompt 函数的用法是这样的：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\n\nv = llm.stream_chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\":\"讲一个100字的故事\"\n}])\n\nfor s  in v:\n    print(s[0], end=\"\")\n```\n\n如果你像用 prompt 函数，可以这么用：\n\n\n```python\nimport byzerllm\nimport json\nimport base64\nfrom typing import Generator\n\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\n\n@byzerllm.prompt()\ndef tell_story() -> Generator[str, None, None]:\n    '''\n    给我讲一个一百多字的故事\n    '''\n\nv = tell_story.with_llm(llm).run()    \nfor i in v:\n    print(i, end=\"\")\n```\n\n可以看到，和普通的 prompt 函数的区别在于，返回值是一个生成器，然后你可以通过 for 循环来获取结果。\n\n## 向量化模型\n\nbyzerllm 支持向量化模型,你可以这样启动一个本地的模型：\n\n```bash\n!byzerllm deploy --pretrained_model_type custom/bge \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--worker_concurrency 10 \\\n--model_path /home/winubuntu/.auto-coder/storage/models/AI-ModelScope/bge-large-zh \\\n--infer_backend transformers \\\n--num_workers 1 \\\n--model emb\n```\n\n注意两个参数:\n\n1. --infer_backend transformers: 表示使用 transformers 作为推理后端。\n2. --model_path: 表示模型的路径。\n\n也可以启动一个 SaaS 的emb模型,比如 qwen 的 emb 模型：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/qianwen \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 2 \\\n--infer_params saas.api_key=${MODEL_QIANWEN_TOKEN}  saas.model=text-embedding-v2 \\\n--model qianwen_emb\n```\n\n或者 openai 的 emb 模型：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--worker_concurrency 10 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=text-embedding-3-small \\\n--model gpt_emb\n```\nSaaS 模型无需配置 `--infer_backend` 参数。\n\n无论是本地模型还是 SaaS 模型，我们都可以这样使用：\n\n```python\nimport byzerllm\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\nllm.setup_default_emb_model_name(\"emb\")\nllm.emb_query(\"你好\")\n```\n\n如果你配置 byzerllm 中的 Storage 使用，比如你这样启动了存储：\n\n```bash\nbyzerllm storage start --enable_emb\n```\n\n那么需要这么使用：\n\n```python\nfrom byzerllm.apps.byzer_storage.simple_api import ByzerStorage, DataType, FieldOption,SortOption\nstorage = ByzerStorage(\"byzerai_store\", \"memory\", \"memory\")\nstorage.emb(\"你好\")\n```\n\n## 一些辅助工具\n\n当调用 prompt 函数返回字符串的时候，如果想从里面抽取代码，可以使用如下方式：\n\n```python\nfrom byzerllm.utils.client import code_utils\ntext_with_markdown = '''\n```shell\nls -l\n```\n'''\ncode_blocks = code_utils.extract_code(text_with_markdown)\nfor code_block in code_blocks:\n    if code_block[0] == \"shell\":\n        print(code_block[1])\n##output: ls -l        \n```\n\n\n## 注意事项\n\n1. prompt函数方法体返回只能是dict，实际的返回类型和方法签名可以不一样，但是方法体返回只能是dict。\n2. 大部分情况prompt函数体为空，如果一定要有方法体，可以返回一个空字典。\n3. 调用prompt方法的时候，如果在@byzerllm.prompt()里没有指定llm对象，那么需要在调用的时候通过with_llm方法指定llm对象。\n\n===========================\n\n# Byzer Storage: 最易用的AI存储引擎\n\nByzer Storage是一个为AI应用设计的高性能存储引擎,它提供了简单易用的API,支持向量搜索、全文检索以及结构化查询。本文将详细介绍Byzer Storage的使用方法和主要特性。\n\n## 0. 安装和启动\n\n```bash\npip install byzerllm\nbyzerllm storage start\n```\n\nThat's it! Byzer Storage已经安装并启动成功,现在我们可以开始使用它了。默认会启动一个 byzerai_store 的集群。\n\n注意，如果你是这样启动的：\n\n```bash\nbyzerllm storage start --enable_emb\n```\n\n那么会自动启动一个 emb 模型，名字就叫 emb， ByzerStorage 会自动使用该模型，无需做任何其他配置。\n\n## 1. 初始化\n\n创建一个 ByzerStorage 对象，链接 byzerai_store 集群，并且指定数据库和表名（可以不存在）。\n\n```python\nfrom byzerllm.apps.byzer_storage.simple_api import ByzerStorage, DataType, FieldOption, SortOption\n\nstorage = ByzerStorage(\"byzerai_store\", \"my_database1\", \"my_table4s\")\n```\n\n\n## 2. 创建库表（可选）\n\nByzer Storage使用Schema来定义数据结构。我们可以使用SchemaBuilder来构建Schema:\n\n```python\n_ = (\n    storage.schema_builder()\n    .add_field(\"_id\", DataType.STRING)\n    .add_field(\"name\", DataType.STRING)\n    .add_field(\"content\", DataType.STRING, [FieldOption.ANALYZE])\n    .add_field(\"raw_content\", DataType.STRING, [FieldOption.NO_INDEX])    \n    .add_array_field(\"summary\", DataType.FLOAT)    \n    .add_field(\"created_time\", DataType.LONG, [FieldOption.SORT])    \n    .execute()\n)\n```\n\n这个Schema定义了以下字段:\n- `_id`: 字符串类型的主键\n- `name`: 字符串类型,可用于过滤条件\n- `content`: 字符串类型,带有ANALYZE选项,用于全文搜索\n- `raw_content`: 字符串类型,带有NO_INDEX选项,不会被索引\n- `summary`: 浮点数组类型,用于存储向量\n- `created_time`: 长整型,带有SORT选项,可用于排序\n\n需要注意的是：\n\n1. 如果一个字段带有ANALYZE选项,则该字段会被分词,并且可以用于全文搜索，但是就无法返回原始的文本了。所以你需要添加一个新字段专门用来存储原文，比如在我们这个例子里，我们新增了 raw_content 字段，并且显示的指定了 NO_INDEX 选项，这样就不会被索引，也就不会被分词，可以后续被检索后用来获取原始文本。\n2. 对于需要作为向量检索和存储的字段，需要指定为数组类型，比如我们这个例子里的 summary 字段。\n3. 如果你需要拿到向量字段的原始文本，那么你也需要添加一个新字段专门用来存储原文，就像我们这个例子里的 raw_content 字段一样。\n\n\n## 3. 写入数据\n\n准备数据并使用WriteBuilder写入Storage:\n\n```python\ndata = [\n    {\"_id\": \"1\", \"name\": \"Hello\", \"content\": \"Hello, world!\", \"raw_content\": \"Hello, world!\", \"summary\": \"hello world\", \"created_time\": 1612137600},\n    {\"_id\": \"2\", \"name\": \"Byzer\", \"content\": \"Byzer, world!\", \"raw_content\": \"Byzer, world!\", \"summary\": \"byzer\", \"created_time\": 1612137601},\n    {\"_id\": \"3\", \"name\": \"AI\", \"content\": \"AI, world!\", \"raw_content\": \"AI, world!\", \"summary\": \"AI\", \"created_time\": 16121376002},\n    {\"_id\": \"4\", \"name\": \"ByzerAI\", \"content\": \"ByzerAI, world!\", \"raw_content\": \"ByzerAI, world!\", \"summary\": \"ByzerAi\", \"created_time\": 16121376003},\n]\n\nstorage.write_builder().add_items(data, vector_fields=[\"summary\"], search_fields=[\"content\"]).execute()\nstorage.commit()\n```\n\n这里我们使用`add_items`方法批量添加数据,并指定了`summary`为向量字段,`content`为搜索字段。最后调用`commit()`来确保数据被持久化。\n\n\n从上面我们要写入到 byzer storage 的数据我们可以看到如下几个特点：\n\n1. 需要包含我们之前定义的 Schema中罗列的所有的字段，同时需要指定哪些是向量字段，哪些是检索字段。\n2. 向量和检索不能是同一个字段。\n3. 对于向量，检索字段，我们给到 write_builder 的都是文本，ByzerStorage 会根据 Schema 的定义自动将其转换为向量，检索字段需要的格式。\n\n## 4. 查询数据\n\nByzer Storage支持多种查询方式,包括向量搜索、全文检索、过滤和排序。\n\n### 4.1 向量搜索 + 全文检索\n\n```python\nquery = storage.query_builder()\nquery.set_vector_query(\"ByzerAI\", fields=[\"summary\"])\nresults = query.set_search_query(\"Hello\", fields=[\"content\"]).execute()\nprint(results)\n```\n\n这个查询结合了向量搜索和全文检索,它会在`summary`字段中搜索与\"ByzerAI\"相似的向量,同时在`content`字段中搜索包含\"Hello\"的文档。\n\n### 4.2 过滤 + 向量搜索 + 全文检索\n\n```python\nquery = storage.query_builder()\nquery.and_filter().add_condition(\"name\", \"AI\").build()\nquery.set_vector_query(\"ByzerAI\", fields=\"summary\")\nresults = query.set_search_query(\"Hello\", fields=[\"content\"]).execute()\nprint(results)\n```\n\n这个查询首先过滤`name`字段等于\"AI\"的文档,然后在结果中进行向量搜索和全文检索。\n\n### 4.3 过滤 + 排序\n\n```python\nquery = storage.query_builder()\nquery.and_filter().add_condition(\"name\", \"AI\").build().sort(\"created_time\", SortOption.DESC)\nresults = query.execute()\nprint(results)\n```\n\n这个查询过滤`name`字段等于\"AI\"的文档,然后按`created_time`字段降序排序。\n\n## 5. 删除数据\n\n### 5.1 根据ID删除\n\n```python\nstorage.delete_by_ids([\"3\"])\n\nquery = storage.query_builder()\nquery.and_filter().add_condition(\"name\", \"AI\").build()\nresults = query.execute()\nprint(results)\n```\n\n这里我们删除了ID为\"3\"的文档,然后查询验证删除结果。\n\n### 5.2 删除整个表\n\n```python\nstorage.drop()\n```\n\n这个操作会删除整个表及其所有数据,请谨慎使用。\n\n## 结论\n\nByzer Storage提供了一套简洁而强大的API,能够轻松实现向量搜索、全文检索、结构化查询等功能。它的设计非常适合AI应用场景,可以有效地存储和检索各种类型的数据。通过本文的介绍,相信读者已经对Byzer Storage有了基本的了解,并能够开始在自己的项目中使用这个强大的存储引擎。\n\n\n     \n\n下面是用户的需求：\n\n提供一个新的convert.py , 支持 super-analysis.convert  命令,支持两个参数 --data_dir --doc_dir, 递归读取 --data_dir 里的csv 文件,然后用pandas读取,然后提取scheme,最后生成 schema 文本写入到 --doc_dir,名字可以用文本md5\n"}, {"role": "assistant", "content": "好的，我理解你的需求。我将为您创建一个新的 `convert.py` 文件，实现 `super-analysis.convert` 命令，支持 `--data_dir` 和 `--doc_dir` 参数，递归读取 CSV 文件，使用 pandas 读取数据，提取 schema，并将 schema 文本写入到指定目录。我将使用 *SEARCH/REPLACE block* 格式来生成代码。\n\n首先，让我们创建新的 `convert.py` 文件：\n\n```python\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/convert.py\n<<<<<<< SEARCH\n=======\nimport os\nimport argparse\nimport pandas as pd\nimport hashlib\nfrom typing import List, Dict\n\ndef get_csv_files(directory: str) -> List[str]:\n    \"\"\"Recursively get all CSV files in the given directory.\"\"\"\n    csv_files = []\n    for root, _, files in os.walk(directory):\n        for file in files:\n            if file.endswith('.csv'):\n                csv_files.append(os.path.join(root, file))\n    return csv_files\n\ndef extract_schema(df: pd.DataFrame) -> Dict[str, str]:\n    \"\"\"Extract schema from pandas DataFrame.\"\"\"\n    schema = {}\n    for column in df.columns:\n        dtype = str(df[column].dtype)\n        if dtype == 'object':\n            schema[column] = 'STRING'\n        elif dtype.startswith('int'):\n            schema[column] = 'INTEGER'\n        elif dtype.startswith('float'):\n            schema[column] = 'FLOAT'\n        elif dtype == 'bool':\n            schema[column] = 'BOOLEAN'\n        elif dtype.startswith('datetime'):\n            schema[column] = 'TIMESTAMP'\n        else:\n            schema[column] = 'STRING'  # Default to STRING for unknown types\n    return schema\n\ndef generate_schema_text(schema: Dict[str, str]) -> str:\n    \"\"\"Generate schema text from the schema dictionary.\"\"\"\n    schema_lines = [f\"{field}: {dtype}\" for field, dtype in schema.items()]\n    return \"\\n\".join(schema_lines)\n\ndef save_schema(schema_text: str, output_dir: str) -> None:\n    \"\"\"Save schema text to a file in the output directory.\"\"\"\n    # Generate MD5 hash of the schema text\n    file_name = hashlib.md5(schema_text.encode()).hexdigest() + '.md'\n    output_path = os.path.join(output_dir, file_name)\n    \n    with open(output_path, 'w') as f:\n        f.write(schema_text)\n    print(f\"Schema saved to {output_path}\")\n\ndef convert(data_dir: str, doc_dir: str) -> None:\n    \"\"\"Main conversion function.\"\"\"\n    csv_files = get_csv_files(data_dir)\n    \n    for csv_file in csv_files:\n        print(f\"Processing {csv_file}\")\n        df = pd.read_csv(csv_file)\n        schema = extract_schema(df)\n        schema_text = generate_schema_text(schema)\n        save_schema(schema_text, doc_dir)\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Convert CSV files to schema documents\")\n    parser.add_argument(\"--data_dir\", required=True, help=\"Directory containing CSV files\")\n    parser.add_argument(\"--doc_dir\", required=True, help=\"Output directory for schema documents\")\n    \n    args = parser.parse_args()\n    \n    if not os.path.exists(args.doc_dir):\n        os.makedirs(args.doc_dir)\n    \n    convert(args.data_dir, args.doc_dir)\n\nif __name__ == \"__main__\":\n    main()\n>>>>>>> REPLACE\n```\n\n现在，我们需要更新 `setup.py` 文件来添加新的命令行入口点：\n\n```python\n##File: /Users/allwefantasy/projects/super_analysis/setup.py\n<<<<<<< SEARCH\n    entry_points={\n        \"console_scripts\": [\n            \"super-analysis.serve=superanalysis.serve.cli:main\",\n        ],\n    },\n=======\n    entry_points={\n        \"console_scripts\": [\n            \"super-analysis.serve=superanalysis.serve.cli:main\",\n            \"super-analysis.convert=superanalysis.serve.convert:main\",\n        ],\n    },\n>>>>>>> REPLACE\n```\n\n这些更改将添加一个新的 `convert.py` 文件，实现了 `super-analysis.convert` 命令。该命令支持 `--data_dir` 和 `--doc_dir` 参数，递归读取 CSV 文件，使用 pandas 读取数据，提取 schema，并将 schema 文本写入到指定目录，文件名使用 schema 文本的 MD5 哈希值。\n\n主要功能包括：\n\n1. 递归获取指定目录下的所有 CSV 文件\n2. 使用 pandas 读取 CSV 文件\n3. 从 DataFrame 中提取 schema\n4. 生成 schema 文本\n5. 将 schema 文本保存到指定目录，文件名使用 MD5 哈希值\n\n用户可以通过以下方式使用这个新命令：\n\n```\nsuper-analysis.convert --data_dir /path/to/csv/files --doc_dir /path/to/output/schema\n```\n\n这将处理 `/path/to/csv/files` 目录下的所有 CSV 文件，并将生成的 schema 文档保存到 `/path/to/output/schema` 目录中。"}], "model": "human", "yaml_file": "actions/110_chat_action.yml"}
{"instruction": "在 @types.py 新增一个 ByzerSQLConfig 里面有个 byzer_sql_url ,然后同时传递给 @@SQLGenerator", "conversations": [{"role": "user", "content": "\n如果你需要生成代码，对于每个需要更改的文件,你需要按 *SEARCH/REPLACE block* 的格式进行生成。\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The opening fence and code language, eg: ```python\n2. The file path alone on a line, starting with \"##File:\" and verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n*SEARCH/REPLACE* blocks will replace *all* matching occurrences.\nInclude enough lines to make the SEARCH blocks unique.\n\nInclude *ALL* the code being searched and replaced!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n下面我们来看一个例子：\n\n当前项目目录结构：\n1. 项目根目录： /tmp/projects/mathweb\n2. 项目子目录/文件列表(类似tree 命令输出)\nflask/\n    app.py\n    templates/\n        index.html\n    static/\n        style.css\n\n用户需求： Change get_factorial() to use math.factorial\n\n回答： To make this change we need to modify `/tmp/projects/mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n用户需求： Refactor hello() into its own file.\n\n回答：To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/hello.py\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/main.py\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n现在让我们开始一个新的任务:\n\n当前项目目录结构：\n1. 项目根目录： /Users/allwefantasy/projects/super_analysis\n2. 项目子目录/文件列表(类似tree 命令输出)：\nsetup.py\n.auto-coder/\n    libs/\n        llm_friendly_packages/\n            src/\n                example1/\n                    main.py\nsrc/\n    superanalysis/\n        serve/\n            convert.py\n            openai_serve.py\n            __init__.py\n            llm_wrapper.py\n            api_server.py\n            cli.py\n        sql/\n            __init__.py\n            types.py\n            generator.py\n下面是一些文件路径以及每个文件对应的源码：\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/types.py\nfrom pydantic import BaseModel\n\nclass RAGConfig(BaseModel):\n    rag_base_url: str = \"\"\n    rag_api_key: str = \"\"\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/cli.py\nimport argparse\nimport byzerllm\nfrom byzerllm.utils.client import ByzerLLM\nfrom superanalysis.serve.api_server import serve, ServerArgs\nfrom superanalysis.sql.types import RAGConfig\nfrom superanalysis.sql.generator import SQLGenerator\nfrom superanalysis.serve.llm_wrapper import LLWrapper\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"ByzerLLM API Server\")\n    parser.add_argument(\"--host\", type=str, default=\"0.0.0.0\", help=\"Host to bind the server to\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"Port to bind the server to\")\n    parser.add_argument(\"--uvicorn-log-level\", type=str, default=\"info\", help=\"Uvicorn log level\")\n    parser.add_argument(\"--allow-credentials\", action=\"store_true\", help=\"Allow credentials\")\n    parser.add_argument(\"--allowed-origins\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed origins\")\n    parser.add_argument(\"--allowed-methods\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed methods\")\n    parser.add_argument(\"--allowed-headers\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed headers\")\n    parser.add_argument(\"--api-key\", type=str, default=\"\", help=\"API key for authentication\")\n    parser.add_argument(\"--served-model-name\", type=str, required=True, help=\"Name of the model to be served\")\n    parser.add_argument(\"--prompt-template\", type=str, default=\"\", help=\"Prompt template\")\n    parser.add_argument(\"--response-role\", type=str, default=\"assistant\", help=\"Response role\")\n    parser.add_argument(\"--ssl-keyfile\", type=str, default=\"\", help=\"SSL key file\")\n    parser.add_argument(\"--ssl-certfile\", type=str, default=\"\", help=\"SSL certificate file\")\n    \n    # Add new arguments for RAGConfig\n    parser.add_argument(\"--rag-base-url\", type=str, default=\"\", help=\"RAG base URL\")\n    parser.add_argument(\"--rag-api-key\", type=str, default=\"\", help=\"RAG API key\")\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile\n    )\n\n    # Create RAGConfig instance\n    rag_config = RAGConfig(\n        rag_base_url=args.rag_base_url,\n        rag_api_key=args.rag_api_key\n    )\n\n    llm = ByzerLLM()\n    llm.setup_default_model_name(args.served_model_name)\n    \n    byzerllm.connect_cluster(address=args.ray_address)\n    llm = byzerllm.ByzerLLM()\n    if not server_args.served_model_name:\n        raise ValueError(\"served_model_name is required\")\n    llm.setup_default_model_name(server_args.served_model_name)\n    rag = SQLGenerator(llm=llm)    \n    llm_wrapper = LLWrapper(llm=llm, rag=rag)\n    serve(llm_wrapper, server_args, rag_config)\n\nif __name__ == \"__main__\":\n    main()\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/__init__.py\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/__init__.py\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/llm_wrapper.py\nfrom typing import Any, Dict, List, Optional, Union,Callable\nfrom byzerllm.utils.client.types import (    \n    LLMFunctionCallResponse,\n    LLMClassResponse,LLMResponse    \n)\nimport pydantic\nfrom byzerllm import ByzerLLM\nfrom byzerllm.utils.client import LLMResponse\nfrom byzerllm.utils.types import SingleOutputMeta\nfrom byzerllm.utils.langutil import asyncfy_with_semaphore\nfrom superanalysis.sql.generator import SQLGenerator\nfrom loguru import logger\n\nclass LLWrapper:\n\n    def __init__(self,llm:ByzerLLM,entry:SQLGenerator):\n        self.entry = entry\n        self.llm = llm\n        \n\n    def chat_oai(self,\n                 conversations,\n                 tools:List[Union[Callable,str]]=[], \n                 tool_choice:Optional[Union[Callable,str]]=None,\n                 execute_tool:bool=False,  \n                 impl_func:Optional[Callable]=None,\n                 execute_impl_func:bool=False,\n                 impl_func_params:Optional[Dict[str,Any]]=None,\n                 func_params:Optional[Dict[str,Any]]=None,\n                 response_class:Optional[Union[pydantic.BaseModel,str]] = None, \n                 response_after_chat:Optional[Union[pydantic.BaseModel,str]] = False,\n                 enable_default_sys_message:bool=True,                 \n                 model:Optional[str] = None,\n                 role_mapping=None,llm_config:Dict[str,Any]={}\n                 )->Union[List[LLMResponse],List[LLMFunctionCallResponse],List[LLMClassResponse]]: \n        res,contexts = self.entry.stream_chat_oai(conversations)\n        s = \"\".join(res)        \n        return [LLMResponse(output=s,metadata={},input=\"\")]\n\n    def stream_chat_oai(self,conversations, \n                        model:Optional[str]=None, \n                        role_mapping=None,\n                        delta_mode=False,\n                        llm_config:Dict[str,Any]={}): \n        res,contexts = self.entry.stream_chat_oai(conversations)        \n        for t in res:                        \n            yield (t,SingleOutputMeta(0,0))\n\n    async def async_stream_chat_oai(self,conversations, \n                        model:Optional[str]=None, \n                        role_mapping=None,\n                        delta_mode=False,\n                        llm_config:Dict[str,Any]={}): \n        res,contexts = self.rag.stream_chat_oai(conversations)                \n        for t in res:                                    \n            yield (t,SingleOutputMeta(0,0))\n                         \n\n    def __getattr__(self, name):        \n        return getattr(self.llm, name)\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/openai_serve.py\nimport argparse\nfrom service_server.api_server import ServerArgs, serve\nfrom service_server.llm_wrapper import LLWrapper\nfrom service_server.summary_rag import SummaryRAG\nimport byzerllm\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"OpenAI Serve\")\n    parser.add_argument(\n        \"--model\",\n        default=\"\",\n    )\n    parser.add_argument(\"--emb_model\", default=\"\")\n    parser.add_argument(\n        \"--ray_address\",\n        default=\"auto\",\n    )\n    parser.add_argument(\"--source_dir\", default=\".\", help=\"\")\n    parser.add_argument(\"--host\", default=\"\", help=\"\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"\")\n    parser.add_argument(\"--uvicorn_log_level\", default=\"info\", help=\"\")\n    parser.add_argument(\"--allow_credentials\", action=\"store_true\", help=\"\")\n    parser.add_argument(\"--allowed_origins\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--allowed_methods\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--allowed_headers\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--api_key\", default=\"\", help=\"\")\n    parser.add_argument(\"--served_model_name\", default=\"\", help=\"\")\n    parser.add_argument(\"--prompt_template\", default=\"\", help=\"\")\n    parser.add_argument(\"--ssl_keyfile\", default=\"\", help=\"\")\n    parser.add_argument(\"--ssl_certfile\", default=\"\", help=\"\")\n    parser.add_argument(\"--response_role\", default=\"assistant\", help=\"\")\n    parser.add_argument(\n        \"--collections\", default=\"\", help=\"Collection name for indexing\"\n    )\n    parser.add_argument(\n        \"--base_dir\",\n        default=\"\",\n        help=\"Path where the processed text embeddings were stored\",\n    )\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name or args.model,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile,\n    )\n    byzerllm.connect_cluster(address=args.ray_address)\n    llm = byzerllm.ByzerLLM()\n    if not server_args.served_model_name:\n        raise ValueError(\"served_model_name is required\")\n    llm.setup_default_model_name(server_args.served_model_name)\n    rag = SummaryRAG(llm=llm)    \n    llm_wrapper = LLWrapper(llm=llm, rag=rag)\n    serve(llm=llm_wrapper, args=server_args)\n\nif __name__ == \"__main__\":\n    main()\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/api_server.py\nimport os\nimport time\nimport uvicorn\nfrom fastapi import Request\nfrom fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import JSONResponse, StreamingResponse, Response\n\nfrom byzerllm.log import init_logger\nfrom byzerllm.utils import random_uuid\nfrom byzerllm.version import __version__ as version\nfrom byzerllm.utils.client import ByzerLLM, LLMRequest\nfrom byzerllm.utils.client.entrypoints.openai.serving_chat import OpenAIServingChat\nfrom byzerllm.utils.client.entrypoints.openai.serving_completion import OpenAIServingCompletion\nfrom byzerllm.utils.client.entrypoints.openai.protocol import (\n    ModelList,\n    ModelCard,\n    ModelPermission,\n    ChatCompletionRequest,\n    ErrorResponse,\n    CompletionRequest,\n    Embeddings,\n    EmbeddingsOutput,\n    EmbeddingsData,\n    EmbeddingsUsage,\n)\nfrom pydantic import BaseModel\nfrom typing import List\n\nlogger = init_logger(__name__)\n\nllm_client: ByzerLLM = None\nopenai_serving_chat: OpenAIServingChat = None\nopenai_serving_completion: OpenAIServingCompletion = None\n\nTIMEOUT_KEEP_ALIVE = 5  # seconds\n# timeout in 10 minutes. Streaming can take longer than 3 min\nTIMEOUT = float(os.environ.get(\"BYZERLLM_APISERVER_HTTP_TIMEOUT\", 600))\n\nrouter_app = FastAPI()\n\n\n@router_app.get(\"/health\")\nasync def health() -> Response:\n    \"\"\"Health check.\"\"\"\n    return Response(status_code=200)\n\n\n@router_app.get(\"/v1/models\")\nasync def show_available_models():\n    models = await openai_serving_chat.show_available_models()\n    return JSONResponse(content=models.model_dump())\n\n\n@router_app.get(\"/version\")\nasync def show_version():\n    return JSONResponse(content={\"version\": version})\n\n\n@router_app.get(\"/v1/models\", response_model=ModelList)\nasync def models() -> ModelList:\n    \"\"\"Show available models. Right now we only have one model.\"\"\"\n    model_cards = [\n        ModelCard(\n            id=\"\",\n            root=\"\",\n            permission=[ModelPermission()]\n        )\n    ]\n    return ModelList(data=model_cards)\n\n\n@router_app.post(\"/v1/completions\")\nasync def create_completion(\n        body: CompletionRequest,\n        request: Request\n):\n    generator = await openai_serving_completion.create_completion(body, request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            status_code=generator.code\n        )\n    if body.stream:\n        return StreamingResponse(\n            content=generator,\n            media_type=\"text/event-stream\"\n        )\n    else:\n        return JSONResponse(content=generator.model_dump())\n\n\n@router_app.post(\"/v1/chat/completions\")\nasync def create_chat_completion(\n        body: ChatCompletionRequest,\n        request: Request,\n):\n    \"\"\"Completion API similar to OpenAI's API.\n\n    See  https://platform.openai.com/docs/api-reference/chat/create\n    for the API specification. This API mimics the OpenAI ChatCompletion API.\n\n    NOTE: Currently we do not support the following features:\n        - function_call (Users should implement this by themselves)\n        - logit_bias (to be supported by vLLM engine)\n    \"\"\"\n    # async with async_timeout.timeout(TIMEOUT):\n\n    generator = await openai_serving_chat.create_chat_completion(body, request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            status_code=generator.code\n        )\n    if body.stream:\n        return StreamingResponse(\n            content=generator,\n            media_type=\"text/event-stream\"\n        )\n    else:\n        return JSONResponse(content=generator.model_dump())\n\n\n@router_app.post(\"/v1/embeddings\")\nasync def embed(body: Embeddings):\n    \"\"\"Given a prompt, the model will return one embedding.\n\n    Returns:\n        A response object with an embedding.\n    \"\"\"\n    embedding_id = f\"embed-{random_uuid()}\"\n\n    results_list = llm_client.emb(body.model, request=LLMRequest(instruction=body.input))\n    tokens = 0\n\n    return EmbeddingsOutput(\n        data=[\n            EmbeddingsData(\n                embedding=results.output,\n                index=i,\n                object=\"embedding\",\n            )\n            for i, results in enumerate(results_list)\n        ],\n        id=embedding_id,\n        object=\"list\",\n        created=int(time.time()),\n        model=body.model,\n        usage=EmbeddingsUsage(\n            prompt_tokens=tokens,\n            total_tokens=tokens,\n        ),\n    )\n\nclass ServerArgs(BaseModel):\n    host: str = None\n    port: int = 8000\n    uvicorn_log_level: str = \"info\"\n    allow_credentials: bool = False\n    allowed_origins: List[str] = [\"*\"]  \n    allowed_methods: List[str] = [\"*\"]\n    allowed_headers: List[str] = [\"*\"]\n    api_key: str = None\n    served_model_name: str = None\n    prompt_template: str = None\n    response_role: str = \"assistant\"\n    ssl_keyfile: str = None\n    ssl_certfile: str = None    \n\ndef serve(llm:ByzerLLM, args: ServerArgs,rag_config):\n    \n    logger.info(f\"ByzerLLM API server version {version}\")\n    logger.info(f\"args: {args}\")\n\n    router_app.add_middleware(\n        CORSMiddleware,\n        allow_origins=args.allowed_origins,\n        allow_credentials=args.allow_credentials,\n        allow_methods=args.allowed_methods,\n        allow_headers=args.allowed_headers,\n    )\n    \n    if token := os.environ.get(\"BYZERLLM_API_KEY\") or args.api_key:\n\n        @router_app.middleware(\"http\")\n        async def authentication(request: Request, call_next):\n            if not request.url.path.startswith(\"/v1\"):\n                return await call_next(request)\n            if request.headers.get(\"Authorization\") != \"Bearer \" + token:\n                return JSONResponse(\n                    content={\"error\": \"Unauthorized\"},\n                    status_code=401\n                )\n            return await call_next(request)\n\n    # Register labels for metrics\n    # add_global_metrics_labels(model_name=engine_args.model)\n    global llm_client\n    llm_client = llm\n    \n    global openai_serving_chat\n    openai_serving_chat = OpenAIServingChat(\n        llm_client=llm_client,\n        response_role=args.response_role,\n        server_model_name=args.served_model_name,\n        prompt_template=args.prompt_template\n    )\n    global openai_serving_completion\n    openai_serving_completion = OpenAIServingCompletion(\n        llm_client=llm_client,\n        server_model_name=args.served_model_name,\n        prompt_template=args.prompt_template\n    )\n\n    uvicorn.run(\n        router_app,\n        host=args.host,\n        port=args.port,\n        log_level=args.uvicorn_log_level,\n        timeout_keep_alive=TIMEOUT_KEEP_ALIVE,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile\n    )\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/generator.py\nfrom typing import Optional, Dict, Any\nfrom openai import OpenAI\nimport json\nfrom .types import RAGConfig\nfrom byzerllm.utils.client import code_utils\n\n\nclass SQLGenerator:\n\n    def __init__(self, rag_config: RAGConfig) -> None:        \n        self.rag_client = OpenAI(api_key=rag_config.rag_api_key, base_url=rag_config.rag_base_url)\n\n    def stream_chat_oai(\n        self,\n        conversations,\n        model: Optional[str] = None,\n        role_mapping=None,\n        delta_mode=False,\n        llm_config: Dict[str, Any] = {},\n    ):\n        if not model:\n            model = \"deepseek_chat\"\n        response = self.rag_client.chat.completions.create(\n        messages=conversations,\n        model=model,\n    )\n        sql_str = response.choices[0].message.content\n        pass\n\n\n\n##File: /Users/allwefantasy/projects/super_analysis/setup.py\nfrom setuptools import setup, find_packages\n\nsetup(\n    name=\"super-analysis\",\n    version=\"0.1.0\",\n    packages=find_packages(where=\"src\"),\n    package_dir={\"\": \"src\"},\n    install_requires=[\n        \"byzerllm\",\n        \"fastapi\",\n        \"uvicorn\",\n        \"argparse\",\n    ],\n    entry_points={\n        \"console_scripts\": [\n            \"super-analysis.serve=superanalysis.serve.cli:main\",\n            \"super-analysis.convert=superanalysis.serve.convert:main\",\n        ],\n    },\n    author=\"allwefantasy\",\n    author_email=\"allwefantasy@gmail.com\",\n    description=\"A package for super analysis with ByzerLLM\",\n    long_description=open(\"README.md\").read(),\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/yourusername/super-analysis\",\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n    ],\n    python_requires='>=3.6',\n)\n\n##File: .auto-coder/libs/llm_friendly_packages/github.com/allwefantasy/byzer-llm/README.md\n# byzerllm 大模型编程快速指南\n\n本文示例 [Notebook](../../notebooks/003_byzerllm_大模型编程快速指南.ipynb)\n\n## 安装\n\n```bash\npip install -U byzerllm\nray start --head\n```\n## 启动一个模型代理\n\nbyzerllm 支持私有化模型或者SaaS模型的部署。\n\n这里以 deepseek 官方API 为例：\n\n```bash\neasy-byzerllm deploy deepseek-chat --token xxxxx --alias deepseek_chat\n```\n\n或者跬基流动API:\n\n```bash\neasy-byzerllm deploy alibaba/Qwen1.5-110B-Chat --token xxxxx --alias qwen110b_chat\n```\n\n将上面的 API KEY 替换成你们自己的。\n\n如果你想部署私有化模型或者对接 Ollama 等更多需求，参考 [002_使用byzerllm进行模型部署.md](./002_使用byzerllm进行模型部署.md) 或者 [README.md](../../README.md)。\n\n之后，你就可以在代码里使用  deepseek_chat 或者 qwen110b_chat  访问模型了。\n\n## hello world\n\n来和我们的大模型打个招呼:\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\n@byzerllm.prompt(llm=llm)\ndef hello(q:str) ->str:\n    '''\n    你好, {{ q }}\n    '''\n\ns = hello(\"你是谁\")    \nprint(s)\n\n## 输出:\n## '你好！我是一个人工智能助手，专门设计来回答问题、提供信息和帮助解决问题。如果你有任何疑问或需要帮助，请随时告诉我。'\n```\n\n恭喜，你和大模型成功打了招呼！\n\n可以看到，我们通过 `@byzerllm.prompt` 装饰器，将一个方法转换成了一个大模型的调用，然后这个方法的主题是一段文本，文本中\n使用了 jinja2 模板语法，来获得方法的参数。当正常调用该方法时，实际上就发起了和大模型的交互，并且返回了大模型的结果。\n\n在 byzerllm 中，我们把这种方法称之为 prompt 函数。\n\n## 查看发送给大模型的prompt\n\n很多情况你可能需要调试，查看自己的 prompt 渲染后到底是什么样子的，这个时候你可以通过如下方式\n获取渲染后的prompt:\n\n```python\nhello.prompt(\"你是谁\")\n## '你好, 你是谁'\n```            \n\n## 动态换一个模型\n\n前面的 hello 方法在初始化的时候，我们使用了默认的模型 deepseek_chat，如果我们想换一个模型，可以这样做：\n\n```python\nhello.with_llm(llm).run(\"你是谁\")\n## '你好！我是一个人工智能助手，专门设计来回答问题、提供信息和帮助解决问题。如果你有任何疑问或需要帮助，请随时告诉我。'\n```\n\n通过 with_llm 你可以设置一个新的 llm 对象，然后调用 run 方法，就可以使用新的模型了。\n\n## 超长文本生成\n\n我们知道，大模型一次生成的长度其实是有限的，如果你想生成超长文本，你可能需手动的不断获得\n生成结果，然后把他转化为输入，然后再次生成，这样的方式是比较麻烦的。\n\nbyzerllm 提供了更加易用的 API :\n\n```python\nimport byzerllm\nfrom byzerllm import ByzerLLM\n\nllm = ByzerLLM.from_default_model(\"deepseek_chat\")\n\n@byzerllm.prompt()\ndef tell_story() -> str:\n    \"\"\"\n    讲一个100字的故事。    \n    \"\"\"\n\n\ns = (\n    tell_story.with_llm(llm)\n    .with_response_markers()\n    .options({\"llm_config\": {\"max_length\": 10}})\n    .run()\n)\nprint(s)\n\n## 从前，森林里住着一只聪明的小狐狸。一天，它发现了一块闪闪发光的宝石。小狐狸决定用这块宝石帮助森林里的动物们。它用宝石的光芒指引迷路的小鹿找到了回家的路，用宝石的温暖治愈了受伤的小鸟。从此，小狐狸成了森林里的英雄，动物们都感激它的善良和智慧。\n```\n\n实际核心部分就是这一行：\n\n```python\ntell_story.with_llm(llm)\n    .with_response_markers()    \n    .run()\n```\n\n我们只需要调用 `with_response_markers` 方法，系统就会自动的帮我们生成超长文本。\n在上面的案例中，我们通过\n\n```python\n.options({\"llm_config\": {\"max_length\": 10}})\n```\n\n认为的限制大模型一次交互最多只能输出10个字符，但是系统依然自动完成了远超过10个字符的文本生成。\n\n## 对象输出\n\n前面我们的例子都是返回字符串，但是我们也可以返回对象，这样我们就可以更加灵活的处理返回结果。\n\n```python\nimport pydantic \n\nclass Story(pydantic.BaseModel):\n    '''\n    故事\n    '''\n\n    title: str = pydantic.Field(description=\"故事的标题\")\n    body: str = pydantic.Field(description=\"故事主体\")\n\n@byzerllm.prompt()\ndef tell_story()->Story:\n    '''\n    讲一个100字的故事。    \n    '''\n\ns = tell_story.with_llm(llm).run()\nprint(isinstance(s, Story))\nprint(s.title)\n\n## True\n## 勇敢的小鸟\n```\n\n可以看到，我们很轻松的将输出转化为格式化输出。\n\n## 自定义字段抽取\n\n前面的结构化输出，其实会消耗更多token,还有一种更加精准的结构化输出方式。\n比如让大模型生成一个正则表达式，但实际上大模型很难准确只输出一个正则表达式，这个时候我们可以通过自定义抽取函数来获取我们想要的结果。\n\n\n```python\nfrom loguru import logger\nimport re\n\n@byzerllm.prompt()\ndef generate_regex_pattern(desc: str) -> str:\n    \"\"\"\n    根据下面的描述生成一个正则表达式，要符合python re.compile 库的要求。\n\n    {{ desc }}\n\n    最后生成的正则表达式要在<REGEX></REGEX>标签对里。\n    \"\"\"    \n\ndef extract_regex_pattern(regex_block: str) -> str:    \n    pattern = re.search(r\"<REGEX>(.*)</REGEX>\", regex_block, re.DOTALL)\n    if pattern is None:\n        logger.warning(\"No regex pattern found in the generated block:\\n {regex_block}\")\n        raise None\n    return pattern.group(1)\n\npattern = \"匹配一个邮箱地址\"\nv = generate_regex_pattern.with_llm(llm).with_extractor(extract_regex_pattern).run(desc=pattern)\nprint(v)\n## ^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\n```\n\n在上面的例子里，我们根据一句话生成一个正则表达式。我们通过 `with_extractor` 方法，传入了一个自定义的抽取函数，这个函数会在大模型生成结果后，对结果进行处理，然后返回我们想要的结果。\n\n我们在 prompt 明确说了，生成的结果要放到 `<REGEX></REGEX>` 标签对里，然后我们通过 extract_regex_pattern 函数，从结果中提取出了我们想要的正则表达式。\n\n## 在实例方法中使用大模型\n\n```python\nimport byzerllm\ndata = {\n    'name': 'Jane Doe',\n    'task_count': 3,\n    'tasks': [\n        {'name': 'Submit report', 'due_date': '2024-03-10'},\n        {'name': 'Finish project', 'due_date': '2024-03-15'},\n        {'name': 'Reply to emails', 'due_date': '2024-03-08'}\n    ]\n}\n\n\nclass RAG():\n    def __init__(self):        \n        self.llm = byzerllm.ByzerLLM()\n        self.llm.setup_template(model=\"deepseek_chat\",template=\"auto\")\n        self.llm.setup_default_model_name(\"deepseek_chat\")        \n    \n    @byzerllm.prompt(lambda self:self.llm)\n    def generate_answer(self,name,task_count,tasks)->str:\n        '''\n        Hello {{ name }},\n\n        This is a reminder that you have {{ task_count }} pending tasks:\n        {% for task in tasks %}\n        - Task: {{ task.name }} | Due: {{ task.due_date }}\n        {% endfor %}\n\n        Best regards,\n        Your Reminder System\n        '''        \n\nt = RAG()\n\nresponse = t.generate_answer(**data)\nprint(response)\n\n## 输出:\n## Hello Jane Doe,\n##I hope this message finds you well. I wanted to remind you of your 3 pending tasks to ensure you stay on track:\n## 1. **Submit report** - This task is due on **2024-03-10**. Please ensure that you allocat\n```\n\n这里我们给了个比较复杂的例子，但我们可以看到，给一个实例prompt方法和普通prompt 方法差异不大。\n唯一的区别是如果你希望在定义的时候就指定大模型，使用一个lambda函数返回实例的 llm 对象即可。\n\n```python\n@byzerllm.prompt(lambda self:self.llm)\n```\n\n你也可以不返回，在调用的时候通过 `with_llm` 方法指定 llm 对象。\n\n此外，这个例子也展示了如何通过jinja2模板语法，来处理复杂的结构化数据。\n\n## 通过 Python 代码处理复杂入参\n\n上面的一个例子中，我们通过 jinja2 模板语法，来处理复杂的结构化数据，但是有时候我们可能需要更加复杂的处理，这个时候我们可以通过 Python 代码来处理。\n\n```python\nimport byzerllm\n\ndata = {\n    'name': 'Jane Doe',\n    'task_count': 3,\n    'tasks': [\n        {'name': 'Submit report', 'due_date': '2024-03-10'},\n        {'name': 'Finish project', 'due_date': '2024-03-15'},\n        {'name': 'Reply to emails', 'due_date': '2024-03-08'}\n    ]\n}\n\n\nclass RAG():\n    def __init__(self):        \n        self.llm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n    \n    @byzerllm.prompt()\n    def generate_answer(self,name,task_count,tasks)->str:\n        '''\n        Hello {{ name }},\n\n        This is a reminder that you have {{ task_count }} pending tasks:\n            \n        {{ tasks }}\n\n        Best regards,\n        Your Reminder System\n        '''\n        \n        tasks_str = \"\\n\".join([f\"- Task: {task['name']} | Due: { task['due_date'] }\" for task in tasks])\n        return {\"tasks\": tasks_str}\n\nt = RAG()\n\nresponse = t.generate_answer.with_llm(t.llm).run(**data)\nprint(response)\n\n## Just a gentle nudge to keep you on track with your pending tasks. Here's a quick recap:....\n```\n\n在这个例子里，我们直接把 tasks 在方法体里进行处理，然后作为一个字符串返回，最够构建一个字典，字典的key为 tasks,然后\n你就可以在 docstring 里使用 `{{ tasks }}` 来引用这个字符串。\n\n这样对于很复杂的入参，就不用谢繁琐的 jinja2 模板语法了。\n\n## 如何自动实现一个方法\n\n比如我定义一个签名，但是我不想自己实现里面的逻辑，让大模型来实现。这个在 byzerllm 中叫 function impl。我们来看看怎么\n实现:\n\n```python\nimport pydantic\nclass Time(pydantic.BaseModel):\n    time: str = pydantic.Field(...,description=\"时间，时间格式为 yyyy-MM-dd\")\n\n\n@llm.impl()\ndef calculate_current_time()->Time:\n    '''\n    计算当前时间\n    '''\n    pass \n\n\ncalculate_current_time()\n#output: Time(time='2024-06-14')\n```\n\n在这个例子里，我们定义了一个 calculate_current_time 方法，但是我们没有实现里面的逻辑，我们通过 `@llm.impl()` 装饰器，让大模型来实现这个方法。\n为了避免每次都要“生成”这个方法，导致无法适用，我们提供了缓存，用户可以按如下方式打印速度：\n\n```python\nstart = time.monotonic()\ncalculate_current_time()\nprint(f\"first time cost: {time.monotonic()-start}\")\n\nstart = time.monotonic()\ncalculate_current_time()\nprint(f\"second time cost: {time.monotonic()-start}\")\n\n# output:\n# first time cost: 6.067266260739416\n# second time cost: 4.347506910562515e-05\n```\n可以看到，第一次执行花费了6s,第二次几乎是瞬间完成的，这是因为第一次执行的时候，我们实际上是在生成这个方法，第二次执行的时候，我们是执行已经生成好的代码，所以速度会非常快。你可以显示的调用 `llm.clear_impl_cache()` 清理掉函数缓存。\n\n## Stream 模式\n\n前面的例子都是一次性生成结果，但是有时候我们可能需要一个流式的输出，这个时候我们可能需要用底层一点的API来完成了：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\nv = llm.stream_chat_oai(model=\"deepseek_chat\",conversations=[{\n    \"role\":\"user\",\n    \"content\":\"你好，你是谁\",\n}],delta_mode=True)\n\nfor t in v:\n    print(t,flush=True)  \n\n# 你好\n# ！\n# 我\n# 是一个\n# 人工智能\n# 助手\n# ，\n# 旨在\n# 提供\n# 信息\n# 、\n# 解答\n# 问题....\n```\n\n如果你不想要流式输出，但是想用底层一点的API，你可以使用 `llm.chat_oai` 方法：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\nv = llm.chat_oai(model=\"deepseek_chat\",conversations=[{\n    \"role\":\"user\",\n    \"content\":\"你好，你是谁\",\n}])\n\nprint(v[0].output)\n## 你好！我是一个人工智能助手，旨在提供信息、解答问题和帮助用户解决问题。如果你有任何问题或需要帮助，请随时告诉我。\n```\n\n## Function Calling \n\nbyzerllm 可以不依赖模型自身就能提供 function calling 支持，我们来看个例子：\n\n\n```python\nfrom typing import List,Dict,Any,Annotated\nimport pydantic \nimport datetime\nfrom dateutil.relativedelta import relativedelta\n\ndef compute_date_range(count:Annotated[int,\"时间跨度，数值类型\"],\n                       unit:Annotated[str,\"时间单位，字符串类型\",{\"enum\":[\"day\",\"week\",\"month\",\"year\"]}])->List[str]:\n    '''\n    计算日期范围\n\n    Args:\n        count: 时间跨度，数值类型\n        unit: 时间单位，字符串类型，可选值为 day,week,month,year\n    '''        \n    now = datetime.datetime.now()\n    now_str = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n    if unit == \"day\":\n        return [(now - relativedelta(days=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"week\":\n        return [(now - relativedelta(weeks=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"month\":\n        return [(now - relativedelta(months=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"year\":\n        return [(now - relativedelta(years=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    return [\"\",\"\"]\n\ndef compute_now()->str:\n    '''\n    计算当前时间\n    '''\n    return datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n```\n\n我们定义了两个方法，一个是计算日期范围，一个是计算当前时间。\n\n现在我么可以来测试下，系统如何根据自然语言决定调用哪个方法：\n\n```python\nt = llm.chat_oai([{\n    \"content\":'''计算当前时间''',\n    \"role\":\"user\"    \n}],tools=[compute_date_range,compute_now],execute_tool=True)\n\nt[0].values\n\n## output: ['2024-06-14 15:18:02']\n```\n\n我们可以看到，他正确的选择了 compute_now 方法。\n\n接着我们再试一个：\n\n```python\nt = llm.chat_oai([{\n    \"content\":'''最近三个月趋势''',\n    \"role\":\"user\"    \n}],tools=[compute_date_range,compute_now],execute_tool=True)\n\nt[0].values\n\n## output: [['2024-03-14 15:19:13', '2024-06-14 15:19:13']]\n```\n\n模型正确的选择了 compute_date_range 方法。\n\n## 多模态\n\nbyerllm 也能很好的支持多模态的交互，而且统一了多模态大模型的接口，比如你可以用一样的方式使用 openai 或者 claude 的图片转文字能力， 或者一致的方式使用火山，azuer, openai的语音合成接口。\n\n### 图生文\n\n```python\nimport byzerllm\nfrom byzerllm.types import ImagePath\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\n\n\n@byzerllm.prompt()\ndef what_in_image(image_path: ImagePath) -> str:\n    \"\"\"\n    {{ image_path }}\n    这个图片里有什么？\n    \"\"\"    \n\n\nv = what_in_image.with_llm(vl_llm).run(\n    ImagePath(value=\"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\")\n)\nv\n## OUTPUT: 这张图片展示了多只可爱的猫咪，采用了艺术风格的绘画。猫咪们有不同的颜色和花纹，背景是浅棕色，上面还点缀着一些红色的花朵。整体画面给人一种温馨和谐的感觉\n```\n\n可以看到，我们只需要把 prompt 函数的图片地址入参使用 byzerllm.types.ImagePath里进行包装，就可以直接在 prompt 函数体里\n带上图片。\n\n或者你可以这样：\n\n```python\nimport byzerllm\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\n\n\n@byzerllm.prompt()\ndef what_in_image(image_path: str) -> str:\n    \"\"\"\n    {{ image }}\n    这个图片里有什么？\n    \"\"\"\n    return {\"image\": byzerllm.Image.load_image_from_path(image_path)}\n\n\nv = what_in_image.with_llm(vl_llm).run(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv\n```\n\n通过 `image_path` 参数，然后通过 `byzerllm.Image.load_image_from_path` 方法，转化为一个图片对象 image，最后在 prompt 函数体里\n使用 `{{ image }}` 引用这个图片对象。\n\n另外我们也是可以支持配置多张图片的。\n\n另外，我们也可以使用基础的 `llm.chat_oai` 方法来实现：\n\n```python\nimport byzerllm\nimport json\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\nimage = byzerllm.Image.load_image_from_path(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv = vl_llm.chat_oai(\n    conversations=[\n        {\n            \"role\": \"user\",\n            \"content\": json.dumps(\n                [{\"image\": image, \"text\": \"这个图片里有什么？\"}], ensure_ascii=False\n            ),\n        }\n    ]\n)\nv[0].output\n```\n\n还可以这么写：\n\n```python\nimport byzerllm\nimport json\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\nimage = byzerllm.Image.load_image_from_path(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv = vl_llm.chat_oai(\n    conversations=[\n        {\n            \"role\": \"user\",\n            \"content\": json.dumps(\n                [\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\"url\": image, \"detail\": \"high\"},\n                    },\n                    {\"text\": \"这个图片里有什么？\", \"type\": \"text\"},\n                ],\n                ensure_ascii=False,\n            ),\n        }\n    ]\n)\nv[0].output\n```\n\n### 语音合成\n\n这里以 openai 的 tts 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=tts-1 \\\n--model openai_tts\n```\n\n此外，byzerllm 支持 azure,火山引擎等 tts 语音合成引擎。\n\n接着你可以这么用：\n\n```python\nimport byzerllm\nimport base64\nimport json\n\nllm = byzerllm.ByzerLLM.from_default_model(\"openai_tts\")\n\n\nt = llm.chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\": json.dumps({\n        \"input\":\"hello, open_tts\",\n        \"voice\": \"alloy\",\n        \"response_format\": \"mp3\"\n    },ensure_ascii=False)\n}])\n\nwith open(\"voice.mp3\",\"wb\") as f:\n    f.write(base64.b64decode(t[0].output))\n```\n\ntts 模型生成没有prompt函数可以用，你需要直接使用 chat_oai。\n\n\n### 语音识别\n\n这里以 openai 的 whisper-1 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--worker_concurrency 10 \\\n--infer_params saas.model=whisper-1 saas.api_key=${MODEL_OPENAI_TOKEN} \\\n--model speech_to_text\n```\n\n语音识别的使用方式和图生文类似，我们可以直接在 prompt 函数体里带上音频文件。\n\n```python\nimport byzerllm\nimport json\nimport base64\nfrom byzerllm.types import AudioPath\n\nllm = byzerllm.ByzerLLM.from_default_model(\"speech_to_text\")\n\naudio_file = \"/Users/allwefantasy/videos/output_audio.mp3\"\n\n@byzerllm.prompt(llm=llm)\ndef audio_to_text(audio_file: AudioPath):\n    \"\"\"\n    {{ audio_file }}\n    \"\"\"\n\nv = audio_to_text(AudioPath(value=audio_file))\njson.loads(v)\n```\n输出的数据格式略微复杂：\n\n```\n{'text': 'In the last chapter, you and I started to step through the internal workings of a transformer. This is one of the key pieces of technology inside large language models, and a lot of other tools in the modern wave of AI.',\n 'task': 'transcribe',\n 'language': 'english',\n 'duration': 10.0,\n 'segments': [{'id': 0,\n   'seek': 0,\n   'start': 0.0,\n   'end': 4.78000020980835,\n   'text': ' In the last chapter, you and I started to step through the internal workings of a transformer.',\n   'tokens': [50364,\n    682,\n    264,\n    1036,\n    7187,\n    11,\n    291,\n    293,\n    286,\n    1409,\n    281,\n    1823,\n    807,\n    264,\n    6920,\n    589,\n    1109,\n    295,\n    257,\n    31782,\n    13,\n    50586],\n   'temperature': 0.0,\n   'avg_logprob': -0.28872039914131165,\n   'compression_ratio': 1.4220778942108154,\n   'no_speech_prob': 0.016033057123422623},\n  ....\n  {'id': 2,\n   'seek': 0,\n   'start': 8.579999923706055,\n   'end': 9.979999542236328,\n   'text': ' and a lot of other tools in the modern wave of AI.',\n   'tokens': [50759,\n    293,\n    257,\n    688,\n    295,\n    661,\n    3873,\n    294,\n    264,\n    4363,\n    5772,\n    295,\n    7318,\n    13,\n    50867],\n   'temperature': 0.0,\n   'avg_logprob': -0.28872039914131165,\n   'compression_ratio': 1.4220778942108154,\n   'no_speech_prob': 0.016033057123422623}],\n 'words': [{'word': 'In', 'start': 0.0, 'end': 0.18000000715255737},\n  {'word': 'the', 'start': 0.18000000715255737, 'end': 0.23999999463558197},\n  {'word': 'last', 'start': 0.23999999463558197, 'end': 0.5400000214576721},\n  {'word': 'chapter', 'start': 0.5400000214576721, 'end': 0.800000011920929},\n  ....\n  {'word': 'AI', 'start': 9.920000076293945, 'end': 9.979999542236328}]}\n```\n\n会输出每一句话以及每一个字所在的起始时间和截止时间。你可以根据需要来使用。\n\n\n### 文生图\n\n文生图和语音合成类似，首先要启动合适的模型,以openai 的 dall-e-3 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=dall-e-3 \\\n--model openai_image_gen\n```\n\n启动模型后，只需要记住几个模板参数即可使用，这里直接使用 chat_oai 方法来使用：\n\n```python\n\nimport byzerllm\nimport json\nimport base64\n\nllm = byzerllm.ByzerLLM.from_default_model(\"openai_image_gen\")\nt = llm.chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\": json.dumps({\n        \"input\":\"a white siamese cat\",\n        \"size\": \"1024x1024\",\n        \"quality\": \"standard\"\n    },ensure_ascii=False)\n}])\n\nwith open(\"output1.jpg\",\"wb\") as f:\n    f.write(base64.b64decode(t[0].output))\n\n\nimport matplotlib.pyplot as plt\n\nimage_path = \"output1.jpg\"\nimage = plt.imread(image_path)\n\nplt.imshow(image)\nplt.axis('off')\nplt.show()\n```\n\n## Prompt 函数的流式输出\n\nbyzerllm 底层支持流式输出，非 prompt 函数的用法是这样的：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\n\nv = llm.stream_chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\":\"讲一个100字的故事\"\n}])\n\nfor s  in v:\n    print(s[0], end=\"\")\n```\n\n如果你像用 prompt 函数，可以这么用：\n\n\n```python\nimport byzerllm\nimport json\nimport base64\nfrom typing import Generator\n\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\n\n@byzerllm.prompt()\ndef tell_story() -> Generator[str, None, None]:\n    '''\n    给我讲一个一百多字的故事\n    '''\n\nv = tell_story.with_llm(llm).run()    \nfor i in v:\n    print(i, end=\"\")\n```\n\n可以看到，和普通的 prompt 函数的区别在于，返回值是一个生成器，然后你可以通过 for 循环来获取结果。\n\n## 向量化模型\n\nbyzerllm 支持向量化模型,你可以这样启动一个本地的模型：\n\n```bash\n!byzerllm deploy --pretrained_model_type custom/bge \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--worker_concurrency 10 \\\n--model_path /home/winubuntu/.auto-coder/storage/models/AI-ModelScope/bge-large-zh \\\n--infer_backend transformers \\\n--num_workers 1 \\\n--model emb\n```\n\n注意两个参数:\n\n1. --infer_backend transformers: 表示使用 transformers 作为推理后端。\n2. --model_path: 表示模型的路径。\n\n也可以启动一个 SaaS 的emb模型,比如 qwen 的 emb 模型：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/qianwen \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 2 \\\n--infer_params saas.api_key=${MODEL_QIANWEN_TOKEN}  saas.model=text-embedding-v2 \\\n--model qianwen_emb\n```\n\n或者 openai 的 emb 模型：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--worker_concurrency 10 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=text-embedding-3-small \\\n--model gpt_emb\n```\nSaaS 模型无需配置 `--infer_backend` 参数。\n\n无论是本地模型还是 SaaS 模型，我们都可以这样使用：\n\n```python\nimport byzerllm\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\nllm.setup_default_emb_model_name(\"emb\")\nllm.emb_query(\"你好\")\n```\n\n如果你配置 byzerllm 中的 Storage 使用，比如你这样启动了存储：\n\n```bash\nbyzerllm storage start --enable_emb\n```\n\n那么需要这么使用：\n\n```python\nfrom byzerllm.apps.byzer_storage.simple_api import ByzerStorage, DataType, FieldOption,SortOption\nstorage = ByzerStorage(\"byzerai_store\", \"memory\", \"memory\")\nstorage.emb(\"你好\")\n```\n\n## 一些辅助工具\n\n当调用 prompt 函数返回字符串的时候，如果想从里面抽取代码，可以使用如下方式：\n\n```python\nfrom byzerllm.utils.client import code_utils\ntext_with_markdown = '''\n```shell\nls -l\n```\n'''\ncode_blocks = code_utils.extract_code(text_with_markdown)\nfor code_block in code_blocks:\n    if code_block[0] == \"shell\":\n        print(code_block[1])\n##output: ls -l        \n```\n\n\n## 注意事项\n\n1. prompt函数方法体返回只能是dict，实际的返回类型和方法签名可以不一样，但是方法体返回只能是dict。\n2. 大部分情况prompt函数体为空，如果一定要有方法体，可以返回一个空字典。\n3. 调用prompt方法的时候，如果在@byzerllm.prompt()里没有指定llm对象，那么需要在调用的时候通过with_llm方法指定llm对象。\n\n===========================\n\n# Byzer Storage: 最易用的AI存储引擎\n\nByzer Storage是一个为AI应用设计的高性能存储引擎,它提供了简单易用的API,支持向量搜索、全文检索以及结构化查询。本文将详细介绍Byzer Storage的使用方法和主要特性。\n\n## 0. 安装和启动\n\n```bash\npip install byzerllm\nbyzerllm storage start\n```\n\nThat's it! Byzer Storage已经安装并启动成功,现在我们可以开始使用它了。默认会启动一个 byzerai_store 的集群。\n\n注意，如果你是这样启动的：\n\n```bash\nbyzerllm storage start --enable_emb\n```\n\n那么会自动启动一个 emb 模型，名字就叫 emb， ByzerStorage 会自动使用该模型，无需做任何其他配置。\n\n## 1. 初始化\n\n创建一个 ByzerStorage 对象，链接 byzerai_store 集群，并且指定数据库和表名（可以不存在）。\n\n```python\nfrom byzerllm.apps.byzer_storage.simple_api import ByzerStorage, DataType, FieldOption, SortOption\n\nstorage = ByzerStorage(\"byzerai_store\", \"my_database1\", \"my_table4s\")\n```\n\n\n## 2. 创建库表（可选）\n\nByzer Storage使用Schema来定义数据结构。我们可以使用SchemaBuilder来构建Schema:\n\n```python\n_ = (\n    storage.schema_builder()\n    .add_field(\"_id\", DataType.STRING)\n    .add_field(\"name\", DataType.STRING)\n    .add_field(\"content\", DataType.STRING, [FieldOption.ANALYZE])\n    .add_field(\"raw_content\", DataType.STRING, [FieldOption.NO_INDEX])    \n    .add_array_field(\"summary\", DataType.FLOAT)    \n    .add_field(\"created_time\", DataType.LONG, [FieldOption.SORT])    \n    .execute()\n)\n```\n\n这个Schema定义了以下字段:\n- `_id`: 字符串类型的主键\n- `name`: 字符串类型,可用于过滤条件\n- `content`: 字符串类型,带有ANALYZE选项,用于全文搜索\n- `raw_content`: 字符串类型,带有NO_INDEX选项,不会被索引\n- `summary`: 浮点数组类型,用于存储向量\n- `created_time`: 长整型,带有SORT选项,可用于排序\n\n需要注意的是：\n\n1. 如果一个字段带有ANALYZE选项,则该字段会被分词,并且可以用于全文搜索，但是就无法返回原始的文本了。所以你需要添加一个新字段专门用来存储原文，比如在我们这个例子里，我们新增了 raw_content 字段，并且显示的指定了 NO_INDEX 选项，这样就不会被索引，也就不会被分词，可以后续被检索后用来获取原始文本。\n2. 对于需要作为向量检索和存储的字段，需要指定为数组类型，比如我们这个例子里的 summary 字段。\n3. 如果你需要拿到向量字段的原始文本，那么你也需要添加一个新字段专门用来存储原文，就像我们这个例子里的 raw_content 字段一样。\n\n\n## 3. 写入数据\n\n准备数据并使用WriteBuilder写入Storage:\n\n```python\ndata = [\n    {\"_id\": \"1\", \"name\": \"Hello\", \"content\": \"Hello, world!\", \"raw_content\": \"Hello, world!\", \"summary\": \"hello world\", \"created_time\": 1612137600},\n    {\"_id\": \"2\", \"name\": \"Byzer\", \"content\": \"Byzer, world!\", \"raw_content\": \"Byzer, world!\", \"summary\": \"byzer\", \"created_time\": 1612137601},\n    {\"_id\": \"3\", \"name\": \"AI\", \"content\": \"AI, world!\", \"raw_content\": \"AI, world!\", \"summary\": \"AI\", \"created_time\": 16121376002},\n    {\"_id\": \"4\", \"name\": \"ByzerAI\", \"content\": \"ByzerAI, world!\", \"raw_content\": \"ByzerAI, world!\", \"summary\": \"ByzerAi\", \"created_time\": 16121376003},\n]\n\nstorage.write_builder().add_items(data, vector_fields=[\"summary\"], search_fields=[\"content\"]).execute()\nstorage.commit()\n```\n\n这里我们使用`add_items`方法批量添加数据,并指定了`summary`为向量字段,`content`为搜索字段。最后调用`commit()`来确保数据被持久化。\n\n\n从上面我们要写入到 byzer storage 的数据我们可以看到如下几个特点：\n\n1. 需要包含我们之前定义的 Schema中罗列的所有的字段，同时需要指定哪些是向量字段，哪些是检索字段。\n2. 向量和检索不能是同一个字段。\n3. 对于向量，检索字段，我们给到 write_builder 的都是文本，ByzerStorage 会根据 Schema 的定义自动将其转换为向量，检索字段需要的格式。\n\n## 4. 查询数据\n\nByzer Storage支持多种查询方式,包括向量搜索、全文检索、过滤和排序。\n\n### 4.1 向量搜索 + 全文检索\n\n```python\nquery = storage.query_builder()\nquery.set_vector_query(\"ByzerAI\", fields=[\"summary\"])\nresults = query.set_search_query(\"Hello\", fields=[\"content\"]).execute()\nprint(results)\n```\n\n这个查询结合了向量搜索和全文检索,它会在`summary`字段中搜索与\"ByzerAI\"相似的向量,同时在`content`字段中搜索包含\"Hello\"的文档。\n\n### 4.2 过滤 + 向量搜索 + 全文检索\n\n```python\nquery = storage.query_builder()\nquery.and_filter().add_condition(\"name\", \"AI\").build()\nquery.set_vector_query(\"ByzerAI\", fields=\"summary\")\nresults = query.set_search_query(\"Hello\", fields=[\"content\"]).execute()\nprint(results)\n```\n\n这个查询首先过滤`name`字段等于\"AI\"的文档,然后在结果中进行向量搜索和全文检索。\n\n### 4.3 过滤 + 排序\n\n```python\nquery = storage.query_builder()\nquery.and_filter().add_condition(\"name\", \"AI\").build().sort(\"created_time\", SortOption.DESC)\nresults = query.execute()\nprint(results)\n```\n\n这个查询过滤`name`字段等于\"AI\"的文档,然后按`created_time`字段降序排序。\n\n## 5. 删除数据\n\n### 5.1 根据ID删除\n\n```python\nstorage.delete_by_ids([\"3\"])\n\nquery = storage.query_builder()\nquery.and_filter().add_condition(\"name\", \"AI\").build()\nresults = query.execute()\nprint(results)\n```\n\n这里我们删除了ID为\"3\"的文档,然后查询验证删除结果。\n\n### 5.2 删除整个表\n\n```python\nstorage.drop()\n```\n\n这个操作会删除整个表及其所有数据,请谨慎使用。\n\n## 结论\n\nByzer Storage提供了一套简洁而强大的API,能够轻松实现向量搜索、全文检索、结构化查询等功能。它的设计非常适合AI应用场景,可以有效地存储和检索各种类型的数据。通过本文的介绍,相信读者已经对Byzer Storage有了基本的了解,并能够开始在自己的项目中使用这个强大的存储引擎。\n\n\n     \n\n下面是用户的需求：\n\n在 @types.py 新增一个 ByzerSQLConfig 里面有个 byzer_sql_url ,然后同时传递给 @@SQLGenerator\n"}, {"role": "assistant", "content": "好的，我理解您的需求。我们需要在 `types.py` 文件中新增一个 `ByzerSQLConfig` 类，并在其中添加 `byzer_sql_url` 字段。然后，我们需要将这个配置传递给 `SQLGenerator` 类。让我为您实现这些更改。\n\n首先，让我们修改 `types.py` 文件：\n\n```python\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/types.py\n<<<<<<< SEARCH\nfrom pydantic import BaseModel\n\nclass RAGConfig(BaseModel):\n    rag_base_url: str = \"\"\n    rag_api_key: str = \"\"\n=======\nfrom pydantic import BaseModel\n\nclass RAGConfig(BaseModel):\n    rag_base_url: str = \"\"\n    rag_api_key: str = \"\"\n\nclass ByzerSQLConfig(BaseModel):\n    byzer_sql_url: str = \"\"\n>>>>>>> REPLACE\n```\n\n现在，让我们更新 `generator.py` 文件以接收并使用 `ByzerSQLConfig`：\n\n```python\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/generator.py\n<<<<<<< SEARCH\nfrom typing import Optional, Dict, Any\nfrom openai import OpenAI\nimport json\nfrom .types import RAGConfig\nfrom byzerllm.utils.client import code_utils\n\n\nclass SQLGenerator:\n\n    def __init__(self, rag_config: RAGConfig) -> None:        \n        self.rag_client = OpenAI(api_key=rag_config.rag_api_key, base_url=rag_config.rag_base_url)\n=======\nfrom typing import Optional, Dict, Any\nfrom openai import OpenAI\nimport json\nfrom .types import RAGConfig, ByzerSQLConfig\nfrom byzerllm.utils.client import code_utils\n\n\nclass SQLGenerator:\n\n    def __init__(self, rag_config: RAGConfig, byzer_sql_config: ByzerSQLConfig) -> None:        \n        self.rag_client = OpenAI(api_key=rag_config.rag_api_key, base_url=rag_config.rag_base_url)\n        self.byzer_sql_url = byzer_sql_config.byzer_sql_url\n>>>>>>> REPLACE\n```\n\n最后，我们需要更新 `cli.py` 文件以传递 `ByzerSQLConfig` 到 `SQLGenerator`：\n\n```python\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/cli.py\n<<<<<<< SEARCH\nfrom superanalysis.sql.types import RAGConfig\n=======\nfrom superanalysis.sql.types import RAGConfig, ByzerSQLConfig\n>>>>>>> REPLACE\n```\n\n```python\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/cli.py\n<<<<<<< SEARCH\n    # Add new arguments for RAGConfig\n    parser.add_argument(\"--rag-base-url\", type=str, default=\"\", help=\"RAG base URL\")\n    parser.add_argument(\"--rag-api-key\", type=str, default=\"\", help=\"RAG API key\")\n=======\n    # Add new arguments for RAGConfig\n    parser.add_argument(\"--rag-base-url\", type=str, default=\"\", help=\"RAG base URL\")\n    parser.add_argument(\"--rag-api-key\", type=str, default=\"\", help=\"RAG API key\")\n    \n    # Add new argument for ByzerSQLConfig\n    parser.add_argument(\"--byzer-sql-url\", type=str, default=\"\", help=\"Byzer SQL URL\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/cli.py\n<<<<<<< SEARCH\n    # Create RAGConfig instance\n    rag_config = RAGConfig(\n        rag_base_url=args.rag_base_url,\n        rag_api_key=args.rag_api_key\n    )\n=======\n    # Create RAGConfig instance\n    rag_config = RAGConfig(\n        rag_base_url=args.rag_base_url,\n        rag_api_key=args.rag_api_key\n    )\n\n    # Create ByzerSQLConfig instance\n    byzer_sql_config = ByzerSQLConfig(\n        byzer_sql_url=args.byzer_sql_url\n    )\n>>>>>>> REPLACE\n```\n\n```python\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/cli.py\n<<<<<<< SEARCH\n    llm.setup_default_model_name(server_args.served_model_name)\n    rag = SQLGenerator(llm=llm)    \n    llm_wrapper = LLWrapper(llm=llm, rag=rag)\n    serve(llm_wrapper, server_args, rag_config)\n=======\n    llm.setup_default_model_name(server_args.served_model_name)\n    rag = SQLGenerator(rag_config=rag_config, byzer_sql_config=byzer_sql_config)    \n    llm_wrapper = LLWrapper(llm=llm, rag=rag)\n    serve(llm_wrapper, server_args, rag_config)\n>>>>>>> REPLACE\n```\n\n这些更改完成后，我们已经：\n\n1. 在 `types.py` 中新增了 `ByzerSQLConfig` 类，包含 `byzer_sql_url` 字段。\n2. 更新了 `generator.py` 中的 `SQLGenerator` 类，使其接收 `ByzerSQLConfig` 并存储 `byzer_sql_url`。\n3. 在 `cli.py` 中添加了新的命令行参数 `--byzer-sql-url`，并创建了 `ByzerSQLConfig` 实例。\n4. 将 `ByzerSQLConfig` 传递给 `SQLGenerator` 的构造函数。\n\n现在，您可以通过命令行参数 `--byzer-sql-url` 来指定 Byzer SQL URL，这个 URL 将被传递给 `SQLGenerator` 类。"}], "model": "human", "yaml_file": "actions/111_chat_action.yml"}
{"instruction": "在 @cli.py 中添加 --data_dir 参数,然后给 @@ByzerSQLConfig 添加相同的参数", "conversations": [{"role": "user", "content": "\n如果你需要生成代码，对于每个需要更改的文件,你需要按 *SEARCH/REPLACE block* 的格式进行生成。\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The opening fence and code language, eg: ```python\n2. The file path alone on a line, starting with \"##File:\" and verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n*SEARCH/REPLACE* blocks will replace *all* matching occurrences.\nInclude enough lines to make the SEARCH blocks unique.\n\nInclude *ALL* the code being searched and replaced!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n下面我们来看一个例子：\n\n当前项目目录结构：\n1. 项目根目录： /tmp/projects/mathweb\n2. 项目子目录/文件列表(类似tree 命令输出)\nflask/\n    app.py\n    templates/\n        index.html\n    static/\n        style.css\n\n用户需求： Change get_factorial() to use math.factorial\n\n回答： To make this change we need to modify `/tmp/projects/mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n用户需求： Refactor hello() into its own file.\n\n回答：To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/hello.py\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/main.py\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n现在让我们开始一个新的任务:\n\n当前项目目录结构：\n1. 项目根目录： /Users/allwefantasy/projects/super_analysis\n2. 项目子目录/文件列表(类似tree 命令输出)：\nsetup.py\n.auto-coder/\n    libs/\n        llm_friendly_packages/\n            src/\n                example1/\n                    main.py\nsrc/\n    superanalysis/\n        serve/\n            convert.py\n            openai_serve.py\n            __init__.py\n            llm_wrapper.py\n            api_server.py\n            cli.py\n        sql/\n            __init__.py\n            types.py\n            generator.py\n下面是一些文件路径以及每个文件对应的源码：\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/types.py\nfrom pydantic import BaseModel\n\nclass RAGConfig(BaseModel):\n    rag_base_url: str = \"\"\n    rag_api_key: str = \"\"\n\nclass ByzerSQLConfig(BaseModel):\n    byzer_sql_url: str = \"\"\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/cli.py\nimport argparse\nimport byzerllm\nfrom byzerllm.utils.client import ByzerLLM\nfrom superanalysis.serve.api_server import serve, ServerArgs\nfrom superanalysis.sql.types import RAGConfig, ByzerSQLConfig\nfrom superanalysis.sql.generator import SQLGenerator\nfrom superanalysis.serve.llm_wrapper import LLWrapper\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"ByzerLLM API Server\")\n    parser.add_argument(\"--host\", type=str, default=\"0.0.0.0\", help=\"Host to bind the server to\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"Port to bind the server to\")\n    parser.add_argument(\"--uvicorn-log-level\", type=str, default=\"info\", help=\"Uvicorn log level\")\n    parser.add_argument(\"--allow-credentials\", action=\"store_true\", help=\"Allow credentials\")\n    parser.add_argument(\"--allowed-origins\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed origins\")\n    parser.add_argument(\"--allowed-methods\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed methods\")\n    parser.add_argument(\"--allowed-headers\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed headers\")\n    parser.add_argument(\"--api-key\", type=str, default=\"\", help=\"API key for authentication\")\n    parser.add_argument(\"--served-model-name\", type=str, required=True, help=\"Name of the model to be served\")\n    parser.add_argument(\"--prompt-template\", type=str, default=\"\", help=\"Prompt template\")\n    parser.add_argument(\"--response-role\", type=str, default=\"assistant\", help=\"Response role\")\n    parser.add_argument(\"--ssl-keyfile\", type=str, default=\"\", help=\"SSL key file\")\n    parser.add_argument(\"--ssl-certfile\", type=str, default=\"\", help=\"SSL certificate file\")\n    \n    # Add new arguments for RAGConfig\n    parser.add_argument(\"--rag-base-url\", type=str, default=\"\", help=\"RAG base URL\")\n    parser.add_argument(\"--rag-api-key\", type=str, default=\"\", help=\"RAG API key\")\n    \n    # Add new argument for ByzerSQLConfig\n    parser.add_argument(\"--byzer-sql-url\", type=str, default=\"\", help=\"Byzer SQL URL\")\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile\n    )\n\n    # Create RAGConfig instance\n    rag_config = RAGConfig(\n        rag_base_url=args.rag_base_url,\n        rag_api_key=args.rag_api_key\n    )\n\n    # Create ByzerSQLConfig instance\n    byzer_sql_config = ByzerSQLConfig(\n        byzer_sql_url=args.byzer_sql_url\n    )\n\n    llm = ByzerLLM()\n    llm.setup_default_model_name(args.served_model_name)\n    \n    byzerllm.connect_cluster(address=args.ray_address)\n    llm = byzerllm.ByzerLLM()\n    if not server_args.served_model_name:\n        raise ValueError(\"served_model_name is required\")\n    llm.setup_default_model_name(server_args.served_model_name)\n    rag = SQLGenerator(rag_config=rag_config, byzer_sql_config=byzer_sql_config)    \n    llm_wrapper = LLWrapper(llm=llm, rag=rag)\n    serve(llm_wrapper, server_args)\n\nif __name__ == \"__main__\":\n    main()\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/__init__.py\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/__init__.py\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/llm_wrapper.py\nfrom typing import Any, Dict, List, Optional, Union,Callable\nfrom byzerllm.utils.client.types import (    \n    LLMFunctionCallResponse,\n    LLMClassResponse,LLMResponse    \n)\nimport pydantic\nfrom byzerllm import ByzerLLM\nfrom byzerllm.utils.client import LLMResponse\nfrom byzerllm.utils.types import SingleOutputMeta\nfrom byzerllm.utils.langutil import asyncfy_with_semaphore\nfrom superanalysis.sql.generator import SQLGenerator\nfrom loguru import logger\n\nclass LLWrapper:\n\n    def __init__(self,llm:ByzerLLM,entry:SQLGenerator):\n        self.entry = entry\n        self.llm = llm\n        \n\n    def chat_oai(self,\n                 conversations,\n                 tools:List[Union[Callable,str]]=[], \n                 tool_choice:Optional[Union[Callable,str]]=None,\n                 execute_tool:bool=False,  \n                 impl_func:Optional[Callable]=None,\n                 execute_impl_func:bool=False,\n                 impl_func_params:Optional[Dict[str,Any]]=None,\n                 func_params:Optional[Dict[str,Any]]=None,\n                 response_class:Optional[Union[pydantic.BaseModel,str]] = None, \n                 response_after_chat:Optional[Union[pydantic.BaseModel,str]] = False,\n                 enable_default_sys_message:bool=True,                 \n                 model:Optional[str] = None,\n                 role_mapping=None,llm_config:Dict[str,Any]={}\n                 )->Union[List[LLMResponse],List[LLMFunctionCallResponse],List[LLMClassResponse]]: \n        res,contexts = self.entry.stream_chat_oai(conversations)\n        s = \"\".join(res)        \n        return [LLMResponse(output=s,metadata={},input=\"\")]\n\n    def stream_chat_oai(self,conversations, \n                        model:Optional[str]=None, \n                        role_mapping=None,\n                        delta_mode=False,\n                        llm_config:Dict[str,Any]={}): \n        res,contexts = self.entry.stream_chat_oai(conversations)        \n        for t in res:                        \n            yield (t,SingleOutputMeta(0,0))\n\n    async def async_stream_chat_oai(self,conversations, \n                        model:Optional[str]=None, \n                        role_mapping=None,\n                        delta_mode=False,\n                        llm_config:Dict[str,Any]={}): \n        res,contexts = self.rag.stream_chat_oai(conversations)                \n        for t in res:                                    \n            yield (t,SingleOutputMeta(0,0))\n                         \n\n    def __getattr__(self, name):        \n        return getattr(self.llm, name)\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/openai_serve.py\nimport argparse\nfrom service_server.api_server import ServerArgs, serve\nfrom service_server.llm_wrapper import LLWrapper\nfrom service_server.summary_rag import SummaryRAG\nimport byzerllm\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"OpenAI Serve\")\n    parser.add_argument(\n        \"--model\",\n        default=\"\",\n    )\n    parser.add_argument(\"--emb_model\", default=\"\")\n    parser.add_argument(\n        \"--ray_address\",\n        default=\"auto\",\n    )\n    parser.add_argument(\"--source_dir\", default=\".\", help=\"\")\n    parser.add_argument(\"--host\", default=\"\", help=\"\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"\")\n    parser.add_argument(\"--uvicorn_log_level\", default=\"info\", help=\"\")\n    parser.add_argument(\"--allow_credentials\", action=\"store_true\", help=\"\")\n    parser.add_argument(\"--allowed_origins\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--allowed_methods\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--allowed_headers\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--api_key\", default=\"\", help=\"\")\n    parser.add_argument(\"--served_model_name\", default=\"\", help=\"\")\n    parser.add_argument(\"--prompt_template\", default=\"\", help=\"\")\n    parser.add_argument(\"--ssl_keyfile\", default=\"\", help=\"\")\n    parser.add_argument(\"--ssl_certfile\", default=\"\", help=\"\")\n    parser.add_argument(\"--response_role\", default=\"assistant\", help=\"\")\n    parser.add_argument(\n        \"--collections\", default=\"\", help=\"Collection name for indexing\"\n    )\n    parser.add_argument(\n        \"--base_dir\",\n        default=\"\",\n        help=\"Path where the processed text embeddings were stored\",\n    )\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name or args.model,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile,\n    )\n    byzerllm.connect_cluster(address=args.ray_address)\n    llm = byzerllm.ByzerLLM()\n    if not server_args.served_model_name:\n        raise ValueError(\"served_model_name is required\")\n    llm.setup_default_model_name(server_args.served_model_name)\n    rag = SummaryRAG(llm=llm)    \n    llm_wrapper = LLWrapper(llm=llm, rag=rag)\n    serve(llm=llm_wrapper, args=server_args)\n\nif __name__ == \"__main__\":\n    main()\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/api_server.py\nimport os\nimport time\nimport uvicorn\nfrom fastapi import Request\nfrom fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import JSONResponse, StreamingResponse, Response\n\nfrom byzerllm.log import init_logger\nfrom byzerllm.utils import random_uuid\nfrom byzerllm.version import __version__ as version\nfrom byzerllm.utils.client import ByzerLLM, LLMRequest\nfrom byzerllm.utils.client.entrypoints.openai.serving_chat import OpenAIServingChat\nfrom byzerllm.utils.client.entrypoints.openai.serving_completion import OpenAIServingCompletion\nfrom byzerllm.utils.client.entrypoints.openai.protocol import (\n    ModelList,\n    ModelCard,\n    ModelPermission,\n    ChatCompletionRequest,\n    ErrorResponse,\n    CompletionRequest,\n    Embeddings,\n    EmbeddingsOutput,\n    EmbeddingsData,\n    EmbeddingsUsage,\n)\nfrom pydantic import BaseModel\nfrom typing import List\n\nlogger = init_logger(__name__)\n\nllm_client: ByzerLLM = None\nopenai_serving_chat: OpenAIServingChat = None\nopenai_serving_completion: OpenAIServingCompletion = None\n\nTIMEOUT_KEEP_ALIVE = 5  # seconds\n# timeout in 10 minutes. Streaming can take longer than 3 min\nTIMEOUT = float(os.environ.get(\"BYZERLLM_APISERVER_HTTP_TIMEOUT\", 600))\n\nrouter_app = FastAPI()\n\n\n@router_app.get(\"/health\")\nasync def health() -> Response:\n    \"\"\"Health check.\"\"\"\n    return Response(status_code=200)\n\n\n@router_app.get(\"/v1/models\")\nasync def show_available_models():\n    models = await openai_serving_chat.show_available_models()\n    return JSONResponse(content=models.model_dump())\n\n\n@router_app.get(\"/version\")\nasync def show_version():\n    return JSONResponse(content={\"version\": version})\n\n\n@router_app.get(\"/v1/models\", response_model=ModelList)\nasync def models() -> ModelList:\n    \"\"\"Show available models. Right now we only have one model.\"\"\"\n    model_cards = [\n        ModelCard(\n            id=\"\",\n            root=\"\",\n            permission=[ModelPermission()]\n        )\n    ]\n    return ModelList(data=model_cards)\n\n\n@router_app.post(\"/v1/completions\")\nasync def create_completion(\n        body: CompletionRequest,\n        request: Request\n):\n    generator = await openai_serving_completion.create_completion(body, request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            status_code=generator.code\n        )\n    if body.stream:\n        return StreamingResponse(\n            content=generator,\n            media_type=\"text/event-stream\"\n        )\n    else:\n        return JSONResponse(content=generator.model_dump())\n\n\n@router_app.post(\"/v1/chat/completions\")\nasync def create_chat_completion(\n        body: ChatCompletionRequest,\n        request: Request,\n):\n    \"\"\"Completion API similar to OpenAI's API.\n\n    See  https://platform.openai.com/docs/api-reference/chat/create\n    for the API specification. This API mimics the OpenAI ChatCompletion API.\n\n    NOTE: Currently we do not support the following features:\n        - function_call (Users should implement this by themselves)\n        - logit_bias (to be supported by vLLM engine)\n    \"\"\"\n    # async with async_timeout.timeout(TIMEOUT):\n\n    generator = await openai_serving_chat.create_chat_completion(body, request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            status_code=generator.code\n        )\n    if body.stream:\n        return StreamingResponse(\n            content=generator,\n            media_type=\"text/event-stream\"\n        )\n    else:\n        return JSONResponse(content=generator.model_dump())\n\n\n@router_app.post(\"/v1/embeddings\")\nasync def embed(body: Embeddings):\n    \"\"\"Given a prompt, the model will return one embedding.\n\n    Returns:\n        A response object with an embedding.\n    \"\"\"\n    embedding_id = f\"embed-{random_uuid()}\"\n\n    results_list = llm_client.emb(body.model, request=LLMRequest(instruction=body.input))\n    tokens = 0\n\n    return EmbeddingsOutput(\n        data=[\n            EmbeddingsData(\n                embedding=results.output,\n                index=i,\n                object=\"embedding\",\n            )\n            for i, results in enumerate(results_list)\n        ],\n        id=embedding_id,\n        object=\"list\",\n        created=int(time.time()),\n        model=body.model,\n        usage=EmbeddingsUsage(\n            prompt_tokens=tokens,\n            total_tokens=tokens,\n        ),\n    )\n\nclass ServerArgs(BaseModel):\n    host: str = None\n    port: int = 8000\n    uvicorn_log_level: str = \"info\"\n    allow_credentials: bool = False\n    allowed_origins: List[str] = [\"*\"]  \n    allowed_methods: List[str] = [\"*\"]\n    allowed_headers: List[str] = [\"*\"]\n    api_key: str = None\n    served_model_name: str = None\n    prompt_template: str = None\n    response_role: str = \"assistant\"\n    ssl_keyfile: str = None\n    ssl_certfile: str = None    \n\ndef serve(llm:ByzerLLM, args: ServerArgs):\n    \n    logger.info(f\"ByzerLLM API server version {version}\")\n    logger.info(f\"args: {args}\")\n\n    router_app.add_middleware(\n        CORSMiddleware,\n        allow_origins=args.allowed_origins,\n        allow_credentials=args.allow_credentials,\n        allow_methods=args.allowed_methods,\n        allow_headers=args.allowed_headers,\n    )\n    \n    if token := os.environ.get(\"BYZERLLM_API_KEY\") or args.api_key:\n\n        @router_app.middleware(\"http\")\n        async def authentication(request: Request, call_next):\n            if not request.url.path.startswith(\"/v1\"):\n                return await call_next(request)\n            if request.headers.get(\"Authorization\") != \"Bearer \" + token:\n                return JSONResponse(\n                    content={\"error\": \"Unauthorized\"},\n                    status_code=401\n                )\n            return await call_next(request)\n\n    # Register labels for metrics\n    # add_global_metrics_labels(model_name=engine_args.model)\n    global llm_client\n    llm_client = llm\n    \n    global openai_serving_chat\n    openai_serving_chat = OpenAIServingChat(\n        llm_client=llm_client,\n        response_role=args.response_role,\n        server_model_name=args.served_model_name,\n        prompt_template=args.prompt_template\n    )\n    global openai_serving_completion\n    openai_serving_completion = OpenAIServingCompletion(\n        llm_client=llm_client,\n        server_model_name=args.served_model_name,\n        prompt_template=args.prompt_template\n    )\n\n    uvicorn.run(\n        router_app,\n        host=args.host,\n        port=args.port,\n        log_level=args.uvicorn_log_level,\n        timeout_keep_alive=TIMEOUT_KEEP_ALIVE,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile\n    )\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/generator.py\nfrom typing import Optional, Dict, Any\nfrom openai import OpenAI\nimport json\nfrom .types import RAGConfig, ByzerSQLConfig\nfrom byzerllm.utils.client import code_utils\n\n\nclass SQLGenerator:\n\n    def __init__(self, rag_config: RAGConfig, byzer_sql_config: ByzerSQLConfig) -> None:        \n        self.rag_client = OpenAI(api_key=rag_config.rag_api_key, base_url=rag_config.rag_base_url)\n        self.byzer_sql_url = byzer_sql_config.byzer_sql_url\n\n    def stream_chat_oai(\n        self,\n        conversations,\n        model: Optional[str] = None,\n        role_mapping=None,\n        delta_mode=False,\n        llm_config: Dict[str, Any] = {},\n    ):\n        if not model:\n            model = \"deepseek_chat\"\n        response = self.rag_client.chat.completions.create(\n        messages=conversations,\n        model=model,\n    )\n        sql_str = response.choices[0].message.content\n        pass\n\n\n\n##File: /Users/allwefantasy/projects/super_analysis/setup.py\nfrom setuptools import setup, find_packages\n\nsetup(\n    name=\"super-analysis\",\n    version=\"0.1.0\",\n    packages=find_packages(where=\"src\"),\n    package_dir={\"\": \"src\"},\n    install_requires=[\n        \"byzerllm\",\n        \"fastapi\",\n        \"uvicorn\",\n        \"argparse\",\n    ],\n    entry_points={\n        \"console_scripts\": [\n            \"super-analysis.serve=superanalysis.serve.cli:main\",\n            \"super-analysis.convert=superanalysis.serve.convert:main\",\n        ],\n    },\n    author=\"allwefantasy\",\n    author_email=\"allwefantasy@gmail.com\",\n    description=\"A package for super analysis with ByzerLLM\",\n    long_description=open(\"README.md\").read(),\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/yourusername/super-analysis\",\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n    ],\n    python_requires='>=3.6',\n)\n\n##File: .auto-coder/libs/llm_friendly_packages/github.com/allwefantasy/byzer-llm/README.md\n# byzerllm 大模型编程快速指南\n\n本文示例 [Notebook](../../notebooks/003_byzerllm_大模型编程快速指南.ipynb)\n\n## 安装\n\n```bash\npip install -U byzerllm\nray start --head\n```\n## 启动一个模型代理\n\nbyzerllm 支持私有化模型或者SaaS模型的部署。\n\n这里以 deepseek 官方API 为例：\n\n```bash\neasy-byzerllm deploy deepseek-chat --token xxxxx --alias deepseek_chat\n```\n\n或者跬基流动API:\n\n```bash\neasy-byzerllm deploy alibaba/Qwen1.5-110B-Chat --token xxxxx --alias qwen110b_chat\n```\n\n将上面的 API KEY 替换成你们自己的。\n\n如果你想部署私有化模型或者对接 Ollama 等更多需求，参考 [002_使用byzerllm进行模型部署.md](./002_使用byzerllm进行模型部署.md) 或者 [README.md](../../README.md)。\n\n之后，你就可以在代码里使用  deepseek_chat 或者 qwen110b_chat  访问模型了。\n\n## hello world\n\n来和我们的大模型打个招呼:\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\n@byzerllm.prompt(llm=llm)\ndef hello(q:str) ->str:\n    '''\n    你好, {{ q }}\n    '''\n\ns = hello(\"你是谁\")    \nprint(s)\n\n## 输出:\n## '你好！我是一个人工智能助手，专门设计来回答问题、提供信息和帮助解决问题。如果你有任何疑问或需要帮助，请随时告诉我。'\n```\n\n恭喜，你和大模型成功打了招呼！\n\n可以看到，我们通过 `@byzerllm.prompt` 装饰器，将一个方法转换成了一个大模型的调用，然后这个方法的主题是一段文本，文本中\n使用了 jinja2 模板语法，来获得方法的参数。当正常调用该方法时，实际上就发起了和大模型的交互，并且返回了大模型的结果。\n\n在 byzerllm 中，我们把这种方法称之为 prompt 函数。\n\n## 查看发送给大模型的prompt\n\n很多情况你可能需要调试，查看自己的 prompt 渲染后到底是什么样子的，这个时候你可以通过如下方式\n获取渲染后的prompt:\n\n```python\nhello.prompt(\"你是谁\")\n## '你好, 你是谁'\n```            \n\n## 动态换一个模型\n\n前面的 hello 方法在初始化的时候，我们使用了默认的模型 deepseek_chat，如果我们想换一个模型，可以这样做：\n\n```python\nhello.with_llm(llm).run(\"你是谁\")\n## '你好！我是一个人工智能助手，专门设计来回答问题、提供信息和帮助解决问题。如果你有任何疑问或需要帮助，请随时告诉我。'\n```\n\n通过 with_llm 你可以设置一个新的 llm 对象，然后调用 run 方法，就可以使用新的模型了。\n\n## 超长文本生成\n\n我们知道，大模型一次生成的长度其实是有限的，如果你想生成超长文本，你可能需手动的不断获得\n生成结果，然后把他转化为输入，然后再次生成，这样的方式是比较麻烦的。\n\nbyzerllm 提供了更加易用的 API :\n\n```python\nimport byzerllm\nfrom byzerllm import ByzerLLM\n\nllm = ByzerLLM.from_default_model(\"deepseek_chat\")\n\n@byzerllm.prompt()\ndef tell_story() -> str:\n    \"\"\"\n    讲一个100字的故事。    \n    \"\"\"\n\n\ns = (\n    tell_story.with_llm(llm)\n    .with_response_markers()\n    .options({\"llm_config\": {\"max_length\": 10}})\n    .run()\n)\nprint(s)\n\n## 从前，森林里住着一只聪明的小狐狸。一天，它发现了一块闪闪发光的宝石。小狐狸决定用这块宝石帮助森林里的动物们。它用宝石的光芒指引迷路的小鹿找到了回家的路，用宝石的温暖治愈了受伤的小鸟。从此，小狐狸成了森林里的英雄，动物们都感激它的善良和智慧。\n```\n\n实际核心部分就是这一行：\n\n```python\ntell_story.with_llm(llm)\n    .with_response_markers()    \n    .run()\n```\n\n我们只需要调用 `with_response_markers` 方法，系统就会自动的帮我们生成超长文本。\n在上面的案例中，我们通过\n\n```python\n.options({\"llm_config\": {\"max_length\": 10}})\n```\n\n认为的限制大模型一次交互最多只能输出10个字符，但是系统依然自动完成了远超过10个字符的文本生成。\n\n## 对象输出\n\n前面我们的例子都是返回字符串，但是我们也可以返回对象，这样我们就可以更加灵活的处理返回结果。\n\n```python\nimport pydantic \n\nclass Story(pydantic.BaseModel):\n    '''\n    故事\n    '''\n\n    title: str = pydantic.Field(description=\"故事的标题\")\n    body: str = pydantic.Field(description=\"故事主体\")\n\n@byzerllm.prompt()\ndef tell_story()->Story:\n    '''\n    讲一个100字的故事。    \n    '''\n\ns = tell_story.with_llm(llm).run()\nprint(isinstance(s, Story))\nprint(s.title)\n\n## True\n## 勇敢的小鸟\n```\n\n可以看到，我们很轻松的将输出转化为格式化输出。\n\n## 自定义字段抽取\n\n前面的结构化输出，其实会消耗更多token,还有一种更加精准的结构化输出方式。\n比如让大模型生成一个正则表达式，但实际上大模型很难准确只输出一个正则表达式，这个时候我们可以通过自定义抽取函数来获取我们想要的结果。\n\n\n```python\nfrom loguru import logger\nimport re\n\n@byzerllm.prompt()\ndef generate_regex_pattern(desc: str) -> str:\n    \"\"\"\n    根据下面的描述生成一个正则表达式，要符合python re.compile 库的要求。\n\n    {{ desc }}\n\n    最后生成的正则表达式要在<REGEX></REGEX>标签对里。\n    \"\"\"    \n\ndef extract_regex_pattern(regex_block: str) -> str:    \n    pattern = re.search(r\"<REGEX>(.*)</REGEX>\", regex_block, re.DOTALL)\n    if pattern is None:\n        logger.warning(\"No regex pattern found in the generated block:\\n {regex_block}\")\n        raise None\n    return pattern.group(1)\n\npattern = \"匹配一个邮箱地址\"\nv = generate_regex_pattern.with_llm(llm).with_extractor(extract_regex_pattern).run(desc=pattern)\nprint(v)\n## ^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\n```\n\n在上面的例子里，我们根据一句话生成一个正则表达式。我们通过 `with_extractor` 方法，传入了一个自定义的抽取函数，这个函数会在大模型生成结果后，对结果进行处理，然后返回我们想要的结果。\n\n我们在 prompt 明确说了，生成的结果要放到 `<REGEX></REGEX>` 标签对里，然后我们通过 extract_regex_pattern 函数，从结果中提取出了我们想要的正则表达式。\n\n## 在实例方法中使用大模型\n\n```python\nimport byzerllm\ndata = {\n    'name': 'Jane Doe',\n    'task_count': 3,\n    'tasks': [\n        {'name': 'Submit report', 'due_date': '2024-03-10'},\n        {'name': 'Finish project', 'due_date': '2024-03-15'},\n        {'name': 'Reply to emails', 'due_date': '2024-03-08'}\n    ]\n}\n\n\nclass RAG():\n    def __init__(self):        \n        self.llm = byzerllm.ByzerLLM()\n        self.llm.setup_template(model=\"deepseek_chat\",template=\"auto\")\n        self.llm.setup_default_model_name(\"deepseek_chat\")        \n    \n    @byzerllm.prompt(lambda self:self.llm)\n    def generate_answer(self,name,task_count,tasks)->str:\n        '''\n        Hello {{ name }},\n\n        This is a reminder that you have {{ task_count }} pending tasks:\n        {% for task in tasks %}\n        - Task: {{ task.name }} | Due: {{ task.due_date }}\n        {% endfor %}\n\n        Best regards,\n        Your Reminder System\n        '''        \n\nt = RAG()\n\nresponse = t.generate_answer(**data)\nprint(response)\n\n## 输出:\n## Hello Jane Doe,\n##I hope this message finds you well. I wanted to remind you of your 3 pending tasks to ensure you stay on track:\n## 1. **Submit report** - This task is due on **2024-03-10**. Please ensure that you allocat\n```\n\n这里我们给了个比较复杂的例子，但我们可以看到，给一个实例prompt方法和普通prompt 方法差异不大。\n唯一的区别是如果你希望在定义的时候就指定大模型，使用一个lambda函数返回实例的 llm 对象即可。\n\n```python\n@byzerllm.prompt(lambda self:self.llm)\n```\n\n你也可以不返回，在调用的时候通过 `with_llm` 方法指定 llm 对象。\n\n此外，这个例子也展示了如何通过jinja2模板语法，来处理复杂的结构化数据。\n\n## 通过 Python 代码处理复杂入参\n\n上面的一个例子中，我们通过 jinja2 模板语法，来处理复杂的结构化数据，但是有时候我们可能需要更加复杂的处理，这个时候我们可以通过 Python 代码来处理。\n\n```python\nimport byzerllm\n\ndata = {\n    'name': 'Jane Doe',\n    'task_count': 3,\n    'tasks': [\n        {'name': 'Submit report', 'due_date': '2024-03-10'},\n        {'name': 'Finish project', 'due_date': '2024-03-15'},\n        {'name': 'Reply to emails', 'due_date': '2024-03-08'}\n    ]\n}\n\n\nclass RAG():\n    def __init__(self):        \n        self.llm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n    \n    @byzerllm.prompt()\n    def generate_answer(self,name,task_count,tasks)->str:\n        '''\n        Hello {{ name }},\n\n        This is a reminder that you have {{ task_count }} pending tasks:\n            \n        {{ tasks }}\n\n        Best regards,\n        Your Reminder System\n        '''\n        \n        tasks_str = \"\\n\".join([f\"- Task: {task['name']} | Due: { task['due_date'] }\" for task in tasks])\n        return {\"tasks\": tasks_str}\n\nt = RAG()\n\nresponse = t.generate_answer.with_llm(t.llm).run(**data)\nprint(response)\n\n## Just a gentle nudge to keep you on track with your pending tasks. Here's a quick recap:....\n```\n\n在这个例子里，我们直接把 tasks 在方法体里进行处理，然后作为一个字符串返回，最够构建一个字典，字典的key为 tasks,然后\n你就可以在 docstring 里使用 `{{ tasks }}` 来引用这个字符串。\n\n这样对于很复杂的入参，就不用谢繁琐的 jinja2 模板语法了。\n\n## 如何自动实现一个方法\n\n比如我定义一个签名，但是我不想自己实现里面的逻辑，让大模型来实现。这个在 byzerllm 中叫 function impl。我们来看看怎么\n实现:\n\n```python\nimport pydantic\nclass Time(pydantic.BaseModel):\n    time: str = pydantic.Field(...,description=\"时间，时间格式为 yyyy-MM-dd\")\n\n\n@llm.impl()\ndef calculate_current_time()->Time:\n    '''\n    计算当前时间\n    '''\n    pass \n\n\ncalculate_current_time()\n#output: Time(time='2024-06-14')\n```\n\n在这个例子里，我们定义了一个 calculate_current_time 方法，但是我们没有实现里面的逻辑，我们通过 `@llm.impl()` 装饰器，让大模型来实现这个方法。\n为了避免每次都要“生成”这个方法，导致无法适用，我们提供了缓存，用户可以按如下方式打印速度：\n\n```python\nstart = time.monotonic()\ncalculate_current_time()\nprint(f\"first time cost: {time.monotonic()-start}\")\n\nstart = time.monotonic()\ncalculate_current_time()\nprint(f\"second time cost: {time.monotonic()-start}\")\n\n# output:\n# first time cost: 6.067266260739416\n# second time cost: 4.347506910562515e-05\n```\n可以看到，第一次执行花费了6s,第二次几乎是瞬间完成的，这是因为第一次执行的时候，我们实际上是在生成这个方法，第二次执行的时候，我们是执行已经生成好的代码，所以速度会非常快。你可以显示的调用 `llm.clear_impl_cache()` 清理掉函数缓存。\n\n## Stream 模式\n\n前面的例子都是一次性生成结果，但是有时候我们可能需要一个流式的输出，这个时候我们可能需要用底层一点的API来完成了：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\nv = llm.stream_chat_oai(model=\"deepseek_chat\",conversations=[{\n    \"role\":\"user\",\n    \"content\":\"你好，你是谁\",\n}],delta_mode=True)\n\nfor t in v:\n    print(t,flush=True)  \n\n# 你好\n# ！\n# 我\n# 是一个\n# 人工智能\n# 助手\n# ，\n# 旨在\n# 提供\n# 信息\n# 、\n# 解答\n# 问题....\n```\n\n如果你不想要流式输出，但是想用底层一点的API，你可以使用 `llm.chat_oai` 方法：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\nv = llm.chat_oai(model=\"deepseek_chat\",conversations=[{\n    \"role\":\"user\",\n    \"content\":\"你好，你是谁\",\n}])\n\nprint(v[0].output)\n## 你好！我是一个人工智能助手，旨在提供信息、解答问题和帮助用户解决问题。如果你有任何问题或需要帮助，请随时告诉我。\n```\n\n## Function Calling \n\nbyzerllm 可以不依赖模型自身就能提供 function calling 支持，我们来看个例子：\n\n\n```python\nfrom typing import List,Dict,Any,Annotated\nimport pydantic \nimport datetime\nfrom dateutil.relativedelta import relativedelta\n\ndef compute_date_range(count:Annotated[int,\"时间跨度，数值类型\"],\n                       unit:Annotated[str,\"时间单位，字符串类型\",{\"enum\":[\"day\",\"week\",\"month\",\"year\"]}])->List[str]:\n    '''\n    计算日期范围\n\n    Args:\n        count: 时间跨度，数值类型\n        unit: 时间单位，字符串类型，可选值为 day,week,month,year\n    '''        \n    now = datetime.datetime.now()\n    now_str = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n    if unit == \"day\":\n        return [(now - relativedelta(days=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"week\":\n        return [(now - relativedelta(weeks=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"month\":\n        return [(now - relativedelta(months=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"year\":\n        return [(now - relativedelta(years=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    return [\"\",\"\"]\n\ndef compute_now()->str:\n    '''\n    计算当前时间\n    '''\n    return datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n```\n\n我们定义了两个方法，一个是计算日期范围，一个是计算当前时间。\n\n现在我么可以来测试下，系统如何根据自然语言决定调用哪个方法：\n\n```python\nt = llm.chat_oai([{\n    \"content\":'''计算当前时间''',\n    \"role\":\"user\"    \n}],tools=[compute_date_range,compute_now],execute_tool=True)\n\nt[0].values\n\n## output: ['2024-06-14 15:18:02']\n```\n\n我们可以看到，他正确的选择了 compute_now 方法。\n\n接着我们再试一个：\n\n```python\nt = llm.chat_oai([{\n    \"content\":'''最近三个月趋势''',\n    \"role\":\"user\"    \n}],tools=[compute_date_range,compute_now],execute_tool=True)\n\nt[0].values\n\n## output: [['2024-03-14 15:19:13', '2024-06-14 15:19:13']]\n```\n\n模型正确的选择了 compute_date_range 方法。\n\n## 多模态\n\nbyerllm 也能很好的支持多模态的交互，而且统一了多模态大模型的接口，比如你可以用一样的方式使用 openai 或者 claude 的图片转文字能力， 或者一致的方式使用火山，azuer, openai的语音合成接口。\n\n### 图生文\n\n```python\nimport byzerllm\nfrom byzerllm.types import ImagePath\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\n\n\n@byzerllm.prompt()\ndef what_in_image(image_path: ImagePath) -> str:\n    \"\"\"\n    {{ image_path }}\n    这个图片里有什么？\n    \"\"\"    \n\n\nv = what_in_image.with_llm(vl_llm).run(\n    ImagePath(value=\"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\")\n)\nv\n## OUTPUT: 这张图片展示了多只可爱的猫咪，采用了艺术风格的绘画。猫咪们有不同的颜色和花纹，背景是浅棕色，上面还点缀着一些红色的花朵。整体画面给人一种温馨和谐的感觉\n```\n\n可以看到，我们只需要把 prompt 函数的图片地址入参使用 byzerllm.types.ImagePath里进行包装，就可以直接在 prompt 函数体里\n带上图片。\n\n或者你可以这样：\n\n```python\nimport byzerllm\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\n\n\n@byzerllm.prompt()\ndef what_in_image(image_path: str) -> str:\n    \"\"\"\n    {{ image }}\n    这个图片里有什么？\n    \"\"\"\n    return {\"image\": byzerllm.Image.load_image_from_path(image_path)}\n\n\nv = what_in_image.with_llm(vl_llm).run(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv\n```\n\n通过 `image_path` 参数，然后通过 `byzerllm.Image.load_image_from_path` 方法，转化为一个图片对象 image，最后在 prompt 函数体里\n使用 `{{ image }}` 引用这个图片对象。\n\n另外我们也是可以支持配置多张图片的。\n\n另外，我们也可以使用基础的 `llm.chat_oai` 方法来实现：\n\n```python\nimport byzerllm\nimport json\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\nimage = byzerllm.Image.load_image_from_path(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv = vl_llm.chat_oai(\n    conversations=[\n        {\n            \"role\": \"user\",\n            \"content\": json.dumps(\n                [{\"image\": image, \"text\": \"这个图片里有什么？\"}], ensure_ascii=False\n            ),\n        }\n    ]\n)\nv[0].output\n```\n\n还可以这么写：\n\n```python\nimport byzerllm\nimport json\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\nimage = byzerllm.Image.load_image_from_path(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv = vl_llm.chat_oai(\n    conversations=[\n        {\n            \"role\": \"user\",\n            \"content\": json.dumps(\n                [\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\"url\": image, \"detail\": \"high\"},\n                    },\n                    {\"text\": \"这个图片里有什么？\", \"type\": \"text\"},\n                ],\n                ensure_ascii=False,\n            ),\n        }\n    ]\n)\nv[0].output\n```\n\n### 语音合成\n\n这里以 openai 的 tts 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=tts-1 \\\n--model openai_tts\n```\n\n此外，byzerllm 支持 azure,火山引擎等 tts 语音合成引擎。\n\n接着你可以这么用：\n\n```python\nimport byzerllm\nimport base64\nimport json\n\nllm = byzerllm.ByzerLLM.from_default_model(\"openai_tts\")\n\n\nt = llm.chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\": json.dumps({\n        \"input\":\"hello, open_tts\",\n        \"voice\": \"alloy\",\n        \"response_format\": \"mp3\"\n    },ensure_ascii=False)\n}])\n\nwith open(\"voice.mp3\",\"wb\") as f:\n    f.write(base64.b64decode(t[0].output))\n```\n\ntts 模型生成没有prompt函数可以用，你需要直接使用 chat_oai。\n\n\n### 语音识别\n\n这里以 openai 的 whisper-1 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--worker_concurrency 10 \\\n--infer_params saas.model=whisper-1 saas.api_key=${MODEL_OPENAI_TOKEN} \\\n--model speech_to_text\n```\n\n语音识别的使用方式和图生文类似，我们可以直接在 prompt 函数体里带上音频文件。\n\n```python\nimport byzerllm\nimport json\nimport base64\nfrom byzerllm.types import AudioPath\n\nllm = byzerllm.ByzerLLM.from_default_model(\"speech_to_text\")\n\naudio_file = \"/Users/allwefantasy/videos/output_audio.mp3\"\n\n@byzerllm.prompt(llm=llm)\ndef audio_to_text(audio_file: AudioPath):\n    \"\"\"\n    {{ audio_file }}\n    \"\"\"\n\nv = audio_to_text(AudioPath(value=audio_file))\njson.loads(v)\n```\n输出的数据格式略微复杂：\n\n```\n{'text': 'In the last chapter, you and I started to step through the internal workings of a transformer. This is one of the key pieces of technology inside large language models, and a lot of other tools in the modern wave of AI.',\n 'task': 'transcribe',\n 'language': 'english',\n 'duration': 10.0,\n 'segments': [{'id': 0,\n   'seek': 0,\n   'start': 0.0,\n   'end': 4.78000020980835,\n   'text': ' In the last chapter, you and I started to step through the internal workings of a transformer.',\n   'tokens': [50364,\n    682,\n    264,\n    1036,\n    7187,\n    11,\n    291,\n    293,\n    286,\n    1409,\n    281,\n    1823,\n    807,\n    264,\n    6920,\n    589,\n    1109,\n    295,\n    257,\n    31782,\n    13,\n    50586],\n   'temperature': 0.0,\n   'avg_logprob': -0.28872039914131165,\n   'compression_ratio': 1.4220778942108154,\n   'no_speech_prob': 0.016033057123422623},\n  ....\n  {'id': 2,\n   'seek': 0,\n   'start': 8.579999923706055,\n   'end': 9.979999542236328,\n   'text': ' and a lot of other tools in the modern wave of AI.',\n   'tokens': [50759,\n    293,\n    257,\n    688,\n    295,\n    661,\n    3873,\n    294,\n    264,\n    4363,\n    5772,\n    295,\n    7318,\n    13,\n    50867],\n   'temperature': 0.0,\n   'avg_logprob': -0.28872039914131165,\n   'compression_ratio': 1.4220778942108154,\n   'no_speech_prob': 0.016033057123422623}],\n 'words': [{'word': 'In', 'start': 0.0, 'end': 0.18000000715255737},\n  {'word': 'the', 'start': 0.18000000715255737, 'end': 0.23999999463558197},\n  {'word': 'last', 'start': 0.23999999463558197, 'end': 0.5400000214576721},\n  {'word': 'chapter', 'start': 0.5400000214576721, 'end': 0.800000011920929},\n  ....\n  {'word': 'AI', 'start': 9.920000076293945, 'end': 9.979999542236328}]}\n```\n\n会输出每一句话以及每一个字所在的起始时间和截止时间。你可以根据需要来使用。\n\n\n### 文生图\n\n文生图和语音合成类似，首先要启动合适的模型,以openai 的 dall-e-3 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=dall-e-3 \\\n--model openai_image_gen\n```\n\n启动模型后，只需要记住几个模板参数即可使用，这里直接使用 chat_oai 方法来使用：\n\n```python\n\nimport byzerllm\nimport json\nimport base64\n\nllm = byzerllm.ByzerLLM.from_default_model(\"openai_image_gen\")\nt = llm.chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\": json.dumps({\n        \"input\":\"a white siamese cat\",\n        \"size\": \"1024x1024\",\n        \"quality\": \"standard\"\n    },ensure_ascii=False)\n}])\n\nwith open(\"output1.jpg\",\"wb\") as f:\n    f.write(base64.b64decode(t[0].output))\n\n\nimport matplotlib.pyplot as plt\n\nimage_path = \"output1.jpg\"\nimage = plt.imread(image_path)\n\nplt.imshow(image)\nplt.axis('off')\nplt.show()\n```\n\n## Prompt 函数的流式输出\n\nbyzerllm 底层支持流式输出，非 prompt 函数的用法是这样的：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\n\nv = llm.stream_chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\":\"讲一个100字的故事\"\n}])\n\nfor s  in v:\n    print(s[0], end=\"\")\n```\n\n如果你像用 prompt 函数，可以这么用：\n\n\n```python\nimport byzerllm\nimport json\nimport base64\nfrom typing import Generator\n\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\n\n@byzerllm.prompt()\ndef tell_story() -> Generator[str, None, None]:\n    '''\n    给我讲一个一百多字的故事\n    '''\n\nv = tell_story.with_llm(llm).run()    \nfor i in v:\n    print(i, end=\"\")\n```\n\n可以看到，和普通的 prompt 函数的区别在于，返回值是一个生成器，然后你可以通过 for 循环来获取结果。\n\n## 向量化模型\n\nbyzerllm 支持向量化模型,你可以这样启动一个本地的模型：\n\n```bash\n!byzerllm deploy --pretrained_model_type custom/bge \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--worker_concurrency 10 \\\n--model_path /home/winubuntu/.auto-coder/storage/models/AI-ModelScope/bge-large-zh \\\n--infer_backend transformers \\\n--num_workers 1 \\\n--model emb\n```\n\n注意两个参数:\n\n1. --infer_backend transformers: 表示使用 transformers 作为推理后端。\n2. --model_path: 表示模型的路径。\n\n也可以启动一个 SaaS 的emb模型,比如 qwen 的 emb 模型：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/qianwen \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 2 \\\n--infer_params saas.api_key=${MODEL_QIANWEN_TOKEN}  saas.model=text-embedding-v2 \\\n--model qianwen_emb\n```\n\n或者 openai 的 emb 模型：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--worker_concurrency 10 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=text-embedding-3-small \\\n--model gpt_emb\n```\nSaaS 模型无需配置 `--infer_backend` 参数。\n\n无论是本地模型还是 SaaS 模型，我们都可以这样使用：\n\n```python\nimport byzerllm\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\nllm.setup_default_emb_model_name(\"emb\")\nllm.emb_query(\"你好\")\n```\n\n如果你配置 byzerllm 中的 Storage 使用，比如你这样启动了存储：\n\n```bash\nbyzerllm storage start --enable_emb\n```\n\n那么需要这么使用：\n\n```python\nfrom byzerllm.apps.byzer_storage.simple_api import ByzerStorage, DataType, FieldOption,SortOption\nstorage = ByzerStorage(\"byzerai_store\", \"memory\", \"memory\")\nstorage.emb(\"你好\")\n```\n\n## 一些辅助工具\n\n当调用 prompt 函数返回字符串的时候，如果想从里面抽取代码，可以使用如下方式：\n\n```python\nfrom byzerllm.utils.client import code_utils\ntext_with_markdown = '''\n```shell\nls -l\n```\n'''\ncode_blocks = code_utils.extract_code(text_with_markdown)\nfor code_block in code_blocks:\n    if code_block[0] == \"shell\":\n        print(code_block[1])\n##output: ls -l        \n```\n\n\n## 注意事项\n\n1. prompt函数方法体返回只能是dict，实际的返回类型和方法签名可以不一样，但是方法体返回只能是dict。\n2. 大部分情况prompt函数体为空，如果一定要有方法体，可以返回一个空字典。\n3. 调用prompt方法的时候，如果在@byzerllm.prompt()里没有指定llm对象，那么需要在调用的时候通过with_llm方法指定llm对象。\n\n===========================\n\n# Byzer Storage: 最易用的AI存储引擎\n\nByzer Storage是一个为AI应用设计的高性能存储引擎,它提供了简单易用的API,支持向量搜索、全文检索以及结构化查询。本文将详细介绍Byzer Storage的使用方法和主要特性。\n\n## 0. 安装和启动\n\n```bash\npip install byzerllm\nbyzerllm storage start\n```\n\nThat's it! Byzer Storage已经安装并启动成功,现在我们可以开始使用它了。默认会启动一个 byzerai_store 的集群。\n\n注意，如果你是这样启动的：\n\n```bash\nbyzerllm storage start --enable_emb\n```\n\n那么会自动启动一个 emb 模型，名字就叫 emb， ByzerStorage 会自动使用该模型，无需做任何其他配置。\n\n## 1. 初始化\n\n创建一个 ByzerStorage 对象，链接 byzerai_store 集群，并且指定数据库和表名（可以不存在）。\n\n```python\nfrom byzerllm.apps.byzer_storage.simple_api import ByzerStorage, DataType, FieldOption, SortOption\n\nstorage = ByzerStorage(\"byzerai_store\", \"my_database1\", \"my_table4s\")\n```\n\n\n## 2. 创建库表（可选）\n\nByzer Storage使用Schema来定义数据结构。我们可以使用SchemaBuilder来构建Schema:\n\n```python\n_ = (\n    storage.schema_builder()\n    .add_field(\"_id\", DataType.STRING)\n    .add_field(\"name\", DataType.STRING)\n    .add_field(\"content\", DataType.STRING, [FieldOption.ANALYZE])\n    .add_field(\"raw_content\", DataType.STRING, [FieldOption.NO_INDEX])    \n    .add_array_field(\"summary\", DataType.FLOAT)    \n    .add_field(\"created_time\", DataType.LONG, [FieldOption.SORT])    \n    .execute()\n)\n```\n\n这个Schema定义了以下字段:\n- `_id`: 字符串类型的主键\n- `name`: 字符串类型,可用于过滤条件\n- `content`: 字符串类型,带有ANALYZE选项,用于全文搜索\n- `raw_content`: 字符串类型,带有NO_INDEX选项,不会被索引\n- `summary`: 浮点数组类型,用于存储向量\n- `created_time`: 长整型,带有SORT选项,可用于排序\n\n需要注意的是：\n\n1. 如果一个字段带有ANALYZE选项,则该字段会被分词,并且可以用于全文搜索，但是就无法返回原始的文本了。所以你需要添加一个新字段专门用来存储原文，比如在我们这个例子里，我们新增了 raw_content 字段，并且显示的指定了 NO_INDEX 选项，这样就不会被索引，也就不会被分词，可以后续被检索后用来获取原始文本。\n2. 对于需要作为向量检索和存储的字段，需要指定为数组类型，比如我们这个例子里的 summary 字段。\n3. 如果你需要拿到向量字段的原始文本，那么你也需要添加一个新字段专门用来存储原文，就像我们这个例子里的 raw_content 字段一样。\n\n\n## 3. 写入数据\n\n准备数据并使用WriteBuilder写入Storage:\n\n```python\ndata = [\n    {\"_id\": \"1\", \"name\": \"Hello\", \"content\": \"Hello, world!\", \"raw_content\": \"Hello, world!\", \"summary\": \"hello world\", \"created_time\": 1612137600},\n    {\"_id\": \"2\", \"name\": \"Byzer\", \"content\": \"Byzer, world!\", \"raw_content\": \"Byzer, world!\", \"summary\": \"byzer\", \"created_time\": 1612137601},\n    {\"_id\": \"3\", \"name\": \"AI\", \"content\": \"AI, world!\", \"raw_content\": \"AI, world!\", \"summary\": \"AI\", \"created_time\": 16121376002},\n    {\"_id\": \"4\", \"name\": \"ByzerAI\", \"content\": \"ByzerAI, world!\", \"raw_content\": \"ByzerAI, world!\", \"summary\": \"ByzerAi\", \"created_time\": 16121376003},\n]\n\nstorage.write_builder().add_items(data, vector_fields=[\"summary\"], search_fields=[\"content\"]).execute()\nstorage.commit()\n```\n\n这里我们使用`add_items`方法批量添加数据,并指定了`summary`为向量字段,`content`为搜索字段。最后调用`commit()`来确保数据被持久化。\n\n\n从上面我们要写入到 byzer storage 的数据我们可以看到如下几个特点：\n\n1. 需要包含我们之前定义的 Schema中罗列的所有的字段，同时需要指定哪些是向量字段，哪些是检索字段。\n2. 向量和检索不能是同一个字段。\n3. 对于向量，检索字段，我们给到 write_builder 的都是文本，ByzerStorage 会根据 Schema 的定义自动将其转换为向量，检索字段需要的格式。\n\n## 4. 查询数据\n\nByzer Storage支持多种查询方式,包括向量搜索、全文检索、过滤和排序。\n\n### 4.1 向量搜索 + 全文检索\n\n```python\nquery = storage.query_builder()\nquery.set_vector_query(\"ByzerAI\", fields=[\"summary\"])\nresults = query.set_search_query(\"Hello\", fields=[\"content\"]).execute()\nprint(results)\n```\n\n这个查询结合了向量搜索和全文检索,它会在`summary`字段中搜索与\"ByzerAI\"相似的向量,同时在`content`字段中搜索包含\"Hello\"的文档。\n\n### 4.2 过滤 + 向量搜索 + 全文检索\n\n```python\nquery = storage.query_builder()\nquery.and_filter().add_condition(\"name\", \"AI\").build()\nquery.set_vector_query(\"ByzerAI\", fields=\"summary\")\nresults = query.set_search_query(\"Hello\", fields=[\"content\"]).execute()\nprint(results)\n```\n\n这个查询首先过滤`name`字段等于\"AI\"的文档,然后在结果中进行向量搜索和全文检索。\n\n### 4.3 过滤 + 排序\n\n```python\nquery = storage.query_builder()\nquery.and_filter().add_condition(\"name\", \"AI\").build().sort(\"created_time\", SortOption.DESC)\nresults = query.execute()\nprint(results)\n```\n\n这个查询过滤`name`字段等于\"AI\"的文档,然后按`created_time`字段降序排序。\n\n## 5. 删除数据\n\n### 5.1 根据ID删除\n\n```python\nstorage.delete_by_ids([\"3\"])\n\nquery = storage.query_builder()\nquery.and_filter().add_condition(\"name\", \"AI\").build()\nresults = query.execute()\nprint(results)\n```\n\n这里我们删除了ID为\"3\"的文档,然后查询验证删除结果。\n\n### 5.2 删除整个表\n\n```python\nstorage.drop()\n```\n\n这个操作会删除整个表及其所有数据,请谨慎使用。\n\n## 结论\n\nByzer Storage提供了一套简洁而强大的API,能够轻松实现向量搜索、全文检索、结构化查询等功能。它的设计非常适合AI应用场景,可以有效地存储和检索各种类型的数据。通过本文的介绍,相信读者已经对Byzer Storage有了基本的了解,并能够开始在自己的项目中使用这个强大的存储引擎。\n\n\n     \n\n下面是用户的需求：\n\n在 @cli.py 中添加 --data_dir 参数,然后给 @@ByzerSQLConfig 添加相同的参数\n"}, {"role": "assistant", "content": "为了满足这个需求,我们需要修改两个文件:\n\n1. `/Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/cli.py`\n2. `/Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/types.py`\n\n让我们逐个进行修改:\n\n```python\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/cli.py\n<<<<<<< SEARCH\n    parser.add_argument(\"--byzer-sql-url\", type=str, default=\"\", help=\"Byzer SQL URL\")\n=======\n    parser.add_argument(\"--byzer-sql-url\", type=str, default=\"\", help=\"Byzer SQL URL\")\n    parser.add_argument(\"--data-dir\", type=str, default=\"\", help=\"Data directory\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/cli.py\n<<<<<<< SEARCH\n    # Create ByzerSQLConfig instance\n    byzer_sql_config = ByzerSQLConfig(\n        byzer_sql_url=args.byzer_sql_url\n    )\n=======\n    # Create ByzerSQLConfig instance\n    byzer_sql_config = ByzerSQLConfig(\n        byzer_sql_url=args.byzer_sql_url,\n        data_dir=args.data_dir\n    )\n>>>>>>> REPLACE\n```\n\n现在让我们修改 `types.py` 文件:\n\n```python\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/types.py\n<<<<<<< SEARCH\nclass ByzerSQLConfig(BaseModel):\n    byzer_sql_url: str = \"\"\n=======\nclass ByzerSQLConfig(BaseModel):\n    byzer_sql_url: str = \"\"\n    data_dir: str = \"\"\n>>>>>>> REPLACE\n```\n\n这些修改完成后:\n\n1. 在 `cli.py` 中，我们添加了 `--data-dir` 参数到参数解析器中。\n2. 在创建 `ByzerSQLConfig` 实例时，我们现在也传入了 `data_dir` 参数。\n3. 在 `types.py` 中，我们在 `ByzerSQLConfig` 类中添加了 `data_dir` 字段。\n\n这样，用户就可以通过命令行参数 `--data-dir` 来指定数据目录，并且这个值会被正确地传递到 `ByzerSQLConfig` 对象中。"}], "model": "human", "yaml_file": "actions/112_chat_action.yml"}
{"instruction": "在生成schema 的时候,对应csv文件的文件名也要保存在文件里.", "conversations": [{"role": "user", "content": "\n如果你需要生成代码，对于每个需要更改的文件,你需要按 *SEARCH/REPLACE block* 的格式进行生成。\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The opening fence and code language, eg: ```python\n2. The file path alone on a line, starting with \"##File:\" and verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n*SEARCH/REPLACE* blocks will replace *all* matching occurrences.\nInclude enough lines to make the SEARCH blocks unique.\n\nInclude *ALL* the code being searched and replaced!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n下面我们来看一个例子：\n\n当前项目目录结构：\n1. 项目根目录： /tmp/projects/mathweb\n2. 项目子目录/文件列表(类似tree 命令输出)\nflask/\n    app.py\n    templates/\n        index.html\n    static/\n        style.css\n\n用户需求： Change get_factorial() to use math.factorial\n\n回答： To make this change we need to modify `/tmp/projects/mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n用户需求： Refactor hello() into its own file.\n\n回答：To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/hello.py\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/main.py\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n现在让我们开始一个新的任务:\n\n当前项目目录结构：\n1. 项目根目录： /Users/allwefantasy/projects/super_analysis\n2. 项目子目录/文件列表(类似tree 命令输出)：\nsetup.py\n.auto-coder/\n    libs/\n        llm_friendly_packages/\n            src/\n                example1/\n                    main.py\nsrc/\n    superanalysis/\n        serve/\n            convert.py\n            openai_serve.py\n            __init__.py\n            llm_wrapper.py\n            api_server.py\n            cli.py\n        sql/\n            __init__.py\n            types.py\n            generator.py\n下面是一些文件路径以及每个文件对应的源码：\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/types.py\nfrom pydantic import BaseModel\n\nclass RAGConfig(BaseModel):\n    rag_base_url: str = \"\"\n    rag_api_key: str = \"\"\n\nclass ByzerSQLConfig(BaseModel):\n    byzer_sql_url: str = \"\"\n    data_dir: str = \"\"\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/cli.py\nimport argparse\nimport byzerllm\nfrom byzerllm.utils.client import ByzerLLM\nfrom superanalysis.serve.api_server import serve, ServerArgs\nfrom superanalysis.sql.types import RAGConfig, ByzerSQLConfig\nfrom superanalysis.sql.generator import SQLGenerator\nfrom superanalysis.serve.llm_wrapper import LLWrapper\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"ByzerLLM API Server\")\n    parser.add_argument(\"--host\", type=str, default=\"0.0.0.0\", help=\"Host to bind the server to\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"Port to bind the server to\")\n    parser.add_argument(\"--uvicorn-log-level\", type=str, default=\"info\", help=\"Uvicorn log level\")\n    parser.add_argument(\"--allow-credentials\", action=\"store_true\", help=\"Allow credentials\")\n    parser.add_argument(\"--allowed-origins\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed origins\")\n    parser.add_argument(\"--allowed-methods\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed methods\")\n    parser.add_argument(\"--allowed-headers\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed headers\")\n    parser.add_argument(\"--api-key\", type=str, default=\"\", help=\"API key for authentication\")\n    parser.add_argument(\"--served-model-name\", type=str, required=True, help=\"Name of the model to be served\")\n    parser.add_argument(\"--prompt-template\", type=str, default=\"\", help=\"Prompt template\")\n    parser.add_argument(\"--response-role\", type=str, default=\"assistant\", help=\"Response role\")\n    parser.add_argument(\"--ssl-keyfile\", type=str, default=\"\", help=\"SSL key file\")\n    parser.add_argument(\"--ssl-certfile\", type=str, default=\"\", help=\"SSL certificate file\")\n    \n    # Add new arguments for RAGConfig\n    parser.add_argument(\"--rag-base-url\", type=str, default=\"\", help=\"RAG base URL\")\n    parser.add_argument(\"--rag-api-key\", type=str, default=\"\", help=\"RAG API key\")\n    \n    # Add new argument for ByzerSQLConfig\n    parser.add_argument(\"--byzer-sql-url\", type=str, default=\"\", help=\"Byzer SQL URL\")\n    parser.add_argument(\"--data-dir\", type=str, default=\"\", help=\"Data directory\")\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile\n    )\n\n    # Create RAGConfig instance\n    rag_config = RAGConfig(\n        rag_base_url=args.rag_base_url,\n        rag_api_key=args.rag_api_key\n    )\n\n    # Create ByzerSQLConfig instance\n    byzer_sql_config = ByzerSQLConfig(\n        byzer_sql_url=args.byzer_sql_url,\n        data_dir=args.data_dir\n    )\n\n    llm = ByzerLLM()\n    llm.setup_default_model_name(args.served_model_name)\n    \n    byzerllm.connect_cluster(address=args.ray_address)\n    llm = byzerllm.ByzerLLM()\n    if not server_args.served_model_name:\n        raise ValueError(\"served_model_name is required\")\n    llm.setup_default_model_name(server_args.served_model_name)\n    rag = SQLGenerator(rag_config=rag_config, byzer_sql_config=byzer_sql_config)    \n    llm_wrapper = LLWrapper(llm=llm, rag=rag)\n    serve(llm_wrapper, server_args)\n\nif __name__ == \"__main__\":\n    main()\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/__init__.py\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/__init__.py\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/llm_wrapper.py\nfrom typing import Any, Dict, List, Optional, Union,Callable\nfrom byzerllm.utils.client.types import (    \n    LLMFunctionCallResponse,\n    LLMClassResponse,LLMResponse    \n)\nimport pydantic\nfrom byzerllm import ByzerLLM\nfrom byzerllm.utils.client import LLMResponse\nfrom byzerllm.utils.types import SingleOutputMeta\nfrom byzerllm.utils.langutil import asyncfy_with_semaphore\nfrom superanalysis.sql.generator import SQLGenerator\nfrom loguru import logger\n\nclass LLWrapper:\n\n    def __init__(self,llm:ByzerLLM,entry:SQLGenerator):\n        self.entry = entry\n        self.llm = llm\n        \n\n    def chat_oai(self,\n                 conversations,\n                 tools:List[Union[Callable,str]]=[], \n                 tool_choice:Optional[Union[Callable,str]]=None,\n                 execute_tool:bool=False,  \n                 impl_func:Optional[Callable]=None,\n                 execute_impl_func:bool=False,\n                 impl_func_params:Optional[Dict[str,Any]]=None,\n                 func_params:Optional[Dict[str,Any]]=None,\n                 response_class:Optional[Union[pydantic.BaseModel,str]] = None, \n                 response_after_chat:Optional[Union[pydantic.BaseModel,str]] = False,\n                 enable_default_sys_message:bool=True,                 \n                 model:Optional[str] = None,\n                 role_mapping=None,llm_config:Dict[str,Any]={}\n                 )->Union[List[LLMResponse],List[LLMFunctionCallResponse],List[LLMClassResponse]]: \n        res,contexts = self.entry.stream_chat_oai(conversations)\n        s = \"\".join(res)        \n        return [LLMResponse(output=s,metadata={},input=\"\")]\n\n    def stream_chat_oai(self,conversations, \n                        model:Optional[str]=None, \n                        role_mapping=None,\n                        delta_mode=False,\n                        llm_config:Dict[str,Any]={}): \n        res,contexts = self.entry.stream_chat_oai(conversations)        \n        for t in res:                        \n            yield (t,SingleOutputMeta(0,0))\n\n    async def async_stream_chat_oai(self,conversations, \n                        model:Optional[str]=None, \n                        role_mapping=None,\n                        delta_mode=False,\n                        llm_config:Dict[str,Any]={}): \n        res,contexts = self.rag.stream_chat_oai(conversations)                \n        for t in res:                                    \n            yield (t,SingleOutputMeta(0,0))\n                         \n\n    def __getattr__(self, name):        \n        return getattr(self.llm, name)\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/openai_serve.py\nimport argparse\nfrom service_server.api_server import ServerArgs, serve\nfrom service_server.llm_wrapper import LLWrapper\nfrom service_server.summary_rag import SummaryRAG\nimport byzerllm\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"OpenAI Serve\")\n    parser.add_argument(\n        \"--model\",\n        default=\"\",\n    )\n    parser.add_argument(\"--emb_model\", default=\"\")\n    parser.add_argument(\n        \"--ray_address\",\n        default=\"auto\",\n    )\n    parser.add_argument(\"--source_dir\", default=\".\", help=\"\")\n    parser.add_argument(\"--host\", default=\"\", help=\"\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"\")\n    parser.add_argument(\"--uvicorn_log_level\", default=\"info\", help=\"\")\n    parser.add_argument(\"--allow_credentials\", action=\"store_true\", help=\"\")\n    parser.add_argument(\"--allowed_origins\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--allowed_methods\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--allowed_headers\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--api_key\", default=\"\", help=\"\")\n    parser.add_argument(\"--served_model_name\", default=\"\", help=\"\")\n    parser.add_argument(\"--prompt_template\", default=\"\", help=\"\")\n    parser.add_argument(\"--ssl_keyfile\", default=\"\", help=\"\")\n    parser.add_argument(\"--ssl_certfile\", default=\"\", help=\"\")\n    parser.add_argument(\"--response_role\", default=\"assistant\", help=\"\")\n    parser.add_argument(\n        \"--collections\", default=\"\", help=\"Collection name for indexing\"\n    )\n    parser.add_argument(\n        \"--base_dir\",\n        default=\"\",\n        help=\"Path where the processed text embeddings were stored\",\n    )\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name or args.model,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile,\n    )\n    byzerllm.connect_cluster(address=args.ray_address)\n    llm = byzerllm.ByzerLLM()\n    if not server_args.served_model_name:\n        raise ValueError(\"served_model_name is required\")\n    llm.setup_default_model_name(server_args.served_model_name)\n    rag = SummaryRAG(llm=llm)    \n    llm_wrapper = LLWrapper(llm=llm, rag=rag)\n    serve(llm=llm_wrapper, args=server_args)\n\nif __name__ == \"__main__\":\n    main()\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/api_server.py\nimport os\nimport time\nimport uvicorn\nfrom fastapi import Request\nfrom fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import JSONResponse, StreamingResponse, Response\n\nfrom byzerllm.log import init_logger\nfrom byzerllm.utils import random_uuid\nfrom byzerllm.version import __version__ as version\nfrom byzerllm.utils.client import ByzerLLM, LLMRequest\nfrom byzerllm.utils.client.entrypoints.openai.serving_chat import OpenAIServingChat\nfrom byzerllm.utils.client.entrypoints.openai.serving_completion import OpenAIServingCompletion\nfrom byzerllm.utils.client.entrypoints.openai.protocol import (\n    ModelList,\n    ModelCard,\n    ModelPermission,\n    ChatCompletionRequest,\n    ErrorResponse,\n    CompletionRequest,\n    Embeddings,\n    EmbeddingsOutput,\n    EmbeddingsData,\n    EmbeddingsUsage,\n)\nfrom pydantic import BaseModel\nfrom typing import List\n\nlogger = init_logger(__name__)\n\nllm_client: ByzerLLM = None\nopenai_serving_chat: OpenAIServingChat = None\nopenai_serving_completion: OpenAIServingCompletion = None\n\nTIMEOUT_KEEP_ALIVE = 5  # seconds\n# timeout in 10 minutes. Streaming can take longer than 3 min\nTIMEOUT = float(os.environ.get(\"BYZERLLM_APISERVER_HTTP_TIMEOUT\", 600))\n\nrouter_app = FastAPI()\n\n\n@router_app.get(\"/health\")\nasync def health() -> Response:\n    \"\"\"Health check.\"\"\"\n    return Response(status_code=200)\n\n\n@router_app.get(\"/v1/models\")\nasync def show_available_models():\n    models = await openai_serving_chat.show_available_models()\n    return JSONResponse(content=models.model_dump())\n\n\n@router_app.get(\"/version\")\nasync def show_version():\n    return JSONResponse(content={\"version\": version})\n\n\n@router_app.get(\"/v1/models\", response_model=ModelList)\nasync def models() -> ModelList:\n    \"\"\"Show available models. Right now we only have one model.\"\"\"\n    model_cards = [\n        ModelCard(\n            id=\"\",\n            root=\"\",\n            permission=[ModelPermission()]\n        )\n    ]\n    return ModelList(data=model_cards)\n\n\n@router_app.post(\"/v1/completions\")\nasync def create_completion(\n        body: CompletionRequest,\n        request: Request\n):\n    generator = await openai_serving_completion.create_completion(body, request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            status_code=generator.code\n        )\n    if body.stream:\n        return StreamingResponse(\n            content=generator,\n            media_type=\"text/event-stream\"\n        )\n    else:\n        return JSONResponse(content=generator.model_dump())\n\n\n@router_app.post(\"/v1/chat/completions\")\nasync def create_chat_completion(\n        body: ChatCompletionRequest,\n        request: Request,\n):\n    \"\"\"Completion API similar to OpenAI's API.\n\n    See  https://platform.openai.com/docs/api-reference/chat/create\n    for the API specification. This API mimics the OpenAI ChatCompletion API.\n\n    NOTE: Currently we do not support the following features:\n        - function_call (Users should implement this by themselves)\n        - logit_bias (to be supported by vLLM engine)\n    \"\"\"\n    # async with async_timeout.timeout(TIMEOUT):\n\n    generator = await openai_serving_chat.create_chat_completion(body, request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            status_code=generator.code\n        )\n    if body.stream:\n        return StreamingResponse(\n            content=generator,\n            media_type=\"text/event-stream\"\n        )\n    else:\n        return JSONResponse(content=generator.model_dump())\n\n\n@router_app.post(\"/v1/embeddings\")\nasync def embed(body: Embeddings):\n    \"\"\"Given a prompt, the model will return one embedding.\n\n    Returns:\n        A response object with an embedding.\n    \"\"\"\n    embedding_id = f\"embed-{random_uuid()}\"\n\n    results_list = llm_client.emb(body.model, request=LLMRequest(instruction=body.input))\n    tokens = 0\n\n    return EmbeddingsOutput(\n        data=[\n            EmbeddingsData(\n                embedding=results.output,\n                index=i,\n                object=\"embedding\",\n            )\n            for i, results in enumerate(results_list)\n        ],\n        id=embedding_id,\n        object=\"list\",\n        created=int(time.time()),\n        model=body.model,\n        usage=EmbeddingsUsage(\n            prompt_tokens=tokens,\n            total_tokens=tokens,\n        ),\n    )\n\nclass ServerArgs(BaseModel):\n    host: str = None\n    port: int = 8000\n    uvicorn_log_level: str = \"info\"\n    allow_credentials: bool = False\n    allowed_origins: List[str] = [\"*\"]  \n    allowed_methods: List[str] = [\"*\"]\n    allowed_headers: List[str] = [\"*\"]\n    api_key: str = None\n    served_model_name: str = None\n    prompt_template: str = None\n    response_role: str = \"assistant\"\n    ssl_keyfile: str = None\n    ssl_certfile: str = None    \n\ndef serve(llm:ByzerLLM, args: ServerArgs):\n    \n    logger.info(f\"ByzerLLM API server version {version}\")\n    logger.info(f\"args: {args}\")\n\n    router_app.add_middleware(\n        CORSMiddleware,\n        allow_origins=args.allowed_origins,\n        allow_credentials=args.allow_credentials,\n        allow_methods=args.allowed_methods,\n        allow_headers=args.allowed_headers,\n    )\n    \n    if token := os.environ.get(\"BYZERLLM_API_KEY\") or args.api_key:\n\n        @router_app.middleware(\"http\")\n        async def authentication(request: Request, call_next):\n            if not request.url.path.startswith(\"/v1\"):\n                return await call_next(request)\n            if request.headers.get(\"Authorization\") != \"Bearer \" + token:\n                return JSONResponse(\n                    content={\"error\": \"Unauthorized\"},\n                    status_code=401\n                )\n            return await call_next(request)\n\n    # Register labels for metrics\n    # add_global_metrics_labels(model_name=engine_args.model)\n    global llm_client\n    llm_client = llm\n    \n    global openai_serving_chat\n    openai_serving_chat = OpenAIServingChat(\n        llm_client=llm_client,\n        response_role=args.response_role,\n        server_model_name=args.served_model_name,\n        prompt_template=args.prompt_template\n    )\n    global openai_serving_completion\n    openai_serving_completion = OpenAIServingCompletion(\n        llm_client=llm_client,\n        server_model_name=args.served_model_name,\n        prompt_template=args.prompt_template\n    )\n\n    uvicorn.run(\n        router_app,\n        host=args.host,\n        port=args.port,\n        log_level=args.uvicorn_log_level,\n        timeout_keep_alive=TIMEOUT_KEEP_ALIVE,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile\n    )\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/generator.py\nfrom typing import Optional, Dict, Any\nfrom openai import OpenAI\nimport json\nfrom .types import RAGConfig, ByzerSQLConfig\nfrom byzerllm.utils.client import code_utils\n\n\nclass SQLGenerator:\n\n    def __init__(self, rag_config: RAGConfig, byzer_sql_config: ByzerSQLConfig) -> None:        \n        self.rag_client = OpenAI(api_key=rag_config.rag_api_key, base_url=rag_config.rag_base_url)\n        self.byzer_sql_url = byzer_sql_config.byzer_sql_url\n\n    def stream_chat_oai(\n        self,\n        conversations,\n        model: Optional[str] = None,\n        role_mapping=None,\n        delta_mode=False,\n        llm_config: Dict[str, Any] = {},\n    ):\n        if not model:\n            model = \"deepseek_chat\"\n        response = self.rag_client.chat.completions.create(\n        messages=conversations,\n        model=model,\n    )\n        sql_str = response.choices[0].message.content\n        pass\n\n\n\n##File: /Users/allwefantasy/projects/super_analysis/setup.py\nfrom setuptools import setup, find_packages\n\nsetup(\n    name=\"super-analysis\",\n    version=\"0.1.0\",\n    packages=find_packages(where=\"src\"),\n    package_dir={\"\": \"src\"},\n    install_requires=[\n        \"byzerllm\",\n        \"fastapi\",\n        \"uvicorn\",\n        \"argparse\",\n    ],\n    entry_points={\n        \"console_scripts\": [\n            \"super-analysis.serve=superanalysis.serve.cli:main\",\n            \"super-analysis.convert=superanalysis.serve.convert:main\",\n        ],\n    },\n    author=\"allwefantasy\",\n    author_email=\"allwefantasy@gmail.com\",\n    description=\"A package for super analysis with ByzerLLM\",\n    long_description=open(\"README.md\").read(),\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/yourusername/super-analysis\",\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n    ],\n    python_requires='>=3.6',\n)\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/convert.py\nimport os\nimport argparse\nimport pandas as pd\nimport hashlib\nfrom typing import List, Dict\n\ndef get_csv_files(directory: str) -> List[str]:\n    \"\"\"Recursively get all CSV files in the given directory.\"\"\"\n    csv_files = []\n    for root, _, files in os.walk(directory):\n        for file in files:\n            if file.endswith('.csv'):\n                csv_files.append(os.path.join(root, file))\n    return csv_files\n\ndef extract_schema(df: pd.DataFrame) -> Dict[str, str]:\n    \"\"\"Extract schema from pandas DataFrame.\"\"\"\n    schema = {}\n    for column in df.columns:\n        dtype = str(df[column].dtype)\n        if dtype == 'object':\n            schema[column] = 'STRING'\n        elif dtype.startswith('int'):\n            schema[column] = 'INTEGER'\n        elif dtype.startswith('float'):\n            schema[column] = 'FLOAT'\n        elif dtype == 'bool':\n            schema[column] = 'BOOLEAN'\n        elif dtype.startswith('datetime'):\n            schema[column] = 'TIMESTAMP'\n        else:\n            schema[column] = 'STRING'  # Default to STRING for unknown types\n    return schema\n\ndef generate_schema_text(schema: Dict[str, str]) -> str:\n    \"\"\"Generate schema text from the schema dictionary.\"\"\"\n    schema_lines = [f\"{field}: {dtype}\" for field, dtype in schema.items()]\n    return \"\\n\".join(schema_lines)\n\ndef save_schema(schema_text: str, output_dir: str) -> None:\n    \"\"\"Save schema text to a file in the output directory.\"\"\"\n    # Generate MD5 hash of the schema text\n    file_name = hashlib.md5(schema_text.encode()).hexdigest() + '.md'\n    output_path = os.path.join(output_dir, file_name)\n    \n    with open(output_path, 'w') as f:\n        f.write(schema_text)\n    print(f\"Schema saved to {output_path}\")\n\ndef convert(data_dir: str, doc_dir: str) -> None:\n    \"\"\"Main conversion function.\"\"\"\n    csv_files = get_csv_files(data_dir)\n    \n    for csv_file in csv_files:\n        print(f\"Processing {csv_file}\")\n        df = pd.read_csv(csv_file)\n        schema = extract_schema(df)\n        schema_text = generate_schema_text(schema)\n        save_schema(schema_text, doc_dir)\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Convert CSV files to schema documents\")\n    parser.add_argument(\"--data_dir\", required=True, help=\"Directory containing CSV files\")\n    parser.add_argument(\"--doc_dir\", required=True, help=\"Output directory for schema documents\")\n    \n    args = parser.parse_args()\n    \n    if not os.path.exists(args.doc_dir):\n        os.makedirs(args.doc_dir)\n    \n    convert(args.data_dir, args.doc_dir)\n\nif __name__ == \"__main__\":\n    main()\n\n     \n\n下面是用户的需求：\n\n在生成schema 的时候,对应csv文件的文件名也要保存在文件里.\n"}, {"role": "assistant", "content": "To include the CSV file name in the generated schema file, we need to modify the `generate_schema_text` and `save_schema` functions in the `convert.py` file. Here's how we can make these changes:\n\n```python\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/convert.py\n<<<<<<< SEARCH\ndef generate_schema_text(schema: Dict[str, str]) -> str:\n    \"\"\"Generate schema text from the schema dictionary.\"\"\"\n    schema_lines = [f\"{field}: {dtype}\" for field, dtype in schema.items()]\n    return \"\\n\".join(schema_lines)\n\ndef save_schema(schema_text: str, output_dir: str) -> None:\n    \"\"\"Save schema text to a file in the output directory.\"\"\"\n    # Generate MD5 hash of the schema text\n    file_name = hashlib.md5(schema_text.encode()).hexdigest() + '.md'\n    output_path = os.path.join(output_dir, file_name)\n    \n    with open(output_path, 'w') as f:\n        f.write(schema_text)\n    print(f\"Schema saved to {output_path}\")\n\ndef convert(data_dir: str, doc_dir: str) -> None:\n    \"\"\"Main conversion function.\"\"\"\n    csv_files = get_csv_files(data_dir)\n    \n    for csv_file in csv_files:\n        print(f\"Processing {csv_file}\")\n        df = pd.read_csv(csv_file)\n        schema = extract_schema(df)\n        schema_text = generate_schema_text(schema)\n        save_schema(schema_text, doc_dir)\n=======\ndef generate_schema_text(schema: Dict[str, str], csv_file: str) -> str:\n    \"\"\"Generate schema text from the schema dictionary and include the CSV file name.\"\"\"\n    schema_lines = [f\"CSV File: {csv_file}\\n\"]\n    schema_lines.extend([f\"{field}: {dtype}\" for field, dtype in schema.items()])\n    return \"\\n\".join(schema_lines)\n\ndef save_schema(schema_text: str, csv_file: str, output_dir: str) -> None:\n    \"\"\"Save schema text to a file in the output directory.\"\"\"\n    # Generate MD5 hash of the schema text\n    file_name = hashlib.md5(schema_text.encode()).hexdigest() + '.md'\n    output_path = os.path.join(output_dir, file_name)\n    \n    with open(output_path, 'w') as f:\n        f.write(schema_text)\n    print(f\"Schema for {csv_file} saved to {output_path}\")\n\ndef convert(data_dir: str, doc_dir: str) -> None:\n    \"\"\"Main conversion function.\"\"\"\n    csv_files = get_csv_files(data_dir)\n    \n    for csv_file in csv_files:\n        print(f\"Processing {csv_file}\")\n        df = pd.read_csv(csv_file)\n        schema = extract_schema(df)\n        schema_text = generate_schema_text(schema, csv_file)\n        save_schema(schema_text, csv_file, doc_dir)\n>>>>>>> REPLACE\n```\n\nThese changes do the following:\n\n1. In the `generate_schema_text` function, we now include the CSV file name at the beginning of the schema text.\n\n2. The `save_schema` function now takes the `csv_file` as an additional parameter, which is used in the print statement to show which CSV file the schema corresponds to.\n\n3. In the `convert` function, we pass the `csv_file` to both `generate_schema_text` and `save_schema` functions.\n\nThese modifications will ensure that each generated schema file includes the name of the corresponding CSV file, making it easier to trace the origin of each schema."}], "model": "human", "yaml_file": "actions/113_chat_action.yml"}
{"instruction": "根据 @convert.py 里的功能,更新 @README.md 添加该命令的使用方式", "conversations": [{"role": "user", "content": "\n如果你需要生成代码，对于每个需要更改的文件,你需要按 *SEARCH/REPLACE block* 的格式进行生成。\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The opening fence and code language, eg: ```python\n2. The file path alone on a line, starting with \"##File:\" and verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n*SEARCH/REPLACE* blocks will replace *all* matching occurrences.\nInclude enough lines to make the SEARCH blocks unique.\n\nInclude *ALL* the code being searched and replaced!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n下面我们来看一个例子：\n\n当前项目目录结构：\n1. 项目根目录： /tmp/projects/mathweb\n2. 项目子目录/文件列表(类似tree 命令输出)\nflask/\n    app.py\n    templates/\n        index.html\n    static/\n        style.css\n\n用户需求： Change get_factorial() to use math.factorial\n\n回答： To make this change we need to modify `/tmp/projects/mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n用户需求： Refactor hello() into its own file.\n\n回答：To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/hello.py\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/main.py\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n现在让我们开始一个新的任务:\n\n当前项目目录结构：\n1. 项目根目录： /Users/allwefantasy/projects/super_analysis\n2. 项目子目录/文件列表(类似tree 命令输出)：\nsetup.py\n.auto-coder/\n    libs/\n        llm_friendly_packages/\n            src/\n                example1/\n                    main.py\nsrc/\n    superanalysis/\n        serve/\n            convert.py\n            openai_serve.py\n            __init__.py\n            llm_wrapper.py\n            api_server.py\n            cli.py\n        sql/\n            __init__.py\n            types.py\n            generator.py\n下面是一些文件路径以及每个文件对应的源码：\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/types.py\nfrom pydantic import BaseModel\n\nclass RAGConfig(BaseModel):\n    rag_base_url: str = \"\"\n    rag_api_key: str = \"\"\n\nclass ByzerSQLConfig(BaseModel):\n    byzer_sql_url: str = \"\"\n    data_dir: str = \"\"\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/cli.py\nimport argparse\nimport byzerllm\nfrom byzerllm.utils.client import ByzerLLM\nfrom superanalysis.serve.api_server import serve, ServerArgs\nfrom superanalysis.sql.types import RAGConfig, ByzerSQLConfig\nfrom superanalysis.sql.generator import SQLGenerator\nfrom superanalysis.serve.llm_wrapper import LLWrapper\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"ByzerLLM API Server\")\n    parser.add_argument(\"--host\", type=str, default=\"0.0.0.0\", help=\"Host to bind the server to\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"Port to bind the server to\")\n    parser.add_argument(\"--uvicorn-log-level\", type=str, default=\"info\", help=\"Uvicorn log level\")\n    parser.add_argument(\"--allow-credentials\", action=\"store_true\", help=\"Allow credentials\")\n    parser.add_argument(\"--allowed-origins\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed origins\")\n    parser.add_argument(\"--allowed-methods\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed methods\")\n    parser.add_argument(\"--allowed-headers\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed headers\")\n    parser.add_argument(\"--api-key\", type=str, default=\"\", help=\"API key for authentication\")\n    parser.add_argument(\"--served-model-name\", type=str, required=True, help=\"Name of the model to be served\")\n    parser.add_argument(\"--prompt-template\", type=str, default=\"\", help=\"Prompt template\")\n    parser.add_argument(\"--response-role\", type=str, default=\"assistant\", help=\"Response role\")\n    parser.add_argument(\"--ssl-keyfile\", type=str, default=\"\", help=\"SSL key file\")\n    parser.add_argument(\"--ssl-certfile\", type=str, default=\"\", help=\"SSL certificate file\")\n    \n    # Add new arguments for RAGConfig\n    parser.add_argument(\"--rag-base-url\", type=str, default=\"\", help=\"RAG base URL\")\n    parser.add_argument(\"--rag-api-key\", type=str, default=\"\", help=\"RAG API key\")\n    \n    # Add new argument for ByzerSQLConfig\n    parser.add_argument(\"--byzer-sql-url\", type=str, default=\"\", help=\"Byzer SQL URL\")\n    parser.add_argument(\"--data-dir\", type=str, default=\"\", help=\"Data directory\")\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile\n    )\n\n    # Create RAGConfig instance\n    rag_config = RAGConfig(\n        rag_base_url=args.rag_base_url,\n        rag_api_key=args.rag_api_key\n    )\n\n    # Create ByzerSQLConfig instance\n    byzer_sql_config = ByzerSQLConfig(\n        byzer_sql_url=args.byzer_sql_url,\n        data_dir=args.data_dir\n    )\n\n    llm = ByzerLLM()\n    llm.setup_default_model_name(args.served_model_name)\n    \n    byzerllm.connect_cluster(address=args.ray_address)\n    llm = byzerllm.ByzerLLM()\n    if not server_args.served_model_name:\n        raise ValueError(\"served_model_name is required\")\n    llm.setup_default_model_name(server_args.served_model_name)\n    rag = SQLGenerator(rag_config=rag_config, byzer_sql_config=byzer_sql_config)    \n    llm_wrapper = LLWrapper(llm=llm, rag=rag)\n    serve(llm_wrapper, server_args)\n\nif __name__ == \"__main__\":\n    main()\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/__init__.py\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/__init__.py\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/llm_wrapper.py\nfrom typing import Any, Dict, List, Optional, Union,Callable\nfrom byzerllm.utils.client.types import (    \n    LLMFunctionCallResponse,\n    LLMClassResponse,LLMResponse    \n)\nimport pydantic\nfrom byzerllm import ByzerLLM\nfrom byzerllm.utils.client import LLMResponse\nfrom byzerllm.utils.types import SingleOutputMeta\nfrom byzerllm.utils.langutil import asyncfy_with_semaphore\nfrom superanalysis.sql.generator import SQLGenerator\nfrom loguru import logger\n\nclass LLWrapper:\n\n    def __init__(self,llm:ByzerLLM,entry:SQLGenerator):\n        self.entry = entry\n        self.llm = llm\n        \n\n    def chat_oai(self,\n                 conversations,\n                 tools:List[Union[Callable,str]]=[], \n                 tool_choice:Optional[Union[Callable,str]]=None,\n                 execute_tool:bool=False,  \n                 impl_func:Optional[Callable]=None,\n                 execute_impl_func:bool=False,\n                 impl_func_params:Optional[Dict[str,Any]]=None,\n                 func_params:Optional[Dict[str,Any]]=None,\n                 response_class:Optional[Union[pydantic.BaseModel,str]] = None, \n                 response_after_chat:Optional[Union[pydantic.BaseModel,str]] = False,\n                 enable_default_sys_message:bool=True,                 \n                 model:Optional[str] = None,\n                 role_mapping=None,llm_config:Dict[str,Any]={}\n                 )->Union[List[LLMResponse],List[LLMFunctionCallResponse],List[LLMClassResponse]]: \n        res,contexts = self.entry.stream_chat_oai(conversations)\n        s = \"\".join(res)        \n        return [LLMResponse(output=s,metadata={},input=\"\")]\n\n    def stream_chat_oai(self,conversations, \n                        model:Optional[str]=None, \n                        role_mapping=None,\n                        delta_mode=False,\n                        llm_config:Dict[str,Any]={}): \n        res,contexts = self.entry.stream_chat_oai(conversations)        \n        for t in res:                        \n            yield (t,SingleOutputMeta(0,0))\n\n    async def async_stream_chat_oai(self,conversations, \n                        model:Optional[str]=None, \n                        role_mapping=None,\n                        delta_mode=False,\n                        llm_config:Dict[str,Any]={}): \n        res,contexts = self.rag.stream_chat_oai(conversations)                \n        for t in res:                                    \n            yield (t,SingleOutputMeta(0,0))\n                         \n\n    def __getattr__(self, name):        \n        return getattr(self.llm, name)\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/openai_serve.py\nimport argparse\nfrom service_server.api_server import ServerArgs, serve\nfrom service_server.llm_wrapper import LLWrapper\nfrom service_server.summary_rag import SummaryRAG\nimport byzerllm\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"OpenAI Serve\")\n    parser.add_argument(\n        \"--model\",\n        default=\"\",\n    )\n    parser.add_argument(\"--emb_model\", default=\"\")\n    parser.add_argument(\n        \"--ray_address\",\n        default=\"auto\",\n    )\n    parser.add_argument(\"--source_dir\", default=\".\", help=\"\")\n    parser.add_argument(\"--host\", default=\"\", help=\"\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"\")\n    parser.add_argument(\"--uvicorn_log_level\", default=\"info\", help=\"\")\n    parser.add_argument(\"--allow_credentials\", action=\"store_true\", help=\"\")\n    parser.add_argument(\"--allowed_origins\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--allowed_methods\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--allowed_headers\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--api_key\", default=\"\", help=\"\")\n    parser.add_argument(\"--served_model_name\", default=\"\", help=\"\")\n    parser.add_argument(\"--prompt_template\", default=\"\", help=\"\")\n    parser.add_argument(\"--ssl_keyfile\", default=\"\", help=\"\")\n    parser.add_argument(\"--ssl_certfile\", default=\"\", help=\"\")\n    parser.add_argument(\"--response_role\", default=\"assistant\", help=\"\")\n    parser.add_argument(\n        \"--collections\", default=\"\", help=\"Collection name for indexing\"\n    )\n    parser.add_argument(\n        \"--base_dir\",\n        default=\"\",\n        help=\"Path where the processed text embeddings were stored\",\n    )\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name or args.model,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile,\n    )\n    byzerllm.connect_cluster(address=args.ray_address)\n    llm = byzerllm.ByzerLLM()\n    if not server_args.served_model_name:\n        raise ValueError(\"served_model_name is required\")\n    llm.setup_default_model_name(server_args.served_model_name)\n    rag = SummaryRAG(llm=llm)    \n    llm_wrapper = LLWrapper(llm=llm, rag=rag)\n    serve(llm=llm_wrapper, args=server_args)\n\nif __name__ == \"__main__\":\n    main()\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/api_server.py\nimport os\nimport time\nimport uvicorn\nfrom fastapi import Request\nfrom fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import JSONResponse, StreamingResponse, Response\n\nfrom byzerllm.log import init_logger\nfrom byzerllm.utils import random_uuid\nfrom byzerllm.version import __version__ as version\nfrom byzerllm.utils.client import ByzerLLM, LLMRequest\nfrom byzerllm.utils.client.entrypoints.openai.serving_chat import OpenAIServingChat\nfrom byzerllm.utils.client.entrypoints.openai.serving_completion import OpenAIServingCompletion\nfrom byzerllm.utils.client.entrypoints.openai.protocol import (\n    ModelList,\n    ModelCard,\n    ModelPermission,\n    ChatCompletionRequest,\n    ErrorResponse,\n    CompletionRequest,\n    Embeddings,\n    EmbeddingsOutput,\n    EmbeddingsData,\n    EmbeddingsUsage,\n)\nfrom pydantic import BaseModel\nfrom typing import List\n\nlogger = init_logger(__name__)\n\nllm_client: ByzerLLM = None\nopenai_serving_chat: OpenAIServingChat = None\nopenai_serving_completion: OpenAIServingCompletion = None\n\nTIMEOUT_KEEP_ALIVE = 5  # seconds\n# timeout in 10 minutes. Streaming can take longer than 3 min\nTIMEOUT = float(os.environ.get(\"BYZERLLM_APISERVER_HTTP_TIMEOUT\", 600))\n\nrouter_app = FastAPI()\n\n\n@router_app.get(\"/health\")\nasync def health() -> Response:\n    \"\"\"Health check.\"\"\"\n    return Response(status_code=200)\n\n\n@router_app.get(\"/v1/models\")\nasync def show_available_models():\n    models = await openai_serving_chat.show_available_models()\n    return JSONResponse(content=models.model_dump())\n\n\n@router_app.get(\"/version\")\nasync def show_version():\n    return JSONResponse(content={\"version\": version})\n\n\n@router_app.get(\"/v1/models\", response_model=ModelList)\nasync def models() -> ModelList:\n    \"\"\"Show available models. Right now we only have one model.\"\"\"\n    model_cards = [\n        ModelCard(\n            id=\"\",\n            root=\"\",\n            permission=[ModelPermission()]\n        )\n    ]\n    return ModelList(data=model_cards)\n\n\n@router_app.post(\"/v1/completions\")\nasync def create_completion(\n        body: CompletionRequest,\n        request: Request\n):\n    generator = await openai_serving_completion.create_completion(body, request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            status_code=generator.code\n        )\n    if body.stream:\n        return StreamingResponse(\n            content=generator,\n            media_type=\"text/event-stream\"\n        )\n    else:\n        return JSONResponse(content=generator.model_dump())\n\n\n@router_app.post(\"/v1/chat/completions\")\nasync def create_chat_completion(\n        body: ChatCompletionRequest,\n        request: Request,\n):\n    \"\"\"Completion API similar to OpenAI's API.\n\n    See  https://platform.openai.com/docs/api-reference/chat/create\n    for the API specification. This API mimics the OpenAI ChatCompletion API.\n\n    NOTE: Currently we do not support the following features:\n        - function_call (Users should implement this by themselves)\n        - logit_bias (to be supported by vLLM engine)\n    \"\"\"\n    # async with async_timeout.timeout(TIMEOUT):\n\n    generator = await openai_serving_chat.create_chat_completion(body, request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            status_code=generator.code\n        )\n    if body.stream:\n        return StreamingResponse(\n            content=generator,\n            media_type=\"text/event-stream\"\n        )\n    else:\n        return JSONResponse(content=generator.model_dump())\n\n\n@router_app.post(\"/v1/embeddings\")\nasync def embed(body: Embeddings):\n    \"\"\"Given a prompt, the model will return one embedding.\n\n    Returns:\n        A response object with an embedding.\n    \"\"\"\n    embedding_id = f\"embed-{random_uuid()}\"\n\n    results_list = llm_client.emb(body.model, request=LLMRequest(instruction=body.input))\n    tokens = 0\n\n    return EmbeddingsOutput(\n        data=[\n            EmbeddingsData(\n                embedding=results.output,\n                index=i,\n                object=\"embedding\",\n            )\n            for i, results in enumerate(results_list)\n        ],\n        id=embedding_id,\n        object=\"list\",\n        created=int(time.time()),\n        model=body.model,\n        usage=EmbeddingsUsage(\n            prompt_tokens=tokens,\n            total_tokens=tokens,\n        ),\n    )\n\nclass ServerArgs(BaseModel):\n    host: str = None\n    port: int = 8000\n    uvicorn_log_level: str = \"info\"\n    allow_credentials: bool = False\n    allowed_origins: List[str] = [\"*\"]  \n    allowed_methods: List[str] = [\"*\"]\n    allowed_headers: List[str] = [\"*\"]\n    api_key: str = None\n    served_model_name: str = None\n    prompt_template: str = None\n    response_role: str = \"assistant\"\n    ssl_keyfile: str = None\n    ssl_certfile: str = None    \n\ndef serve(llm:ByzerLLM, args: ServerArgs):\n    \n    logger.info(f\"ByzerLLM API server version {version}\")\n    logger.info(f\"args: {args}\")\n\n    router_app.add_middleware(\n        CORSMiddleware,\n        allow_origins=args.allowed_origins,\n        allow_credentials=args.allow_credentials,\n        allow_methods=args.allowed_methods,\n        allow_headers=args.allowed_headers,\n    )\n    \n    if token := os.environ.get(\"BYZERLLM_API_KEY\") or args.api_key:\n\n        @router_app.middleware(\"http\")\n        async def authentication(request: Request, call_next):\n            if not request.url.path.startswith(\"/v1\"):\n                return await call_next(request)\n            if request.headers.get(\"Authorization\") != \"Bearer \" + token:\n                return JSONResponse(\n                    content={\"error\": \"Unauthorized\"},\n                    status_code=401\n                )\n            return await call_next(request)\n\n    # Register labels for metrics\n    # add_global_metrics_labels(model_name=engine_args.model)\n    global llm_client\n    llm_client = llm\n    \n    global openai_serving_chat\n    openai_serving_chat = OpenAIServingChat(\n        llm_client=llm_client,\n        response_role=args.response_role,\n        server_model_name=args.served_model_name,\n        prompt_template=args.prompt_template\n    )\n    global openai_serving_completion\n    openai_serving_completion = OpenAIServingCompletion(\n        llm_client=llm_client,\n        server_model_name=args.served_model_name,\n        prompt_template=args.prompt_template\n    )\n\n    uvicorn.run(\n        router_app,\n        host=args.host,\n        port=args.port,\n        log_level=args.uvicorn_log_level,\n        timeout_keep_alive=TIMEOUT_KEEP_ALIVE,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile\n    )\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/generator.py\nfrom typing import Optional, Dict, Any\nfrom openai import OpenAI\nimport json\nfrom .types import RAGConfig, ByzerSQLConfig\nfrom byzerllm.utils.client import code_utils\n\n\nclass SQLGenerator:\n\n    def __init__(self, rag_config: RAGConfig, byzer_sql_config: ByzerSQLConfig) -> None:        \n        self.rag_client = OpenAI(api_key=rag_config.rag_api_key, base_url=rag_config.rag_base_url)\n        self.byzer_sql_url = byzer_sql_config.byzer_sql_url\n\n    def stream_chat_oai(\n        self,\n        conversations,\n        model: Optional[str] = None,\n        role_mapping=None,\n        delta_mode=False,\n        llm_config: Dict[str, Any] = {},\n    ):\n        if not model:\n            model = \"deepseek_chat\"\n        response = self.rag_client.chat.completions.create(\n        messages=conversations,\n        model=model,\n    )\n        sql_str = response.choices[0].message.content\n        pass\n\n\n\n##File: /Users/allwefantasy/projects/super_analysis/setup.py\nfrom setuptools import setup, find_packages\n\nsetup(\n    name=\"super-analysis\",\n    version=\"0.1.0\",\n    packages=find_packages(where=\"src\"),\n    package_dir={\"\": \"src\"},\n    install_requires=[\n        \"byzerllm\",\n        \"fastapi\",\n        \"uvicorn\",\n        \"argparse\",\n    ],\n    entry_points={\n        \"console_scripts\": [\n            \"super-analysis.serve=superanalysis.serve.cli:main\",\n            \"super-analysis.convert=superanalysis.serve.convert:main\",\n        ],\n    },\n    author=\"allwefantasy\",\n    author_email=\"allwefantasy@gmail.com\",\n    description=\"A package for super analysis with ByzerLLM\",\n    long_description=open(\"README.md\").read(),\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/yourusername/super-analysis\",\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n    ],\n    python_requires='>=3.6',\n)\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/convert.py\nimport os\nimport argparse\nimport pandas as pd\nimport hashlib\nfrom typing import List, Dict\n\ndef get_csv_files(directory: str) -> List[str]:\n    \"\"\"Recursively get all CSV files in the given directory.\"\"\"\n    csv_files = []\n    for root, _, files in os.walk(directory):\n        for file in files:\n            if file.endswith('.csv'):\n                csv_files.append(os.path.join(root, file))\n    return csv_files\n\ndef extract_schema(df: pd.DataFrame) -> Dict[str, str]:\n    \"\"\"Extract schema from pandas DataFrame.\"\"\"\n    schema = {}\n    for column in df.columns:\n        dtype = str(df[column].dtype)\n        if dtype == 'object':\n            schema[column] = 'STRING'\n        elif dtype.startswith('int'):\n            schema[column] = 'INTEGER'\n        elif dtype.startswith('float'):\n            schema[column] = 'FLOAT'\n        elif dtype == 'bool':\n            schema[column] = 'BOOLEAN'\n        elif dtype.startswith('datetime'):\n            schema[column] = 'TIMESTAMP'\n        else:\n            schema[column] = 'STRING'  # Default to STRING for unknown types\n    return schema\n\ndef generate_schema_text(schema: Dict[str, str], csv_file: str) -> str:\n    \"\"\"Generate schema text from the schema dictionary and include the CSV file name.\"\"\"\n    schema_lines = [f\"CSV File: {csv_file}\\n\"]\n    schema_lines.extend([f\"{field}: {dtype}\" for field, dtype in schema.items()])\n    return \"\\n\".join(schema_lines)\n\ndef save_schema(schema_text: str, csv_file: str, output_dir: str) -> None:\n    \"\"\"Save schema text to a file in the output directory.\"\"\"\n    # Generate MD5 hash of the schema text\n    file_name = hashlib.md5(schema_text.encode()).hexdigest() + '.md'\n    output_path = os.path.join(output_dir, file_name)\n    \n    with open(output_path, 'w') as f:\n        f.write(schema_text)\n    print(f\"Schema for {csv_file} saved to {output_path}\")\n\ndef convert(data_dir: str, doc_dir: str) -> None:\n    \"\"\"Main conversion function.\"\"\"\n    csv_files = get_csv_files(data_dir)\n    \n    for csv_file in csv_files:\n        print(f\"Processing {csv_file}\")\n        df = pd.read_csv(csv_file)\n        schema = extract_schema(df)\n        schema_text = generate_schema_text(schema, csv_file)\n        save_schema(schema_text, csv_file, doc_dir)\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Convert CSV files to schema documents\")\n    parser.add_argument(\"--data_dir\", required=True, help=\"Directory containing CSV files\")\n    parser.add_argument(\"--doc_dir\", required=True, help=\"Output directory for schema documents\")\n    \n    args = parser.parse_args()\n    \n    if not os.path.exists(args.doc_dir):\n        os.makedirs(args.doc_dir)\n    \n    convert(args.data_dir, args.doc_dir)\n\nif __name__ == \"__main__\":\n    main()\n\n     \n\n下面是用户的需求：\n\n根据 @convert.py 里的功能,更新 @README.md 添加该命令的使用方式\n"}, {"role": "assistant", "content": "Sure, I can help you update the README.md file to include the usage instructions for the convert command. Here's how we can do that:\n\n```python\n##File: /Users/allwefantasy/projects/super_analysis/README.md\n<<<<<<< SEARCH\n# Super Analysis\n\nThis is a Python package for super analysis with ByzerLLM.\n\n## Installation\n\nYou can install the package using pip:\n\n```\npip install super-analysis\n```\n\n## Usage\n\nTo use the package, you can run the following command:\n\n```\nsuper-analysis.serve --model <model_name> --ray_address <ray_address> --host <host> --port <port>\n```\n\nReplace `<model_name>`, `<ray_address>`, `<host>`, and `<port>` with your specific values.\n\nFor more options, run:\n\n```\nsuper-analysis.serve --help\n```\n\n## License\n\nThis project is licensed under the MIT License.\n=======\n# Super Analysis\n\nThis is a Python package for super analysis with ByzerLLM.\n\n## Installation\n\nYou can install the package using pip:\n\n```\npip install super-analysis\n```\n\n## Usage\n\n### Serving the Model\n\nTo serve the model, you can run the following command:\n\n```\nsuper-analysis.serve --model <model_name> --ray_address <ray_address> --host <host> --port <port>\n```\n\nReplace `<model_name>`, `<ray_address>`, `<host>`, and `<port>` with your specific values.\n\nFor more options, run:\n\n```\nsuper-analysis.serve --help\n```\n\n### Converting CSV to Schema Documents\n\nTo convert CSV files to schema documents, use the following command:\n\n```\nsuper-analysis.convert --data_dir <path_to_csv_directory> --doc_dir <output_directory>\n```\n\n- `<path_to_csv_directory>`: The directory containing your CSV files (will be searched recursively).\n- `<output_directory>`: The directory where the schema documents will be saved.\n\nThis command will:\n1. Recursively search for all CSV files in the specified data directory.\n2. For each CSV file:\n   - Extract the schema (column names and data types).\n   - Generate a schema document.\n   - Save the schema document in the specified output directory.\n\nThe schema documents are saved with MD5 hash filenames to ensure uniqueness.\n\n## License\n\nThis project is licensed under the MIT License.\n>>>>>>> REPLACE\n```\n\nThis update adds a new section to the README.md file explaining how to use the `super-analysis.convert` command. It provides a clear explanation of what the command does and how to use it, including the required arguments."}], "model": "human", "yaml_file": "actions/114_chat_action.yml"}
{"instruction": "根据 @convert.py 里的功能,更新根目录下  @README.md 添加该命令的使用方式", "conversations": [{"role": "user", "content": "\n如果你需要生成代码，对于每个需要更改的文件,你需要按 *SEARCH/REPLACE block* 的格式进行生成。\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The opening fence and code language, eg: ```python\n2. The file path alone on a line, starting with \"##File:\" and verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n*SEARCH/REPLACE* blocks will replace *all* matching occurrences.\nInclude enough lines to make the SEARCH blocks unique.\n\nInclude *ALL* the code being searched and replaced!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n下面我们来看一个例子：\n\n当前项目目录结构：\n1. 项目根目录： /tmp/projects/mathweb\n2. 项目子目录/文件列表(类似tree 命令输出)\nflask/\n    app.py\n    templates/\n        index.html\n    static/\n        style.css\n\n用户需求： Change get_factorial() to use math.factorial\n\n回答： To make this change we need to modify `/tmp/projects/mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n用户需求： Refactor hello() into its own file.\n\n回答：To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/hello.py\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/main.py\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n现在让我们开始一个新的任务:\n\n当前项目目录结构：\n1. 项目根目录： /Users/allwefantasy/projects/super_analysis\n2. 项目子目录/文件列表(类似tree 命令输出)：\nsetup.py\n.auto-coder/\n    libs/\n        llm_friendly_packages/\n            src/\n                example1/\n                    main.py\nsrc/\n    superanalysis/\n        serve/\n            convert.py\n            openai_serve.py\n            __init__.py\n            llm_wrapper.py\n            api_server.py\n            cli.py\n        sql/\n            __init__.py\n            types.py\n            generator.py\n下面是一些文件路径以及每个文件对应的源码：\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/types.py\nfrom pydantic import BaseModel\n\nclass RAGConfig(BaseModel):\n    rag_base_url: str = \"\"\n    rag_api_key: str = \"\"\n\nclass ByzerSQLConfig(BaseModel):\n    byzer_sql_url: str = \"\"\n    data_dir: str = \"\"\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/cli.py\nimport argparse\nimport byzerllm\nfrom byzerllm.utils.client import ByzerLLM\nfrom superanalysis.serve.api_server import serve, ServerArgs\nfrom superanalysis.sql.types import RAGConfig, ByzerSQLConfig\nfrom superanalysis.sql.generator import SQLGenerator\nfrom superanalysis.serve.llm_wrapper import LLWrapper\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"ByzerLLM API Server\")\n    parser.add_argument(\"--host\", type=str, default=\"0.0.0.0\", help=\"Host to bind the server to\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"Port to bind the server to\")\n    parser.add_argument(\"--uvicorn-log-level\", type=str, default=\"info\", help=\"Uvicorn log level\")\n    parser.add_argument(\"--allow-credentials\", action=\"store_true\", help=\"Allow credentials\")\n    parser.add_argument(\"--allowed-origins\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed origins\")\n    parser.add_argument(\"--allowed-methods\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed methods\")\n    parser.add_argument(\"--allowed-headers\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed headers\")\n    parser.add_argument(\"--api-key\", type=str, default=\"\", help=\"API key for authentication\")\n    parser.add_argument(\"--served-model-name\", type=str, required=True, help=\"Name of the model to be served\")\n    parser.add_argument(\"--prompt-template\", type=str, default=\"\", help=\"Prompt template\")\n    parser.add_argument(\"--response-role\", type=str, default=\"assistant\", help=\"Response role\")\n    parser.add_argument(\"--ssl-keyfile\", type=str, default=\"\", help=\"SSL key file\")\n    parser.add_argument(\"--ssl-certfile\", type=str, default=\"\", help=\"SSL certificate file\")\n    \n    # Add new arguments for RAGConfig\n    parser.add_argument(\"--rag-base-url\", type=str, default=\"\", help=\"RAG base URL\")\n    parser.add_argument(\"--rag-api-key\", type=str, default=\"\", help=\"RAG API key\")\n    \n    # Add new argument for ByzerSQLConfig\n    parser.add_argument(\"--byzer-sql-url\", type=str, default=\"\", help=\"Byzer SQL URL\")\n    parser.add_argument(\"--data-dir\", type=str, default=\"\", help=\"Data directory\")\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile\n    )\n\n    # Create RAGConfig instance\n    rag_config = RAGConfig(\n        rag_base_url=args.rag_base_url,\n        rag_api_key=args.rag_api_key\n    )\n\n    # Create ByzerSQLConfig instance\n    byzer_sql_config = ByzerSQLConfig(\n        byzer_sql_url=args.byzer_sql_url,\n        data_dir=args.data_dir\n    )\n\n    llm = ByzerLLM()\n    llm.setup_default_model_name(args.served_model_name)\n    \n    byzerllm.connect_cluster(address=args.ray_address)\n    llm = byzerllm.ByzerLLM()\n    if not server_args.served_model_name:\n        raise ValueError(\"served_model_name is required\")\n    llm.setup_default_model_name(server_args.served_model_name)\n    rag = SQLGenerator(rag_config=rag_config, byzer_sql_config=byzer_sql_config)    \n    llm_wrapper = LLWrapper(llm=llm, rag=rag)\n    serve(llm_wrapper, server_args)\n\nif __name__ == \"__main__\":\n    main()\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/__init__.py\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/__init__.py\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/llm_wrapper.py\nfrom typing import Any, Dict, List, Optional, Union,Callable\nfrom byzerllm.utils.client.types import (    \n    LLMFunctionCallResponse,\n    LLMClassResponse,LLMResponse    \n)\nimport pydantic\nfrom byzerllm import ByzerLLM\nfrom byzerllm.utils.client import LLMResponse\nfrom byzerllm.utils.types import SingleOutputMeta\nfrom byzerllm.utils.langutil import asyncfy_with_semaphore\nfrom superanalysis.sql.generator import SQLGenerator\nfrom loguru import logger\n\nclass LLWrapper:\n\n    def __init__(self,llm:ByzerLLM,entry:SQLGenerator):\n        self.entry = entry\n        self.llm = llm\n        \n\n    def chat_oai(self,\n                 conversations,\n                 tools:List[Union[Callable,str]]=[], \n                 tool_choice:Optional[Union[Callable,str]]=None,\n                 execute_tool:bool=False,  \n                 impl_func:Optional[Callable]=None,\n                 execute_impl_func:bool=False,\n                 impl_func_params:Optional[Dict[str,Any]]=None,\n                 func_params:Optional[Dict[str,Any]]=None,\n                 response_class:Optional[Union[pydantic.BaseModel,str]] = None, \n                 response_after_chat:Optional[Union[pydantic.BaseModel,str]] = False,\n                 enable_default_sys_message:bool=True,                 \n                 model:Optional[str] = None,\n                 role_mapping=None,llm_config:Dict[str,Any]={}\n                 )->Union[List[LLMResponse],List[LLMFunctionCallResponse],List[LLMClassResponse]]: \n        res,contexts = self.entry.stream_chat_oai(conversations)\n        s = \"\".join(res)        \n        return [LLMResponse(output=s,metadata={},input=\"\")]\n\n    def stream_chat_oai(self,conversations, \n                        model:Optional[str]=None, \n                        role_mapping=None,\n                        delta_mode=False,\n                        llm_config:Dict[str,Any]={}): \n        res,contexts = self.entry.stream_chat_oai(conversations)        \n        for t in res:                        \n            yield (t,SingleOutputMeta(0,0))\n\n    async def async_stream_chat_oai(self,conversations, \n                        model:Optional[str]=None, \n                        role_mapping=None,\n                        delta_mode=False,\n                        llm_config:Dict[str,Any]={}): \n        res,contexts = self.rag.stream_chat_oai(conversations)                \n        for t in res:                                    \n            yield (t,SingleOutputMeta(0,0))\n                         \n\n    def __getattr__(self, name):        \n        return getattr(self.llm, name)\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/openai_serve.py\nimport argparse\nfrom service_server.api_server import ServerArgs, serve\nfrom service_server.llm_wrapper import LLWrapper\nfrom service_server.summary_rag import SummaryRAG\nimport byzerllm\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"OpenAI Serve\")\n    parser.add_argument(\n        \"--model\",\n        default=\"\",\n    )\n    parser.add_argument(\"--emb_model\", default=\"\")\n    parser.add_argument(\n        \"--ray_address\",\n        default=\"auto\",\n    )\n    parser.add_argument(\"--source_dir\", default=\".\", help=\"\")\n    parser.add_argument(\"--host\", default=\"\", help=\"\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"\")\n    parser.add_argument(\"--uvicorn_log_level\", default=\"info\", help=\"\")\n    parser.add_argument(\"--allow_credentials\", action=\"store_true\", help=\"\")\n    parser.add_argument(\"--allowed_origins\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--allowed_methods\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--allowed_headers\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--api_key\", default=\"\", help=\"\")\n    parser.add_argument(\"--served_model_name\", default=\"\", help=\"\")\n    parser.add_argument(\"--prompt_template\", default=\"\", help=\"\")\n    parser.add_argument(\"--ssl_keyfile\", default=\"\", help=\"\")\n    parser.add_argument(\"--ssl_certfile\", default=\"\", help=\"\")\n    parser.add_argument(\"--response_role\", default=\"assistant\", help=\"\")\n    parser.add_argument(\n        \"--collections\", default=\"\", help=\"Collection name for indexing\"\n    )\n    parser.add_argument(\n        \"--base_dir\",\n        default=\"\",\n        help=\"Path where the processed text embeddings were stored\",\n    )\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name or args.model,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile,\n    )\n    byzerllm.connect_cluster(address=args.ray_address)\n    llm = byzerllm.ByzerLLM()\n    if not server_args.served_model_name:\n        raise ValueError(\"served_model_name is required\")\n    llm.setup_default_model_name(server_args.served_model_name)\n    rag = SummaryRAG(llm=llm)    \n    llm_wrapper = LLWrapper(llm=llm, rag=rag)\n    serve(llm=llm_wrapper, args=server_args)\n\nif __name__ == \"__main__\":\n    main()\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/api_server.py\nimport os\nimport time\nimport uvicorn\nfrom fastapi import Request\nfrom fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import JSONResponse, StreamingResponse, Response\n\nfrom byzerllm.log import init_logger\nfrom byzerllm.utils import random_uuid\nfrom byzerllm.version import __version__ as version\nfrom byzerllm.utils.client import ByzerLLM, LLMRequest\nfrom byzerllm.utils.client.entrypoints.openai.serving_chat import OpenAIServingChat\nfrom byzerllm.utils.client.entrypoints.openai.serving_completion import OpenAIServingCompletion\nfrom byzerllm.utils.client.entrypoints.openai.protocol import (\n    ModelList,\n    ModelCard,\n    ModelPermission,\n    ChatCompletionRequest,\n    ErrorResponse,\n    CompletionRequest,\n    Embeddings,\n    EmbeddingsOutput,\n    EmbeddingsData,\n    EmbeddingsUsage,\n)\nfrom pydantic import BaseModel\nfrom typing import List\n\nlogger = init_logger(__name__)\n\nllm_client: ByzerLLM = None\nopenai_serving_chat: OpenAIServingChat = None\nopenai_serving_completion: OpenAIServingCompletion = None\n\nTIMEOUT_KEEP_ALIVE = 5  # seconds\n# timeout in 10 minutes. Streaming can take longer than 3 min\nTIMEOUT = float(os.environ.get(\"BYZERLLM_APISERVER_HTTP_TIMEOUT\", 600))\n\nrouter_app = FastAPI()\n\n\n@router_app.get(\"/health\")\nasync def health() -> Response:\n    \"\"\"Health check.\"\"\"\n    return Response(status_code=200)\n\n\n@router_app.get(\"/v1/models\")\nasync def show_available_models():\n    models = await openai_serving_chat.show_available_models()\n    return JSONResponse(content=models.model_dump())\n\n\n@router_app.get(\"/version\")\nasync def show_version():\n    return JSONResponse(content={\"version\": version})\n\n\n@router_app.get(\"/v1/models\", response_model=ModelList)\nasync def models() -> ModelList:\n    \"\"\"Show available models. Right now we only have one model.\"\"\"\n    model_cards = [\n        ModelCard(\n            id=\"\",\n            root=\"\",\n            permission=[ModelPermission()]\n        )\n    ]\n    return ModelList(data=model_cards)\n\n\n@router_app.post(\"/v1/completions\")\nasync def create_completion(\n        body: CompletionRequest,\n        request: Request\n):\n    generator = await openai_serving_completion.create_completion(body, request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            status_code=generator.code\n        )\n    if body.stream:\n        return StreamingResponse(\n            content=generator,\n            media_type=\"text/event-stream\"\n        )\n    else:\n        return JSONResponse(content=generator.model_dump())\n\n\n@router_app.post(\"/v1/chat/completions\")\nasync def create_chat_completion(\n        body: ChatCompletionRequest,\n        request: Request,\n):\n    \"\"\"Completion API similar to OpenAI's API.\n\n    See  https://platform.openai.com/docs/api-reference/chat/create\n    for the API specification. This API mimics the OpenAI ChatCompletion API.\n\n    NOTE: Currently we do not support the following features:\n        - function_call (Users should implement this by themselves)\n        - logit_bias (to be supported by vLLM engine)\n    \"\"\"\n    # async with async_timeout.timeout(TIMEOUT):\n\n    generator = await openai_serving_chat.create_chat_completion(body, request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            status_code=generator.code\n        )\n    if body.stream:\n        return StreamingResponse(\n            content=generator,\n            media_type=\"text/event-stream\"\n        )\n    else:\n        return JSONResponse(content=generator.model_dump())\n\n\n@router_app.post(\"/v1/embeddings\")\nasync def embed(body: Embeddings):\n    \"\"\"Given a prompt, the model will return one embedding.\n\n    Returns:\n        A response object with an embedding.\n    \"\"\"\n    embedding_id = f\"embed-{random_uuid()}\"\n\n    results_list = llm_client.emb(body.model, request=LLMRequest(instruction=body.input))\n    tokens = 0\n\n    return EmbeddingsOutput(\n        data=[\n            EmbeddingsData(\n                embedding=results.output,\n                index=i,\n                object=\"embedding\",\n            )\n            for i, results in enumerate(results_list)\n        ],\n        id=embedding_id,\n        object=\"list\",\n        created=int(time.time()),\n        model=body.model,\n        usage=EmbeddingsUsage(\n            prompt_tokens=tokens,\n            total_tokens=tokens,\n        ),\n    )\n\nclass ServerArgs(BaseModel):\n    host: str = None\n    port: int = 8000\n    uvicorn_log_level: str = \"info\"\n    allow_credentials: bool = False\n    allowed_origins: List[str] = [\"*\"]  \n    allowed_methods: List[str] = [\"*\"]\n    allowed_headers: List[str] = [\"*\"]\n    api_key: str = None\n    served_model_name: str = None\n    prompt_template: str = None\n    response_role: str = \"assistant\"\n    ssl_keyfile: str = None\n    ssl_certfile: str = None    \n\ndef serve(llm:ByzerLLM, args: ServerArgs):\n    \n    logger.info(f\"ByzerLLM API server version {version}\")\n    logger.info(f\"args: {args}\")\n\n    router_app.add_middleware(\n        CORSMiddleware,\n        allow_origins=args.allowed_origins,\n        allow_credentials=args.allow_credentials,\n        allow_methods=args.allowed_methods,\n        allow_headers=args.allowed_headers,\n    )\n    \n    if token := os.environ.get(\"BYZERLLM_API_KEY\") or args.api_key:\n\n        @router_app.middleware(\"http\")\n        async def authentication(request: Request, call_next):\n            if not request.url.path.startswith(\"/v1\"):\n                return await call_next(request)\n            if request.headers.get(\"Authorization\") != \"Bearer \" + token:\n                return JSONResponse(\n                    content={\"error\": \"Unauthorized\"},\n                    status_code=401\n                )\n            return await call_next(request)\n\n    # Register labels for metrics\n    # add_global_metrics_labels(model_name=engine_args.model)\n    global llm_client\n    llm_client = llm\n    \n    global openai_serving_chat\n    openai_serving_chat = OpenAIServingChat(\n        llm_client=llm_client,\n        response_role=args.response_role,\n        server_model_name=args.served_model_name,\n        prompt_template=args.prompt_template\n    )\n    global openai_serving_completion\n    openai_serving_completion = OpenAIServingCompletion(\n        llm_client=llm_client,\n        server_model_name=args.served_model_name,\n        prompt_template=args.prompt_template\n    )\n\n    uvicorn.run(\n        router_app,\n        host=args.host,\n        port=args.port,\n        log_level=args.uvicorn_log_level,\n        timeout_keep_alive=TIMEOUT_KEEP_ALIVE,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile\n    )\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/generator.py\nfrom typing import Optional, Dict, Any\nfrom openai import OpenAI\nimport json\nfrom .types import RAGConfig, ByzerSQLConfig\nfrom byzerllm.utils.client import code_utils\n\n\nclass SQLGenerator:\n\n    def __init__(self, rag_config: RAGConfig, byzer_sql_config: ByzerSQLConfig) -> None:        \n        self.rag_client = OpenAI(api_key=rag_config.rag_api_key, base_url=rag_config.rag_base_url)\n        self.byzer_sql_url = byzer_sql_config.byzer_sql_url\n\n    def stream_chat_oai(\n        self,\n        conversations,\n        model: Optional[str] = None,\n        role_mapping=None,\n        delta_mode=False,\n        llm_config: Dict[str, Any] = {},\n    ):\n        if not model:\n            model = \"deepseek_chat\"\n        response = self.rag_client.chat.completions.create(\n        messages=conversations,\n        model=model,\n    )\n        sql_str = response.choices[0].message.content\n        pass\n\n\n\n##File: /Users/allwefantasy/projects/super_analysis/setup.py\nfrom setuptools import setup, find_packages\n\nsetup(\n    name=\"super-analysis\",\n    version=\"0.1.0\",\n    packages=find_packages(where=\"src\"),\n    package_dir={\"\": \"src\"},\n    install_requires=[\n        \"byzerllm\",\n        \"fastapi\",\n        \"uvicorn\",\n        \"argparse\",\n    ],\n    entry_points={\n        \"console_scripts\": [\n            \"super-analysis.serve=superanalysis.serve.cli:main\",\n            \"super-analysis.convert=superanalysis.serve.convert:main\",\n        ],\n    },\n    author=\"allwefantasy\",\n    author_email=\"allwefantasy@gmail.com\",\n    description=\"A package for super analysis with ByzerLLM\",\n    long_description=open(\"README.md\").read(),\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/yourusername/super-analysis\",\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n    ],\n    python_requires='>=3.6',\n)\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/convert.py\nimport os\nimport argparse\nimport pandas as pd\nimport hashlib\nfrom typing import List, Dict\n\ndef get_csv_files(directory: str) -> List[str]:\n    \"\"\"Recursively get all CSV files in the given directory.\"\"\"\n    csv_files = []\n    for root, _, files in os.walk(directory):\n        for file in files:\n            if file.endswith('.csv'):\n                csv_files.append(os.path.join(root, file))\n    return csv_files\n\ndef extract_schema(df: pd.DataFrame) -> Dict[str, str]:\n    \"\"\"Extract schema from pandas DataFrame.\"\"\"\n    schema = {}\n    for column in df.columns:\n        dtype = str(df[column].dtype)\n        if dtype == 'object':\n            schema[column] = 'STRING'\n        elif dtype.startswith('int'):\n            schema[column] = 'INTEGER'\n        elif dtype.startswith('float'):\n            schema[column] = 'FLOAT'\n        elif dtype == 'bool':\n            schema[column] = 'BOOLEAN'\n        elif dtype.startswith('datetime'):\n            schema[column] = 'TIMESTAMP'\n        else:\n            schema[column] = 'STRING'  # Default to STRING for unknown types\n    return schema\n\ndef generate_schema_text(schema: Dict[str, str], csv_file: str) -> str:\n    \"\"\"Generate schema text from the schema dictionary and include the CSV file name.\"\"\"\n    schema_lines = [f\"CSV File: {csv_file}\\n\"]\n    schema_lines.extend([f\"{field}: {dtype}\" for field, dtype in schema.items()])\n    return \"\\n\".join(schema_lines)\n\ndef save_schema(schema_text: str, csv_file: str, output_dir: str) -> None:\n    \"\"\"Save schema text to a file in the output directory.\"\"\"\n    # Generate MD5 hash of the schema text\n    file_name = hashlib.md5(schema_text.encode()).hexdigest() + '.md'\n    output_path = os.path.join(output_dir, file_name)\n    \n    with open(output_path, 'w') as f:\n        f.write(schema_text)\n    print(f\"Schema for {csv_file} saved to {output_path}\")\n\ndef convert(data_dir: str, doc_dir: str) -> None:\n    \"\"\"Main conversion function.\"\"\"\n    csv_files = get_csv_files(data_dir)\n    \n    for csv_file in csv_files:\n        print(f\"Processing {csv_file}\")\n        df = pd.read_csv(csv_file)\n        schema = extract_schema(df)\n        schema_text = generate_schema_text(schema, csv_file)\n        save_schema(schema_text, csv_file, doc_dir)\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Convert CSV files to schema documents\")\n    parser.add_argument(\"--data_dir\", required=True, help=\"Directory containing CSV files\")\n    parser.add_argument(\"--doc_dir\", required=True, help=\"Output directory for schema documents\")\n    \n    args = parser.parse_args()\n    \n    if not os.path.exists(args.doc_dir):\n        os.makedirs(args.doc_dir)\n    \n    convert(args.data_dir, args.doc_dir)\n\nif __name__ == \"__main__\":\n    main()\n\n     \n\n下面是用户的需求：\n\n根据 @convert.py 里的功能,更新根目录下  @README.md 添加该命令的使用方式\n"}, {"role": "assistant", "content": "这个更改将在 README.md 文件的末尾添加一个新的部分，描述 `super-analysis.convert` 命令的用法、参数和示例。这将帮助用户了解如何使用这个新功能来将 CSV 文件转换为模式文档。\n为了更新根目录下的 README.md 文件，添加 convert.py 命令的使用方式，我们需要创建一个 SEARCH/REPLACE 块来修改 README.md 文件。由于我们没有看到当前 README.md 的内容，我们将假设我们要在文件末尾添加新的内容。\n\n这里是相应的 SEARCH/REPLACE 块：\n\n```markdown\n##File: /Users/allwefantasy/projects/super_analysis/README.md\n<<<<<<< SEARCH\n=======\n\n## Convert CSV to Schema Documents\n\nThe `super-analysis.convert` command allows you to convert CSV files to schema documents. This is useful for generating metadata about your CSV files that can be used for further analysis or documentation.\n\n### Usage\n\n```bash\nsuper-analysis.convert --data_dir <path_to_csv_files> --doc_dir <output_directory>\n```\n\n### Arguments\n\n- `--data_dir`: Directory containing CSV files (required)\n- `--doc_dir`: Output directory for schema documents (required)\n\n### Example\n\n```bash\nsuper-analysis.convert --data_dir /path/to/csv/files --doc_dir /path/to/output/schemas\n```\n\nThis command will:\n1. Recursively search for all CSV files in the specified `data_dir`.\n2. For each CSV file:\n   - Read the file and infer its schema\n   - Generate a schema document containing the file name and field types\n   - Save the schema document as a markdown file in the specified `doc_dir`\n\nThe output files will be named using an MD5 hash of their content to ensure uniqueness.\n\n>>>>>>> REPLACE\n```\n\n这个更改将在 README.md 文件的末尾添加一个新的部分，描述 `super-analysis.convert` 命令的用法、参数和示例。这将帮助用户了解如何使用这个新功能来将 CSV 文件转换为模式文档。"}], "model": "human", "yaml_file": "actions/115_chat_action.yml"}
{"instruction": "@generator.py 在 #MARK 标记处,使用 requests 访问 byzer-sql 引擎,执行sql,得到的结果.", "conversations": [{"role": "user", "content": "\n如果你需要生成代码，对于每个需要更改的文件,你需要按 *SEARCH/REPLACE block* 的格式进行生成。\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The opening fence and code language, eg: ```python\n2. The file path alone on a line, starting with \"##File:\" and verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n*SEARCH/REPLACE* blocks will replace *all* matching occurrences.\nInclude enough lines to make the SEARCH blocks unique.\n\nInclude *ALL* the code being searched and replaced!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n下面我们来看一个例子：\n\n当前项目目录结构：\n1. 项目根目录： /tmp/projects/mathweb\n2. 项目子目录/文件列表(类似tree 命令输出)\nflask/\n    app.py\n    templates/\n        index.html\n    static/\n        style.css\n\n用户需求： Change get_factorial() to use math.factorial\n\n回答： To make this change we need to modify `/tmp/projects/mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n用户需求： Refactor hello() into its own file.\n\n回答：To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/hello.py\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/main.py\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n现在让我们开始一个新的任务:\n\n当前项目目录结构：\n1. 项目根目录： /Users/allwefantasy/projects/super_analysis\n2. 项目子目录/文件列表(类似tree 命令输出)：\nsetup.py\n.auto-coder/\n    libs/\n        llm_friendly_packages/\n            src/\n                example1/\n                    main.py\nsrc/\n    superanalysis/\n        serve/\n            convert.py\n            openai_serve.py\n            __init__.py\n            llm_wrapper.py\n            api_server.py\n            cli.py\n        sql/\n            __init__.py\n            types.py\n            generator.py\n下面是一些文件路径以及每个文件对应的源码：\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/types.py\nfrom pydantic import BaseModel\n\nclass RAGConfig(BaseModel):\n    schema_rag_base_url: str = \"\"\n    schema_rag_api_key: str = \"xxxx\"\n\n    context_rag_base_url: str = \"\"\n    context_rag_api_key: str = \"xxxx\"\n\n    \n\n    \n\nclass ByzerSQLConfig(BaseModel):\n    byzer_sql_url: str = \"\"\n    data_dir: str = \"\"\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/cli.py\nimport argparse\nimport byzerllm\nfrom byzerllm.utils.client import ByzerLLM\nfrom superanalysis.serve.api_server import serve, ServerArgs\nfrom superanalysis.sql.types import RAGConfig, ByzerSQLConfig\nfrom superanalysis.sql.generator import SQLGenerator\nfrom superanalysis.serve.llm_wrapper import LLWrapper\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"ByzerLLM API Server\")\n    parser.add_argument(\"--host\", type=str, default=\"0.0.0.0\", help=\"Host to bind the server to\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"Port to bind the server to\")\n    parser.add_argument(\"--uvicorn-log-level\", type=str, default=\"info\", help=\"Uvicorn log level\")\n    parser.add_argument(\"--allow-credentials\", action=\"store_true\", help=\"Allow credentials\")\n    parser.add_argument(\"--allowed-origins\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed origins\")\n    parser.add_argument(\"--allowed-methods\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed methods\")\n    parser.add_argument(\"--allowed-headers\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed headers\")\n    parser.add_argument(\"--api-key\", type=str, default=\"\", help=\"API key for authentication\")\n    parser.add_argument(\"--served-model-name\", type=str, required=True, help=\"Name of the model to be served\")\n    parser.add_argument(\"--prompt-template\", type=str, default=\"\", help=\"Prompt template\")\n    parser.add_argument(\"--response-role\", type=str, default=\"assistant\", help=\"Response role\")\n    parser.add_argument(\"--ssl-keyfile\", type=str, default=\"\", help=\"SSL key file\")\n    parser.add_argument(\"--ssl-certfile\", type=str, default=\"\", help=\"SSL certificate file\")\n    \n    # Add new arguments for RAGConfig\n    parser.add_argument(\"--rag-base-url\", type=str, default=\"\", help=\"RAG base URL\")\n    parser.add_argument(\"--rag-api-key\", type=str, default=\"\", help=\"RAG API key\")\n    \n    # Add new argument for ByzerSQLConfig\n    parser.add_argument(\"--byzer-sql-url\", type=str, default=\"\", help=\"Byzer SQL URL\")\n    parser.add_argument(\"--data-dir\", type=str, default=\"\", help=\"Data directory\")\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile\n    )\n\n    # Create RAGConfig instance\n    rag_config = RAGConfig(\n        rag_base_url=args.rag_base_url,\n        rag_api_key=args.rag_api_key\n    )\n\n    # Create ByzerSQLConfig instance\n    byzer_sql_config = ByzerSQLConfig(\n        byzer_sql_url=args.byzer_sql_url,\n        data_dir=args.data_dir\n    )\n\n    llm = ByzerLLM()\n    llm.setup_default_model_name(args.served_model_name)\n    \n    byzerllm.connect_cluster(address=args.ray_address)\n    llm = byzerllm.ByzerLLM()\n    if not server_args.served_model_name:\n        raise ValueError(\"served_model_name is required\")\n    llm.setup_default_model_name(server_args.served_model_name)\n    rag = SQLGenerator(rag_config=rag_config, byzer_sql_config=byzer_sql_config)    \n    llm_wrapper = LLWrapper(llm=llm, rag=rag)\n    serve(llm_wrapper, server_args)\n\nif __name__ == \"__main__\":\n    main()\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/__init__.py\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/__init__.py\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/llm_wrapper.py\nfrom typing import Any, Dict, List, Optional, Union,Callable\nfrom byzerllm.utils.client.types import (    \n    LLMFunctionCallResponse,\n    LLMClassResponse,LLMResponse    \n)\nimport pydantic\nfrom byzerllm import ByzerLLM\nfrom byzerllm.utils.client import LLMResponse\nfrom byzerllm.utils.types import SingleOutputMeta\nfrom byzerllm.utils.langutil import asyncfy_with_semaphore\nfrom superanalysis.sql.generator import SQLGenerator\nfrom loguru import logger\n\nclass LLWrapper:\n\n    def __init__(self,llm:ByzerLLM,entry:SQLGenerator):\n        self.entry = entry\n        self.llm = llm\n        \n\n    def chat_oai(self,\n                 conversations,\n                 tools:List[Union[Callable,str]]=[], \n                 tool_choice:Optional[Union[Callable,str]]=None,\n                 execute_tool:bool=False,  \n                 impl_func:Optional[Callable]=None,\n                 execute_impl_func:bool=False,\n                 impl_func_params:Optional[Dict[str,Any]]=None,\n                 func_params:Optional[Dict[str,Any]]=None,\n                 response_class:Optional[Union[pydantic.BaseModel,str]] = None, \n                 response_after_chat:Optional[Union[pydantic.BaseModel,str]] = False,\n                 enable_default_sys_message:bool=True,                 \n                 model:Optional[str] = None,\n                 role_mapping=None,llm_config:Dict[str,Any]={}\n                 )->Union[List[LLMResponse],List[LLMFunctionCallResponse],List[LLMClassResponse]]: \n        res,contexts = self.entry.stream_chat_oai(conversations)\n        s = \"\".join(res)        \n        return [LLMResponse(output=s,metadata={},input=\"\")]\n\n    def stream_chat_oai(self,conversations, \n                        model:Optional[str]=None, \n                        role_mapping=None,\n                        delta_mode=False,\n                        llm_config:Dict[str,Any]={}): \n        res,contexts = self.entry.stream_chat_oai(conversations)        \n        for t in res:                        \n            yield (t,SingleOutputMeta(0,0))\n\n    async def async_stream_chat_oai(self,conversations, \n                        model:Optional[str]=None, \n                        role_mapping=None,\n                        delta_mode=False,\n                        llm_config:Dict[str,Any]={}): \n        res,contexts = self.rag.stream_chat_oai(conversations)                \n        for t in res:                                    \n            yield (t,SingleOutputMeta(0,0))\n                         \n\n    def __getattr__(self, name):        \n        return getattr(self.llm, name)\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/openai_serve.py\nimport argparse\nfrom service_server.api_server import ServerArgs, serve\nfrom service_server.llm_wrapper import LLWrapper\nfrom service_server.summary_rag import SummaryRAG\nimport byzerllm\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"OpenAI Serve\")\n    parser.add_argument(\n        \"--model\",\n        default=\"\",\n    )\n    parser.add_argument(\"--emb_model\", default=\"\")\n    parser.add_argument(\n        \"--ray_address\",\n        default=\"auto\",\n    )\n    parser.add_argument(\"--source_dir\", default=\".\", help=\"\")\n    parser.add_argument(\"--host\", default=\"\", help=\"\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"\")\n    parser.add_argument(\"--uvicorn_log_level\", default=\"info\", help=\"\")\n    parser.add_argument(\"--allow_credentials\", action=\"store_true\", help=\"\")\n    parser.add_argument(\"--allowed_origins\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--allowed_methods\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--allowed_headers\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--api_key\", default=\"\", help=\"\")\n    parser.add_argument(\"--served_model_name\", default=\"\", help=\"\")\n    parser.add_argument(\"--prompt_template\", default=\"\", help=\"\")\n    parser.add_argument(\"--ssl_keyfile\", default=\"\", help=\"\")\n    parser.add_argument(\"--ssl_certfile\", default=\"\", help=\"\")\n    parser.add_argument(\"--response_role\", default=\"assistant\", help=\"\")\n    parser.add_argument(\n        \"--collections\", default=\"\", help=\"Collection name for indexing\"\n    )\n    parser.add_argument(\n        \"--base_dir\",\n        default=\"\",\n        help=\"Path where the processed text embeddings were stored\",\n    )\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name or args.model,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile,\n    )\n    byzerllm.connect_cluster(address=args.ray_address)\n    llm = byzerllm.ByzerLLM()\n    if not server_args.served_model_name:\n        raise ValueError(\"served_model_name is required\")\n    llm.setup_default_model_name(server_args.served_model_name)\n    rag = SummaryRAG(llm=llm)    \n    llm_wrapper = LLWrapper(llm=llm, rag=rag)\n    serve(llm=llm_wrapper, args=server_args)\n\nif __name__ == \"__main__\":\n    main()\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/api_server.py\nimport os\nimport time\nimport uvicorn\nfrom fastapi import Request\nfrom fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import JSONResponse, StreamingResponse, Response\n\nfrom byzerllm.log import init_logger\nfrom byzerllm.utils import random_uuid\nfrom byzerllm.version import __version__ as version\nfrom byzerllm.utils.client import ByzerLLM, LLMRequest\nfrom byzerllm.utils.client.entrypoints.openai.serving_chat import OpenAIServingChat\nfrom byzerllm.utils.client.entrypoints.openai.serving_completion import OpenAIServingCompletion\nfrom byzerllm.utils.client.entrypoints.openai.protocol import (\n    ModelList,\n    ModelCard,\n    ModelPermission,\n    ChatCompletionRequest,\n    ErrorResponse,\n    CompletionRequest,\n    Embeddings,\n    EmbeddingsOutput,\n    EmbeddingsData,\n    EmbeddingsUsage,\n)\nfrom pydantic import BaseModel\nfrom typing import List\n\nlogger = init_logger(__name__)\n\nllm_client: ByzerLLM = None\nopenai_serving_chat: OpenAIServingChat = None\nopenai_serving_completion: OpenAIServingCompletion = None\n\nTIMEOUT_KEEP_ALIVE = 5  # seconds\n# timeout in 10 minutes. Streaming can take longer than 3 min\nTIMEOUT = float(os.environ.get(\"BYZERLLM_APISERVER_HTTP_TIMEOUT\", 600))\n\nrouter_app = FastAPI()\n\n\n@router_app.get(\"/health\")\nasync def health() -> Response:\n    \"\"\"Health check.\"\"\"\n    return Response(status_code=200)\n\n\n@router_app.get(\"/v1/models\")\nasync def show_available_models():\n    models = await openai_serving_chat.show_available_models()\n    return JSONResponse(content=models.model_dump())\n\n\n@router_app.get(\"/version\")\nasync def show_version():\n    return JSONResponse(content={\"version\": version})\n\n\n@router_app.get(\"/v1/models\", response_model=ModelList)\nasync def models() -> ModelList:\n    \"\"\"Show available models. Right now we only have one model.\"\"\"\n    model_cards = [\n        ModelCard(\n            id=\"\",\n            root=\"\",\n            permission=[ModelPermission()]\n        )\n    ]\n    return ModelList(data=model_cards)\n\n\n@router_app.post(\"/v1/completions\")\nasync def create_completion(\n        body: CompletionRequest,\n        request: Request\n):\n    generator = await openai_serving_completion.create_completion(body, request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            status_code=generator.code\n        )\n    if body.stream:\n        return StreamingResponse(\n            content=generator,\n            media_type=\"text/event-stream\"\n        )\n    else:\n        return JSONResponse(content=generator.model_dump())\n\n\n@router_app.post(\"/v1/chat/completions\")\nasync def create_chat_completion(\n        body: ChatCompletionRequest,\n        request: Request,\n):\n    \"\"\"Completion API similar to OpenAI's API.\n\n    See  https://platform.openai.com/docs/api-reference/chat/create\n    for the API specification. This API mimics the OpenAI ChatCompletion API.\n\n    NOTE: Currently we do not support the following features:\n        - function_call (Users should implement this by themselves)\n        - logit_bias (to be supported by vLLM engine)\n    \"\"\"\n    # async with async_timeout.timeout(TIMEOUT):\n\n    generator = await openai_serving_chat.create_chat_completion(body, request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            status_code=generator.code\n        )\n    if body.stream:\n        return StreamingResponse(\n            content=generator,\n            media_type=\"text/event-stream\"\n        )\n    else:\n        return JSONResponse(content=generator.model_dump())\n\n\n@router_app.post(\"/v1/embeddings\")\nasync def embed(body: Embeddings):\n    \"\"\"Given a prompt, the model will return one embedding.\n\n    Returns:\n        A response object with an embedding.\n    \"\"\"\n    embedding_id = f\"embed-{random_uuid()}\"\n\n    results_list = llm_client.emb(body.model, request=LLMRequest(instruction=body.input))\n    tokens = 0\n\n    return EmbeddingsOutput(\n        data=[\n            EmbeddingsData(\n                embedding=results.output,\n                index=i,\n                object=\"embedding\",\n            )\n            for i, results in enumerate(results_list)\n        ],\n        id=embedding_id,\n        object=\"list\",\n        created=int(time.time()),\n        model=body.model,\n        usage=EmbeddingsUsage(\n            prompt_tokens=tokens,\n            total_tokens=tokens,\n        ),\n    )\n\nclass ServerArgs(BaseModel):\n    host: str = None\n    port: int = 8000\n    uvicorn_log_level: str = \"info\"\n    allow_credentials: bool = False\n    allowed_origins: List[str] = [\"*\"]  \n    allowed_methods: List[str] = [\"*\"]\n    allowed_headers: List[str] = [\"*\"]\n    api_key: str = None\n    served_model_name: str = None\n    prompt_template: str = None\n    response_role: str = \"assistant\"\n    ssl_keyfile: str = None\n    ssl_certfile: str = None    \n\ndef serve(llm:ByzerLLM, args: ServerArgs):\n    \n    logger.info(f\"ByzerLLM API server version {version}\")\n    logger.info(f\"args: {args}\")\n\n    router_app.add_middleware(\n        CORSMiddleware,\n        allow_origins=args.allowed_origins,\n        allow_credentials=args.allow_credentials,\n        allow_methods=args.allowed_methods,\n        allow_headers=args.allowed_headers,\n    )\n    \n    if token := os.environ.get(\"BYZERLLM_API_KEY\") or args.api_key:\n\n        @router_app.middleware(\"http\")\n        async def authentication(request: Request, call_next):\n            if not request.url.path.startswith(\"/v1\"):\n                return await call_next(request)\n            if request.headers.get(\"Authorization\") != \"Bearer \" + token:\n                return JSONResponse(\n                    content={\"error\": \"Unauthorized\"},\n                    status_code=401\n                )\n            return await call_next(request)\n\n    # Register labels for metrics\n    # add_global_metrics_labels(model_name=engine_args.model)\n    global llm_client\n    llm_client = llm\n    \n    global openai_serving_chat\n    openai_serving_chat = OpenAIServingChat(\n        llm_client=llm_client,\n        response_role=args.response_role,\n        server_model_name=args.served_model_name,\n        prompt_template=args.prompt_template\n    )\n    global openai_serving_completion\n    openai_serving_completion = OpenAIServingCompletion(\n        llm_client=llm_client,\n        server_model_name=args.served_model_name,\n        prompt_template=args.prompt_template\n    )\n\n    uvicorn.run(\n        router_app,\n        host=args.host,\n        port=args.port,\n        log_level=args.uvicorn_log_level,\n        timeout_keep_alive=TIMEOUT_KEEP_ALIVE,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile\n    )\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/generator.py\nfrom typing import Optional, Dict, Any,List\nfrom openai import OpenAI\nimport json\nfrom .types import RAGConfig, ByzerSQLConfig\nfrom byzerllm.utils.client import code_utils\nimport byzerllm\n\n\nclass SQLGenerator:\n\n    @byzerllm.prompt()\n    def gen_sql(\n        self, conversations: List[Dict[str, str]]\n    ) -> str:\n        \"\"\"\n        根据下面的对话内容，理解用户最后的问题，然后根据看到的schema 信息，生成 Byzer-SQL语句。\n        \n        {% for msg in conversations %}\n        <{{ msg.role }}>: {{ msg.content }}\n        {% endfor %}\n\n        注意，输出 Byzer-SQL 语句使用 ```sql``` 标记。         \n        \"\"\"\n\n    def __init__(self, rag_config: RAGConfig, byzer_sql_config: ByzerSQLConfig) -> None:\n        self.schema_rag_client = OpenAI(\n            api_key=rag_config.schema_rag_base_url,\n            base_url=rag_config.schema_rag_api_key,\n        )\n        self.context_rag_client = OpenAI(\n            api_key=rag_config.context_rag_api_key,\n            base_url=rag_config.context_rag_base_url,\n        )\n        self.byzer_sql_url = byzer_sql_config.byzer_sql_url\n\n    def stream_chat_oai(\n        self,\n        conversations,\n        model: Optional[str] = None,\n        role_mapping=None,\n        delta_mode=False,\n        llm_config: Dict[str, Any] = {},\n    ):\n        if not model:\n            model = \"deepseek_chat\"\n\n        gen_sql_prompt = self.gen_sql(conversations).prompt()\n\n        response = self.rag_client.chat.completions.create(\n            messages=gen_sql_prompt,\n            model=model,\n        )\n        sql_str = response.choices[0].message.content\n        sql = code_utils.extract_code(sql_str)[0][1]\n        #MARK\n        return [],[]\n\n\n##File: /Users/allwefantasy/projects/super_analysis/setup.py\nfrom setuptools import setup, find_packages\n\nsetup(\n    name=\"super-analysis\",\n    version=\"0.1.0\",\n    packages=find_packages(where=\"src\"),\n    package_dir={\"\": \"src\"},\n    install_requires=[\n        \"byzerllm\",\n        \"fastapi\",\n        \"uvicorn\",\n        \"argparse\",\n    ],\n    entry_points={\n        \"console_scripts\": [\n            \"super-analysis.serve=superanalysis.serve.cli:main\",\n            \"super-analysis.convert=superanalysis.serve.convert:main\",\n        ],\n    },\n    author=\"allwefantasy\",\n    author_email=\"allwefantasy@gmail.com\",\n    description=\"A package for super analysis with ByzerLLM\",\n    long_description=open(\"README.md\").read(),\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/yourusername/super-analysis\",\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n    ],\n    python_requires='>=3.6',\n)\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/convert.py\nimport os\nimport argparse\nimport pandas as pd\nimport hashlib\nfrom typing import List, Dict\n\ndef get_csv_files(directory: str) -> List[str]:\n    \"\"\"Recursively get all CSV files in the given directory.\"\"\"\n    csv_files = []\n    for root, _, files in os.walk(directory):\n        for file in files:\n            if file.endswith('.csv'):\n                csv_files.append(os.path.join(root, file))\n    return csv_files\n\ndef extract_schema(df: pd.DataFrame) -> Dict[str, str]:\n    \"\"\"Extract schema from pandas DataFrame.\"\"\"\n    schema = {}\n    for column in df.columns:\n        dtype = str(df[column].dtype)\n        if dtype == 'object':\n            schema[column] = 'STRING'\n        elif dtype.startswith('int'):\n            schema[column] = 'INTEGER'\n        elif dtype.startswith('float'):\n            schema[column] = 'FLOAT'\n        elif dtype == 'bool':\n            schema[column] = 'BOOLEAN'\n        elif dtype.startswith('datetime'):\n            schema[column] = 'TIMESTAMP'\n        else:\n            schema[column] = 'STRING'  # Default to STRING for unknown types\n    return schema\n\ndef generate_schema_text(schema: Dict[str, str], csv_file: str) -> str:\n    \"\"\"Generate schema text from the schema dictionary and include the CSV file name.\"\"\"\n    schema_lines = [f\"CSV File: {csv_file}\\n\"]\n    schema_lines.extend([f\"{field}: {dtype}\" for field, dtype in schema.items()])\n    return \"\\n\".join(schema_lines)\n\ndef save_schema(schema_text: str, csv_file: str, output_dir: str) -> None:\n    \"\"\"Save schema text to a file in the output directory.\"\"\"\n    # Generate MD5 hash of the schema text\n    file_name = hashlib.md5(schema_text.encode()).hexdigest() + '.md'\n    output_path = os.path.join(output_dir, file_name)\n    \n    with open(output_path, 'w') as f:\n        f.write(schema_text)\n    print(f\"Schema for {csv_file} saved to {output_path}\")\n\ndef convert(data_dir: str, doc_dir: str) -> None:\n    \"\"\"Main conversion function.\"\"\"\n    csv_files = get_csv_files(data_dir)\n    \n    for csv_file in csv_files:\n        print(f\"Processing {csv_file}\")\n        df = pd.read_csv(csv_file)\n        schema = extract_schema(df)\n        schema_text = generate_schema_text(schema, csv_file)\n        save_schema(schema_text, csv_file, doc_dir)\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Convert CSV files to schema documents\")\n    parser.add_argument(\"--data_dir\", required=True, help=\"Directory containing CSV files\")\n    parser.add_argument(\"--doc_dir\", required=True, help=\"Output directory for schema documents\")\n    \n    args = parser.parse_args()\n    \n    if not os.path.exists(args.doc_dir):\n        os.makedirs(args.doc_dir)\n    \n    convert(args.data_dir, args.doc_dir)\n\nif __name__ == \"__main__\":\n    main()\n\n     \n\n下面是用户的需求：\n\n@generator.py 在 #MARK 标记处,使用 requests 访问 byzer-sql 引擎,执行sql,得到的结果.\n"}, {"role": "assistant", "content": "好的，我理解您的需求是要在 `generator.py` 文件的 #MARK 标记处添加代码，使用 requests 库访问 Byzer-SQL 引擎执行 SQL 语句，并获取结果。我们需要修改 `generator.py` 文件来实现这个功能。以下是相应的修改：\n\n```python\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/generator.py\n<<<<<<< SEARCH\n        sql_str = response.choices[0].message.content\n        sql = code_utils.extract_code(sql_str)[0][1]\n        #MARK\n        return [],[]\n=======\n        sql_str = response.choices[0].message.content\n        sql = code_utils.extract_code(sql_str)[0][1]\n        \n        # Execute SQL using Byzer-SQL engine\n        import requests\n        import json\n\n        try:\n            response = requests.post(\n                self.byzer_sql_url,\n                json={\"sql\": sql},\n                headers={\"Content-Type\": \"application/json\"}\n            )\n            response.raise_for_status()\n            result = response.json()\n            \n            # Process the result\n            if isinstance(result, list):\n                return result, [{\"role\": \"assistant\", \"content\": json.dumps(result, indent=2)}]\n            else:\n                return [result], [{\"role\": \"assistant\", \"content\": json.dumps(result, indent=2)}]\n        except requests.RequestException as e:\n            error_message = f\"Error executing SQL: {str(e)}\"\n            return [], [{\"role\": \"assistant\", \"content\": error_message}]\n\n>>>>>>> REPLACE\n```\n\n这个修改做了以下几点：\n\n1. 我们在 #MARK 标记处添加了新的代码来执行 SQL 查询。\n\n2. 使用 `requests.post()` 方法向 `self.byzer_sql_url` 发送 POST 请求，将 SQL 语句作为 JSON 数据发送。\n\n3. 我们假设 Byzer-SQL 引擎接受 JSON 格式的请求，其中包含一个 \"sql\" 键，值为要执行的 SQL 语句。\n\n4. 请求发送后，我们检查响应状态并获取 JSON 格式的结果。\n\n5. 如果结果是一个列表，我们直接返回它；否则，我们将结果包装在一个列表中返回。\n\n6. 我们还返回一个包含 JSON 格式结果的对话消息，作为助手的回复。\n\n7. 如果在执行过程中发生任何错误，我们会捕获异常并返回一个错误消息。\n\n请注意，您可能需要根据实际的 Byzer-SQL 引擎 API 调整请求的格式和处理响应的方式。此外，您可能还需要在文件顶部添加 `import requests` 和 `import json`，如果这些库还没有被导入的话。\n\n最后，请确保 `self.byzer_sql_url` 已经被正确设置为您的 Byzer-SQL 引擎的 URL。"}], "model": "human", "yaml_file": "actions/116_chat_action.yml"}
{"instruction": "@generator.py 中,在 @@stream_chat_oai 方法里打印合适的日志,让我们可以看到详细的信息流动", "conversations": [{"role": "user", "content": "\n如果你需要生成代码，对于每个需要更改的文件,你需要按 *SEARCH/REPLACE block* 的格式进行生成。\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The opening fence and code language, eg: ```python\n2. The file path alone on a line, starting with \"##File:\" and verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n*SEARCH/REPLACE* blocks will replace *all* matching occurrences.\nInclude enough lines to make the SEARCH blocks unique.\n\nInclude *ALL* the code being searched and replaced!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n下面我们来看一个例子：\n\n当前项目目录结构：\n1. 项目根目录： /tmp/projects/mathweb\n2. 项目子目录/文件列表(类似tree 命令输出)\nflask/\n    app.py\n    templates/\n        index.html\n    static/\n        style.css\n\n用户需求： Change get_factorial() to use math.factorial\n\n回答： To make this change we need to modify `/tmp/projects/mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n用户需求： Refactor hello() into its own file.\n\n回答：To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/hello.py\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/main.py\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n现在让我们开始一个新的任务:\n\n当前项目目录结构：\n1. 项目根目录： /Users/allwefantasy/projects/super_analysis\n2. 项目子目录/文件列表(类似tree 命令输出)：\nsetup.py\n.auto-coder/\n    libs/\n        llm_friendly_packages/\n            src/\n                example1/\n                    main.py\nsrc/\n    superanalysis/\n        serve/\n            convert.py\n            openai_serve.py\n            __init__.py\n            llm_wrapper.py\n            api_server.py\n            cli.py\n        sql/\n            __init__.py\n            types.py\n            generator.py\n下面是一些文件路径以及每个文件对应的源码：\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/types.py\nfrom pydantic import BaseModel\n\nclass RAGConfig(BaseModel):\n    schema_rag_base_url: str = \"\"\n    schema_rag_api_key: str = \"xxxx\"\n\n    context_rag_base_url: str = \"\"\n    context_rag_api_key: str = \"xxxx\"\n\n    \n\n    \n\nclass ByzerSQLConfig(BaseModel):\n    byzer_sql_url: str = \"\"\n    data_dir: str = \"\"\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/cli.py\nimport argparse\nimport byzerllm\nfrom byzerllm.utils.client import ByzerLLM\nfrom superanalysis.serve.api_server import serve, ServerArgs\nfrom superanalysis.sql.types import RAGConfig, ByzerSQLConfig\nfrom superanalysis.sql.generator import SQLGenerator\nfrom superanalysis.serve.llm_wrapper import LLWrapper\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"ByzerLLM API Server\")\n    parser.add_argument(\"--host\", type=str, default=\"0.0.0.0\", help=\"Host to bind the server to\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"Port to bind the server to\")\n    parser.add_argument(\"--uvicorn-log-level\", type=str, default=\"info\", help=\"Uvicorn log level\")\n    parser.add_argument(\"--allow-credentials\", action=\"store_true\", help=\"Allow credentials\")\n    parser.add_argument(\"--allowed-origins\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed origins\")\n    parser.add_argument(\"--allowed-methods\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed methods\")\n    parser.add_argument(\"--allowed-headers\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed headers\")\n    parser.add_argument(\"--api-key\", type=str, default=\"\", help=\"API key for authentication\")\n    parser.add_argument(\"--served-model-name\", type=str, required=True, help=\"Name of the model to be served\")\n    parser.add_argument(\"--prompt-template\", type=str, default=\"\", help=\"Prompt template\")\n    parser.add_argument(\"--response-role\", type=str, default=\"assistant\", help=\"Response role\")\n    parser.add_argument(\"--ssl-keyfile\", type=str, default=\"\", help=\"SSL key file\")\n    parser.add_argument(\"--ssl-certfile\", type=str, default=\"\", help=\"SSL certificate file\")\n    \n    # Add new arguments for RAGConfig\n    parser.add_argument(\"--rag-base-url\", type=str, default=\"\", help=\"RAG base URL\")\n    parser.add_argument(\"--rag-api-key\", type=str, default=\"\", help=\"RAG API key\")\n    \n    # Add new argument for ByzerSQLConfig\n    parser.add_argument(\"--byzer-sql-url\", type=str, default=\"\", help=\"Byzer SQL URL\")\n    parser.add_argument(\"--data-dir\", type=str, default=\"\", help=\"Data directory\")\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile\n    )\n\n    # Create RAGConfig instance\n    rag_config = RAGConfig(\n        rag_base_url=args.rag_base_url,\n        rag_api_key=args.rag_api_key\n    )\n\n    # Create ByzerSQLConfig instance\n    byzer_sql_config = ByzerSQLConfig(\n        byzer_sql_url=args.byzer_sql_url,\n        data_dir=args.data_dir\n    )\n\n    llm = ByzerLLM()\n    llm.setup_default_model_name(args.served_model_name)\n    \n    byzerllm.connect_cluster(address=args.ray_address)\n    llm = byzerllm.ByzerLLM()\n    if not server_args.served_model_name:\n        raise ValueError(\"served_model_name is required\")\n    llm.setup_default_model_name(server_args.served_model_name)\n    rag = SQLGenerator(rag_config=rag_config, byzer_sql_config=byzer_sql_config)    \n    llm_wrapper = LLWrapper(llm=llm, rag=rag)\n    serve(llm_wrapper, server_args)\n\nif __name__ == \"__main__\":\n    main()\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/__init__.py\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/__init__.py\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/llm_wrapper.py\nfrom typing import Any, Dict, List, Optional, Union,Callable\nfrom byzerllm.utils.client.types import (    \n    LLMFunctionCallResponse,\n    LLMClassResponse,LLMResponse    \n)\nimport pydantic\nfrom byzerllm import ByzerLLM\nfrom byzerllm.utils.client import LLMResponse\nfrom byzerllm.utils.types import SingleOutputMeta\nfrom byzerllm.utils.langutil import asyncfy_with_semaphore\nfrom superanalysis.sql.generator import SQLGenerator\nfrom loguru import logger\n\nclass LLWrapper:\n\n    def __init__(self,llm:ByzerLLM,entry:SQLGenerator):\n        self.entry = entry\n        self.llm = llm\n        \n\n    def chat_oai(self,\n                 conversations,\n                 tools:List[Union[Callable,str]]=[], \n                 tool_choice:Optional[Union[Callable,str]]=None,\n                 execute_tool:bool=False,  \n                 impl_func:Optional[Callable]=None,\n                 execute_impl_func:bool=False,\n                 impl_func_params:Optional[Dict[str,Any]]=None,\n                 func_params:Optional[Dict[str,Any]]=None,\n                 response_class:Optional[Union[pydantic.BaseModel,str]] = None, \n                 response_after_chat:Optional[Union[pydantic.BaseModel,str]] = False,\n                 enable_default_sys_message:bool=True,                 \n                 model:Optional[str] = None,\n                 role_mapping=None,llm_config:Dict[str,Any]={}\n                 )->Union[List[LLMResponse],List[LLMFunctionCallResponse],List[LLMClassResponse]]: \n        res,contexts = self.entry.stream_chat_oai(conversations)\n        s = \"\".join(res)        \n        return [LLMResponse(output=s,metadata={},input=\"\")]\n\n    def stream_chat_oai(self,conversations, \n                        model:Optional[str]=None, \n                        role_mapping=None,\n                        delta_mode=False,\n                        llm_config:Dict[str,Any]={}): \n        res,contexts = self.entry.stream_chat_oai(conversations)        \n        for t in res:                        \n            yield (t,SingleOutputMeta(0,0))\n\n    async def async_stream_chat_oai(self,conversations, \n                        model:Optional[str]=None, \n                        role_mapping=None,\n                        delta_mode=False,\n                        llm_config:Dict[str,Any]={}): \n        res,contexts = self.rag.stream_chat_oai(conversations)                \n        for t in res:                                    \n            yield (t,SingleOutputMeta(0,0))\n                         \n\n    def __getattr__(self, name):        \n        return getattr(self.llm, name)\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/openai_serve.py\nimport argparse\nfrom service_server.api_server import ServerArgs, serve\nfrom service_server.llm_wrapper import LLWrapper\nfrom service_server.summary_rag import SummaryRAG\nimport byzerllm\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"OpenAI Serve\")\n    parser.add_argument(\n        \"--model\",\n        default=\"\",\n    )\n    parser.add_argument(\"--emb_model\", default=\"\")\n    parser.add_argument(\n        \"--ray_address\",\n        default=\"auto\",\n    )\n    parser.add_argument(\"--source_dir\", default=\".\", help=\"\")\n    parser.add_argument(\"--host\", default=\"\", help=\"\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"\")\n    parser.add_argument(\"--uvicorn_log_level\", default=\"info\", help=\"\")\n    parser.add_argument(\"--allow_credentials\", action=\"store_true\", help=\"\")\n    parser.add_argument(\"--allowed_origins\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--allowed_methods\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--allowed_headers\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--api_key\", default=\"\", help=\"\")\n    parser.add_argument(\"--served_model_name\", default=\"\", help=\"\")\n    parser.add_argument(\"--prompt_template\", default=\"\", help=\"\")\n    parser.add_argument(\"--ssl_keyfile\", default=\"\", help=\"\")\n    parser.add_argument(\"--ssl_certfile\", default=\"\", help=\"\")\n    parser.add_argument(\"--response_role\", default=\"assistant\", help=\"\")\n    parser.add_argument(\n        \"--collections\", default=\"\", help=\"Collection name for indexing\"\n    )\n    parser.add_argument(\n        \"--base_dir\",\n        default=\"\",\n        help=\"Path where the processed text embeddings were stored\",\n    )\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name or args.model,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile,\n    )\n    byzerllm.connect_cluster(address=args.ray_address)\n    llm = byzerllm.ByzerLLM()\n    if not server_args.served_model_name:\n        raise ValueError(\"served_model_name is required\")\n    llm.setup_default_model_name(server_args.served_model_name)\n    rag = SummaryRAG(llm=llm)    \n    llm_wrapper = LLWrapper(llm=llm, rag=rag)\n    serve(llm=llm_wrapper, args=server_args)\n\nif __name__ == \"__main__\":\n    main()\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/api_server.py\nimport os\nimport time\nimport uvicorn\nfrom fastapi import Request\nfrom fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import JSONResponse, StreamingResponse, Response\n\nfrom byzerllm.log import init_logger\nfrom byzerllm.utils import random_uuid\nfrom byzerllm.version import __version__ as version\nfrom byzerllm.utils.client import ByzerLLM, LLMRequest\nfrom byzerllm.utils.client.entrypoints.openai.serving_chat import OpenAIServingChat\nfrom byzerllm.utils.client.entrypoints.openai.serving_completion import OpenAIServingCompletion\nfrom byzerllm.utils.client.entrypoints.openai.protocol import (\n    ModelList,\n    ModelCard,\n    ModelPermission,\n    ChatCompletionRequest,\n    ErrorResponse,\n    CompletionRequest,\n    Embeddings,\n    EmbeddingsOutput,\n    EmbeddingsData,\n    EmbeddingsUsage,\n)\nfrom pydantic import BaseModel\nfrom typing import List\n\nlogger = init_logger(__name__)\n\nllm_client: ByzerLLM = None\nopenai_serving_chat: OpenAIServingChat = None\nopenai_serving_completion: OpenAIServingCompletion = None\n\nTIMEOUT_KEEP_ALIVE = 5  # seconds\n# timeout in 10 minutes. Streaming can take longer than 3 min\nTIMEOUT = float(os.environ.get(\"BYZERLLM_APISERVER_HTTP_TIMEOUT\", 600))\n\nrouter_app = FastAPI()\n\n\n@router_app.get(\"/health\")\nasync def health() -> Response:\n    \"\"\"Health check.\"\"\"\n    return Response(status_code=200)\n\n\n@router_app.get(\"/v1/models\")\nasync def show_available_models():\n    models = await openai_serving_chat.show_available_models()\n    return JSONResponse(content=models.model_dump())\n\n\n@router_app.get(\"/version\")\nasync def show_version():\n    return JSONResponse(content={\"version\": version})\n\n\n@router_app.get(\"/v1/models\", response_model=ModelList)\nasync def models() -> ModelList:\n    \"\"\"Show available models. Right now we only have one model.\"\"\"\n    model_cards = [\n        ModelCard(\n            id=\"\",\n            root=\"\",\n            permission=[ModelPermission()]\n        )\n    ]\n    return ModelList(data=model_cards)\n\n\n@router_app.post(\"/v1/completions\")\nasync def create_completion(\n        body: CompletionRequest,\n        request: Request\n):\n    generator = await openai_serving_completion.create_completion(body, request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            status_code=generator.code\n        )\n    if body.stream:\n        return StreamingResponse(\n            content=generator,\n            media_type=\"text/event-stream\"\n        )\n    else:\n        return JSONResponse(content=generator.model_dump())\n\n\n@router_app.post(\"/v1/chat/completions\")\nasync def create_chat_completion(\n        body: ChatCompletionRequest,\n        request: Request,\n):\n    \"\"\"Completion API similar to OpenAI's API.\n\n    See  https://platform.openai.com/docs/api-reference/chat/create\n    for the API specification. This API mimics the OpenAI ChatCompletion API.\n\n    NOTE: Currently we do not support the following features:\n        - function_call (Users should implement this by themselves)\n        - logit_bias (to be supported by vLLM engine)\n    \"\"\"\n    # async with async_timeout.timeout(TIMEOUT):\n\n    generator = await openai_serving_chat.create_chat_completion(body, request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            status_code=generator.code\n        )\n    if body.stream:\n        return StreamingResponse(\n            content=generator,\n            media_type=\"text/event-stream\"\n        )\n    else:\n        return JSONResponse(content=generator.model_dump())\n\n\n@router_app.post(\"/v1/embeddings\")\nasync def embed(body: Embeddings):\n    \"\"\"Given a prompt, the model will return one embedding.\n\n    Returns:\n        A response object with an embedding.\n    \"\"\"\n    embedding_id = f\"embed-{random_uuid()}\"\n\n    results_list = llm_client.emb(body.model, request=LLMRequest(instruction=body.input))\n    tokens = 0\n\n    return EmbeddingsOutput(\n        data=[\n            EmbeddingsData(\n                embedding=results.output,\n                index=i,\n                object=\"embedding\",\n            )\n            for i, results in enumerate(results_list)\n        ],\n        id=embedding_id,\n        object=\"list\",\n        created=int(time.time()),\n        model=body.model,\n        usage=EmbeddingsUsage(\n            prompt_tokens=tokens,\n            total_tokens=tokens,\n        ),\n    )\n\nclass ServerArgs(BaseModel):\n    host: str = None\n    port: int = 8000\n    uvicorn_log_level: str = \"info\"\n    allow_credentials: bool = False\n    allowed_origins: List[str] = [\"*\"]  \n    allowed_methods: List[str] = [\"*\"]\n    allowed_headers: List[str] = [\"*\"]\n    api_key: str = None\n    served_model_name: str = None\n    prompt_template: str = None\n    response_role: str = \"assistant\"\n    ssl_keyfile: str = None\n    ssl_certfile: str = None    \n\ndef serve(llm:ByzerLLM, args: ServerArgs):\n    \n    logger.info(f\"ByzerLLM API server version {version}\")\n    logger.info(f\"args: {args}\")\n\n    router_app.add_middleware(\n        CORSMiddleware,\n        allow_origins=args.allowed_origins,\n        allow_credentials=args.allow_credentials,\n        allow_methods=args.allowed_methods,\n        allow_headers=args.allowed_headers,\n    )\n    \n    if token := os.environ.get(\"BYZERLLM_API_KEY\") or args.api_key:\n\n        @router_app.middleware(\"http\")\n        async def authentication(request: Request, call_next):\n            if not request.url.path.startswith(\"/v1\"):\n                return await call_next(request)\n            if request.headers.get(\"Authorization\") != \"Bearer \" + token:\n                return JSONResponse(\n                    content={\"error\": \"Unauthorized\"},\n                    status_code=401\n                )\n            return await call_next(request)\n\n    # Register labels for metrics\n    # add_global_metrics_labels(model_name=engine_args.model)\n    global llm_client\n    llm_client = llm\n    \n    global openai_serving_chat\n    openai_serving_chat = OpenAIServingChat(\n        llm_client=llm_client,\n        response_role=args.response_role,\n        server_model_name=args.served_model_name,\n        prompt_template=args.prompt_template\n    )\n    global openai_serving_completion\n    openai_serving_completion = OpenAIServingCompletion(\n        llm_client=llm_client,\n        server_model_name=args.served_model_name,\n        prompt_template=args.prompt_template\n    )\n\n    uvicorn.run(\n        router_app,\n        host=args.host,\n        port=args.port,\n        log_level=args.uvicorn_log_level,\n        timeout_keep_alive=TIMEOUT_KEEP_ALIVE,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile\n    )\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/generator.py\nfrom typing import Optional, Dict, Any,List\nfrom openai import OpenAI\nimport json\nfrom .types import RAGConfig, ByzerSQLConfig\nfrom byzerllm.utils.client import code_utils\nimport byzerllm\nimport requests\nfrom loguru import logger\nclass SQLGenerator:\n\n    @byzerllm.prompt()\n    def gen_sql(\n        self, conversations: List[Dict[str, str]],context:str\n    ) -> str:\n        \"\"\"        \n        对话内容：\n        {% for msg in conversations %}\n        <{{ msg.role }}>: {{ msg.content }}\n        {% endfor %}\n\n        相关知识：\n        {{ context }}\n\n        根据提供的对话内容以及相关知识，理解用户最后的问题，然后根据看到的schema 信息，生成 Byzer-SQL语句。\n        注意，输出 Byzer-SQL 语句使用 ```sql``` 标记。         \n        \"\"\"\n\n    def __init__(self, rag_config: RAGConfig, byzer_sql_config: ByzerSQLConfig) -> None:\n        self.schema_rag_client = OpenAI(\n            api_key=rag_config.schema_rag_base_url,\n            base_url=rag_config.schema_rag_api_key,\n        )\n        self.context_rag_client = OpenAI(\n            api_key=rag_config.context_rag_api_key,\n            base_url=rag_config.context_rag_base_url,\n        )\n        self.byzer_sql_url = byzer_sql_config.byzer_sql_url\n\n    def stream_chat_oai(\n        self,\n        conversations,\n        model: Optional[str] = None,\n        role_mapping=None,\n        delta_mode=False,\n        llm_config: Dict[str, Any] = {},\n    ):\n        if not model:\n            model = \"deepseek_chat\"\n\n    \n        context_response = self.context_rag_client.chat.completions.create(\n            messages=conversations,\n            model=model,\n        )\n\n        context = response.choices[0].message.content\n\n        gen_sql_prompt = self.gen_sql.prompt(conversations=conversations,context=context)\n\n        response = self.schema_rag_client.chat.completions.create(\n            messages=gen_sql_prompt,\n            model=model,\n        )\n        sql_str = response.choices[0].message.content\n        sql = code_utils.extract_code(sql_str)[0][1]\n                        \n        response = requests.post(\n            self.byzer_sql_url,\n            params={\"sql\": sql}                \n        )\n        response.raise_for_status()\n        result = response.content.decode(\"utf-8\")        \n        return [result], []\n\n\n\n##File: /Users/allwefantasy/projects/super_analysis/setup.py\nfrom setuptools import setup, find_packages\n\nsetup(\n    name=\"super-analysis\",\n    version=\"0.1.0\",\n    packages=find_packages(where=\"src\"),\n    package_dir={\"\": \"src\"},\n    install_requires=[\n        \"byzerllm\",\n        \"fastapi\",\n        \"uvicorn\",\n        \"argparse\",\n    ],\n    entry_points={\n        \"console_scripts\": [\n            \"super-analysis.serve=superanalysis.serve.cli:main\",\n            \"super-analysis.convert=superanalysis.serve.convert:main\",\n        ],\n    },\n    author=\"allwefantasy\",\n    author_email=\"allwefantasy@gmail.com\",\n    description=\"A package for super analysis with ByzerLLM\",\n    long_description=open(\"README.md\").read(),\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/yourusername/super-analysis\",\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n    ],\n    python_requires='>=3.6',\n)\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/convert.py\nimport os\nimport argparse\nimport pandas as pd\nimport hashlib\nfrom typing import List, Dict\n\ndef get_csv_files(directory: str) -> List[str]:\n    \"\"\"Recursively get all CSV files in the given directory.\"\"\"\n    csv_files = []\n    for root, _, files in os.walk(directory):\n        for file in files:\n            if file.endswith('.csv'):\n                csv_files.append(os.path.join(root, file))\n    return csv_files\n\ndef extract_schema(df: pd.DataFrame) -> Dict[str, str]:\n    \"\"\"Extract schema from pandas DataFrame.\"\"\"\n    schema = {}\n    for column in df.columns:\n        dtype = str(df[column].dtype)\n        if dtype == 'object':\n            schema[column] = 'STRING'\n        elif dtype.startswith('int'):\n            schema[column] = 'INTEGER'\n        elif dtype.startswith('float'):\n            schema[column] = 'FLOAT'\n        elif dtype == 'bool':\n            schema[column] = 'BOOLEAN'\n        elif dtype.startswith('datetime'):\n            schema[column] = 'TIMESTAMP'\n        else:\n            schema[column] = 'STRING'  # Default to STRING for unknown types\n    return schema\n\ndef generate_schema_text(schema: Dict[str, str], csv_file: str) -> str:\n    \"\"\"Generate schema text from the schema dictionary and include the CSV file name.\"\"\"\n    schema_lines = [f\"CSV File: {csv_file}\\n\"]\n    schema_lines.extend([f\"{field}: {dtype}\" for field, dtype in schema.items()])\n    return \"\\n\".join(schema_lines)\n\ndef save_schema(schema_text: str, csv_file: str, output_dir: str) -> None:\n    \"\"\"Save schema text to a file in the output directory.\"\"\"\n    # Generate MD5 hash of the schema text\n    file_name = hashlib.md5(schema_text.encode()).hexdigest() + '.md'\n    output_path = os.path.join(output_dir, file_name)\n    \n    with open(output_path, 'w') as f:\n        f.write(schema_text)\n    print(f\"Schema for {csv_file} saved to {output_path}\")\n\ndef convert(data_dir: str, doc_dir: str) -> None:\n    \"\"\"Main conversion function.\"\"\"\n    csv_files = get_csv_files(data_dir)\n    \n    for csv_file in csv_files:\n        print(f\"Processing {csv_file}\")\n        df = pd.read_csv(csv_file)\n        schema = extract_schema(df)\n        schema_text = generate_schema_text(schema, csv_file)\n        save_schema(schema_text, csv_file, doc_dir)\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Convert CSV files to schema documents\")\n    parser.add_argument(\"--data_dir\", required=True, help=\"Directory containing CSV files\")\n    parser.add_argument(\"--doc_dir\", required=True, help=\"Output directory for schema documents\")\n    \n    args = parser.parse_args()\n    \n    if not os.path.exists(args.doc_dir):\n        os.makedirs(args.doc_dir)\n    \n    convert(args.data_dir, args.doc_dir)\n\nif __name__ == \"__main__\":\n    main()\n\n     \n\n下面是用户的需求：\n\n@generator.py 中,在 @@stream_chat_oai 方法里打印合适的日志,让我们可以看到详细的信息流动\n"}, {"role": "assistant", "content": "To add appropriate logging in the `stream_chat_oai` method of the `SQLGenerator` class, we'll use the `loguru` logger to print detailed information about the flow of operations. Here's how we can modify the `generator.py` file:\n\n```python\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/generator.py\n<<<<<<< SEARCH\nfrom loguru import logger\nclass SQLGenerator:\n\n    @byzerllm.prompt()\n    def gen_sql(\n        self, conversations: List[Dict[str, str]],context:str\n    ) -> str:\n        \"\"\"        \n        对话内容：\n        {% for msg in conversations %}\n        <{{ msg.role }}>: {{ msg.content }}\n        {% endfor %}\n\n        相关知识：\n        {{ context }}\n\n        根据提供的对话内容以及相关知识，理解用户最后的问题，然后根据看到的schema 信息，生成 Byzer-SQL语句。\n        注意，输出 Byzer-SQL 语句使用 ```sql``` 标记。         \n        \"\"\"\n\n    def __init__(self, rag_config: RAGConfig, byzer_sql_config: ByzerSQLConfig) -> None:\n        self.schema_rag_client = OpenAI(\n            api_key=rag_config.schema_rag_base_url,\n            base_url=rag_config.schema_rag_api_key,\n        )\n        self.context_rag_client = OpenAI(\n            api_key=rag_config.context_rag_api_key,\n            base_url=rag_config.context_rag_base_url,\n        )\n        self.byzer_sql_url = byzer_sql_config.byzer_sql_url\n\n    def stream_chat_oai(\n        self,\n        conversations,\n        model: Optional[str] = None,\n        role_mapping=None,\n        delta_mode=False,\n        llm_config: Dict[str, Any] = {},\n    ):\n        if not model:\n            model = \"deepseek_chat\"\n\n    \n        context_response = self.context_rag_client.chat.completions.create(\n            messages=conversations,\n            model=model,\n        )\n\n        context = response.choices[0].message.content\n\n        gen_sql_prompt = self.gen_sql.prompt(conversations=conversations,context=context)\n\n        response = self.schema_rag_client.chat.completions.create(\n            messages=gen_sql_prompt,\n            model=model,\n        )\n        sql_str = response.choices[0].message.content\n        sql = code_utils.extract_code(sql_str)[0][1]\n                        \n        response = requests.post(\n            self.byzer_sql_url,\n            params={\"sql\": sql}                \n        )\n        response.raise_for_status()\n        result = response.content.decode(\"utf-8\")        \n        return [result], []\n=======\nfrom loguru import logger\nclass SQLGenerator:\n\n    @byzerllm.prompt()\n    def gen_sql(\n        self, conversations: List[Dict[str, str]],context:str\n    ) -> str:\n        \"\"\"        \n        对话内容：\n        {% for msg in conversations %}\n        <{{ msg.role }}>: {{ msg.content }}\n        {% endfor %}\n\n        相关知识：\n        {{ context }}\n\n        根据提供的对话内容以及相关知识，理解用户最后的问题，然后根据看到的schema 信息，生成 Byzer-SQL语句。\n        注意，输出 Byzer-SQL 语句使用 ```sql``` 标记。         \n        \"\"\"\n\n    def __init__(self, rag_config: RAGConfig, byzer_sql_config: ByzerSQLConfig) -> None:\n        self.schema_rag_client = OpenAI(\n            api_key=rag_config.schema_rag_base_url,\n            base_url=rag_config.schema_rag_api_key,\n        )\n        self.context_rag_client = OpenAI(\n            api_key=rag_config.context_rag_api_key,\n            base_url=rag_config.context_rag_base_url,\n        )\n        self.byzer_sql_url = byzer_sql_config.byzer_sql_url\n\n    def stream_chat_oai(\n        self,\n        conversations,\n        model: Optional[str] = None,\n        role_mapping=None,\n        delta_mode=False,\n        llm_config: Dict[str, Any] = {},\n    ):\n        logger.info(f\"Starting stream_chat_oai with model: {model}\")\n        logger.debug(f\"Conversations: {conversations}\")\n        logger.debug(f\"LLM Config: {llm_config}\")\n\n        if not model:\n            model = \"deepseek_chat\"\n        logger.info(f\"Using model: {model}\")\n\n        logger.info(\"Fetching context from context RAG client\")\n        context_response = self.context_rag_client.chat.completions.create(\n            messages=conversations,\n            model=model,\n        )\n        logger.debug(f\"Context RAG response: {context_response}\")\n\n        context = context_response.choices[0].message.content\n        logger.info(\"Context retrieved successfully\")\n\n        logger.info(\"Generating SQL prompt\")\n        gen_sql_prompt = self.gen_sql.prompt(conversations=conversations,context=context)\n        logger.debug(f\"Generated SQL prompt: {gen_sql_prompt}\")\n\n        logger.info(\"Fetching SQL from schema RAG client\")\n        response = self.schema_rag_client.chat.completions.create(\n            messages=gen_sql_prompt,\n            model=model,\n        )\n        logger.debug(f\"Schema RAG response: {response}\")\n\n        sql_str = response.choices[0].message.content\n        logger.info(\"SQL string generated\")\n        logger.debug(f\"Generated SQL string: {sql_str}\")\n\n        sql = code_utils.extract_code(sql_str)[0][1]\n        logger.info(\"SQL code extracted\")\n        logger.debug(f\"Extracted SQL: {sql}\")\n\n        logger.info(f\"Sending SQL to Byzer SQL URL: {self.byzer_sql_url}\")\n        response = requests.post(\n            self.byzer_sql_url,\n            params={\"sql\": sql}                \n        )\n        response.raise_for_status()\n        logger.info(\"Byzer SQL response received\")\n\n        result = response.content.decode(\"utf-8\")\n        logger.info(\"SQL execution result decoded\")\n        logger.debug(f\"SQL execution result: {result}\")\n\n        logger.info(\"stream_chat_oai completed successfully\")\n        return [result], []\n>>>>>>> REPLACE\n```\n\nThese changes add detailed logging throughout the `stream_chat_oai` method. Here's a breakdown of the additions:\n\n1. We've added log messages at the start and end of the method to mark its beginning and completion.\n2. We're logging the input parameters, including the model and conversations.\n3. Each major step in the process (fetching context, generating SQL prompt, fetching SQL, executing SQL) now has informative log messages.\n4. We're using different log levels:\n   - `logger.info()` for high-level process steps\n   - `logger.debug()` for detailed information that might be useful for debugging\n\nThese logs will provide a clear picture of the information flow through the method, making it easier to understand the process and debug any issues that might arise.\n\nRemember to configure the logger appropriately in your main application to control where these logs are output and at what level of detail. You can adjust the log levels (e.g., from `debug` to `info`) if you find the output too verbose for regular operation."}], "model": "human", "yaml_file": "actions/117_chat_action.yml"}
{"instruction": "在 notebooks 目录生成一个jupyter notebook ,使用 openai sdk 访问 http://127.0.0.1:8000/v1 接口", "conversations": [{"role": "user", "content": "\n如果你需要生成代码，对于每个需要更改的文件,你需要按 *SEARCH/REPLACE block* 的格式进行生成。\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The opening fence and code language, eg: ```python\n2. The file path alone on a line, starting with \"##File:\" and verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n*SEARCH/REPLACE* blocks will replace *all* matching occurrences.\nInclude enough lines to make the SEARCH blocks unique.\n\nInclude *ALL* the code being searched and replaced!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n下面我们来看一个例子：\n\n当前项目目录结构：\n1. 项目根目录： /tmp/projects/mathweb\n2. 项目子目录/文件列表(类似tree 命令输出)\nflask/\n    app.py\n    templates/\n        index.html\n    static/\n        style.css\n\n用户需求： Change get_factorial() to use math.factorial\n\n回答： To make this change we need to modify `/tmp/projects/mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n用户需求： Refactor hello() into its own file.\n\n回答：To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/hello.py\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/main.py\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n现在让我们开始一个新的任务:\n\n当前项目目录结构：\n1. 项目根目录： /Users/allwefantasy/projects/super_analysis\n2. 项目子目录/文件列表(类似tree 命令输出)：\nsetup.py\n.auto-coder/\n    libs/\n        llm_friendly_packages/\n            src/\n                example1/\n                    main.py\nsrc/\n    superanalysis/\n        serve/\n            convert.py\n            openai_serve.py\n            __init__.py\n            llm_wrapper.py\n            api_server.py\n            cli.py\n        sql/\n            __init__.py\n            types.py\n            generator.py\n下面是一些文件路径以及每个文件对应的源码：\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/types.py\nfrom pydantic import BaseModel\n\nclass RAGConfig(BaseModel):\n    schema_rag_base_url: str = \"\"\n    schema_rag_api_key: str = \"xxxx\"\n\n    context_rag_base_url: str = \"\"\n    context_rag_api_key: str = \"xxxx\"\n\n    \n\n    \n\nclass ByzerSQLConfig(BaseModel):\n    byzer_sql_url: str = \"\"\n    data_dir: str = \"\"\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/cli.py\nimport argparse\nimport byzerllm\nfrom byzerllm.utils.client import ByzerLLM\nfrom superanalysis.serve.api_server import serve, ServerArgs\nfrom superanalysis.sql.types import RAGConfig, ByzerSQLConfig\nfrom superanalysis.sql.generator import SQLGenerator\nfrom superanalysis.serve.llm_wrapper import LLWrapper\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"ByzerLLM API Server\")\n    parser.add_argument(\"--ray-address\", type=str, default=\"auto\", help=\"ray address\")\n    parser.add_argument(\"--host\", type=str, default=\"0.0.0.0\", help=\"Host to bind the server to\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"Port to bind the server to\")\n    parser.add_argument(\"--uvicorn-log-level\", type=str, default=\"info\", help=\"Uvicorn log level\")\n    parser.add_argument(\"--allow-credentials\", action=\"store_true\", help=\"Allow credentials\")\n    parser.add_argument(\"--allowed-origins\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed origins\")\n    parser.add_argument(\"--allowed-methods\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed methods\")\n    parser.add_argument(\"--allowed-headers\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed headers\")\n    parser.add_argument(\"--api-key\", type=str, default=\"\", help=\"API key for authentication\")\n    parser.add_argument(\"--served-model-name\", type=str, required=True, help=\"Name of the model to be served\")\n    parser.add_argument(\"--prompt-template\", type=str, default=\"\", help=\"Prompt template\")\n    parser.add_argument(\"--response-role\", type=str, default=\"assistant\", help=\"Response role\")\n    parser.add_argument(\"--ssl-keyfile\", type=str, default=\"\", help=\"SSL key file\")\n    parser.add_argument(\"--ssl-certfile\", type=str, default=\"\", help=\"SSL certificate file\")\n    \n    # Add new arguments for RAGConfig\n    parser.add_argument(\"--schema-rag-base-url\", type=str, default=\"\", help=\"RAG base URL\")\n    parser.add_argument(\"--schema-rag-api-key\", type=str, default=\"xxxx\", help=\"RAG API key\")\n\n    parser.add_argument(\"--context-rag-base-url\", type=str, default=\"\", help=\"RAG base URL\")\n    parser.add_argument(\"--context-rag-api-key\", type=str, default=\"xxxx\", help=\"RAG API key\")\n    \n    # Add new argument for ByzerSQLConfig\n    parser.add_argument(\"--byzer-sql-url\", type=str, default=\"\", help=\"Byzer SQL URL\")\n    parser.add_argument(\"--data-dir\", type=str, default=\"\", help=\"Data directory\")\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile\n    )\n\n    # Create RAGConfig instance\n    rag_config = RAGConfig(\n        schema_rag_base_url=args.schema_rag_base_url,\n        schema_rag_api_key=args.schema_rag_api_key,\n        context_rag_base_url=args.context_rag_base_url,\n        context_rag_api_key=args.context_rag_api_key\n    )\n\n    # Create ByzerSQLConfig instance\n    byzer_sql_config = ByzerSQLConfig(\n        byzer_sql_url=args.byzer_sql_url,\n        data_dir=args.data_dir\n    )\n\n    byzerllm.connect_cluster(address=args.ray_address)\n    llm = ByzerLLM()\n    llm.setup_default_model_name(args.served_model_name)        \n    llm = byzerllm.ByzerLLM()\n    if not server_args.served_model_name:\n        raise ValueError(\"served_model_name is required\")\n    llm.setup_default_model_name(server_args.served_model_name)\n\n    print(\"RAG Config: \", rag_config)\n    rag = SQLGenerator(rag_config=rag_config, byzer_sql_config=byzer_sql_config)    \n    llm_wrapper = LLWrapper(llm=llm, entry=rag)\n    serve(llm_wrapper, server_args)\n\nif __name__ == \"__main__\":\n    main()\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/__init__.py\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/__init__.py\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/llm_wrapper.py\nfrom typing import Any, Dict, List, Optional, Union,Callable\nfrom byzerllm.utils.client.types import (    \n    LLMFunctionCallResponse,\n    LLMClassResponse,LLMResponse    \n)\nimport pydantic\nfrom byzerllm import ByzerLLM\nfrom byzerllm.utils.client import LLMResponse\nfrom byzerllm.utils.types import SingleOutputMeta\nfrom byzerllm.utils.langutil import asyncfy_with_semaphore\nfrom superanalysis.sql.generator import SQLGenerator\nfrom loguru import logger\n\nclass LLWrapper:\n\n    def __init__(self,llm:ByzerLLM,entry:SQLGenerator):\n        self.entry = entry\n        self.llm = llm\n        \n\n    def chat_oai(self,\n                 conversations,\n                 tools:List[Union[Callable,str]]=[], \n                 tool_choice:Optional[Union[Callable,str]]=None,\n                 execute_tool:bool=False,  \n                 impl_func:Optional[Callable]=None,\n                 execute_impl_func:bool=False,\n                 impl_func_params:Optional[Dict[str,Any]]=None,\n                 func_params:Optional[Dict[str,Any]]=None,\n                 response_class:Optional[Union[pydantic.BaseModel,str]] = None, \n                 response_after_chat:Optional[Union[pydantic.BaseModel,str]] = False,\n                 enable_default_sys_message:bool=True,                 \n                 model:Optional[str] = None,\n                 role_mapping=None,llm_config:Dict[str,Any]={}\n                 )->Union[List[LLMResponse],List[LLMFunctionCallResponse],List[LLMClassResponse]]: \n        res,contexts = self.entry.stream_chat_oai(conversations)\n        s = \"\".join(res)        \n        return [LLMResponse(output=s,metadata={},input=\"\")]\n\n    def stream_chat_oai(self,conversations, \n                        model:Optional[str]=None, \n                        role_mapping=None,\n                        delta_mode=False,\n                        llm_config:Dict[str,Any]={}): \n        res,contexts = self.entry.stream_chat_oai(conversations)        \n        for t in res:                        \n            yield (t,SingleOutputMeta(0,0))\n\n    async def async_stream_chat_oai(self,conversations, \n                        model:Optional[str]=None, \n                        role_mapping=None,\n                        delta_mode=False,\n                        llm_config:Dict[str,Any]={}): \n        res,contexts = self.rag.stream_chat_oai(conversations)                \n        for t in res:                                    \n            yield (t,SingleOutputMeta(0,0))\n                         \n\n    def __getattr__(self, name):        \n        return getattr(self.llm, name)\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/openai_serve.py\nimport argparse\nfrom service_server.api_server import ServerArgs, serve\nfrom service_server.llm_wrapper import LLWrapper\nfrom service_server.summary_rag import SummaryRAG\nimport byzerllm\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"OpenAI Serve\")\n    parser.add_argument(\n        \"--model\",\n        default=\"\",\n    )\n    parser.add_argument(\"--emb_model\", default=\"\")\n    parser.add_argument(\n        \"--ray_address\",\n        default=\"auto\",\n    )\n    parser.add_argument(\"--source_dir\", default=\".\", help=\"\")\n    parser.add_argument(\"--host\", default=\"\", help=\"\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"\")\n    parser.add_argument(\"--uvicorn_log_level\", default=\"info\", help=\"\")\n    parser.add_argument(\"--allow_credentials\", action=\"store_true\", help=\"\")\n    parser.add_argument(\"--allowed_origins\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--allowed_methods\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--allowed_headers\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--api_key\", default=\"\", help=\"\")\n    parser.add_argument(\"--served_model_name\", default=\"\", help=\"\")\n    parser.add_argument(\"--prompt_template\", default=\"\", help=\"\")\n    parser.add_argument(\"--ssl_keyfile\", default=\"\", help=\"\")\n    parser.add_argument(\"--ssl_certfile\", default=\"\", help=\"\")\n    parser.add_argument(\"--response_role\", default=\"assistant\", help=\"\")\n    parser.add_argument(\n        \"--collections\", default=\"\", help=\"Collection name for indexing\"\n    )\n    parser.add_argument(\n        \"--base_dir\",\n        default=\"\",\n        help=\"Path where the processed text embeddings were stored\",\n    )\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name or args.model,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile,\n    )\n    byzerllm.connect_cluster(address=args.ray_address)\n    llm = byzerllm.ByzerLLM()\n    if not server_args.served_model_name:\n        raise ValueError(\"served_model_name is required\")\n    llm.setup_default_model_name(server_args.served_model_name)\n    rag = SummaryRAG(llm=llm)    \n    llm_wrapper = LLWrapper(llm=llm, rag=rag)\n    serve(llm=llm_wrapper, args=server_args)\n\nif __name__ == \"__main__\":\n    main()\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/api_server.py\nimport os\nimport time\nimport uvicorn\nfrom fastapi import Request\nfrom fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import JSONResponse, StreamingResponse, Response\n\nfrom byzerllm.log import init_logger\nfrom byzerllm.utils import random_uuid\nfrom byzerllm.version import __version__ as version\nfrom byzerllm.utils.client import ByzerLLM, LLMRequest\nfrom byzerllm.utils.client.entrypoints.openai.serving_chat import OpenAIServingChat\nfrom byzerllm.utils.client.entrypoints.openai.serving_completion import OpenAIServingCompletion\nfrom byzerllm.utils.client.entrypoints.openai.protocol import (\n    ModelList,\n    ModelCard,\n    ModelPermission,\n    ChatCompletionRequest,\n    ErrorResponse,\n    CompletionRequest,\n    Embeddings,\n    EmbeddingsOutput,\n    EmbeddingsData,\n    EmbeddingsUsage,\n)\nfrom pydantic import BaseModel\nfrom typing import List\n\nlogger = init_logger(__name__)\n\nllm_client: ByzerLLM = None\nopenai_serving_chat: OpenAIServingChat = None\nopenai_serving_completion: OpenAIServingCompletion = None\n\nTIMEOUT_KEEP_ALIVE = 5  # seconds\n# timeout in 10 minutes. Streaming can take longer than 3 min\nTIMEOUT = float(os.environ.get(\"BYZERLLM_APISERVER_HTTP_TIMEOUT\", 600))\n\nrouter_app = FastAPI()\n\n\n@router_app.get(\"/health\")\nasync def health() -> Response:\n    \"\"\"Health check.\"\"\"\n    return Response(status_code=200)\n\n\n@router_app.get(\"/v1/models\")\nasync def show_available_models():\n    models = await openai_serving_chat.show_available_models()\n    return JSONResponse(content=models.model_dump())\n\n\n@router_app.get(\"/version\")\nasync def show_version():\n    return JSONResponse(content={\"version\": version})\n\n\n@router_app.get(\"/v1/models\", response_model=ModelList)\nasync def models() -> ModelList:\n    \"\"\"Show available models. Right now we only have one model.\"\"\"\n    model_cards = [\n        ModelCard(\n            id=\"\",\n            root=\"\",\n            permission=[ModelPermission()]\n        )\n    ]\n    return ModelList(data=model_cards)\n\n\n@router_app.post(\"/v1/completions\")\nasync def create_completion(\n        body: CompletionRequest,\n        request: Request\n):\n    generator = await openai_serving_completion.create_completion(body, request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            status_code=generator.code\n        )\n    if body.stream:\n        return StreamingResponse(\n            content=generator,\n            media_type=\"text/event-stream\"\n        )\n    else:\n        return JSONResponse(content=generator.model_dump())\n\n\n@router_app.post(\"/v1/chat/completions\")\nasync def create_chat_completion(\n        body: ChatCompletionRequest,\n        request: Request,\n):\n    \"\"\"Completion API similar to OpenAI's API.\n\n    See  https://platform.openai.com/docs/api-reference/chat/create\n    for the API specification. This API mimics the OpenAI ChatCompletion API.\n\n    NOTE: Currently we do not support the following features:\n        - function_call (Users should implement this by themselves)\n        - logit_bias (to be supported by vLLM engine)\n    \"\"\"\n    # async with async_timeout.timeout(TIMEOUT):\n\n    generator = await openai_serving_chat.create_chat_completion(body, request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            status_code=generator.code\n        )\n    if body.stream:\n        return StreamingResponse(\n            content=generator,\n            media_type=\"text/event-stream\"\n        )\n    else:\n        return JSONResponse(content=generator.model_dump())\n\n\n@router_app.post(\"/v1/embeddings\")\nasync def embed(body: Embeddings):\n    \"\"\"Given a prompt, the model will return one embedding.\n\n    Returns:\n        A response object with an embedding.\n    \"\"\"\n    embedding_id = f\"embed-{random_uuid()}\"\n\n    results_list = llm_client.emb(body.model, request=LLMRequest(instruction=body.input))\n    tokens = 0\n\n    return EmbeddingsOutput(\n        data=[\n            EmbeddingsData(\n                embedding=results.output,\n                index=i,\n                object=\"embedding\",\n            )\n            for i, results in enumerate(results_list)\n        ],\n        id=embedding_id,\n        object=\"list\",\n        created=int(time.time()),\n        model=body.model,\n        usage=EmbeddingsUsage(\n            prompt_tokens=tokens,\n            total_tokens=tokens,\n        ),\n    )\n\nclass ServerArgs(BaseModel):\n    host: str = None\n    port: int = 8000\n    uvicorn_log_level: str = \"info\"\n    allow_credentials: bool = False\n    allowed_origins: List[str] = [\"*\"]  \n    allowed_methods: List[str] = [\"*\"]\n    allowed_headers: List[str] = [\"*\"]\n    api_key: str = None\n    served_model_name: str = None\n    prompt_template: str = None\n    response_role: str = \"assistant\"\n    ssl_keyfile: str = None\n    ssl_certfile: str = None    \n\ndef serve(llm:ByzerLLM, args: ServerArgs):\n    \n    logger.info(f\"ByzerLLM API server version {version}\")\n    logger.info(f\"args: {args}\")\n\n    router_app.add_middleware(\n        CORSMiddleware,\n        allow_origins=args.allowed_origins,\n        allow_credentials=args.allow_credentials,\n        allow_methods=args.allowed_methods,\n        allow_headers=args.allowed_headers,\n    )\n    \n    if token := os.environ.get(\"BYZERLLM_API_KEY\") or args.api_key:\n\n        @router_app.middleware(\"http\")\n        async def authentication(request: Request, call_next):\n            if not request.url.path.startswith(\"/v1\"):\n                return await call_next(request)\n            if request.headers.get(\"Authorization\") != \"Bearer \" + token:\n                return JSONResponse(\n                    content={\"error\": \"Unauthorized\"},\n                    status_code=401\n                )\n            return await call_next(request)\n\n    # Register labels for metrics\n    # add_global_metrics_labels(model_name=engine_args.model)\n    global llm_client\n    llm_client = llm\n    \n    global openai_serving_chat\n    openai_serving_chat = OpenAIServingChat(\n        llm_client=llm_client,\n        response_role=args.response_role,\n        server_model_name=args.served_model_name,\n        prompt_template=args.prompt_template\n    )\n    global openai_serving_completion\n    openai_serving_completion = OpenAIServingCompletion(\n        llm_client=llm_client,\n        server_model_name=args.served_model_name,\n        prompt_template=args.prompt_template\n    )\n\n    uvicorn.run(\n        router_app,\n        host=args.host,\n        port=args.port,\n        log_level=args.uvicorn_log_level,\n        timeout_keep_alive=TIMEOUT_KEEP_ALIVE,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile\n    )\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/generator.py\nfrom typing import Optional, Dict, Any,List\nfrom openai import OpenAI\nimport json\nfrom .types import RAGConfig, ByzerSQLConfig\nfrom byzerllm.utils.client import code_utils\nimport byzerllm\nimport requests\nfrom loguru import logger\nclass SQLGenerator:\n\n    @byzerllm.prompt()\n    def gen_sql(\n        self, conversations: List[Dict[str, str]],context:str\n    ) -> str:\n        \"\"\"        \n        对话内容：\n        {% for msg in conversations %}\n        <{{ msg.role }}>: {{ msg.content }}\n        {% endfor %}\n\n        相关知识：\n        {{ context }}\n\n        根据提供的对话内容以及相关知识，理解用户最后的问题，然后根据看到的schema 信息，生成 Byzer-SQL语句。\n        注意，输出 Byzer-SQL 语句使用 ```sql``` 标记。         \n        \"\"\"\n\n    def __init__(self, rag_config: RAGConfig, byzer_sql_config: ByzerSQLConfig) -> None:\n        self.schema_rag_client = OpenAI(\n            api_key=rag_config.schema_rag_base_url,\n            base_url=rag_config.schema_rag_api_key,\n        )\n        self.context_rag_client = OpenAI(\n            api_key=rag_config.context_rag_api_key,\n            base_url=rag_config.context_rag_base_url,\n        )\n        self.byzer_sql_url = byzer_sql_config.byzer_sql_url\n\n    def stream_chat_oai(\n        self,\n        conversations,\n        model: Optional[str] = None,\n        role_mapping=None,\n        delta_mode=False,\n        llm_config: Dict[str, Any] = {},\n    ):   \n        try :            \n            v = self._stream_chat_oai(\n                conversations=conversations,\n                model=model,\n                role_mapping=role_mapping,\n                delta_mode=delta_mode,\n                llm_config=llm_config,\n            )\n        except Exception as e:\n            logger.error(f\"Error in stream_chat_oai: {e}\")\n            v = [str(e)], []         \n\n        return v    \n\n    def _stream_chat_oai(\n        self,\n        conversations,\n        model: Optional[str] = None,\n        role_mapping=None,\n        delta_mode=False,\n        llm_config: Dict[str, Any] = {},\n    ):\n        logger.info(f\"Starting stream_chat_oai with model: {model}\")\n        logger.debug(f\"Conversations: {conversations}\")\n        logger.debug(f\"LLM Config: {llm_config}\")\n\n        if not model:\n            model = \"deepseek_chat\"\n        logger.info(f\"Using model: {model}\")\n\n        logger.info(\"Fetching context from context RAG client\")\n        context_response = self.context_rag_client.chat.completions.create(\n            messages=conversations,\n            model=model,\n        )\n        logger.debug(f\"Context RAG response: {context_response}\")\n\n        context = context_response.choices[0].message.content\n        logger.info(\"Context retrieved successfully\")\n\n        logger.info(\"Generating SQL prompt\")\n        gen_sql_prompt = self.gen_sql.prompt(conversations=conversations,context=context)\n        logger.debug(f\"Generated SQL prompt: {gen_sql_prompt}\")\n\n        logger.info(\"Fetching SQL from schema RAG client\")\n        response = self.schema_rag_client.chat.completions.create(\n            messages=gen_sql_prompt,\n            model=model,\n        )\n        logger.debug(f\"Schema RAG response: {response}\")\n\n        sql_str = response.choices[0].message.content\n        logger.info(\"SQL string generated\")\n        logger.debug(f\"Generated SQL string: {sql_str}\")\n\n        sql = code_utils.extract_code(sql_str)[0][1]\n        logger.info(\"SQL code extracted\")\n        logger.debug(f\"Extracted SQL: {sql}\")\n\n        logger.info(f\"Sending SQL to Byzer SQL URL: {self.byzer_sql_url}\")\n        response = requests.post(\n            self.byzer_sql_url,\n            params={\"sql\": sql}                \n        )\n        response.raise_for_status()\n        logger.info(\"Byzer SQL response received\")\n\n        result = response.content.decode(\"utf-8\")\n        logger.info(\"SQL execution result decoded\")\n        logger.debug(f\"SQL execution result: {result}\")\n        \n        return [result], []\n\n\n\n##File: /Users/allwefantasy/projects/super_analysis/setup.py\nfrom setuptools import setup, find_packages\n\nsetup(\n    name=\"super-analysis\",\n    version=\"0.1.0\",\n    packages=find_packages(where=\"src\"),\n    package_dir={\"\": \"src\"},\n    install_requires=[\n        \"byzerllm\",\n        \"fastapi\",\n        \"uvicorn\",\n        \"argparse\",\n    ],\n    entry_points={\n        \"console_scripts\": [\n            \"super-analysis.serve=superanalysis.serve.cli:main\",\n            \"super-analysis.convert=superanalysis.serve.convert:main\",\n        ],\n    },\n    author=\"allwefantasy\",\n    author_email=\"allwefantasy@gmail.com\",\n    description=\"A package for super analysis with ByzerLLM\",\n    long_description=open(\"README.md\").read(),\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/yourusername/super-analysis\",\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n    ],\n    python_requires='>=3.6',\n)\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/convert.py\nimport os\nimport argparse\nimport pandas as pd\nimport hashlib\nfrom typing import List, Dict\n\ndef get_csv_files(directory: str) -> List[str]:\n    \"\"\"Recursively get all CSV files in the given directory.\"\"\"\n    csv_files = []\n    for root, _, files in os.walk(directory):\n        for file in files:\n            if file.endswith('.csv'):\n                csv_files.append(os.path.join(root, file))\n    return csv_files\n\ndef extract_schema(df: pd.DataFrame) -> Dict[str, str]:\n    \"\"\"Extract schema from pandas DataFrame.\"\"\"\n    schema = {}\n    for column in df.columns:\n        dtype = str(df[column].dtype)\n        if dtype == 'object':\n            schema[column] = 'STRING'\n        elif dtype.startswith('int'):\n            schema[column] = 'INTEGER'\n        elif dtype.startswith('float'):\n            schema[column] = 'FLOAT'\n        elif dtype == 'bool':\n            schema[column] = 'BOOLEAN'\n        elif dtype.startswith('datetime'):\n            schema[column] = 'TIMESTAMP'\n        else:\n            schema[column] = 'STRING'  # Default to STRING for unknown types\n    return schema\n\ndef generate_schema_text(schema: Dict[str, str], csv_file: str) -> str:\n    \"\"\"Generate schema text from the schema dictionary and include the CSV file name.\"\"\"\n    schema_lines = [f\"CSV File: {csv_file}\\n\"]\n    schema_lines.extend([f\"{field}: {dtype}\" for field, dtype in schema.items()])\n    return \"\\n\".join(schema_lines)\n\ndef save_schema(schema_text: str, csv_file: str, output_dir: str) -> None:\n    \"\"\"Save schema text to a file in the output directory.\"\"\"\n    # Generate MD5 hash of the schema text\n    file_name = hashlib.md5(schema_text.encode()).hexdigest() + '.md'\n    output_path = os.path.join(output_dir, file_name)\n    \n    with open(output_path, 'w') as f:\n        f.write(schema_text)\n    print(f\"Schema for {csv_file} saved to {output_path}\")\n\ndef convert(data_dir: str, doc_dir: str) -> None:\n    \"\"\"Main conversion function.\"\"\"\n    csv_files = get_csv_files(data_dir)\n    \n    for csv_file in csv_files:\n        print(f\"Processing {csv_file}\")\n        df = pd.read_csv(csv_file)\n        schema = extract_schema(df)\n        schema_text = generate_schema_text(schema, csv_file)\n        save_schema(schema_text, csv_file, doc_dir)\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Convert CSV files to schema documents\")\n    parser.add_argument(\"--data_dir\", required=True, help=\"Directory containing CSV files\")\n    parser.add_argument(\"--doc_dir\", required=True, help=\"Output directory for schema documents\")\n    \n    args = parser.parse_args()\n    \n    if not os.path.exists(args.doc_dir):\n        os.makedirs(args.doc_dir)\n    \n    convert(args.data_dir, args.doc_dir)\n\nif __name__ == \"__main__\":\n    main()\n\n     \n\n下面是用户的需求：\n\n在 notebooks 目录生成一个jupyter notebook ,使用 openai sdk 访问 http://127.0.0.1:8000/v1 接口\n"}, {"role": "assistant", "content": "Certainly! I'll create a Jupyter notebook that uses the OpenAI SDK to access the http://127.0.0.1:8000/v1 interface. Here's how we can do that:\n\n1. First, let's create a new directory called `notebooks` if it doesn't exist.\n2. Then, we'll create a new Jupyter notebook file inside this directory.\n3. Finally, we'll add the content to the notebook.\n\nHere are the *SEARCH/REPLACE* blocks to accomplish this:\n\n```python\n##File: /Users/allwefantasy/projects/super_analysis/notebooks/openai_local_api.ipynb\n<<<<<<< SEARCH\n=======\n{\n \"cells\": [\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"# Using OpenAI SDK to Access Local API\\n\",\n    \"\\n\",\n    \"This notebook demonstrates how to use the OpenAI SDK to interact with a local API running at http://127.0.0.1:8000/v1.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"import openai\\n\",\n    \"\\n\",\n    \"# Configure the client to use our local API\\n\",\n    \"client = openai.OpenAI(base_url=\\\"http://127.0.0.1:8000/v1\\\", api_key=\\\"your-api-key\\\")\\n\",\n    \"\\n\",\n    \"# You might need to adjust the model name based on what your local API supports\\n\",\n    \"model_name = \\\"your-model-name\\\"\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## List Available Models\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"models = client.models.list()\\n\",\n    \"print(\\\"Available models:\\\")\\n\",\n    \"for model in models:\\n\",\n    \"    print(model)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Chat Completion\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"chat_completion = client.chat.completions.create(\\n\",\n    \"    model=model_name,\\n\",\n    \"    messages=[\\n\",\n    \"        {\\\"role\\\": \\\"system\\\", \\\"content\\\": \\\"You are a helpful assistant.\\\"},\\n\",\n    \"        {\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"What's the capital of France?\\\"}\\n\",\n    \"    ]\\n\",\n    \")\\n\",\n    \"\\n\",\n    \"print(chat_completion.choices[0].message.content)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Text Completion\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"completion = client.completions.create(\\n\",\n    \"    model=model_name,\\n\",\n    \"    prompt=\\\"Once upon a time\\\",\\n\",\n    \"    max_tokens=50\\n\",\n    \")\\n\",\n    \"\\n\",\n    \"print(completion.choices[0].text)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Embeddings\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"text = \\\"Hello, world!\\\"\\n\",\n    \"embedding = client.embeddings.create(input=text, model=model_name)\\n\",\n    \"print(f\\\"Embedding for '{text}':\\\")\\n\",\n    \"print(embedding.data[0].embedding)\"\n   ]\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\\n\",\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.8.10\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 4\n}\n>>>>>>> REPLACE\n```\n\nThis will create a new Jupyter notebook file named `openai_local_api.ipynb` in the `/Users/allwefantasy/projects/super_analysis/notebooks/` directory. The notebook contains:\n\n1. An introduction markdown cell explaining the purpose of the notebook.\n2. A code cell to set up the OpenAI client with the local API URL.\n3. Sections demonstrating how to:\n   - List available models\n   - Perform chat completion\n   - Perform text completion\n   - Generate embeddings\n\nNote that you'll need to replace `\"your-api-key\"` with the actual API key if your local server requires authentication, and `\"your-model-name\"` with the name of the model supported by your local API.\n\nTo use this notebook:\n\n1. Make sure you have Jupyter installed (`pip install jupyter` if you don't).\n2. Navigate to the `/Users/allwefantasy/projects/super_analysis/` directory in your terminal.\n3. Run `jupyter notebook` to start the Jupyter server.\n4. Open the `notebooks/openai_local_api.ipynb` file in your browser.\n5. Before running the cells, make sure your local API server is running at http://127.0.0.1:8000/v1.\n6. Adjust the `model_name` variable if needed to match the model name supported by your local API.\n7. Execute the cells one by one to interact with your local API using the OpenAI SDK.\n\nThis notebook provides a starting point for interacting with your local API. You can modify and expand it as needed based on the specific capabilities of your local API implementation."}], "model": "human", "yaml_file": "actions/118_chat_action.yml"}
{"instruction": "在 notebooks 目录生成一个jupyter notebook ,使用 openai sdk 访问 http://127.0.0.1:8000/v1 接口", "conversations": [{"role": "user", "content": "\n如果你需要生成代码，对于每个需要更改的文件,你需要按 *SEARCH/REPLACE block* 的格式进行生成。\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The opening fence and code language, eg: ```python\n2. The file path alone on a line, starting with \"##File:\" and verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n*SEARCH/REPLACE* blocks will replace *all* matching occurrences.\nInclude enough lines to make the SEARCH blocks unique.\n\nInclude *ALL* the code being searched and replaced!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n下面我们来看一个例子：\n\n当前项目目录结构：\n1. 项目根目录： /tmp/projects/mathweb\n2. 项目子目录/文件列表(类似tree 命令输出)\nflask/\n    app.py\n    templates/\n        index.html\n    static/\n        style.css\n\n用户需求： Change get_factorial() to use math.factorial\n\n回答： To make this change we need to modify `/tmp/projects/mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n用户需求： Refactor hello() into its own file.\n\n回答：To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/hello.py\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/main.py\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n现在让我们开始一个新的任务:\n\n当前项目目录结构：\n1. 项目根目录： /Users/allwefantasy/projects/super_analysis\n2. 项目子目录/文件列表(类似tree 命令输出)：\nsetup.py\n.auto-coder/\n    libs/\n        llm_friendly_packages/\n            src/\n                example1/\n                    main.py\nsrc/\n    superanalysis/\n        serve/\n            convert.py\n            openai_serve.py\n            __init__.py\n            llm_wrapper.py\n            api_server.py\n            cli.py\n        sql/\n            __init__.py\n            types.py\n            generator.py\n下面是一些文件路径以及每个文件对应的源码：\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/types.py\nfrom pydantic import BaseModel\n\nclass RAGConfig(BaseModel):\n    schema_rag_base_url: str = \"\"\n    schema_rag_api_key: str = \"xxxx\"\n\n    context_rag_base_url: str = \"\"\n    context_rag_api_key: str = \"xxxx\"\n\n    \n\n    \n\nclass ByzerSQLConfig(BaseModel):\n    byzer_sql_url: str = \"\"\n    data_dir: str = \"\"\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/cli.py\nimport argparse\nimport byzerllm\nfrom byzerllm.utils.client import ByzerLLM\nfrom superanalysis.serve.api_server import serve, ServerArgs\nfrom superanalysis.sql.types import RAGConfig, ByzerSQLConfig\nfrom superanalysis.sql.generator import SQLGenerator\nfrom superanalysis.serve.llm_wrapper import LLWrapper\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"ByzerLLM API Server\")\n    parser.add_argument(\"--ray-address\", type=str, default=\"auto\", help=\"ray address\")\n    parser.add_argument(\"--host\", type=str, default=\"0.0.0.0\", help=\"Host to bind the server to\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"Port to bind the server to\")\n    parser.add_argument(\"--uvicorn-log-level\", type=str, default=\"info\", help=\"Uvicorn log level\")\n    parser.add_argument(\"--allow-credentials\", action=\"store_true\", help=\"Allow credentials\")\n    parser.add_argument(\"--allowed-origins\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed origins\")\n    parser.add_argument(\"--allowed-methods\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed methods\")\n    parser.add_argument(\"--allowed-headers\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed headers\")\n    parser.add_argument(\"--api-key\", type=str, default=\"\", help=\"API key for authentication\")\n    parser.add_argument(\"--served-model-name\", type=str, required=True, help=\"Name of the model to be served\")\n    parser.add_argument(\"--prompt-template\", type=str, default=\"\", help=\"Prompt template\")\n    parser.add_argument(\"--response-role\", type=str, default=\"assistant\", help=\"Response role\")\n    parser.add_argument(\"--ssl-keyfile\", type=str, default=\"\", help=\"SSL key file\")\n    parser.add_argument(\"--ssl-certfile\", type=str, default=\"\", help=\"SSL certificate file\")\n    \n    # Add new arguments for RAGConfig\n    parser.add_argument(\"--schema-rag-base-url\", type=str, default=\"\", help=\"RAG base URL\")\n    parser.add_argument(\"--schema-rag-api-key\", type=str, default=\"xxxx\", help=\"RAG API key\")\n\n    parser.add_argument(\"--context-rag-base-url\", type=str, default=\"\", help=\"RAG base URL\")\n    parser.add_argument(\"--context-rag-api-key\", type=str, default=\"xxxx\", help=\"RAG API key\")\n    \n    # Add new argument for ByzerSQLConfig\n    parser.add_argument(\"--byzer-sql-url\", type=str, default=\"\", help=\"Byzer SQL URL\")\n    parser.add_argument(\"--data-dir\", type=str, default=\"\", help=\"Data directory\")\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile\n    )\n\n    # Create RAGConfig instance\n    rag_config = RAGConfig(\n        schema_rag_base_url=args.schema_rag_base_url,\n        schema_rag_api_key=args.schema_rag_api_key,\n        context_rag_base_url=args.context_rag_base_url,\n        context_rag_api_key=args.context_rag_api_key\n    )\n\n    # Create ByzerSQLConfig instance\n    byzer_sql_config = ByzerSQLConfig(\n        byzer_sql_url=args.byzer_sql_url,\n        data_dir=args.data_dir\n    )\n\n    byzerllm.connect_cluster(address=args.ray_address)\n    llm = ByzerLLM()\n    llm.setup_default_model_name(args.served_model_name)        \n    llm = byzerllm.ByzerLLM()\n    if not server_args.served_model_name:\n        raise ValueError(\"served_model_name is required\")\n    llm.setup_default_model_name(server_args.served_model_name)\n\n    print(\"RAG Config: \", rag_config)\n    rag = SQLGenerator(rag_config=rag_config, byzer_sql_config=byzer_sql_config)    \n    llm_wrapper = LLWrapper(llm=llm, entry=rag)\n    serve(llm_wrapper, server_args)\n\nif __name__ == \"__main__\":\n    main()\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/__init__.py\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/__init__.py\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/llm_wrapper.py\nfrom typing import Any, Dict, List, Optional, Union,Callable\nfrom byzerllm.utils.client.types import (    \n    LLMFunctionCallResponse,\n    LLMClassResponse,LLMResponse    \n)\nimport pydantic\nfrom byzerllm import ByzerLLM\nfrom byzerllm.utils.client import LLMResponse\nfrom byzerllm.utils.types import SingleOutputMeta\nfrom byzerllm.utils.langutil import asyncfy_with_semaphore\nfrom superanalysis.sql.generator import SQLGenerator\nfrom loguru import logger\n\nclass LLWrapper:\n\n    def __init__(self,llm:ByzerLLM,entry:SQLGenerator):\n        self.entry = entry\n        self.llm = llm\n        \n\n    def chat_oai(self,\n                 conversations,\n                 tools:List[Union[Callable,str]]=[], \n                 tool_choice:Optional[Union[Callable,str]]=None,\n                 execute_tool:bool=False,  \n                 impl_func:Optional[Callable]=None,\n                 execute_impl_func:bool=False,\n                 impl_func_params:Optional[Dict[str,Any]]=None,\n                 func_params:Optional[Dict[str,Any]]=None,\n                 response_class:Optional[Union[pydantic.BaseModel,str]] = None, \n                 response_after_chat:Optional[Union[pydantic.BaseModel,str]] = False,\n                 enable_default_sys_message:bool=True,                 \n                 model:Optional[str] = None,\n                 role_mapping=None,llm_config:Dict[str,Any]={}\n                 )->Union[List[LLMResponse],List[LLMFunctionCallResponse],List[LLMClassResponse]]: \n        res,contexts = self.entry.stream_chat_oai(conversations)\n        s = \"\".join(res)        \n        return [LLMResponse(output=s,metadata={},input=\"\")]\n\n    def stream_chat_oai(self,conversations, \n                        model:Optional[str]=None, \n                        role_mapping=None,\n                        delta_mode=False,\n                        llm_config:Dict[str,Any]={}): \n        res,contexts = self.entry.stream_chat_oai(conversations)        \n        for t in res:                        \n            yield (t,SingleOutputMeta(0,0))\n\n    async def async_stream_chat_oai(self,conversations, \n                        model:Optional[str]=None, \n                        role_mapping=None,\n                        delta_mode=False,\n                        llm_config:Dict[str,Any]={}): \n        res,contexts = self.rag.stream_chat_oai(conversations)                \n        for t in res:                                    \n            yield (t,SingleOutputMeta(0,0))\n                         \n\n    def __getattr__(self, name):        \n        return getattr(self.llm, name)\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/openai_serve.py\nimport argparse\nfrom service_server.api_server import ServerArgs, serve\nfrom service_server.llm_wrapper import LLWrapper\nfrom service_server.summary_rag import SummaryRAG\nimport byzerllm\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"OpenAI Serve\")\n    parser.add_argument(\n        \"--model\",\n        default=\"\",\n    )\n    parser.add_argument(\"--emb_model\", default=\"\")\n    parser.add_argument(\n        \"--ray_address\",\n        default=\"auto\",\n    )\n    parser.add_argument(\"--source_dir\", default=\".\", help=\"\")\n    parser.add_argument(\"--host\", default=\"\", help=\"\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"\")\n    parser.add_argument(\"--uvicorn_log_level\", default=\"info\", help=\"\")\n    parser.add_argument(\"--allow_credentials\", action=\"store_true\", help=\"\")\n    parser.add_argument(\"--allowed_origins\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--allowed_methods\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--allowed_headers\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--api_key\", default=\"\", help=\"\")\n    parser.add_argument(\"--served_model_name\", default=\"\", help=\"\")\n    parser.add_argument(\"--prompt_template\", default=\"\", help=\"\")\n    parser.add_argument(\"--ssl_keyfile\", default=\"\", help=\"\")\n    parser.add_argument(\"--ssl_certfile\", default=\"\", help=\"\")\n    parser.add_argument(\"--response_role\", default=\"assistant\", help=\"\")\n    parser.add_argument(\n        \"--collections\", default=\"\", help=\"Collection name for indexing\"\n    )\n    parser.add_argument(\n        \"--base_dir\",\n        default=\"\",\n        help=\"Path where the processed text embeddings were stored\",\n    )\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name or args.model,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile,\n    )\n    byzerllm.connect_cluster(address=args.ray_address)\n    llm = byzerllm.ByzerLLM()\n    if not server_args.served_model_name:\n        raise ValueError(\"served_model_name is required\")\n    llm.setup_default_model_name(server_args.served_model_name)\n    rag = SummaryRAG(llm=llm)    \n    llm_wrapper = LLWrapper(llm=llm, rag=rag)\n    serve(llm=llm_wrapper, args=server_args)\n\nif __name__ == \"__main__\":\n    main()\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/api_server.py\nimport os\nimport time\nimport uvicorn\nfrom fastapi import Request\nfrom fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import JSONResponse, StreamingResponse, Response\n\nfrom byzerllm.log import init_logger\nfrom byzerllm.utils import random_uuid\nfrom byzerllm.version import __version__ as version\nfrom byzerllm.utils.client import ByzerLLM, LLMRequest\nfrom byzerllm.utils.client.entrypoints.openai.serving_chat import OpenAIServingChat\nfrom byzerllm.utils.client.entrypoints.openai.serving_completion import OpenAIServingCompletion\nfrom byzerllm.utils.client.entrypoints.openai.protocol import (\n    ModelList,\n    ModelCard,\n    ModelPermission,\n    ChatCompletionRequest,\n    ErrorResponse,\n    CompletionRequest,\n    Embeddings,\n    EmbeddingsOutput,\n    EmbeddingsData,\n    EmbeddingsUsage,\n)\nfrom pydantic import BaseModel\nfrom typing import List\n\nlogger = init_logger(__name__)\n\nllm_client: ByzerLLM = None\nopenai_serving_chat: OpenAIServingChat = None\nopenai_serving_completion: OpenAIServingCompletion = None\n\nTIMEOUT_KEEP_ALIVE = 5  # seconds\n# timeout in 10 minutes. Streaming can take longer than 3 min\nTIMEOUT = float(os.environ.get(\"BYZERLLM_APISERVER_HTTP_TIMEOUT\", 600))\n\nrouter_app = FastAPI()\n\n\n@router_app.get(\"/health\")\nasync def health() -> Response:\n    \"\"\"Health check.\"\"\"\n    return Response(status_code=200)\n\n\n@router_app.get(\"/v1/models\")\nasync def show_available_models():\n    models = await openai_serving_chat.show_available_models()\n    return JSONResponse(content=models.model_dump())\n\n\n@router_app.get(\"/version\")\nasync def show_version():\n    return JSONResponse(content={\"version\": version})\n\n\n@router_app.get(\"/v1/models\", response_model=ModelList)\nasync def models() -> ModelList:\n    \"\"\"Show available models. Right now we only have one model.\"\"\"\n    model_cards = [\n        ModelCard(\n            id=\"\",\n            root=\"\",\n            permission=[ModelPermission()]\n        )\n    ]\n    return ModelList(data=model_cards)\n\n\n@router_app.post(\"/v1/completions\")\nasync def create_completion(\n        body: CompletionRequest,\n        request: Request\n):\n    generator = await openai_serving_completion.create_completion(body, request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            status_code=generator.code\n        )\n    if body.stream:\n        return StreamingResponse(\n            content=generator,\n            media_type=\"text/event-stream\"\n        )\n    else:\n        return JSONResponse(content=generator.model_dump())\n\n\n@router_app.post(\"/v1/chat/completions\")\nasync def create_chat_completion(\n        body: ChatCompletionRequest,\n        request: Request,\n):\n    \"\"\"Completion API similar to OpenAI's API.\n\n    See  https://platform.openai.com/docs/api-reference/chat/create\n    for the API specification. This API mimics the OpenAI ChatCompletion API.\n\n    NOTE: Currently we do not support the following features:\n        - function_call (Users should implement this by themselves)\n        - logit_bias (to be supported by vLLM engine)\n    \"\"\"\n    # async with async_timeout.timeout(TIMEOUT):\n\n    generator = await openai_serving_chat.create_chat_completion(body, request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            status_code=generator.code\n        )\n    if body.stream:\n        return StreamingResponse(\n            content=generator,\n            media_type=\"text/event-stream\"\n        )\n    else:\n        return JSONResponse(content=generator.model_dump())\n\n\n@router_app.post(\"/v1/embeddings\")\nasync def embed(body: Embeddings):\n    \"\"\"Given a prompt, the model will return one embedding.\n\n    Returns:\n        A response object with an embedding.\n    \"\"\"\n    embedding_id = f\"embed-{random_uuid()}\"\n\n    results_list = llm_client.emb(body.model, request=LLMRequest(instruction=body.input))\n    tokens = 0\n\n    return EmbeddingsOutput(\n        data=[\n            EmbeddingsData(\n                embedding=results.output,\n                index=i,\n                object=\"embedding\",\n            )\n            for i, results in enumerate(results_list)\n        ],\n        id=embedding_id,\n        object=\"list\",\n        created=int(time.time()),\n        model=body.model,\n        usage=EmbeddingsUsage(\n            prompt_tokens=tokens,\n            total_tokens=tokens,\n        ),\n    )\n\nclass ServerArgs(BaseModel):\n    host: str = None\n    port: int = 8000\n    uvicorn_log_level: str = \"info\"\n    allow_credentials: bool = False\n    allowed_origins: List[str] = [\"*\"]  \n    allowed_methods: List[str] = [\"*\"]\n    allowed_headers: List[str] = [\"*\"]\n    api_key: str = None\n    served_model_name: str = None\n    prompt_template: str = None\n    response_role: str = \"assistant\"\n    ssl_keyfile: str = None\n    ssl_certfile: str = None    \n\ndef serve(llm:ByzerLLM, args: ServerArgs):\n    \n    logger.info(f\"ByzerLLM API server version {version}\")\n    logger.info(f\"args: {args}\")\n\n    router_app.add_middleware(\n        CORSMiddleware,\n        allow_origins=args.allowed_origins,\n        allow_credentials=args.allow_credentials,\n        allow_methods=args.allowed_methods,\n        allow_headers=args.allowed_headers,\n    )\n    \n    if token := os.environ.get(\"BYZERLLM_API_KEY\") or args.api_key:\n\n        @router_app.middleware(\"http\")\n        async def authentication(request: Request, call_next):\n            if not request.url.path.startswith(\"/v1\"):\n                return await call_next(request)\n            if request.headers.get(\"Authorization\") != \"Bearer \" + token:\n                return JSONResponse(\n                    content={\"error\": \"Unauthorized\"},\n                    status_code=401\n                )\n            return await call_next(request)\n\n    # Register labels for metrics\n    # add_global_metrics_labels(model_name=engine_args.model)\n    global llm_client\n    llm_client = llm\n    \n    global openai_serving_chat\n    openai_serving_chat = OpenAIServingChat(\n        llm_client=llm_client,\n        response_role=args.response_role,\n        server_model_name=args.served_model_name,\n        prompt_template=args.prompt_template\n    )\n    global openai_serving_completion\n    openai_serving_completion = OpenAIServingCompletion(\n        llm_client=llm_client,\n        server_model_name=args.served_model_name,\n        prompt_template=args.prompt_template\n    )\n\n    uvicorn.run(\n        router_app,\n        host=args.host,\n        port=args.port,\n        log_level=args.uvicorn_log_level,\n        timeout_keep_alive=TIMEOUT_KEEP_ALIVE,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile\n    )\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/generator.py\nfrom typing import Optional, Dict, Any,List\nfrom openai import OpenAI\nimport json\nfrom .types import RAGConfig, ByzerSQLConfig\nfrom byzerllm.utils.client import code_utils\nimport byzerllm\nimport requests\nfrom loguru import logger\nclass SQLGenerator:\n\n    @byzerllm.prompt()\n    def gen_sql(\n        self, conversations: List[Dict[str, str]],context:str\n    ) -> str:\n        \"\"\"        \n        对话内容：\n        {% for msg in conversations %}\n        <{{ msg.role }}>: {{ msg.content }}\n        {% endfor %}\n\n        相关知识：\n        {{ context }}\n\n        根据提供的对话内容以及相关知识，理解用户最后的问题，然后根据看到的schema 信息，生成 Byzer-SQL语句。\n        注意，输出 Byzer-SQL 语句使用 ```sql``` 标记。         \n        \"\"\"\n\n    def __init__(self, rag_config: RAGConfig, byzer_sql_config: ByzerSQLConfig) -> None:\n        self.schema_rag_client = OpenAI(\n            api_key=rag_config.schema_rag_base_url,\n            base_url=rag_config.schema_rag_api_key,\n        )\n        self.context_rag_client = OpenAI(\n            api_key=rag_config.context_rag_api_key,\n            base_url=rag_config.context_rag_base_url,\n        )\n        self.byzer_sql_url = byzer_sql_config.byzer_sql_url\n\n    def stream_chat_oai(\n        self,\n        conversations,\n        model: Optional[str] = None,\n        role_mapping=None,\n        delta_mode=False,\n        llm_config: Dict[str, Any] = {},\n    ):   \n        try :            \n            v = self._stream_chat_oai(\n                conversations=conversations,\n                model=model,\n                role_mapping=role_mapping,\n                delta_mode=delta_mode,\n                llm_config=llm_config,\n            )\n        except Exception as e:\n            logger.error(f\"Error in stream_chat_oai: {e}\")\n            v = [str(e)], []         \n\n        return v    \n\n    def _stream_chat_oai(\n        self,\n        conversations,\n        model: Optional[str] = None,\n        role_mapping=None,\n        delta_mode=False,\n        llm_config: Dict[str, Any] = {},\n    ):\n        logger.info(f\"Starting stream_chat_oai with model: {model}\")\n        logger.debug(f\"Conversations: {conversations}\")\n        logger.debug(f\"LLM Config: {llm_config}\")\n\n        if not model:\n            model = \"deepseek_chat\"\n        logger.info(f\"Using model: {model}\")\n\n        logger.info(\"Fetching context from context RAG client\")\n        context_response = self.context_rag_client.chat.completions.create(\n            messages=conversations,\n            model=model,\n        )\n        logger.debug(f\"Context RAG response: {context_response}\")\n\n        context = context_response.choices[0].message.content\n        logger.info(\"Context retrieved successfully\")\n\n        logger.info(\"Generating SQL prompt\")\n        gen_sql_prompt = self.gen_sql.prompt(conversations=conversations,context=context)\n        logger.debug(f\"Generated SQL prompt: {gen_sql_prompt}\")\n\n        logger.info(\"Fetching SQL from schema RAG client\")\n        response = self.schema_rag_client.chat.completions.create(\n            messages=gen_sql_prompt,\n            model=model,\n        )\n        logger.debug(f\"Schema RAG response: {response}\")\n\n        sql_str = response.choices[0].message.content\n        logger.info(\"SQL string generated\")\n        logger.debug(f\"Generated SQL string: {sql_str}\")\n\n        sql = code_utils.extract_code(sql_str)[0][1]\n        logger.info(\"SQL code extracted\")\n        logger.debug(f\"Extracted SQL: {sql}\")\n\n        logger.info(f\"Sending SQL to Byzer SQL URL: {self.byzer_sql_url}\")\n        response = requests.post(\n            self.byzer_sql_url,\n            params={\"sql\": sql}                \n        )\n        response.raise_for_status()\n        logger.info(\"Byzer SQL response received\")\n\n        result = response.content.decode(\"utf-8\")\n        logger.info(\"SQL execution result decoded\")\n        logger.debug(f\"SQL execution result: {result}\")\n        \n        return [result], []\n\n\n\n##File: /Users/allwefantasy/projects/super_analysis/setup.py\nfrom setuptools import setup, find_packages\n\nsetup(\n    name=\"super-analysis\",\n    version=\"0.1.0\",\n    packages=find_packages(where=\"src\"),\n    package_dir={\"\": \"src\"},\n    install_requires=[\n        \"byzerllm\",\n        \"fastapi\",\n        \"uvicorn\",\n        \"argparse\",\n    ],\n    entry_points={\n        \"console_scripts\": [\n            \"super-analysis.serve=superanalysis.serve.cli:main\",\n            \"super-analysis.convert=superanalysis.serve.convert:main\",\n        ],\n    },\n    author=\"allwefantasy\",\n    author_email=\"allwefantasy@gmail.com\",\n    description=\"A package for super analysis with ByzerLLM\",\n    long_description=open(\"README.md\").read(),\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/yourusername/super-analysis\",\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n    ],\n    python_requires='>=3.6',\n)\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/convert.py\nimport os\nimport argparse\nimport pandas as pd\nimport hashlib\nfrom typing import List, Dict\n\ndef get_csv_files(directory: str) -> List[str]:\n    \"\"\"Recursively get all CSV files in the given directory.\"\"\"\n    csv_files = []\n    for root, _, files in os.walk(directory):\n        for file in files:\n            if file.endswith('.csv'):\n                csv_files.append(os.path.join(root, file))\n    return csv_files\n\ndef extract_schema(df: pd.DataFrame) -> Dict[str, str]:\n    \"\"\"Extract schema from pandas DataFrame.\"\"\"\n    schema = {}\n    for column in df.columns:\n        dtype = str(df[column].dtype)\n        if dtype == 'object':\n            schema[column] = 'STRING'\n        elif dtype.startswith('int'):\n            schema[column] = 'INTEGER'\n        elif dtype.startswith('float'):\n            schema[column] = 'FLOAT'\n        elif dtype == 'bool':\n            schema[column] = 'BOOLEAN'\n        elif dtype.startswith('datetime'):\n            schema[column] = 'TIMESTAMP'\n        else:\n            schema[column] = 'STRING'  # Default to STRING for unknown types\n    return schema\n\ndef generate_schema_text(schema: Dict[str, str], csv_file: str) -> str:\n    \"\"\"Generate schema text from the schema dictionary and include the CSV file name.\"\"\"\n    schema_lines = [f\"CSV File: {csv_file}\\n\"]\n    schema_lines.extend([f\"{field}: {dtype}\" for field, dtype in schema.items()])\n    return \"\\n\".join(schema_lines)\n\ndef save_schema(schema_text: str, csv_file: str, output_dir: str) -> None:\n    \"\"\"Save schema text to a file in the output directory.\"\"\"\n    # Generate MD5 hash of the schema text\n    file_name = hashlib.md5(schema_text.encode()).hexdigest() + '.md'\n    output_path = os.path.join(output_dir, file_name)\n    \n    with open(output_path, 'w') as f:\n        f.write(schema_text)\n    print(f\"Schema for {csv_file} saved to {output_path}\")\n\ndef convert(data_dir: str, doc_dir: str) -> None:\n    \"\"\"Main conversion function.\"\"\"\n    csv_files = get_csv_files(data_dir)\n    \n    for csv_file in csv_files:\n        print(f\"Processing {csv_file}\")\n        df = pd.read_csv(csv_file)\n        schema = extract_schema(df)\n        schema_text = generate_schema_text(schema, csv_file)\n        save_schema(schema_text, csv_file, doc_dir)\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Convert CSV files to schema documents\")\n    parser.add_argument(\"--data_dir\", required=True, help=\"Directory containing CSV files\")\n    parser.add_argument(\"--doc_dir\", required=True, help=\"Output directory for schema documents\")\n    \n    args = parser.parse_args()\n    \n    if not os.path.exists(args.doc_dir):\n        os.makedirs(args.doc_dir)\n    \n    convert(args.data_dir, args.doc_dir)\n\nif __name__ == \"__main__\":\n    main()\n\n     \n\n下面是用户的需求：\n\n在 notebooks 目录生成一个jupyter notebook ,使用 openai sdk 访问 http://127.0.0.1:8000/v1 接口\n"}, {"role": "assistant", "content": "为了满足您的需求，我将在 notebooks 目录下创建一个 Jupyter Notebook，使用 OpenAI SDK 来访问 http://127.0.0.1:8000/v1 接口。以下是创建这个 notebook 的步骤：\n\n1. 首先，我们需要在项目中创建 notebooks 目录（如果还不存在的话）。\n2. 然后，我们将在 notebooks 目录下创建一个新的 Jupyter Notebook 文件。\n3. 在这个 notebook 中，我们将使用 OpenAI SDK 来连接到本地运行的服务。\n\n让我们开始创建所需的文件：\n\n```python\n##File: /Users/allwefantasy/projects/super_analysis/notebooks/openai_local_api.ipynb\n<<<<<<< SEARCH\n=======\n{\n \"cells\": [\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"# Using OpenAI SDK to Access Local API\\n\",\n    \"\\n\",\n    \"This notebook demonstrates how to use the OpenAI SDK to interact with a locally running API at http://127.0.0.1:8000/v1.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Import required libraries\\n\",\n    \"from openai import OpenAI\\n\",\n    \"import os\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Configure the OpenAI client to use the local API\\n\",\n    \"client = OpenAI(\\n\",\n    \"    base_url=\\\"http://127.0.0.1:8000/v1\\\",\\n\",\n    \"    api_key=\\\"your-api-key-here\\\"  # Replace with your actual API key if required\\n\",\n    \")\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Test the connection by listing available models\\n\",\n    \"models = client.models.list()\\n\",\n    \"print(\\\"Available models:\\\")\\n\",\n    \"for model in models:\\n\",\n    \"    print(f\\\"- {model.id}\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Example: Create a chat completion\\n\",\n    \"chat_completion = client.chat.completions.create(\\n\",\n    \"    model=\\\"gpt-3.5-turbo\\\",  # Replace with an available model from your local API\\n\",\n    \"    messages=[\\n\",\n    \"        {\\\"role\\\": \\\"system\\\", \\\"content\\\": \\\"You are a helpful assistant.\\\"},\\n\",\n    \"        {\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"What is the capital of France?\\\"}\\n\",\n    \"    ]\\n\",\n    \")\\n\",\n    \"\\n\",\n    \"print(chat_completion.choices[0].message.content)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"Note: Make sure to replace 'your-api-key-here' with the actual API key if your local service requires authentication. Also, replace 'gpt-3.5-turbo' with a model that is available in your local API.\"\n   ]\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.8.5\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 4\n}\n>>>>>>> REPLACE\n```\n\n这个 Jupyter Notebook 文件 (`openai_local_api.ipynb`) 现在已经被创建在 `/Users/allwefantasy/projects/super_analysis/notebooks/` 目录下。这个 notebook 包含了以下内容：\n\n1. 一个简短的介绍\n2. 导入必要的库\n3. 配置 OpenAI 客户端以使用本地 API\n4. 测试连接并列出可用的模型\n5. 一个使用聊天完成 API 的示例\n\n请注意，您可能需要根据您的本地 API 的具体设置来调整一些细节：\n\n- 如果您的本地 API 需要认证，请将 `\"your-api-key-here\"` 替换为实际的 API 密钥。\n- 在聊天完成的示例中，我使用了 `\"gpt-3.5-turbo\"` 作为模型名称。您可能需要将其替换为您本地 API 中可用的模型名称。\n\n要使用这个 notebook，您需要：\n\n1. 确保您的本地 API 服务器正在运行并监听 `http://127.0.0.1:8000`。\n2. 在您的开发环境中打开这个 notebook。\n3. 运行每个代码单元，观察输出结果。\n\n如果您需要对 notebook 进行任何修改或者有任何其他需求，请随时告诉我。"}], "model": "human", "yaml_file": "actions/119_chat_action.yml"}
{"instruction": "修改  response = requests.post(self.byzer_sql_url, params={\"sql\": sql}) 改成 form data 参数", "conversations": [{"role": "user", "content": "\n如果你需要生成代码，对于每个需要更改的文件,你需要按 *SEARCH/REPLACE block* 的格式进行生成。\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The opening fence and code language, eg: ```python\n2. The file path alone on a line, starting with \"##File:\" and verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n*SEARCH/REPLACE* blocks will replace *all* matching occurrences.\nInclude enough lines to make the SEARCH blocks unique.\n\nInclude *ALL* the code being searched and replaced!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n下面我们来看一个例子：\n\n当前项目目录结构：\n1. 项目根目录： /tmp/projects/mathweb\n2. 项目子目录/文件列表(类似tree 命令输出)\nflask/\n    app.py\n    templates/\n        index.html\n    static/\n        style.css\n\n用户需求： Change get_factorial() to use math.factorial\n\n回答： To make this change we need to modify `/tmp/projects/mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n用户需求： Refactor hello() into its own file.\n\n回答：To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/hello.py\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/main.py\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n现在让我们开始一个新的任务:\n\n当前项目目录结构：\n1. 项目根目录： /Users/allwefantasy/projects/super_analysis\n2. 项目子目录/文件列表(类似tree 命令输出)：\nsetup.py\n.auto-coder/\n    libs/\n        llm_friendly_packages/\n            src/\n                example1/\n                    main.py\nsrc/\n    superanalysis/\n        serve/\n            convert.py\n            openai_serve.py\n            __init__.py\n            llm_wrapper.py\n            api_server.py\n            cli.py\n        sql/\n            __init__.py\n            types.py\n            generator.py\n下面是一些文件路径以及每个文件对应的源码：\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/types.py\nfrom pydantic import BaseModel\n\nclass RAGConfig(BaseModel):\n    schema_rag_base_url: str = \"\"\n    schema_rag_api_key: str = \"xxxx\"\n\n    context_rag_base_url: str = \"\"\n    context_rag_api_key: str = \"xxxx\"\n\n    \n\n    \n\nclass ByzerSQLConfig(BaseModel):\n    byzer_sql_url: str = \"\"\n    data_dir: str = \"\"\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/cli.py\nimport argparse\nimport byzerllm\nfrom byzerllm.utils.client import ByzerLLM\nfrom superanalysis.serve.api_server import serve, ServerArgs\nfrom superanalysis.sql.types import RAGConfig, ByzerSQLConfig\nfrom superanalysis.sql.generator import SQLGenerator\nfrom superanalysis.serve.llm_wrapper import LLWrapper\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"ByzerLLM API Server\")\n    parser.add_argument(\"--ray-address\", type=str, default=\"auto\", help=\"ray address\")\n    parser.add_argument(\"--host\", type=str, default=\"0.0.0.0\", help=\"Host to bind the server to\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"Port to bind the server to\")\n    parser.add_argument(\"--uvicorn-log-level\", type=str, default=\"info\", help=\"Uvicorn log level\")\n    parser.add_argument(\"--allow-credentials\", action=\"store_true\", help=\"Allow credentials\")\n    parser.add_argument(\"--allowed-origins\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed origins\")\n    parser.add_argument(\"--allowed-methods\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed methods\")\n    parser.add_argument(\"--allowed-headers\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed headers\")\n    parser.add_argument(\"--api-key\", type=str, default=\"\", help=\"API key for authentication\")\n    parser.add_argument(\"--served-model-name\", type=str, required=True, help=\"Name of the model to be served\")\n    parser.add_argument(\"--prompt-template\", type=str, default=\"\", help=\"Prompt template\")\n    parser.add_argument(\"--response-role\", type=str, default=\"assistant\", help=\"Response role\")\n    parser.add_argument(\"--ssl-keyfile\", type=str, default=\"\", help=\"SSL key file\")\n    parser.add_argument(\"--ssl-certfile\", type=str, default=\"\", help=\"SSL certificate file\")\n    \n    # Add new arguments for RAGConfig\n    parser.add_argument(\"--schema-rag-base-url\", type=str, default=\"\", help=\"RAG base URL\")\n    parser.add_argument(\"--schema-rag-api-key\", type=str, default=\"xxxx\", help=\"RAG API key\")\n\n    parser.add_argument(\"--context-rag-base-url\", type=str, default=\"\", help=\"RAG base URL\")\n    parser.add_argument(\"--context-rag-api-key\", type=str, default=\"xxxx\", help=\"RAG API key\")\n    \n    # Add new argument for ByzerSQLConfig\n    parser.add_argument(\"--byzer-sql-url\", type=str, default=\"\", help=\"Byzer SQL URL\")\n    parser.add_argument(\"--data-dir\", type=str, default=\"\", help=\"Data directory\")\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile\n    )\n\n    # Create RAGConfig instance\n    rag_config = RAGConfig(\n        schema_rag_base_url=args.schema_rag_base_url,\n        schema_rag_api_key=args.schema_rag_api_key,\n        context_rag_base_url=args.context_rag_base_url,\n        context_rag_api_key=args.context_rag_api_key\n    )\n\n    # Create ByzerSQLConfig instance\n    byzer_sql_config = ByzerSQLConfig(\n        byzer_sql_url=args.byzer_sql_url,\n        data_dir=args.data_dir\n    )\n\n    byzerllm.connect_cluster(address=args.ray_address)\n    llm = ByzerLLM()\n    llm.setup_default_model_name(args.served_model_name)        \n    llm = byzerllm.ByzerLLM()\n    if not server_args.served_model_name:\n        raise ValueError(\"served_model_name is required\")\n    llm.setup_default_model_name(server_args.served_model_name)\n\n    print(\"RAG Config: \", rag_config)\n    rag = SQLGenerator(rag_config=rag_config, byzer_sql_config=byzer_sql_config)    \n    llm_wrapper = LLWrapper(llm=llm, entry=rag)\n    serve(llm_wrapper, server_args)\n\nif __name__ == \"__main__\":\n    main()\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/__init__.py\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/__init__.py\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/llm_wrapper.py\nfrom typing import Any, Dict, List, Optional, Union,Callable\nfrom byzerllm.utils.client.types import (    \n    LLMFunctionCallResponse,\n    LLMClassResponse,LLMResponse    \n)\nimport pydantic\nfrom byzerllm import ByzerLLM\nfrom byzerllm.utils.client import LLMResponse\nfrom byzerllm.utils.types import SingleOutputMeta\nfrom byzerllm.utils.langutil import asyncfy_with_semaphore\nfrom superanalysis.sql.generator import SQLGenerator\nfrom loguru import logger\n\nclass LLWrapper:\n\n    def __init__(self,llm:ByzerLLM,entry:SQLGenerator):\n        self.entry = entry\n        self.llm = llm\n        \n\n    def chat_oai(self,\n                 conversations,\n                 tools:List[Union[Callable,str]]=[], \n                 tool_choice:Optional[Union[Callable,str]]=None,\n                 execute_tool:bool=False,  \n                 impl_func:Optional[Callable]=None,\n                 execute_impl_func:bool=False,\n                 impl_func_params:Optional[Dict[str,Any]]=None,\n                 func_params:Optional[Dict[str,Any]]=None,\n                 response_class:Optional[Union[pydantic.BaseModel,str]] = None, \n                 response_after_chat:Optional[Union[pydantic.BaseModel,str]] = False,\n                 enable_default_sys_message:bool=True,                 \n                 model:Optional[str] = None,\n                 role_mapping=None,llm_config:Dict[str,Any]={}\n                 )->Union[List[LLMResponse],List[LLMFunctionCallResponse],List[LLMClassResponse]]: \n        res,contexts = self.entry.stream_chat_oai(conversations)\n        s = \"\".join(res)        \n        return [LLMResponse(output=s,metadata={},input=\"\")]\n\n    def stream_chat_oai(self,conversations, \n                        model:Optional[str]=None, \n                        role_mapping=None,\n                        delta_mode=False,\n                        llm_config:Dict[str,Any]={}): \n        res,contexts = self.entry.stream_chat_oai(conversations)        \n        for t in res:                        \n            yield (t,SingleOutputMeta(0,0))\n\n    async def async_stream_chat_oai(self,conversations, \n                        model:Optional[str]=None, \n                        role_mapping=None,\n                        delta_mode=False,\n                        llm_config:Dict[str,Any]={}): \n        res,contexts = self.rag.stream_chat_oai(conversations)                \n        for t in res:                                    \n            yield (t,SingleOutputMeta(0,0))\n                         \n\n    def __getattr__(self, name):        \n        return getattr(self.llm, name)\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/openai_serve.py\nimport argparse\nfrom service_server.api_server import ServerArgs, serve\nfrom service_server.llm_wrapper import LLWrapper\nfrom service_server.summary_rag import SummaryRAG\nimport byzerllm\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"OpenAI Serve\")\n    parser.add_argument(\n        \"--model\",\n        default=\"\",\n    )\n    parser.add_argument(\"--emb_model\", default=\"\")\n    parser.add_argument(\n        \"--ray_address\",\n        default=\"auto\",\n    )\n    parser.add_argument(\"--source_dir\", default=\".\", help=\"\")\n    parser.add_argument(\"--host\", default=\"\", help=\"\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"\")\n    parser.add_argument(\"--uvicorn_log_level\", default=\"info\", help=\"\")\n    parser.add_argument(\"--allow_credentials\", action=\"store_true\", help=\"\")\n    parser.add_argument(\"--allowed_origins\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--allowed_methods\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--allowed_headers\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--api_key\", default=\"\", help=\"\")\n    parser.add_argument(\"--served_model_name\", default=\"\", help=\"\")\n    parser.add_argument(\"--prompt_template\", default=\"\", help=\"\")\n    parser.add_argument(\"--ssl_keyfile\", default=\"\", help=\"\")\n    parser.add_argument(\"--ssl_certfile\", default=\"\", help=\"\")\n    parser.add_argument(\"--response_role\", default=\"assistant\", help=\"\")\n    parser.add_argument(\n        \"--collections\", default=\"\", help=\"Collection name for indexing\"\n    )\n    parser.add_argument(\n        \"--base_dir\",\n        default=\"\",\n        help=\"Path where the processed text embeddings were stored\",\n    )\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name or args.model,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile,\n    )\n    byzerllm.connect_cluster(address=args.ray_address)\n    llm = byzerllm.ByzerLLM()\n    if not server_args.served_model_name:\n        raise ValueError(\"served_model_name is required\")\n    llm.setup_default_model_name(server_args.served_model_name)\n    rag = SummaryRAG(llm=llm)    \n    llm_wrapper = LLWrapper(llm=llm, rag=rag)\n    serve(llm=llm_wrapper, args=server_args)\n\nif __name__ == \"__main__\":\n    main()\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/api_server.py\nimport os\nimport time\nimport uvicorn\nfrom fastapi import Request\nfrom fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import JSONResponse, StreamingResponse, Response\n\nfrom byzerllm.log import init_logger\nfrom byzerllm.utils import random_uuid\nfrom byzerllm.version import __version__ as version\nfrom byzerllm.utils.client import ByzerLLM, LLMRequest\nfrom byzerllm.utils.client.entrypoints.openai.serving_chat import OpenAIServingChat\nfrom byzerllm.utils.client.entrypoints.openai.serving_completion import OpenAIServingCompletion\nfrom byzerllm.utils.client.entrypoints.openai.protocol import (\n    ModelList,\n    ModelCard,\n    ModelPermission,\n    ChatCompletionRequest,\n    ErrorResponse,\n    CompletionRequest,\n    Embeddings,\n    EmbeddingsOutput,\n    EmbeddingsData,\n    EmbeddingsUsage,\n)\nfrom pydantic import BaseModel\nfrom typing import List\n\nlogger = init_logger(__name__)\n\nllm_client: ByzerLLM = None\nopenai_serving_chat: OpenAIServingChat = None\nopenai_serving_completion: OpenAIServingCompletion = None\n\nTIMEOUT_KEEP_ALIVE = 5  # seconds\n# timeout in 10 minutes. Streaming can take longer than 3 min\nTIMEOUT = float(os.environ.get(\"BYZERLLM_APISERVER_HTTP_TIMEOUT\", 600))\n\nrouter_app = FastAPI()\n\n\n@router_app.get(\"/health\")\nasync def health() -> Response:\n    \"\"\"Health check.\"\"\"\n    return Response(status_code=200)\n\n\n@router_app.get(\"/v1/models\")\nasync def show_available_models():\n    models = await openai_serving_chat.show_available_models()\n    return JSONResponse(content=models.model_dump())\n\n\n@router_app.get(\"/version\")\nasync def show_version():\n    return JSONResponse(content={\"version\": version})\n\n\n@router_app.get(\"/v1/models\", response_model=ModelList)\nasync def models() -> ModelList:\n    \"\"\"Show available models. Right now we only have one model.\"\"\"\n    model_cards = [\n        ModelCard(\n            id=\"\",\n            root=\"\",\n            permission=[ModelPermission()]\n        )\n    ]\n    return ModelList(data=model_cards)\n\n\n@router_app.post(\"/v1/completions\")\nasync def create_completion(\n        body: CompletionRequest,\n        request: Request\n):\n    generator = await openai_serving_completion.create_completion(body, request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            status_code=generator.code\n        )\n    if body.stream:\n        return StreamingResponse(\n            content=generator,\n            media_type=\"text/event-stream\"\n        )\n    else:\n        return JSONResponse(content=generator.model_dump())\n\n\n@router_app.post(\"/v1/chat/completions\")\nasync def create_chat_completion(\n        body: ChatCompletionRequest,\n        request: Request,\n):\n    \"\"\"Completion API similar to OpenAI's API.\n\n    See  https://platform.openai.com/docs/api-reference/chat/create\n    for the API specification. This API mimics the OpenAI ChatCompletion API.\n\n    NOTE: Currently we do not support the following features:\n        - function_call (Users should implement this by themselves)\n        - logit_bias (to be supported by vLLM engine)\n    \"\"\"\n    # async with async_timeout.timeout(TIMEOUT):\n\n    generator = await openai_serving_chat.create_chat_completion(body, request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            status_code=generator.code\n        )\n    if body.stream:\n        return StreamingResponse(\n            content=generator,\n            media_type=\"text/event-stream\"\n        )\n    else:\n        return JSONResponse(content=generator.model_dump())\n\n\n@router_app.post(\"/v1/embeddings\")\nasync def embed(body: Embeddings):\n    \"\"\"Given a prompt, the model will return one embedding.\n\n    Returns:\n        A response object with an embedding.\n    \"\"\"\n    embedding_id = f\"embed-{random_uuid()}\"\n\n    results_list = llm_client.emb(body.model, request=LLMRequest(instruction=body.input))\n    tokens = 0\n\n    return EmbeddingsOutput(\n        data=[\n            EmbeddingsData(\n                embedding=results.output,\n                index=i,\n                object=\"embedding\",\n            )\n            for i, results in enumerate(results_list)\n        ],\n        id=embedding_id,\n        object=\"list\",\n        created=int(time.time()),\n        model=body.model,\n        usage=EmbeddingsUsage(\n            prompt_tokens=tokens,\n            total_tokens=tokens,\n        ),\n    )\n\nclass ServerArgs(BaseModel):\n    host: str = None\n    port: int = 8000\n    uvicorn_log_level: str = \"info\"\n    allow_credentials: bool = False\n    allowed_origins: List[str] = [\"*\"]  \n    allowed_methods: List[str] = [\"*\"]\n    allowed_headers: List[str] = [\"*\"]\n    api_key: str = None\n    served_model_name: str = None\n    prompt_template: str = None\n    response_role: str = \"assistant\"\n    ssl_keyfile: str = None\n    ssl_certfile: str = None    \n\ndef serve(llm:ByzerLLM, args: ServerArgs):\n    \n    logger.info(f\"ByzerLLM API server version {version}\")\n    logger.info(f\"args: {args}\")\n\n    router_app.add_middleware(\n        CORSMiddleware,\n        allow_origins=args.allowed_origins,\n        allow_credentials=args.allow_credentials,\n        allow_methods=args.allowed_methods,\n        allow_headers=args.allowed_headers,\n    )\n    \n    if token := os.environ.get(\"BYZERLLM_API_KEY\") or args.api_key:\n\n        @router_app.middleware(\"http\")\n        async def authentication(request: Request, call_next):\n            if not request.url.path.startswith(\"/v1\"):\n                return await call_next(request)\n            if request.headers.get(\"Authorization\") != \"Bearer \" + token:\n                return JSONResponse(\n                    content={\"error\": \"Unauthorized\"},\n                    status_code=401\n                )\n            return await call_next(request)\n\n    # Register labels for metrics\n    # add_global_metrics_labels(model_name=engine_args.model)\n    global llm_client\n    llm_client = llm\n    \n    global openai_serving_chat\n    openai_serving_chat = OpenAIServingChat(\n        llm_client=llm_client,\n        response_role=args.response_role,\n        server_model_name=args.served_model_name,\n        prompt_template=args.prompt_template\n    )\n    global openai_serving_completion\n    openai_serving_completion = OpenAIServingCompletion(\n        llm_client=llm_client,\n        server_model_name=args.served_model_name,\n        prompt_template=args.prompt_template\n    )\n\n    uvicorn.run(\n        router_app,\n        host=args.host,\n        port=args.port,\n        log_level=args.uvicorn_log_level,\n        timeout_keep_alive=TIMEOUT_KEEP_ALIVE,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile\n    )\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/generator.py\nfrom typing import Optional, Dict, Any, List\nfrom openai import OpenAI\nimport json\nfrom .types import RAGConfig, ByzerSQLConfig\nfrom byzerllm.utils.client import code_utils\nimport byzerllm\nimport requests\nfrom loguru import logger\nimport traceback\nclass SQLGenerator:\n\n    @byzerllm.prompt()\n    def gen_context(self, conversations: List[Dict[str, str]]) -> str:\n        \"\"\"\n        对话内容：\n        {% for msg in conversations %}\n        <{{ msg.role }}>: {{ msg.content }}\n        {% endfor %}\n\n        根据提供的对话内容，理解用户最后的问题，然后提取可能会用到的 Byzer-SQL 相关的知识。\n        \"\"\"\n\n    @byzerllm.prompt()\n    def gen_sql(self, conversations: List[Dict[str, str]],context:str) -> str:\n        \"\"\"\n        对话内容：\n        {% for msg in conversations %}\n        <{{ msg.role }}>: {{ msg.content }}\n        {% endfor %}\n\n        相关知识：\n        {{ context }}\n\n        根据提供的对话内容以及相关知识，理解用户最后的问题，然后根据看到的schema 信息，生成 Byzer-SQL语句。\n        注意，输出 Byzer-SQL 语句使用 ```sql``` 标记。\n        \"\"\"\n\n    def __init__(self, rag_config: RAGConfig, byzer_sql_config: ByzerSQLConfig) -> None:        \n        self.schema_rag_client = OpenAI(\n            api_key=rag_config.schema_rag_api_key,\n            base_url=rag_config.schema_rag_base_url,\n        )\n        self.context_rag_client = OpenAI(\n            api_key=rag_config.context_rag_api_key,\n            base_url=rag_config.context_rag_base_url,\n        )\n        self.byzer_sql_url = byzer_sql_config.byzer_sql_url\n\n    def stream_chat_oai(\n        self,\n        conversations,\n        model: Optional[str] = None,\n        role_mapping=None,\n        delta_mode=False,\n        llm_config: Dict[str, Any] = {},\n    ):\n        try:\n            v = self._stream_chat_oai(\n                conversations=conversations,\n                model=model,\n                role_mapping=role_mapping,\n                delta_mode=delta_mode,\n                llm_config=llm_config,\n            )\n        except Exception as e:\n            error_msg = f\"Error in stream_chat_oai: {e}\\n{traceback.format_exc()}\"\n            logger.error(error_msg)\n            v = [str(e)], []\n\n        return v\n\n    def _stream_chat_oai(\n        self,\n        conversations,\n        model: Optional[str] = None,\n        role_mapping=None,\n        delta_mode=False,\n        llm_config: Dict[str, Any] = {},\n    ):\n        logger.info(f\"Starting stream_chat_oai with model: {model}\")\n        logger.debug(f\"Conversations: {conversations}\")\n        logger.debug(f\"LLM Config: {llm_config}\")\n\n        if not model:\n            model = \"deepseek_chat\"\n        logger.info(f\"Using model: {model}\")\n\n        query = conversations[-1][\"content\"]\n\n        if (\n            \"使用四到五个字直接返回这句话的简要主题，不要解释、不要标点、不要语气词、不要多余文本，不要加粗，如果没有主题\"\n            in query\n            or \"简要总结一下对话内容，用作后续的上下文提示 prompt，控制在 200 字以内\"\n            in query\n        ):\n            chunks = self.llm.stream_chat_oai(\n                conversations=conversations,\n                model=model,\n                role_mapping=role_mapping,\n                llm_config=llm_config,\n                delta_mode=True,\n            )\n            return (chunk[0] for chunk in chunks), []\n\n        logger.info(\"Fetching context from context RAG client\")\n\n        context_query = self.gen_context.prompt(conversations=conversations)\n        context_response = self.context_rag_client.chat.completions.create(\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": json.dumps(\n                        {\"query\": context_query, \"only_contexts\": True},\n                        ensure_ascii=False,\n                    ),\n                }\n            ],\n            model=model,\n        )\n        \n        context = context_response.choices[0].message.content\n        new_context = \"\"\n        for msg in context.split(\"\\n\"):\n            if msg.strip():\n                v = json.loads(msg)\n                new_context += v[\"source_code\"]\n        logger.info(f\"context {new_context[0:100]}...\")\n        logger.info(\"Context retrieved successfully\")\n\n        logger.info(\"Generating SQL prompt\")\n        gen_sql_prompt = self.gen_sql.prompt(\n            conversations=conversations, context=new_context\n        )        \n\n        logger.info(\"Fetching SQL from schema RAG client\")\n        response = self.schema_rag_client.chat.completions.create(\n            messages=[{\"role\": \"user\", \"content\": gen_sql_prompt}],\n            model=model,\n        )\n        logger.debug(f\"Schema RAG response: {response}\")\n\n        sql_str = response.choices[0].message.content\n        logger.info(\"SQL string generated\")\n        logger.debug(f\"Generated SQL string: {sql_str}\")\n\n        sql = code_utils.extract_code(sql_str)[0][1]\n        logger.info(\"SQL code extracted\")\n        logger.debug(f\"Extracted SQL: {sql}\")\n\n        logger.info(f\"Sending SQL to Byzer SQL URL: {self.byzer_sql_url}\")\n        response = requests.post(self.byzer_sql_url, params={\"sql\": sql})\n        response.raise_for_status()\n        logger.info(\"Byzer SQL response received\")\n\n        result = response.content.decode(\"utf-8\")\n        logger.info(\"SQL execution result decoded\")\n        logger.debug(f\"SQL execution result: {result}\")\n\n        return [result], []\n\n\n##File: /Users/allwefantasy/projects/super_analysis/setup.py\nfrom setuptools import setup, find_packages\n\nsetup(\n    name=\"super-analysis\",\n    version=\"0.1.0\",\n    packages=find_packages(where=\"src\"),\n    package_dir={\"\": \"src\"},\n    install_requires=[\n        \"byzerllm\",\n        \"fastapi\",\n        \"uvicorn\",\n        \"argparse\",\n    ],\n    entry_points={\n        \"console_scripts\": [\n            \"super-analysis.serve=superanalysis.serve.cli:main\",\n            \"super-analysis.convert=superanalysis.serve.convert:main\",\n        ],\n    },\n    author=\"allwefantasy\",\n    author_email=\"allwefantasy@gmail.com\",\n    description=\"A package for super analysis with ByzerLLM\",\n    long_description=open(\"README.md\").read(),\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/yourusername/super-analysis\",\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n    ],\n    python_requires='>=3.6',\n)\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/convert.py\nimport os\nimport argparse\nimport pandas as pd\nimport hashlib\nfrom typing import List, Dict\n\ndef get_csv_files(directory: str) -> List[str]:\n    \"\"\"Recursively get all CSV files in the given directory.\"\"\"\n    csv_files = []\n    for root, _, files in os.walk(directory):\n        for file in files:\n            if file.endswith('.csv'):\n                csv_files.append(os.path.join(root, file))\n    return csv_files\n\ndef extract_schema(df: pd.DataFrame) -> Dict[str, str]:\n    \"\"\"Extract schema from pandas DataFrame.\"\"\"\n    schema = {}\n    for column in df.columns:\n        dtype = str(df[column].dtype)\n        if dtype == 'object':\n            schema[column] = 'STRING'\n        elif dtype.startswith('int'):\n            schema[column] = 'INTEGER'\n        elif dtype.startswith('float'):\n            schema[column] = 'FLOAT'\n        elif dtype == 'bool':\n            schema[column] = 'BOOLEAN'\n        elif dtype.startswith('datetime'):\n            schema[column] = 'TIMESTAMP'\n        else:\n            schema[column] = 'STRING'  # Default to STRING for unknown types\n    return schema\n\ndef generate_schema_text(schema: Dict[str, str], csv_file: str) -> str:\n    \"\"\"Generate schema text from the schema dictionary and include the CSV file name.\"\"\"\n    schema_lines = [f\"CSV File: {csv_file}\\n\"]\n    schema_lines.extend([f\"{field}: {dtype}\" for field, dtype in schema.items()])\n    return \"\\n\".join(schema_lines)\n\ndef save_schema(schema_text: str, csv_file: str, output_dir: str) -> None:\n    \"\"\"Save schema text to a file in the output directory.\"\"\"\n    # Generate MD5 hash of the schema text\n    file_name = hashlib.md5(schema_text.encode()).hexdigest() + '.md'\n    output_path = os.path.join(output_dir, file_name)\n    \n    with open(output_path, 'w') as f:\n        f.write(schema_text)\n    print(f\"Schema for {csv_file} saved to {output_path}\")\n\ndef convert(data_dir: str, doc_dir: str) -> None:\n    \"\"\"Main conversion function.\"\"\"\n    csv_files = get_csv_files(data_dir)\n    \n    for csv_file in csv_files:\n        print(f\"Processing {csv_file}\")\n        df = pd.read_csv(csv_file)\n        schema = extract_schema(df)\n        schema_text = generate_schema_text(schema, csv_file)\n        save_schema(schema_text, csv_file, doc_dir)\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Convert CSV files to schema documents\")\n    parser.add_argument(\"--data_dir\", required=True, help=\"Directory containing CSV files\")\n    parser.add_argument(\"--doc_dir\", required=True, help=\"Output directory for schema documents\")\n    \n    args = parser.parse_args()\n    \n    if not os.path.exists(args.doc_dir):\n        os.makedirs(args.doc_dir)\n    \n    convert(args.data_dir, args.doc_dir)\n\nif __name__ == \"__main__\":\n    main()\n\n     \n\n下面是用户的需求：\n\n修改  response = requests.post(self.byzer_sql_url, params={\"sql\": sql}) 改成 form data 参数\n"}, {"role": "assistant", "content": "To modify the request to use form data instead of query parameters, we need to update the `_stream_chat_oai` method in the `SQLGenerator` class. Here's the *SEARCH/REPLACE* block to make this change:\n\n```python\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/generator.py\n<<<<<<< SEARCH\n        logger.info(f\"Sending SQL to Byzer SQL URL: {self.byzer_sql_url}\")\n        response = requests.post(self.byzer_sql_url, params={\"sql\": sql})\n        response.raise_for_status()\n        logger.info(\"Byzer SQL response received\")\n=======\n        logger.info(f\"Sending SQL to Byzer SQL URL: {self.byzer_sql_url}\")\n        response = requests.post(self.byzer_sql_url, data={\"sql\": sql})\n        response.raise_for_status()\n        logger.info(\"Byzer SQL response received\")\n>>>>>>> REPLACE\n```\n\nThis change replaces the `params` argument with `data` in the `requests.post()` call. When using `data`, the parameters will be sent as form data in the request body instead of as query parameters in the URL.\n\nWould you like me to explain this change or provide any additional information?"}], "model": "human", "yaml_file": "actions/121_chat_action.yml"}
{"instruction": "response = requests.post(self.byzer_sql_url, data={\"sql\": sql}) 相关的代码要更友好,如果请求失败,打印详细的服务端日志", "conversations": [{"role": "user", "content": "\n如果你需要生成代码，对于每个需要更改的文件,你需要按 *SEARCH/REPLACE block* 的格式进行生成。\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The opening fence and code language, eg: ```python\n2. The file path alone on a line, starting with \"##File:\" and verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n*SEARCH/REPLACE* blocks will replace *all* matching occurrences.\nInclude enough lines to make the SEARCH blocks unique.\n\nInclude *ALL* the code being searched and replaced!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n下面我们来看一个例子：\n\n当前项目目录结构：\n1. 项目根目录： /tmp/projects/mathweb\n2. 项目子目录/文件列表(类似tree 命令输出)\nflask/\n    app.py\n    templates/\n        index.html\n    static/\n        style.css\n\n用户需求： Change get_factorial() to use math.factorial\n\n回答： To make this change we need to modify `/tmp/projects/mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n用户需求： Refactor hello() into its own file.\n\n回答：To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/hello.py\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/main.py\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n现在让我们开始一个新的任务:\n\n当前项目目录结构：\n1. 项目根目录： /Users/allwefantasy/projects/super_analysis\n2. 项目子目录/文件列表(类似tree 命令输出)：\nsetup.py\n.auto-coder/\n    libs/\n        llm_friendly_packages/\n            src/\n                example1/\n                    main.py\nsrc/\n    superanalysis/\n        serve/\n            convert.py\n            openai_serve.py\n            __init__.py\n            llm_wrapper.py\n            api_server.py\n            cli.py\n        sql/\n            __init__.py\n            types.py\n            generator.py\n下面是一些文件路径以及每个文件对应的源码：\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/types.py\nfrom pydantic import BaseModel\n\nclass RAGConfig(BaseModel):\n    schema_rag_base_url: str = \"\"\n    schema_rag_api_key: str = \"xxxx\"\n\n    context_rag_base_url: str = \"\"\n    context_rag_api_key: str = \"xxxx\"\n\n    \n\n    \n\nclass ByzerSQLConfig(BaseModel):\n    byzer_sql_url: str = \"\"\n    data_dir: str = \"\"\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/cli.py\nimport argparse\nimport byzerllm\nfrom byzerllm.utils.client import ByzerLLM\nfrom superanalysis.serve.api_server import serve, ServerArgs\nfrom superanalysis.sql.types import RAGConfig, ByzerSQLConfig\nfrom superanalysis.sql.generator import SQLGenerator\nfrom superanalysis.serve.llm_wrapper import LLWrapper\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"ByzerLLM API Server\")\n    parser.add_argument(\"--ray-address\", type=str, default=\"auto\", help=\"ray address\")\n    parser.add_argument(\"--host\", type=str, default=\"0.0.0.0\", help=\"Host to bind the server to\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"Port to bind the server to\")\n    parser.add_argument(\"--uvicorn-log-level\", type=str, default=\"info\", help=\"Uvicorn log level\")\n    parser.add_argument(\"--allow-credentials\", action=\"store_true\", help=\"Allow credentials\")\n    parser.add_argument(\"--allowed-origins\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed origins\")\n    parser.add_argument(\"--allowed-methods\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed methods\")\n    parser.add_argument(\"--allowed-headers\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed headers\")\n    parser.add_argument(\"--api-key\", type=str, default=\"\", help=\"API key for authentication\")\n    parser.add_argument(\"--served-model-name\", type=str, required=True, help=\"Name of the model to be served\")\n    parser.add_argument(\"--prompt-template\", type=str, default=\"\", help=\"Prompt template\")\n    parser.add_argument(\"--response-role\", type=str, default=\"assistant\", help=\"Response role\")\n    parser.add_argument(\"--ssl-keyfile\", type=str, default=\"\", help=\"SSL key file\")\n    parser.add_argument(\"--ssl-certfile\", type=str, default=\"\", help=\"SSL certificate file\")\n    \n    # Add new arguments for RAGConfig\n    parser.add_argument(\"--schema-rag-base-url\", type=str, default=\"\", help=\"RAG base URL\")\n    parser.add_argument(\"--schema-rag-api-key\", type=str, default=\"xxxx\", help=\"RAG API key\")\n\n    parser.add_argument(\"--context-rag-base-url\", type=str, default=\"\", help=\"RAG base URL\")\n    parser.add_argument(\"--context-rag-api-key\", type=str, default=\"xxxx\", help=\"RAG API key\")\n    \n    # Add new argument for ByzerSQLConfig\n    parser.add_argument(\"--byzer-sql-url\", type=str, default=\"\", help=\"Byzer SQL URL\")\n    parser.add_argument(\"--data-dir\", type=str, default=\"\", help=\"Data directory\")\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile\n    )\n\n    # Create RAGConfig instance\n    rag_config = RAGConfig(\n        schema_rag_base_url=args.schema_rag_base_url,\n        schema_rag_api_key=args.schema_rag_api_key,\n        context_rag_base_url=args.context_rag_base_url,\n        context_rag_api_key=args.context_rag_api_key\n    )\n\n    # Create ByzerSQLConfig instance\n    byzer_sql_config = ByzerSQLConfig(\n        byzer_sql_url=args.byzer_sql_url,\n        data_dir=args.data_dir\n    )\n\n    byzerllm.connect_cluster(address=args.ray_address)\n    llm = ByzerLLM()\n    llm.setup_default_model_name(args.served_model_name)        \n    llm = byzerllm.ByzerLLM()\n    if not server_args.served_model_name:\n        raise ValueError(\"served_model_name is required\")\n    llm.setup_default_model_name(server_args.served_model_name)\n\n    print(\"RAG Config: \", rag_config)\n    rag = SQLGenerator(rag_config=rag_config, byzer_sql_config=byzer_sql_config)    \n    llm_wrapper = LLWrapper(llm=llm, entry=rag)\n    serve(llm_wrapper, server_args)\n\nif __name__ == \"__main__\":\n    main()\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/__init__.py\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/__init__.py\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/llm_wrapper.py\nfrom typing import Any, Dict, List, Optional, Union,Callable\nfrom byzerllm.utils.client.types import (    \n    LLMFunctionCallResponse,\n    LLMClassResponse,LLMResponse    \n)\nimport pydantic\nfrom byzerllm import ByzerLLM\nfrom byzerllm.utils.client import LLMResponse\nfrom byzerllm.utils.types import SingleOutputMeta\nfrom byzerllm.utils.langutil import asyncfy_with_semaphore\nfrom superanalysis.sql.generator import SQLGenerator\nfrom loguru import logger\n\nclass LLWrapper:\n\n    def __init__(self,llm:ByzerLLM,entry:SQLGenerator):\n        self.entry = entry\n        self.llm = llm\n        \n\n    def chat_oai(self,\n                 conversations,\n                 tools:List[Union[Callable,str]]=[], \n                 tool_choice:Optional[Union[Callable,str]]=None,\n                 execute_tool:bool=False,  \n                 impl_func:Optional[Callable]=None,\n                 execute_impl_func:bool=False,\n                 impl_func_params:Optional[Dict[str,Any]]=None,\n                 func_params:Optional[Dict[str,Any]]=None,\n                 response_class:Optional[Union[pydantic.BaseModel,str]] = None, \n                 response_after_chat:Optional[Union[pydantic.BaseModel,str]] = False,\n                 enable_default_sys_message:bool=True,                 \n                 model:Optional[str] = None,\n                 role_mapping=None,llm_config:Dict[str,Any]={}\n                 )->Union[List[LLMResponse],List[LLMFunctionCallResponse],List[LLMClassResponse]]: \n        res,contexts = self.entry.stream_chat_oai(conversations)\n        s = \"\".join(res)        \n        return [LLMResponse(output=s,metadata={},input=\"\")]\n\n    def stream_chat_oai(self,conversations, \n                        model:Optional[str]=None, \n                        role_mapping=None,\n                        delta_mode=False,\n                        llm_config:Dict[str,Any]={}): \n        res,contexts = self.entry.stream_chat_oai(conversations)        \n        for t in res:                        \n            yield (t,SingleOutputMeta(0,0))\n\n    async def async_stream_chat_oai(self,conversations, \n                        model:Optional[str]=None, \n                        role_mapping=None,\n                        delta_mode=False,\n                        llm_config:Dict[str,Any]={}): \n        res,contexts = self.rag.stream_chat_oai(conversations)                \n        for t in res:                                    \n            yield (t,SingleOutputMeta(0,0))\n                         \n\n    def __getattr__(self, name):        \n        return getattr(self.llm, name)\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/openai_serve.py\nimport argparse\nfrom service_server.api_server import ServerArgs, serve\nfrom service_server.llm_wrapper import LLWrapper\nfrom service_server.summary_rag import SummaryRAG\nimport byzerllm\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"OpenAI Serve\")\n    parser.add_argument(\n        \"--model\",\n        default=\"\",\n    )\n    parser.add_argument(\"--emb_model\", default=\"\")\n    parser.add_argument(\n        \"--ray_address\",\n        default=\"auto\",\n    )\n    parser.add_argument(\"--source_dir\", default=\".\", help=\"\")\n    parser.add_argument(\"--host\", default=\"\", help=\"\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"\")\n    parser.add_argument(\"--uvicorn_log_level\", default=\"info\", help=\"\")\n    parser.add_argument(\"--allow_credentials\", action=\"store_true\", help=\"\")\n    parser.add_argument(\"--allowed_origins\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--allowed_methods\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--allowed_headers\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--api_key\", default=\"\", help=\"\")\n    parser.add_argument(\"--served_model_name\", default=\"\", help=\"\")\n    parser.add_argument(\"--prompt_template\", default=\"\", help=\"\")\n    parser.add_argument(\"--ssl_keyfile\", default=\"\", help=\"\")\n    parser.add_argument(\"--ssl_certfile\", default=\"\", help=\"\")\n    parser.add_argument(\"--response_role\", default=\"assistant\", help=\"\")\n    parser.add_argument(\n        \"--collections\", default=\"\", help=\"Collection name for indexing\"\n    )\n    parser.add_argument(\n        \"--base_dir\",\n        default=\"\",\n        help=\"Path where the processed text embeddings were stored\",\n    )\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name or args.model,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile,\n    )\n    byzerllm.connect_cluster(address=args.ray_address)\n    llm = byzerllm.ByzerLLM()\n    if not server_args.served_model_name:\n        raise ValueError(\"served_model_name is required\")\n    llm.setup_default_model_name(server_args.served_model_name)\n    rag = SummaryRAG(llm=llm)    \n    llm_wrapper = LLWrapper(llm=llm, rag=rag)\n    serve(llm=llm_wrapper, args=server_args)\n\nif __name__ == \"__main__\":\n    main()\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/api_server.py\nimport os\nimport time\nimport uvicorn\nfrom fastapi import Request\nfrom fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import JSONResponse, StreamingResponse, Response\n\nfrom byzerllm.log import init_logger\nfrom byzerllm.utils import random_uuid\nfrom byzerllm.version import __version__ as version\nfrom byzerllm.utils.client import ByzerLLM, LLMRequest\nfrom byzerllm.utils.client.entrypoints.openai.serving_chat import OpenAIServingChat\nfrom byzerllm.utils.client.entrypoints.openai.serving_completion import OpenAIServingCompletion\nfrom byzerllm.utils.client.entrypoints.openai.protocol import (\n    ModelList,\n    ModelCard,\n    ModelPermission,\n    ChatCompletionRequest,\n    ErrorResponse,\n    CompletionRequest,\n    Embeddings,\n    EmbeddingsOutput,\n    EmbeddingsData,\n    EmbeddingsUsage,\n)\nfrom pydantic import BaseModel\nfrom typing import List\n\nlogger = init_logger(__name__)\n\nllm_client: ByzerLLM = None\nopenai_serving_chat: OpenAIServingChat = None\nopenai_serving_completion: OpenAIServingCompletion = None\n\nTIMEOUT_KEEP_ALIVE = 5  # seconds\n# timeout in 10 minutes. Streaming can take longer than 3 min\nTIMEOUT = float(os.environ.get(\"BYZERLLM_APISERVER_HTTP_TIMEOUT\", 600))\n\nrouter_app = FastAPI()\n\n\n@router_app.get(\"/health\")\nasync def health() -> Response:\n    \"\"\"Health check.\"\"\"\n    return Response(status_code=200)\n\n\n@router_app.get(\"/v1/models\")\nasync def show_available_models():\n    models = await openai_serving_chat.show_available_models()\n    return JSONResponse(content=models.model_dump())\n\n\n@router_app.get(\"/version\")\nasync def show_version():\n    return JSONResponse(content={\"version\": version})\n\n\n@router_app.get(\"/v1/models\", response_model=ModelList)\nasync def models() -> ModelList:\n    \"\"\"Show available models. Right now we only have one model.\"\"\"\n    model_cards = [\n        ModelCard(\n            id=\"\",\n            root=\"\",\n            permission=[ModelPermission()]\n        )\n    ]\n    return ModelList(data=model_cards)\n\n\n@router_app.post(\"/v1/completions\")\nasync def create_completion(\n        body: CompletionRequest,\n        request: Request\n):\n    generator = await openai_serving_completion.create_completion(body, request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            status_code=generator.code\n        )\n    if body.stream:\n        return StreamingResponse(\n            content=generator,\n            media_type=\"text/event-stream\"\n        )\n    else:\n        return JSONResponse(content=generator.model_dump())\n\n\n@router_app.post(\"/v1/chat/completions\")\nasync def create_chat_completion(\n        body: ChatCompletionRequest,\n        request: Request,\n):\n    \"\"\"Completion API similar to OpenAI's API.\n\n    See  https://platform.openai.com/docs/api-reference/chat/create\n    for the API specification. This API mimics the OpenAI ChatCompletion API.\n\n    NOTE: Currently we do not support the following features:\n        - function_call (Users should implement this by themselves)\n        - logit_bias (to be supported by vLLM engine)\n    \"\"\"\n    # async with async_timeout.timeout(TIMEOUT):\n\n    generator = await openai_serving_chat.create_chat_completion(body, request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            status_code=generator.code\n        )\n    if body.stream:\n        return StreamingResponse(\n            content=generator,\n            media_type=\"text/event-stream\"\n        )\n    else:\n        return JSONResponse(content=generator.model_dump())\n\n\n@router_app.post(\"/v1/embeddings\")\nasync def embed(body: Embeddings):\n    \"\"\"Given a prompt, the model will return one embedding.\n\n    Returns:\n        A response object with an embedding.\n    \"\"\"\n    embedding_id = f\"embed-{random_uuid()}\"\n\n    results_list = llm_client.emb(body.model, request=LLMRequest(instruction=body.input))\n    tokens = 0\n\n    return EmbeddingsOutput(\n        data=[\n            EmbeddingsData(\n                embedding=results.output,\n                index=i,\n                object=\"embedding\",\n            )\n            for i, results in enumerate(results_list)\n        ],\n        id=embedding_id,\n        object=\"list\",\n        created=int(time.time()),\n        model=body.model,\n        usage=EmbeddingsUsage(\n            prompt_tokens=tokens,\n            total_tokens=tokens,\n        ),\n    )\n\nclass ServerArgs(BaseModel):\n    host: str = None\n    port: int = 8000\n    uvicorn_log_level: str = \"info\"\n    allow_credentials: bool = False\n    allowed_origins: List[str] = [\"*\"]  \n    allowed_methods: List[str] = [\"*\"]\n    allowed_headers: List[str] = [\"*\"]\n    api_key: str = None\n    served_model_name: str = None\n    prompt_template: str = None\n    response_role: str = \"assistant\"\n    ssl_keyfile: str = None\n    ssl_certfile: str = None    \n\ndef serve(llm:ByzerLLM, args: ServerArgs):\n    \n    logger.info(f\"ByzerLLM API server version {version}\")\n    logger.info(f\"args: {args}\")\n\n    router_app.add_middleware(\n        CORSMiddleware,\n        allow_origins=args.allowed_origins,\n        allow_credentials=args.allow_credentials,\n        allow_methods=args.allowed_methods,\n        allow_headers=args.allowed_headers,\n    )\n    \n    if token := os.environ.get(\"BYZERLLM_API_KEY\") or args.api_key:\n\n        @router_app.middleware(\"http\")\n        async def authentication(request: Request, call_next):\n            if not request.url.path.startswith(\"/v1\"):\n                return await call_next(request)\n            if request.headers.get(\"Authorization\") != \"Bearer \" + token:\n                return JSONResponse(\n                    content={\"error\": \"Unauthorized\"},\n                    status_code=401\n                )\n            return await call_next(request)\n\n    # Register labels for metrics\n    # add_global_metrics_labels(model_name=engine_args.model)\n    global llm_client\n    llm_client = llm\n    \n    global openai_serving_chat\n    openai_serving_chat = OpenAIServingChat(\n        llm_client=llm_client,\n        response_role=args.response_role,\n        server_model_name=args.served_model_name,\n        prompt_template=args.prompt_template\n    )\n    global openai_serving_completion\n    openai_serving_completion = OpenAIServingCompletion(\n        llm_client=llm_client,\n        server_model_name=args.served_model_name,\n        prompt_template=args.prompt_template\n    )\n\n    uvicorn.run(\n        router_app,\n        host=args.host,\n        port=args.port,\n        log_level=args.uvicorn_log_level,\n        timeout_keep_alive=TIMEOUT_KEEP_ALIVE,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile\n    )\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/generator.py\nfrom typing import Optional, Dict, Any, List\nfrom openai import OpenAI\nimport json\nfrom .types import RAGConfig, ByzerSQLConfig\nfrom byzerllm.utils.client import code_utils\nimport byzerllm\nimport requests\nfrom loguru import logger\nimport traceback\nclass SQLGenerator:\n\n    @byzerllm.prompt()\n    def gen_context(self, conversations: List[Dict[str, str]]) -> str:\n        \"\"\"\n        对话内容：\n        {% for msg in conversations %}\n        <{{ msg.role }}>: {{ msg.content }}\n        {% endfor %}\n\n        根据提供的对话内容，理解用户最后的问题，然后提取可能会用到的 Byzer-SQL 相关的知识。\n        \"\"\"\n\n    @byzerllm.prompt()\n    def gen_sql(self, conversations: List[Dict[str, str]],context:str) -> str:\n        \"\"\"\n        对话内容：\n        {% for msg in conversations %}\n        <{{ msg.role }}>: {{ msg.content }}\n        {% endfor %}\n\n        相关知识：\n        {{ context }}\n\n        根据提供的对话内容以及相关知识，理解用户最后的问题，然后根据看到的schema 信息，生成 Byzer-SQL语句。\n        注意，输出 Byzer-SQL 语句使用 ```sql``` 标记。\n        \"\"\"\n\n    def __init__(self, rag_config: RAGConfig, byzer_sql_config: ByzerSQLConfig) -> None:        \n        self.schema_rag_client = OpenAI(\n            api_key=rag_config.schema_rag_api_key,\n            base_url=rag_config.schema_rag_base_url,\n        )\n        self.context_rag_client = OpenAI(\n            api_key=rag_config.context_rag_api_key,\n            base_url=rag_config.context_rag_base_url,\n        )\n        self.byzer_sql_url = byzer_sql_config.byzer_sql_url\n\n    def stream_chat_oai(\n        self,\n        conversations,\n        model: Optional[str] = None,\n        role_mapping=None,\n        delta_mode=False,\n        llm_config: Dict[str, Any] = {},\n    ):\n        try:\n            v = self._stream_chat_oai(\n                conversations=conversations,\n                model=model,\n                role_mapping=role_mapping,\n                delta_mode=delta_mode,\n                llm_config=llm_config,\n            )\n        except Exception as e:\n            error_msg = f\"Error in stream_chat_oai: {e}\\n{traceback.format_exc()}\"\n            logger.error(error_msg)\n            v = [str(e)], []\n\n        return v\n\n    def _stream_chat_oai(\n        self,\n        conversations,\n        model: Optional[str] = None,\n        role_mapping=None,\n        delta_mode=False,\n        llm_config: Dict[str, Any] = {},\n    ):\n        logger.info(f\"Starting stream_chat_oai with model: {model}\")\n        logger.debug(f\"Conversations: {conversations}\")\n        logger.debug(f\"LLM Config: {llm_config}\")\n\n        if not model:\n            model = \"deepseek_chat\"\n        logger.info(f\"Using model: {model}\")\n\n        query = conversations[-1][\"content\"]\n\n        if (\n            \"使用四到五个字直接返回这句话的简要主题，不要解释、不要标点、不要语气词、不要多余文本，不要加粗，如果没有主题\"\n            in query\n            or \"简要总结一下对话内容，用作后续的上下文提示 prompt，控制在 200 字以内\"\n            in query\n        ):\n            chunks = self.llm.stream_chat_oai(\n                conversations=conversations,\n                model=model,\n                role_mapping=role_mapping,\n                llm_config=llm_config,\n                delta_mode=True,\n            )\n            return (chunk[0] for chunk in chunks), []\n\n        logger.info(\"Fetching context from context RAG client\")\n\n        context_query = self.gen_context.prompt(conversations=conversations)\n        context_response = self.context_rag_client.chat.completions.create(\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": json.dumps(\n                        {\"query\": context_query, \"only_contexts\": True},\n                        ensure_ascii=False,\n                    ),\n                }\n            ],\n            model=model,\n        )\n        \n        context = context_response.choices[0].message.content\n        new_context = \"\"\n        for msg in context.split(\"\\n\"):\n            if msg.strip():\n                v = json.loads(msg)\n                new_context += v[\"source_code\"]\n        logger.info(f\"context {new_context[0:100]}...\")\n        logger.info(\"Context retrieved successfully\")\n\n        logger.info(\"Generating SQL prompt\")\n        gen_sql_prompt = self.gen_sql.prompt(\n            conversations=conversations, context=new_context\n        )        \n\n        logger.info(\"Fetching SQL from schema RAG client\")\n        response = self.schema_rag_client.chat.completions.create(\n            messages=[{\"role\": \"user\", \"content\": gen_sql_prompt}],\n            model=model,\n        )\n        logger.debug(f\"Schema RAG response: {response}\")\n\n        sql_str = response.choices[0].message.content\n        logger.info(\"SQL string generated\")\n        logger.debug(f\"Generated SQL string: {sql_str}\")\n\n        sql = code_utils.extract_code(sql_str)[0][1]\n        logger.info(\"SQL code extracted\")\n        logger.debug(f\"Extracted SQL: {sql}\")\n\n        logger.info(f\"Sending SQL to Byzer SQL URL: {self.byzer_sql_url}\")\n        response = requests.post(self.byzer_sql_url, data={\"sql\": sql})\n        response.raise_for_status()\n        logger.info(\"Byzer SQL response received\")\n\n        result = response.content.decode(\"utf-8\")\n        logger.info(\"SQL execution result decoded\")\n        logger.debug(f\"SQL execution result: {result}\")\n\n        return [result], []\n\n\n##File: /Users/allwefantasy/projects/super_analysis/setup.py\nfrom setuptools import setup, find_packages\n\nsetup(\n    name=\"super-analysis\",\n    version=\"0.1.0\",\n    packages=find_packages(where=\"src\"),\n    package_dir={\"\": \"src\"},\n    install_requires=[\n        \"byzerllm\",\n        \"fastapi\",\n        \"uvicorn\",\n        \"argparse\",\n    ],\n    entry_points={\n        \"console_scripts\": [\n            \"super-analysis.serve=superanalysis.serve.cli:main\",\n            \"super-analysis.convert=superanalysis.serve.convert:main\",\n        ],\n    },\n    author=\"allwefantasy\",\n    author_email=\"allwefantasy@gmail.com\",\n    description=\"A package for super analysis with ByzerLLM\",\n    long_description=open(\"README.md\").read(),\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/yourusername/super-analysis\",\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n    ],\n    python_requires='>=3.6',\n)\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/convert.py\nimport os\nimport argparse\nimport pandas as pd\nimport hashlib\nfrom typing import List, Dict\n\ndef get_csv_files(directory: str) -> List[str]:\n    \"\"\"Recursively get all CSV files in the given directory.\"\"\"\n    csv_files = []\n    for root, _, files in os.walk(directory):\n        for file in files:\n            if file.endswith('.csv'):\n                csv_files.append(os.path.join(root, file))\n    return csv_files\n\ndef extract_schema(df: pd.DataFrame) -> Dict[str, str]:\n    \"\"\"Extract schema from pandas DataFrame.\"\"\"\n    schema = {}\n    for column in df.columns:\n        dtype = str(df[column].dtype)\n        if dtype == 'object':\n            schema[column] = 'STRING'\n        elif dtype.startswith('int'):\n            schema[column] = 'INTEGER'\n        elif dtype.startswith('float'):\n            schema[column] = 'FLOAT'\n        elif dtype == 'bool':\n            schema[column] = 'BOOLEAN'\n        elif dtype.startswith('datetime'):\n            schema[column] = 'TIMESTAMP'\n        else:\n            schema[column] = 'STRING'  # Default to STRING for unknown types\n    return schema\n\ndef generate_schema_text(schema: Dict[str, str], csv_file: str) -> str:\n    \"\"\"Generate schema text from the schema dictionary and include the CSV file name.\"\"\"\n    schema_lines = [f\"CSV File: {csv_file}\\n\"]\n    schema_lines.extend([f\"{field}: {dtype}\" for field, dtype in schema.items()])\n    return \"\\n\".join(schema_lines)\n\ndef save_schema(schema_text: str, csv_file: str, output_dir: str) -> None:\n    \"\"\"Save schema text to a file in the output directory.\"\"\"\n    # Generate MD5 hash of the schema text\n    file_name = hashlib.md5(schema_text.encode()).hexdigest() + '.md'\n    output_path = os.path.join(output_dir, file_name)\n    \n    with open(output_path, 'w') as f:\n        f.write(schema_text)\n    print(f\"Schema for {csv_file} saved to {output_path}\")\n\ndef convert(data_dir: str, doc_dir: str) -> None:\n    \"\"\"Main conversion function.\"\"\"\n    csv_files = get_csv_files(data_dir)\n    \n    for csv_file in csv_files:\n        print(f\"Processing {csv_file}\")\n        df = pd.read_csv(csv_file)\n        schema = extract_schema(df)\n        schema_text = generate_schema_text(schema, csv_file)\n        save_schema(schema_text, csv_file, doc_dir)\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Convert CSV files to schema documents\")\n    parser.add_argument(\"--data_dir\", required=True, help=\"Directory containing CSV files\")\n    parser.add_argument(\"--doc_dir\", required=True, help=\"Output directory for schema documents\")\n    \n    args = parser.parse_args()\n    \n    if not os.path.exists(args.doc_dir):\n        os.makedirs(args.doc_dir)\n    \n    convert(args.data_dir, args.doc_dir)\n\nif __name__ == \"__main__\":\n    main()\n\n     \n\n下面是用户的需求：\n\nresponse = requests.post(self.byzer_sql_url, data={\"sql\": sql}) 相关的代码要更友好,如果请求失败,打印详细的服务端日志\n"}, {"role": "assistant", "content": "为了实现您的需求，我们需要修改 `/Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/generator.py` 文件中的相关代码，使其在请求失败时能够打印详细的服务端日志。以下是修改后的代码：\n\n```python\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/generator.py\n<<<<<<< SEARCH\n        logger.info(f\"Sending SQL to Byzer SQL URL: {self.byzer_sql_url}\")\n        response = requests.post(self.byzer_sql_url, data={\"sql\": sql})\n        response.raise_for_status()\n        logger.info(\"Byzer SQL response received\")\n\n        result = response.content.decode(\"utf-8\")\n        logger.info(\"SQL execution result decoded\")\n        logger.debug(f\"SQL execution result: {result}\")\n\n        return [result], []\n=======\n        logger.info(f\"Sending SQL to Byzer SQL URL: {self.byzer_sql_url}\")\n        try:\n            response = requests.post(self.byzer_sql_url, data={\"sql\": sql}, timeout=30)\n            response.raise_for_status()\n            logger.info(\"Byzer SQL response received successfully\")\n            result = response.content.decode(\"utf-8\")\n            logger.info(\"SQL execution result decoded\")\n            logger.debug(f\"SQL execution result: {result}\")\n            return [result], []\n        except requests.exceptions.RequestException as e:\n            logger.error(f\"Failed to send SQL to Byzer SQL URL: {e}\")\n            if hasattr(e.response, 'content'):\n                error_content = e.response.content.decode(\"utf-8\")\n                logger.error(f\"Server response: {error_content}\")\n            else:\n                logger.error(\"No server response available\")\n            raise Exception(f\"Byzer SQL request failed: {str(e)}\") from e\n>>>>>>> REPLACE\n```\n\n这个修改做了以下几点改进：\n\n1. 使用 `try-except` 块来捕获可能的异常。\n2. 在 `requests.post()` 调用中添加了 `timeout` 参数，以避免请求无限期等待。\n3. 如果请求失败，会捕获 `requests.exceptions.RequestException` 异常。\n4. 在异常处理中，我们尝试获取并打印服务器的详细响应内容。\n5. 如果无法获取服务器响应，我们会记录这一情况。\n6. 最后，我们抛出一个新的异常，包含原始错误信息，以便上层代码可以继续处理这个错误。\n\n这些修改将使得代码在处理 Byzer SQL 请求时更加健壮，并在出现问题时提供更详细的错误信息。这将有助于调试和解决潜在的问题。\n\n需要我解释这些更改或进行其他修改吗？"}], "model": "human", "yaml_file": "actions/122_chat_action.yml"}
{"instruction": "模型UDF函数：\n        {{ model }} 这部分加一个if 判断 jinja2 语法", "conversations": [{"role": "user", "content": "\n如果你需要生成代码，对于每个需要更改的文件,你需要按 *SEARCH/REPLACE block* 的格式进行生成。\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The opening fence and code language, eg: ```python\n2. The file path alone on a line, starting with \"##File:\" and verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n*SEARCH/REPLACE* blocks will replace *all* matching occurrences.\nInclude enough lines to make the SEARCH blocks unique.\n\nInclude *ALL* the code being searched and replaced!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n下面我们来看一个例子：\n\n当前项目目录结构：\n1. 项目根目录： /tmp/projects/mathweb\n2. 项目子目录/文件列表(类似tree 命令输出)\nflask/\n    app.py\n    templates/\n        index.html\n    static/\n        style.css\n\n用户需求： Change get_factorial() to use math.factorial\n\n回答： To make this change we need to modify `/tmp/projects/mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n用户需求： Refactor hello() into its own file.\n\n回答：To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/hello.py\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/main.py\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n现在让我们开始一个新的任务:\n\n当前项目目录结构：\n1. 项目根目录： /Users/allwefantasy/projects/super_analysis\n2. 项目子目录/文件列表(类似tree 命令输出)：\nsetup.py\n.auto-coder/\n    libs/\n        llm_friendly_packages/\n            src/\n                example1/\n                    main.py\nsrc/\n    superanalysis/\n        serve/\n            convert.py\n            openai_serve.py\n            __init__.py\n            llm_wrapper.py\n            api_server.py\n            cli.py\n        sql/\n            __init__.py\n            types.py\n            generator.py\n下面是一些文件路径以及每个文件对应的源码：\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/types.py\nfrom pydantic import BaseModel\n\nclass RAGConfig(BaseModel):\n    schema_rag_base_url: str = \"\"\n    schema_rag_api_key: str = \"xxxx\"\n\n    context_rag_base_url: str = \"\"\n    context_rag_api_key: str = \"xxxx\"\n\n    \n\n    \n\nclass ByzerSQLConfig(BaseModel):\n    byzer_sql_url: str = \"\"\n    data_dir: str = \"\"\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/cli.py\nimport argparse\nimport byzerllm\nfrom byzerllm.utils.client import ByzerLLM\nfrom superanalysis.serve.api_server import serve, ServerArgs\nfrom superanalysis.sql.types import RAGConfig, ByzerSQLConfig\nfrom superanalysis.sql.generator import SQLGenerator\nfrom superanalysis.serve.llm_wrapper import LLWrapper\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"ByzerLLM API Server\")\n    parser.add_argument(\"--ray-address\", type=str, default=\"auto\", help=\"ray address\")\n    parser.add_argument(\"--host\", type=str, default=\"0.0.0.0\", help=\"Host to bind the server to\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"Port to bind the server to\")\n    parser.add_argument(\"--uvicorn-log-level\", type=str, default=\"info\", help=\"Uvicorn log level\")\n    parser.add_argument(\"--allow-credentials\", action=\"store_true\", help=\"Allow credentials\")\n    parser.add_argument(\"--allowed-origins\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed origins\")\n    parser.add_argument(\"--allowed-methods\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed methods\")\n    parser.add_argument(\"--allowed-headers\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed headers\")\n    parser.add_argument(\"--api-key\", type=str, default=\"\", help=\"API key for authentication\")\n    parser.add_argument(\"--served-model-name\", type=str, required=True, help=\"Name of the model to be served\")\n    parser.add_argument(\"--prompt-template\", type=str, default=\"\", help=\"Prompt template\")\n    parser.add_argument(\"--response-role\", type=str, default=\"assistant\", help=\"Response role\")\n    parser.add_argument(\"--ssl-keyfile\", type=str, default=\"\", help=\"SSL key file\")\n    parser.add_argument(\"--ssl-certfile\", type=str, default=\"\", help=\"SSL certificate file\")\n    \n    # Add new arguments for RAGConfig\n    parser.add_argument(\"--schema-rag-base-url\", type=str, default=\"\", help=\"RAG base URL\")\n    parser.add_argument(\"--schema-rag-api-key\", type=str, default=\"xxxx\", help=\"RAG API key\")\n\n    parser.add_argument(\"--context-rag-base-url\", type=str, default=\"\", help=\"RAG base URL\")\n    parser.add_argument(\"--context-rag-api-key\", type=str, default=\"xxxx\", help=\"RAG API key\")\n    \n    # Add new argument for ByzerSQLConfig\n    parser.add_argument(\"--byzer-sql-url\", type=str, default=\"\", help=\"Byzer SQL URL\")\n    parser.add_argument(\"--data-dir\", type=str, default=\"\", help=\"Data directory\")\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile\n    )\n\n    # Create RAGConfig instance\n    rag_config = RAGConfig(\n        schema_rag_base_url=args.schema_rag_base_url,\n        schema_rag_api_key=args.schema_rag_api_key,\n        context_rag_base_url=args.context_rag_base_url,\n        context_rag_api_key=args.context_rag_api_key\n    )\n\n    # Create ByzerSQLConfig instance\n    byzer_sql_config = ByzerSQLConfig(\n        byzer_sql_url=args.byzer_sql_url,\n        data_dir=args.data_dir\n    )\n\n    byzerllm.connect_cluster(address=args.ray_address)\n    llm = ByzerLLM()\n    llm.setup_default_model_name(args.served_model_name)        \n    llm = byzerllm.ByzerLLM()\n    if not server_args.served_model_name:\n        raise ValueError(\"served_model_name is required\")\n    llm.setup_default_model_name(server_args.served_model_name)\n\n    print(\"RAG Config: \", rag_config)\n    rag = SQLGenerator(llm=llm,rag_config=rag_config, byzer_sql_config=byzer_sql_config)    \n    llm_wrapper = LLWrapper(llm=llm, entry=rag)\n    serve(llm_wrapper, server_args)\n\nif __name__ == \"__main__\":\n    main()\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/__init__.py\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/__init__.py\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/llm_wrapper.py\nfrom typing import Any, Dict, List, Optional, Union,Callable\nfrom byzerllm.utils.client.types import (    \n    LLMFunctionCallResponse,\n    LLMClassResponse,LLMResponse    \n)\nimport pydantic\nfrom byzerllm import ByzerLLM\nfrom byzerllm.utils.client import LLMResponse\nfrom byzerllm.utils.types import SingleOutputMeta\nfrom byzerllm.utils.langutil import asyncfy_with_semaphore\nfrom superanalysis.sql.generator import SQLGenerator\nfrom loguru import logger\n\nclass LLWrapper:\n\n    def __init__(self,llm:ByzerLLM,entry:SQLGenerator):\n        self.entry = entry\n        self.llm = llm\n        \n\n    def chat_oai(self,\n                 conversations,\n                 tools:List[Union[Callable,str]]=[], \n                 tool_choice:Optional[Union[Callable,str]]=None,\n                 execute_tool:bool=False,  \n                 impl_func:Optional[Callable]=None,\n                 execute_impl_func:bool=False,\n                 impl_func_params:Optional[Dict[str,Any]]=None,\n                 func_params:Optional[Dict[str,Any]]=None,\n                 response_class:Optional[Union[pydantic.BaseModel,str]] = None, \n                 response_after_chat:Optional[Union[pydantic.BaseModel,str]] = False,\n                 enable_default_sys_message:bool=True,                 \n                 model:Optional[str] = None,\n                 role_mapping=None,llm_config:Dict[str,Any]={}\n                 )->Union[List[LLMResponse],List[LLMFunctionCallResponse],List[LLMClassResponse]]: \n        res,contexts = self.entry.stream_chat_oai(conversations)\n        s = \"\".join(res)        \n        return [LLMResponse(output=s,metadata={},input=\"\")]\n\n    def stream_chat_oai(self,conversations, \n                        model:Optional[str]=None, \n                        role_mapping=None,\n                        delta_mode=False,\n                        llm_config:Dict[str,Any]={}): \n        res,contexts = self.entry.stream_chat_oai(conversations)        \n        for t in res:                        \n            yield (t,SingleOutputMeta(0,0))\n\n    async def async_stream_chat_oai(self,conversations, \n                        model:Optional[str]=None, \n                        role_mapping=None,\n                        delta_mode=False,\n                        llm_config:Dict[str,Any]={}): \n        res,contexts = self.rag.stream_chat_oai(conversations)                \n        for t in res:                                    \n            yield (t,SingleOutputMeta(0,0))\n                         \n\n    def __getattr__(self, name):        \n        return getattr(self.llm, name)\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/openai_serve.py\nimport argparse\nfrom service_server.api_server import ServerArgs, serve\nfrom service_server.llm_wrapper import LLWrapper\nfrom service_server.summary_rag import SummaryRAG\nimport byzerllm\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"OpenAI Serve\")\n    parser.add_argument(\n        \"--model\",\n        default=\"\",\n    )\n    parser.add_argument(\"--emb_model\", default=\"\")\n    parser.add_argument(\n        \"--ray_address\",\n        default=\"auto\",\n    )\n    parser.add_argument(\"--source_dir\", default=\".\", help=\"\")\n    parser.add_argument(\"--host\", default=\"\", help=\"\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"\")\n    parser.add_argument(\"--uvicorn_log_level\", default=\"info\", help=\"\")\n    parser.add_argument(\"--allow_credentials\", action=\"store_true\", help=\"\")\n    parser.add_argument(\"--allowed_origins\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--allowed_methods\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--allowed_headers\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--api_key\", default=\"\", help=\"\")\n    parser.add_argument(\"--served_model_name\", default=\"\", help=\"\")\n    parser.add_argument(\"--prompt_template\", default=\"\", help=\"\")\n    parser.add_argument(\"--ssl_keyfile\", default=\"\", help=\"\")\n    parser.add_argument(\"--ssl_certfile\", default=\"\", help=\"\")\n    parser.add_argument(\"--response_role\", default=\"assistant\", help=\"\")\n    parser.add_argument(\n        \"--collections\", default=\"\", help=\"Collection name for indexing\"\n    )\n    parser.add_argument(\n        \"--base_dir\",\n        default=\"\",\n        help=\"Path where the processed text embeddings were stored\",\n    )\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name or args.model,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile,\n    )\n    byzerllm.connect_cluster(address=args.ray_address)\n    llm = byzerllm.ByzerLLM()\n    if not server_args.served_model_name:\n        raise ValueError(\"served_model_name is required\")\n    llm.setup_default_model_name(server_args.served_model_name)\n    rag = SummaryRAG(llm=llm)    \n    llm_wrapper = LLWrapper(llm=llm, rag=rag)\n    serve(llm=llm_wrapper, args=server_args)\n\nif __name__ == \"__main__\":\n    main()\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/api_server.py\nimport os\nimport time\nimport uvicorn\nfrom fastapi import Request\nfrom fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import JSONResponse, StreamingResponse, Response\n\nfrom byzerllm.log import init_logger\nfrom byzerllm.utils import random_uuid\nfrom byzerllm.version import __version__ as version\nfrom byzerllm.utils.client import ByzerLLM, LLMRequest\nfrom byzerllm.utils.client.entrypoints.openai.serving_chat import OpenAIServingChat\nfrom byzerllm.utils.client.entrypoints.openai.serving_completion import OpenAIServingCompletion\nfrom byzerllm.utils.client.entrypoints.openai.protocol import (\n    ModelList,\n    ModelCard,\n    ModelPermission,\n    ChatCompletionRequest,\n    ErrorResponse,\n    CompletionRequest,\n    Embeddings,\n    EmbeddingsOutput,\n    EmbeddingsData,\n    EmbeddingsUsage,\n)\nfrom pydantic import BaseModel\nfrom typing import List\n\nlogger = init_logger(__name__)\n\nllm_client: ByzerLLM = None\nopenai_serving_chat: OpenAIServingChat = None\nopenai_serving_completion: OpenAIServingCompletion = None\n\nTIMEOUT_KEEP_ALIVE = 5  # seconds\n# timeout in 10 minutes. Streaming can take longer than 3 min\nTIMEOUT = float(os.environ.get(\"BYZERLLM_APISERVER_HTTP_TIMEOUT\", 600))\n\nrouter_app = FastAPI()\n\n\n@router_app.get(\"/health\")\nasync def health() -> Response:\n    \"\"\"Health check.\"\"\"\n    return Response(status_code=200)\n\n\n@router_app.get(\"/v1/models\")\nasync def show_available_models():\n    models = await openai_serving_chat.show_available_models()\n    return JSONResponse(content=models.model_dump())\n\n\n@router_app.get(\"/version\")\nasync def show_version():\n    return JSONResponse(content={\"version\": version})\n\n\n@router_app.get(\"/v1/models\", response_model=ModelList)\nasync def models() -> ModelList:\n    \"\"\"Show available models. Right now we only have one model.\"\"\"\n    model_cards = [\n        ModelCard(\n            id=\"\",\n            root=\"\",\n            permission=[ModelPermission()]\n        )\n    ]\n    return ModelList(data=model_cards)\n\n\n@router_app.post(\"/v1/completions\")\nasync def create_completion(\n        body: CompletionRequest,\n        request: Request\n):\n    generator = await openai_serving_completion.create_completion(body, request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            status_code=generator.code\n        )\n    if body.stream:\n        return StreamingResponse(\n            content=generator,\n            media_type=\"text/event-stream\"\n        )\n    else:\n        return JSONResponse(content=generator.model_dump())\n\n\n@router_app.post(\"/v1/chat/completions\")\nasync def create_chat_completion(\n        body: ChatCompletionRequest,\n        request: Request,\n):\n    \"\"\"Completion API similar to OpenAI's API.\n\n    See  https://platform.openai.com/docs/api-reference/chat/create\n    for the API specification. This API mimics the OpenAI ChatCompletion API.\n\n    NOTE: Currently we do not support the following features:\n        - function_call (Users should implement this by themselves)\n        - logit_bias (to be supported by vLLM engine)\n    \"\"\"\n    # async with async_timeout.timeout(TIMEOUT):\n\n    generator = await openai_serving_chat.create_chat_completion(body, request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            status_code=generator.code\n        )\n    if body.stream:\n        return StreamingResponse(\n            content=generator,\n            media_type=\"text/event-stream\"\n        )\n    else:\n        return JSONResponse(content=generator.model_dump())\n\n\n@router_app.post(\"/v1/embeddings\")\nasync def embed(body: Embeddings):\n    \"\"\"Given a prompt, the model will return one embedding.\n\n    Returns:\n        A response object with an embedding.\n    \"\"\"\n    embedding_id = f\"embed-{random_uuid()}\"\n\n    results_list = llm_client.emb(body.model, request=LLMRequest(instruction=body.input))\n    tokens = 0\n\n    return EmbeddingsOutput(\n        data=[\n            EmbeddingsData(\n                embedding=results.output,\n                index=i,\n                object=\"embedding\",\n            )\n            for i, results in enumerate(results_list)\n        ],\n        id=embedding_id,\n        object=\"list\",\n        created=int(time.time()),\n        model=body.model,\n        usage=EmbeddingsUsage(\n            prompt_tokens=tokens,\n            total_tokens=tokens,\n        ),\n    )\n\nclass ServerArgs(BaseModel):\n    host: str = None\n    port: int = 8000\n    uvicorn_log_level: str = \"info\"\n    allow_credentials: bool = False\n    allowed_origins: List[str] = [\"*\"]  \n    allowed_methods: List[str] = [\"*\"]\n    allowed_headers: List[str] = [\"*\"]\n    api_key: str = None\n    served_model_name: str = None\n    prompt_template: str = None\n    response_role: str = \"assistant\"\n    ssl_keyfile: str = None\n    ssl_certfile: str = None    \n\ndef serve(llm:ByzerLLM, args: ServerArgs):\n    \n    logger.info(f\"ByzerLLM API server version {version}\")\n    logger.info(f\"args: {args}\")\n\n    router_app.add_middleware(\n        CORSMiddleware,\n        allow_origins=args.allowed_origins,\n        allow_credentials=args.allow_credentials,\n        allow_methods=args.allowed_methods,\n        allow_headers=args.allowed_headers,\n    )\n    \n    if token := os.environ.get(\"BYZERLLM_API_KEY\") or args.api_key:\n\n        @router_app.middleware(\"http\")\n        async def authentication(request: Request, call_next):\n            if not request.url.path.startswith(\"/v1\"):\n                return await call_next(request)\n            if request.headers.get(\"Authorization\") != \"Bearer \" + token:\n                return JSONResponse(\n                    content={\"error\": \"Unauthorized\"},\n                    status_code=401\n                )\n            return await call_next(request)\n\n    # Register labels for metrics\n    # add_global_metrics_labels(model_name=engine_args.model)\n    global llm_client\n    llm_client = llm\n    \n    global openai_serving_chat\n    openai_serving_chat = OpenAIServingChat(\n        llm_client=llm_client,\n        response_role=args.response_role,\n        server_model_name=args.served_model_name,\n        prompt_template=args.prompt_template\n    )\n    global openai_serving_completion\n    openai_serving_completion = OpenAIServingCompletion(\n        llm_client=llm_client,\n        server_model_name=args.served_model_name,\n        prompt_template=args.prompt_template\n    )\n\n    uvicorn.run(\n        router_app,\n        host=args.host,\n        port=args.port,\n        log_level=args.uvicorn_log_level,\n        timeout_keep_alive=TIMEOUT_KEEP_ALIVE,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile\n    )\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/generator.py\nfrom typing import Optional, Dict, Any, List\nfrom openai import OpenAI\nimport json\nfrom .types import RAGConfig, ByzerSQLConfig\nfrom byzerllm.utils.client import code_utils\nimport byzerllm\nimport requests\nfrom loguru import logger\nimport traceback\n\n\nclass SQLGenerator:\n\n    @byzerllm.prompt()\n    def gen_context(self, conversations: List[Dict[str, str]]) -> str:\n        \"\"\"\n        对话内容：\n        {% for msg in conversations %}\n        <{{ msg.role }}>: {{ msg.content }}\n        {% endfor %}\n\n        根据提供的对话内容，理解用户最后的问题，然后提取可能会用到的 Byzer-SQL 相关的知识。\n        \"\"\"\n\n    @byzerllm.prompt()\n    def gen_sql(self, conversations: List[Dict[str, str]], context: str,schema:str, model:str) -> str:\n        \"\"\"        \n        相关知识：\n        {{ context }}\n\n        Schema信息：\n        {{ schema }}\n\n        对话内容：\n        {% for msg in conversations %}\n        <{{ msg.role }}>: {{ msg.content }}\n        {% endfor %}\n\n        模型UDF函数：\n        {{ model }}\n\n        根据提供的相关知识以及对话内容，理解用户最后的问题，然后根据看到的schema 信息，生成 Byzer-SQL 语句。\n        Byzer-SQL 首先需要用 load 语法加载 schema 中的路径映射成表格，然后根据表格的信息生成对应的 Byzer-SQL 语句。\n        特别需要关注的是，用户的很多需求是要对表格的文本内容做自然语言处理，你需要使用 Byzer-SQL 中的大模型辅助函数来完成这个需求，如果你不知道用哪个\n        模型 UDF 函数，默认使用 deepseek_chat 这个名字。\n        注意，输出 Byzer-SQL 语句使用 ```sql``` 标记。\n        \"\"\"\n\n    @byzerllm.prompt()\n    def fetch_schema(self, context: str) -> str:\n        \"\"\"\n        下面是一些基础知识，包括名词术语等等：\n        {{ context }}\n\n        当理解对话内容的时候，需要参考上面的基础知识，然后更好的理解对应的表格schema信息是不是有用。\n        \"\"\"\n\n    def __init__(self, llm:byzerllm.ByzerLLM,rag_config: RAGConfig, byzer_sql_config: ByzerSQLConfig) -> None:\n        self.llm = llm\n        self.schema_rag_client = OpenAI(\n            api_key=rag_config.schema_rag_api_key,\n            base_url=rag_config.schema_rag_base_url,\n        )\n        self.context_rag_client = OpenAI(\n            api_key=rag_config.context_rag_api_key,\n            base_url=rag_config.context_rag_base_url,\n        )\n        self.byzer_sql_url = byzer_sql_config.byzer_sql_url\n\n    def stream_chat_oai(\n        self,\n        conversations,\n        model: Optional[str] = None,\n        role_mapping=None,\n        delta_mode=False,\n        llm_config: Dict[str, Any] = {},\n    ):\n        try:\n            v, c = self._stream_chat_oai(\n                conversations=conversations,\n                model=model,\n                role_mapping=role_mapping,\n                delta_mode=delta_mode,\n                llm_config=llm_config,\n            )\n        except Exception as e:\n            error_msg = f\"Error in stream_chat_oai: {e}\\n{traceback.format_exc()}\"\n            logger.error(error_msg)\n            v, c = [str(e)], []\n\n        return v, c\n\n    def _stream_chat_oai(\n        self,\n        conversations,\n        model: Optional[str] = None,\n        role_mapping=None,\n        delta_mode=False,\n        llm_config: Dict[str, Any] = {},\n    ):\n        logger.info(f\"Starting stream_chat_oai with model: {model}\")\n        logger.debug(f\"Conversations: {conversations}\")\n        logger.debug(f\"LLM Config: {llm_config}\")\n\n        if not model:\n            model = \"deepseek_chat\"\n        logger.info(f\"Using model: {model}\")\n\n        query = conversations[-1][\"content\"]\n\n        if (\n            \"使用四到五个字直接返回这句话的简要主题，不要解释、不要标点、不要语气词、不要多余文本，不要加粗，如果没有主题\"\n            in query\n            or \"简要总结一下对话内容，用作后续的上下文提示 prompt，控制在 200 字以内\"\n            in query\n        ):\n            chunks = self.llm.stream_chat_oai(\n                conversations=conversations,\n                model=model,\n                role_mapping=role_mapping,\n                llm_config=llm_config,\n                delta_mode=True,\n            )\n            return (chunk[0] for chunk in chunks), []\n\n        logger.info(\"Fetching context from context RAG client\")\n        context_query = self.gen_context.prompt(conversations=conversations)\n        context_response = self.context_rag_client.chat.completions.create(\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": json.dumps(\n                        {\"query\": context_query, \"only_contexts\": True},\n                        ensure_ascii=False,\n                    ),\n                }\n            ],\n            model=model,\n        )\n\n        context = context_response.choices[0].message.content\n        new_context = \"\"\n        for msg in context.split(\"\\n\"):\n            if msg.strip():\n                v = json.loads(msg)\n                new_context += v[\"source_code\"]\n        logger.info(f\"context {new_context[0:100]}...\")\n        logger.info(\"Context retrieved successfully\")\n        \n        logger.info(\"Fetching schemas from schema RAG client\")\n        schema_query = self.fetch_schema.prompt(context=new_context)\n\n        response = self.schema_rag_client.chat.completions.create(\n            messages=conversations[0:-1]\n            + [\n                {\n                    \"role\": \"user\",\n                    \"content\": json.dumps(\n                        {\"query\": schema_query, \"only_contexts\": True},\n                        ensure_ascii=False,\n                    ),\n                }\n            ],\n            model=model,\n        )\n        logger.debug(f\"Schema RAG response: {response}\")\n\n        schema_str = response.choices[0].message.content\n        logger.info(\"Schema retrieved successfully\")\n\n        logger.info(\"Generating SQL string\")\n        sql_str = self.gen_sql.with_llm(self.llm).run(\n            conversations=conversations, context=new_context, schema=schema_str\n        )\n\n\n        logger.info(\"SQL string generated\")\n        logger.debug(f\"Generated SQL string: {sql_str}\")\n\n        sql = code_utils.extract_code(sql_str)[0][1]\n        logger.info(\"SQL code extracted\")\n        logger.debug(f\"Extracted SQL: {sql}\")\n\n        logger.info(f\"Sending SQL to Byzer SQL URL: {self.byzer_sql_url}\")\n        try:\n            response = requests.post(\n                self.byzer_sql_url,\n                data={\"sql\": sql, \"owner\": \"allwefantasy\"},\n                timeout=600,\n            )\n            response.raise_for_status()\n            result = response.content.decode(\"utf-8\")\n            logger.info(\"SQL execution result decoded\")\n            logger.debug(f\"SQL execution result: {result}\")\n            return (item for item in [result]), []\n        except requests.exceptions.RequestException as e:\n            logger.error(f\"Failed to send SQL to Byzer SQL URL: {e}\")\n            if hasattr(e.response, \"content\"):\n                error_content = e.response.content.decode(\"utf-8\")\n                logger.error(f\"Server response: {error_content}\")\n            else:\n                logger.error(\"No server response available\")\n            raise Exception(f\"Byzer SQL request failed: {str(e)}\") from e\n\n\n##File: /Users/allwefantasy/projects/super_analysis/setup.py\nfrom setuptools import setup, find_packages\n\nsetup(\n    name=\"super-analysis\",\n    version=\"0.1.0\",\n    packages=find_packages(where=\"src\"),\n    package_dir={\"\": \"src\"},\n    install_requires=[\n        \"byzerllm\",\n        \"fastapi\",\n        \"uvicorn\",\n        \"argparse\",\n    ],\n    entry_points={\n        \"console_scripts\": [\n            \"super-analysis.serve=superanalysis.serve.cli:main\",\n            \"super-analysis.convert=superanalysis.serve.convert:main\",\n        ],\n    },\n    author=\"allwefantasy\",\n    author_email=\"allwefantasy@gmail.com\",\n    description=\"A package for super analysis with ByzerLLM\",\n    long_description=open(\"README.md\").read(),\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/yourusername/super-analysis\",\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n    ],\n    python_requires='>=3.6',\n)\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/convert.py\nimport os\nimport argparse\nimport pandas as pd\nimport hashlib\nfrom typing import List, Dict\n\ndef get_csv_files(directory: str) -> List[str]:\n    \"\"\"Recursively get all CSV files in the given directory.\"\"\"\n    csv_files = []\n    for root, _, files in os.walk(directory):\n        for file in files:\n            if file.endswith('.csv'):\n                csv_files.append(os.path.join(root, file))\n    return csv_files\n\ndef extract_schema(df: pd.DataFrame) -> Dict[str, str]:\n    \"\"\"Extract schema from pandas DataFrame.\"\"\"\n    schema = {}\n    for column in df.columns:\n        dtype = str(df[column].dtype)\n        if dtype == 'object':\n            schema[column] = 'STRING'\n        elif dtype.startswith('int'):\n            schema[column] = 'INTEGER'\n        elif dtype.startswith('float'):\n            schema[column] = 'FLOAT'\n        elif dtype == 'bool':\n            schema[column] = 'BOOLEAN'\n        elif dtype.startswith('datetime'):\n            schema[column] = 'TIMESTAMP'\n        else:\n            schema[column] = 'STRING'  # Default to STRING for unknown types\n    return schema\n\ndef generate_schema_text(schema: Dict[str, str], csv_file: str) -> str:\n    \"\"\"Generate schema text from the schema dictionary and include the CSV file name.\"\"\"\n    schema_lines = [f\"CSV File: {csv_file}\\n\"]\n    schema_lines.extend([f\"{field}: {dtype}\" for field, dtype in schema.items()])\n    return \"\\n\".join(schema_lines)\n\ndef save_schema(schema_text: str, csv_file: str, output_dir: str) -> None:\n    \"\"\"Save schema text to a file in the output directory.\"\"\"\n    # Generate MD5 hash of the schema text\n    file_name = hashlib.md5(schema_text.encode()).hexdigest() + '.md'\n    output_path = os.path.join(output_dir, file_name)\n    \n    with open(output_path, 'w') as f:\n        f.write(schema_text)\n    print(f\"Schema for {csv_file} saved to {output_path}\")\n\ndef convert(data_dir: str, doc_dir: str) -> None:\n    \"\"\"Main conversion function.\"\"\"\n    csv_files = get_csv_files(data_dir)\n    \n    for csv_file in csv_files:\n        print(f\"Processing {csv_file}\")\n        df = pd.read_csv(csv_file)\n        schema = extract_schema(df)\n        schema_text = generate_schema_text(schema, csv_file)\n        save_schema(schema_text, csv_file, doc_dir)\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Convert CSV files to schema documents\")\n    parser.add_argument(\"--data_dir\", required=True, help=\"Directory containing CSV files\")\n    parser.add_argument(\"--doc_dir\", required=True, help=\"Output directory for schema documents\")\n    \n    args = parser.parse_args()\n    \n    if not os.path.exists(args.doc_dir):\n        os.makedirs(args.doc_dir)\n    \n    convert(args.data_dir, args.doc_dir)\n\nif __name__ == \"__main__\":\n    main()\n\n     \n\n下面是用户的需求：\n\n模型UDF函数：\n        {{ model }} 这部分加一个if 判断 jinja2 语法\n"}, {"role": "assistant", "content": "好的，我理解您的需求是要在 `gen_sql` 函数的模板中为模型 UDF 函数部分添加一个 if 判断，使用 Jinja2 语法。我们需要修改 `/Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/generator.py` 文件中的 `gen_sql` 方法。下面是相应的修改：\n\n```python\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/generator.py\n<<<<<<< SEARCH\n    @byzerllm.prompt()\n    def gen_sql(self, conversations: List[Dict[str, str]], context: str,schema:str, model:str) -> str:\n        \"\"\"        \n        相关知识：\n        {{ context }}\n\n        Schema信息：\n        {{ schema }}\n\n        对话内容：\n        {% for msg in conversations %}\n        <{{ msg.role }}>: {{ msg.content }}\n        {% endfor %}\n\n        模型UDF函数：\n        {{ model }}\n\n        根据提供的相关知识以及对话内容，理解用户最后的问题，然后根据看到的schema 信息，生成 Byzer-SQL 语句。\n        Byzer-SQL 首先需要用 load 语法加载 schema 中的路径映射成表格，然后根据表格的信息生成对应的 Byzer-SQL 语句。\n        特别需要关注的是，用户的很多需求是要对表格的文本内容做自然语言处理，你需要使用 Byzer-SQL 中的大模型辅助函数来完成这个需求，如果你不知道用哪个\n        模型 UDF 函数，默认使用 deepseek_chat 这个名字。\n        注意，输出 Byzer-SQL 语句使用 ```sql``` 标记。\n        \"\"\"\n=======\n    @byzerllm.prompt()\n    def gen_sql(self, conversations: List[Dict[str, str]], context: str,schema:str, model:str) -> str:\n        \"\"\"        \n        相关知识：\n        {{ context }}\n\n        Schema信息：\n        {{ schema }}\n\n        对话内容：\n        {% for msg in conversations %}\n        <{{ msg.role }}>: {{ msg.content }}\n        {% endfor %}\n\n        模型UDF函数：\n        {% if model %}\n        {{ model }}\n        {% else %}\n        默认使用 deepseek_chat\n        {% endif %}\n\n        根据提供的相关知识以及对话内容，理解用户最后的问题，然后根据看到的schema 信息，生成 Byzer-SQL 语句。\n        Byzer-SQL 首先需要用 load 语法加载 schema 中的路径映射成表格，然后根据表格的信息生成对应的 Byzer-SQL 语句。\n        特别需要关注的是，用户的很多需求是要对表格的文本内容做自然语言处理，你需要使用 Byzer-SQL 中的大模型辅助函数来完成这个需求，如果你不知道用哪个\n        模型 UDF 函数，默认使用 deepseek_chat 这个名字。\n        注意，输出 Byzer-SQL 语句使用 ```sql``` 标记。\n        \"\"\"\n>>>>>>> REPLACE\n```\n\n这个修改添加了一个 Jinja2 if 语句来检查 `model` 参数是否有值。如果 `model` 有值，它会显示传入的模型信息；如果 `model` 没有值或为空，它会显示默认使用 deepseek_chat。\n\n这样的修改使得模板更加灵活，能够根据是否提供了特定的模型 UDF 函数来调整提示。如果在调用 `gen_sql` 方法时没有提供 `model` 参数或提供了空值，模板会默认建议使用 deepseek_chat。\n\n是否还需要其他修改或有任何问题？"}], "model": "human", "yaml_file": "actions/123_chat_action.yml"}
{"instruction": "使用 TagExtractor 抽取 <_byzer_sql_></_byzer_sql_> 里的内容,里面只会有一个 标签对,在 #MARK 标记处实现", "conversations": [{"role": "user", "content": "\n如果你需要生成代码，对于每个需要更改的文件,你需要按 *SEARCH/REPLACE block* 的格式进行生成。\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The opening fence and code language, eg: ```python\n2. The file path alone on a line, starting with \"##File:\" and verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n*SEARCH/REPLACE* blocks will replace *all* matching occurrences.\nInclude enough lines to make the SEARCH blocks unique.\n\nInclude *ALL* the code being searched and replaced!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n下面我们来看一个例子：\n\n当前项目目录结构：\n1. 项目根目录： /tmp/projects/mathweb\n2. 项目子目录/文件列表(类似tree 命令输出)\nflask/\n    app.py\n    templates/\n        index.html\n    static/\n        style.css\n\n用户需求： Change get_factorial() to use math.factorial\n\n回答： To make this change we need to modify `/tmp/projects/mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n用户需求： Refactor hello() into its own file.\n\n回答：To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/hello.py\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/main.py\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n现在让我们开始一个新的任务:\n\n当前项目目录结构：\n1. 项目根目录： /Users/allwefantasy/projects/super_analysis\n2. 项目子目录/文件列表(类似tree 命令输出)：\nsetup.py\n.auto-coder/\n    libs/\n        llm_friendly_packages/\n            src/\n                example1/\n                    main.py\nsrc/\n    superanalysis/\n        serve/\n            convert.py\n            openai_serve.py\n            __init__.py\n            llm_wrapper.py\n            api_server.py\n            cli.py\n        sql/\n            __init__.py\n            types.py\n            generator.py\n下面是一些文件路径以及每个文件对应的源码：\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/types.py\nfrom pydantic import BaseModel\n\nclass RAGConfig(BaseModel):\n    schema_rag_base_url: str = \"\"\n    schema_rag_api_key: str = \"xxxx\"\n\n    context_rag_base_url: str = \"\"\n    context_rag_api_key: str = \"xxxx\"\n\n    \n\n    \n\nclass ByzerSQLConfig(BaseModel):\n    byzer_sql_url: str = \"\"\n    data_dir: str = \"\"\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/cli.py\nimport argparse\nimport byzerllm\nfrom byzerllm.utils.client import ByzerLLM\nfrom superanalysis.serve.api_server import serve, ServerArgs\nfrom superanalysis.sql.types import RAGConfig, ByzerSQLConfig\nfrom superanalysis.sql.generator import SQLGenerator\nfrom superanalysis.serve.llm_wrapper import LLWrapper\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"ByzerLLM API Server\")\n    parser.add_argument(\"--ray-address\", type=str, default=\"auto\", help=\"ray address\")\n    parser.add_argument(\"--host\", type=str, default=\"0.0.0.0\", help=\"Host to bind the server to\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"Port to bind the server to\")\n    parser.add_argument(\"--uvicorn-log-level\", type=str, default=\"info\", help=\"Uvicorn log level\")\n    parser.add_argument(\"--allow-credentials\", action=\"store_true\", help=\"Allow credentials\")\n    parser.add_argument(\"--allowed-origins\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed origins\")\n    parser.add_argument(\"--allowed-methods\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed methods\")\n    parser.add_argument(\"--allowed-headers\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed headers\")\n    parser.add_argument(\"--api-key\", type=str, default=\"\", help=\"API key for authentication\")\n    parser.add_argument(\"--served-model-name\", type=str, required=True, help=\"Name of the model to be served\")\n    parser.add_argument(\"--prompt-template\", type=str, default=\"\", help=\"Prompt template\")\n    parser.add_argument(\"--response-role\", type=str, default=\"assistant\", help=\"Response role\")\n    parser.add_argument(\"--ssl-keyfile\", type=str, default=\"\", help=\"SSL key file\")\n    parser.add_argument(\"--ssl-certfile\", type=str, default=\"\", help=\"SSL certificate file\")\n    \n    # Add new arguments for RAGConfig\n    parser.add_argument(\"--schema-rag-base-url\", type=str, default=\"\", help=\"RAG base URL\")\n    parser.add_argument(\"--schema-rag-api-key\", type=str, default=\"xxxx\", help=\"RAG API key\")\n\n    parser.add_argument(\"--context-rag-base-url\", type=str, default=\"\", help=\"RAG base URL\")\n    parser.add_argument(\"--context-rag-api-key\", type=str, default=\"xxxx\", help=\"RAG API key\")\n    \n    # Add new argument for ByzerSQLConfig\n    parser.add_argument(\"--byzer-sql-url\", type=str, default=\"\", help=\"Byzer SQL URL\")\n    parser.add_argument(\"--data-dir\", type=str, default=\"\", help=\"Data directory\")\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile\n    )\n\n    # Create RAGConfig instance\n    rag_config = RAGConfig(\n        schema_rag_base_url=args.schema_rag_base_url,\n        schema_rag_api_key=args.schema_rag_api_key,\n        context_rag_base_url=args.context_rag_base_url,\n        context_rag_api_key=args.context_rag_api_key\n    )\n\n    # Create ByzerSQLConfig instance\n    byzer_sql_config = ByzerSQLConfig(\n        byzer_sql_url=args.byzer_sql_url,\n        data_dir=args.data_dir\n    )\n\n    byzerllm.connect_cluster(address=args.ray_address)\n    llm = ByzerLLM()\n    llm.setup_default_model_name(args.served_model_name)        \n    llm = byzerllm.ByzerLLM()\n    if not server_args.served_model_name:\n        raise ValueError(\"served_model_name is required\")\n    llm.setup_default_model_name(server_args.served_model_name)\n\n    print(\"RAG Config: \", rag_config)\n    rag = SQLGenerator(llm=llm,rag_config=rag_config, byzer_sql_config=byzer_sql_config)    \n    llm_wrapper = LLWrapper(llm=llm, entry=rag)\n    serve(llm_wrapper, server_args)\n\nif __name__ == \"__main__\":\n    main()\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/__init__.py\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/__init__.py\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/llm_wrapper.py\nfrom typing import Any, Dict, List, Optional, Union,Callable\nfrom byzerllm.utils.client.types import (    \n    LLMFunctionCallResponse,\n    LLMClassResponse,LLMResponse    \n)\nimport pydantic\nfrom byzerllm import ByzerLLM\nfrom byzerllm.utils.client import LLMResponse\nfrom byzerllm.utils.types import SingleOutputMeta\nfrom byzerllm.utils.langutil import asyncfy_with_semaphore\nfrom superanalysis.sql.generator import SQLGenerator\nfrom loguru import logger\n\nclass LLWrapper:\n\n    def __init__(self,llm:ByzerLLM,entry:SQLGenerator):\n        self.entry = entry\n        self.llm = llm\n        \n\n    def chat_oai(self,\n                 conversations,\n                 tools:List[Union[Callable,str]]=[], \n                 tool_choice:Optional[Union[Callable,str]]=None,\n                 execute_tool:bool=False,  \n                 impl_func:Optional[Callable]=None,\n                 execute_impl_func:bool=False,\n                 impl_func_params:Optional[Dict[str,Any]]=None,\n                 func_params:Optional[Dict[str,Any]]=None,\n                 response_class:Optional[Union[pydantic.BaseModel,str]] = None, \n                 response_after_chat:Optional[Union[pydantic.BaseModel,str]] = False,\n                 enable_default_sys_message:bool=True,                 \n                 model:Optional[str] = None,\n                 role_mapping=None,llm_config:Dict[str,Any]={}\n                 )->Union[List[LLMResponse],List[LLMFunctionCallResponse],List[LLMClassResponse]]: \n        res,contexts = self.entry.stream_chat_oai(conversations)\n        s = \"\".join(res)        \n        return [LLMResponse(output=s,metadata={},input=\"\")]\n\n    def stream_chat_oai(self,conversations, \n                        model:Optional[str]=None, \n                        role_mapping=None,\n                        delta_mode=False,\n                        llm_config:Dict[str,Any]={}): \n        res,contexts = self.entry.stream_chat_oai(conversations)        \n        for t in res:                        \n            yield (t,SingleOutputMeta(0,0))\n\n    async def async_stream_chat_oai(self,conversations, \n                        model:Optional[str]=None, \n                        role_mapping=None,\n                        delta_mode=False,\n                        llm_config:Dict[str,Any]={}): \n        res,contexts = self.rag.stream_chat_oai(conversations)                \n        for t in res:                                    \n            yield (t,SingleOutputMeta(0,0))\n                         \n\n    def __getattr__(self, name):        \n        return getattr(self.llm, name)\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/openai_serve.py\nimport argparse\nfrom service_server.api_server import ServerArgs, serve\nfrom service_server.llm_wrapper import LLWrapper\nfrom service_server.summary_rag import SummaryRAG\nimport byzerllm\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"OpenAI Serve\")\n    parser.add_argument(\n        \"--model\",\n        default=\"\",\n    )\n    parser.add_argument(\"--emb_model\", default=\"\")\n    parser.add_argument(\n        \"--ray_address\",\n        default=\"auto\",\n    )\n    parser.add_argument(\"--source_dir\", default=\".\", help=\"\")\n    parser.add_argument(\"--host\", default=\"\", help=\"\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"\")\n    parser.add_argument(\"--uvicorn_log_level\", default=\"info\", help=\"\")\n    parser.add_argument(\"--allow_credentials\", action=\"store_true\", help=\"\")\n    parser.add_argument(\"--allowed_origins\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--allowed_methods\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--allowed_headers\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--api_key\", default=\"\", help=\"\")\n    parser.add_argument(\"--served_model_name\", default=\"\", help=\"\")\n    parser.add_argument(\"--prompt_template\", default=\"\", help=\"\")\n    parser.add_argument(\"--ssl_keyfile\", default=\"\", help=\"\")\n    parser.add_argument(\"--ssl_certfile\", default=\"\", help=\"\")\n    parser.add_argument(\"--response_role\", default=\"assistant\", help=\"\")\n    parser.add_argument(\n        \"--collections\", default=\"\", help=\"Collection name for indexing\"\n    )\n    parser.add_argument(\n        \"--base_dir\",\n        default=\"\",\n        help=\"Path where the processed text embeddings were stored\",\n    )\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name or args.model,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile,\n    )\n    byzerllm.connect_cluster(address=args.ray_address)\n    llm = byzerllm.ByzerLLM()\n    if not server_args.served_model_name:\n        raise ValueError(\"served_model_name is required\")\n    llm.setup_default_model_name(server_args.served_model_name)\n    rag = SummaryRAG(llm=llm)    \n    llm_wrapper = LLWrapper(llm=llm, rag=rag)\n    serve(llm=llm_wrapper, args=server_args)\n\nif __name__ == \"__main__\":\n    main()\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/api_server.py\nimport os\nimport time\nimport uvicorn\nfrom fastapi import Request\nfrom fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import JSONResponse, StreamingResponse, Response\n\nfrom byzerllm.log import init_logger\nfrom byzerllm.utils import random_uuid\nfrom byzerllm.version import __version__ as version\nfrom byzerllm.utils.client import ByzerLLM, LLMRequest\nfrom byzerllm.utils.client.entrypoints.openai.serving_chat import OpenAIServingChat\nfrom byzerllm.utils.client.entrypoints.openai.serving_completion import OpenAIServingCompletion\nfrom byzerllm.utils.client.entrypoints.openai.protocol import (\n    ModelList,\n    ModelCard,\n    ModelPermission,\n    ChatCompletionRequest,\n    ErrorResponse,\n    CompletionRequest,\n    Embeddings,\n    EmbeddingsOutput,\n    EmbeddingsData,\n    EmbeddingsUsage,\n)\nfrom pydantic import BaseModel\nfrom typing import List\n\nlogger = init_logger(__name__)\n\nllm_client: ByzerLLM = None\nopenai_serving_chat: OpenAIServingChat = None\nopenai_serving_completion: OpenAIServingCompletion = None\n\nTIMEOUT_KEEP_ALIVE = 5  # seconds\n# timeout in 10 minutes. Streaming can take longer than 3 min\nTIMEOUT = float(os.environ.get(\"BYZERLLM_APISERVER_HTTP_TIMEOUT\", 600))\n\nrouter_app = FastAPI()\n\n\n@router_app.get(\"/health\")\nasync def health() -> Response:\n    \"\"\"Health check.\"\"\"\n    return Response(status_code=200)\n\n\n@router_app.get(\"/v1/models\")\nasync def show_available_models():\n    models = await openai_serving_chat.show_available_models()\n    return JSONResponse(content=models.model_dump())\n\n\n@router_app.get(\"/version\")\nasync def show_version():\n    return JSONResponse(content={\"version\": version})\n\n\n@router_app.get(\"/v1/models\", response_model=ModelList)\nasync def models() -> ModelList:\n    \"\"\"Show available models. Right now we only have one model.\"\"\"\n    model_cards = [\n        ModelCard(\n            id=\"\",\n            root=\"\",\n            permission=[ModelPermission()]\n        )\n    ]\n    return ModelList(data=model_cards)\n\n\n@router_app.post(\"/v1/completions\")\nasync def create_completion(\n        body: CompletionRequest,\n        request: Request\n):\n    generator = await openai_serving_completion.create_completion(body, request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            status_code=generator.code\n        )\n    if body.stream:\n        return StreamingResponse(\n            content=generator,\n            media_type=\"text/event-stream\"\n        )\n    else:\n        return JSONResponse(content=generator.model_dump())\n\n\n@router_app.post(\"/v1/chat/completions\")\nasync def create_chat_completion(\n        body: ChatCompletionRequest,\n        request: Request,\n):\n    \"\"\"Completion API similar to OpenAI's API.\n\n    See  https://platform.openai.com/docs/api-reference/chat/create\n    for the API specification. This API mimics the OpenAI ChatCompletion API.\n\n    NOTE: Currently we do not support the following features:\n        - function_call (Users should implement this by themselves)\n        - logit_bias (to be supported by vLLM engine)\n    \"\"\"\n    # async with async_timeout.timeout(TIMEOUT):\n\n    generator = await openai_serving_chat.create_chat_completion(body, request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            status_code=generator.code\n        )\n    if body.stream:\n        return StreamingResponse(\n            content=generator,\n            media_type=\"text/event-stream\"\n        )\n    else:\n        return JSONResponse(content=generator.model_dump())\n\n\n@router_app.post(\"/v1/embeddings\")\nasync def embed(body: Embeddings):\n    \"\"\"Given a prompt, the model will return one embedding.\n\n    Returns:\n        A response object with an embedding.\n    \"\"\"\n    embedding_id = f\"embed-{random_uuid()}\"\n\n    results_list = llm_client.emb(body.model, request=LLMRequest(instruction=body.input))\n    tokens = 0\n\n    return EmbeddingsOutput(\n        data=[\n            EmbeddingsData(\n                embedding=results.output,\n                index=i,\n                object=\"embedding\",\n            )\n            for i, results in enumerate(results_list)\n        ],\n        id=embedding_id,\n        object=\"list\",\n        created=int(time.time()),\n        model=body.model,\n        usage=EmbeddingsUsage(\n            prompt_tokens=tokens,\n            total_tokens=tokens,\n        ),\n    )\n\nclass ServerArgs(BaseModel):\n    host: str = None\n    port: int = 8000\n    uvicorn_log_level: str = \"info\"\n    allow_credentials: bool = False\n    allowed_origins: List[str] = [\"*\"]  \n    allowed_methods: List[str] = [\"*\"]\n    allowed_headers: List[str] = [\"*\"]\n    api_key: str = None\n    served_model_name: str = None\n    prompt_template: str = None\n    response_role: str = \"assistant\"\n    ssl_keyfile: str = None\n    ssl_certfile: str = None    \n\ndef serve(llm:ByzerLLM, args: ServerArgs):\n    \n    logger.info(f\"ByzerLLM API server version {version}\")\n    logger.info(f\"args: {args}\")\n\n    router_app.add_middleware(\n        CORSMiddleware,\n        allow_origins=args.allowed_origins,\n        allow_credentials=args.allow_credentials,\n        allow_methods=args.allowed_methods,\n        allow_headers=args.allowed_headers,\n    )\n    \n    if token := os.environ.get(\"BYZERLLM_API_KEY\") or args.api_key:\n\n        @router_app.middleware(\"http\")\n        async def authentication(request: Request, call_next):\n            if not request.url.path.startswith(\"/v1\"):\n                return await call_next(request)\n            if request.headers.get(\"Authorization\") != \"Bearer \" + token:\n                return JSONResponse(\n                    content={\"error\": \"Unauthorized\"},\n                    status_code=401\n                )\n            return await call_next(request)\n\n    # Register labels for metrics\n    # add_global_metrics_labels(model_name=engine_args.model)\n    global llm_client\n    llm_client = llm\n    \n    global openai_serving_chat\n    openai_serving_chat = OpenAIServingChat(\n        llm_client=llm_client,\n        response_role=args.response_role,\n        server_model_name=args.served_model_name,\n        prompt_template=args.prompt_template\n    )\n    global openai_serving_completion\n    openai_serving_completion = OpenAIServingCompletion(\n        llm_client=llm_client,\n        server_model_name=args.served_model_name,\n        prompt_template=args.prompt_template\n    )\n\n    uvicorn.run(\n        router_app,\n        host=args.host,\n        port=args.port,\n        log_level=args.uvicorn_log_level,\n        timeout_keep_alive=TIMEOUT_KEEP_ALIVE,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile\n    )\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/generator.py\nfrom typing import Optional, Dict, Any, List\nfrom openai import OpenAI\nimport json\nfrom .types import RAGConfig, ByzerSQLConfig\nfrom byzerllm.utils.client import code_utils\nimport byzerllm\nimport requests\nfrom loguru import logger\nimport traceback\nfrom byzerllm.utils.nontext import TagExtractor\n\n\nclass SQLGenerator:\n\n    @byzerllm.prompt()\n    def gen_context(self, conversations: List[Dict[str, str]]) -> str:\n        \"\"\"\n        对话内容：\n        {% for msg in conversations %}\n        <{{ msg.role }}>: {{ msg.content }}\n        {% endfor %}\n\n        根据提供的对话内容，理解用户最后的问题，然后提取可能会用到的 Byzer-SQL 相关的知识。\n        \"\"\"\n\n    @byzerllm.prompt()\n    def gen_sql(self, conversations: List[Dict[str, str]], context: str,schema:str, model:str=\"deepseek_chat\") -> str:\n        \"\"\"        \n        相关知识：\n        {{ context }}\n\n        Schema信息：\n        {{ schema }}\n\n        对话内容：\n        {% for msg in conversations %}\n        <{{ msg.role }}>: {{ msg.content }}\n        {% endfor %}\n\n        模型UDF函数：        \n        {{ model }}        \n\n        根据提供的相关知识以及对话内容，理解用户最后的问题，然后根据看到的schema 信息，生成 Byzer-SQL 语句。\n        Byzer-SQL 首先需要用 load 语法加载 schema 中的路径映射成表格，然后根据表格的信息生成对应的 Byzer-SQL 语句。\n        特别需要关注的是，用户的很多需求是要对表格的文本内容做自然语言处理，你需要使用 Byzer-SQL 中的大模型辅助函数来完成这个需求。\n        注意，输出的 Byzer-SQL 语句需要使用 <_byzer_sql_></_byzer_sql_> 标签包裹起来。\n        \"\"\"\n\n    @byzerllm.prompt()\n    def fetch_schema(self, context: str) -> str:\n        \"\"\"\n        下面是一些基础知识，包括名词术语等等：\n        {{ context }}\n\n        当理解对话内容的时候，需要参考上面的基础知识，然后更好的理解对应的表格schema信息是不是有用。\n        \"\"\"\n\n    def __init__(self, llm:byzerllm.ByzerLLM,rag_config: RAGConfig, byzer_sql_config: ByzerSQLConfig) -> None:\n        self.llm = llm\n        self.schema_rag_client = OpenAI(\n            api_key=rag_config.schema_rag_api_key,\n            base_url=rag_config.schema_rag_base_url,\n        )\n        self.context_rag_client = OpenAI(\n            api_key=rag_config.context_rag_api_key,\n            base_url=rag_config.context_rag_base_url,\n        )\n        self.byzer_sql_url = byzer_sql_config.byzer_sql_url\n\n    def stream_chat_oai(\n        self,\n        conversations,\n        model: Optional[str] = None,\n        role_mapping=None,\n        delta_mode=False,\n        llm_config: Dict[str, Any] = {},\n    ):\n        try:\n            v, c = self._stream_chat_oai(\n                conversations=conversations,\n                model=model,\n                role_mapping=role_mapping,\n                delta_mode=delta_mode,\n                llm_config=llm_config,\n            )\n        except Exception as e:\n            error_msg = f\"Error in stream_chat_oai: {e}\\n{traceback.format_exc()}\"\n            logger.error(error_msg)\n            v, c = [str(e)], []\n\n        return v, c\n\n    def _stream_chat_oai(\n        self,\n        conversations,\n        model: Optional[str] = None,\n        role_mapping=None,\n        delta_mode=False,\n        llm_config: Dict[str, Any] = {},\n    ):\n        logger.info(f\"Starting stream_chat_oai with model: {model}\")\n        logger.debug(f\"Conversations: {conversations}\")\n        logger.debug(f\"LLM Config: {llm_config}\")\n\n        if not model:\n            model = \"deepseek_chat\"\n        logger.info(f\"Using model: {model}\")\n\n        query = conversations[-1][\"content\"]\n\n        if (\n            \"使用四到五个字直接返回这句话的简要主题，不要解释、不要标点、不要语气词、不要多余文本，不要加粗，如果没有主题\"\n            in query\n            or \"简要总结一下对话内容，用作后续的上下文提示 prompt，控制在 200 字以内\"\n            in query\n        ):\n            chunks = self.llm.stream_chat_oai(\n                conversations=conversations,\n                model=model,\n                role_mapping=role_mapping,\n                llm_config=llm_config,\n                delta_mode=True,\n            )\n            return (chunk[0] for chunk in chunks), []\n\n        logger.info(\"Fetching context from context RAG client\")\n        context_query = self.gen_context.prompt(conversations=conversations)\n        context_response = self.context_rag_client.chat.completions.create(\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": json.dumps(\n                        {\"query\": context_query, \"only_contexts\": True},\n                        ensure_ascii=False,\n                    ),\n                }\n            ],\n            model=model,\n        )\n\n        context = context_response.choices[0].message.content\n        new_context = \"\"\n        for msg in context.split(\"\\n\"):\n            if msg.strip():\n                v = json.loads(msg)\n                new_context += v[\"source_code\"]\n        logger.info(f\"context {new_context[0:100]}...\")\n        logger.info(\"Context retrieved successfully\")\n        \n        logger.info(\"Fetching schemas from schema RAG client\")\n        schema_query = self.fetch_schema.prompt(context=new_context)\n\n        response = self.schema_rag_client.chat.completions.create(\n            messages=conversations[0:-1]\n            + [\n                {\n                    \"role\": \"user\",\n                    \"content\": json.dumps(\n                        {\"query\": schema_query, \"only_contexts\": True},\n                        ensure_ascii=False,\n                    ),\n                }\n            ],\n            model=model,\n        )\n        logger.debug(f\"Schema RAG response: {response}\")\n\n        schema_str = response.choices[0].message.content\n        logger.info(\"Schema retrieved successfully\")\n\n        logger.info(\"Generating SQL string\")\n        sql_str = self.gen_sql.with_llm(self.llm).run(\n            conversations=conversations, context=new_context, schema=schema_str\n        )\n\n\n        logger.info(\"SQL string generated\")\n        logger.debug(f\"Generated SQL string: {sql_str}\")\n\n        #MARK\n        logger.info(\"SQL code extracted\")\n        logger.debug(f\"Extracted SQL: {sql}\")\n\n        logger.info(f\"Sending SQL to Byzer SQL URL: {self.byzer_sql_url}\")\n        try:\n            response = requests.post(\n                self.byzer_sql_url,\n                data={\"sql\": sql, \"owner\": \"allwefantasy\"},\n                timeout=600,\n            )\n            response.raise_for_status()\n            result = response.content.decode(\"utf-8\")\n            logger.info(\"SQL execution result decoded\")\n            logger.debug(f\"SQL execution result: {result}\")\n            return (item for item in [result]), []\n        except requests.exceptions.RequestException as e:\n            logger.error(f\"Failed to send SQL to Byzer SQL URL: {e}\")\n            if hasattr(e.response, \"content\"):\n                error_content = e.response.content.decode(\"utf-8\")\n                logger.error(f\"Server response: {error_content}\")\n            else:\n                logger.error(\"No server response available\")\n            raise Exception(f\"Byzer SQL request failed: {str(e)}\") from e\n\n\n##File: /Users/allwefantasy/projects/super_analysis/setup.py\nfrom setuptools import setup, find_packages\n\nsetup(\n    name=\"super-analysis\",\n    version=\"0.1.0\",\n    packages=find_packages(where=\"src\"),\n    package_dir={\"\": \"src\"},\n    install_requires=[\n        \"byzerllm\",\n        \"fastapi\",\n        \"uvicorn\",\n        \"argparse\",\n    ],\n    entry_points={\n        \"console_scripts\": [\n            \"super-analysis.serve=superanalysis.serve.cli:main\",\n            \"super-analysis.convert=superanalysis.serve.convert:main\",\n        ],\n    },\n    author=\"allwefantasy\",\n    author_email=\"allwefantasy@gmail.com\",\n    description=\"A package for super analysis with ByzerLLM\",\n    long_description=open(\"README.md\").read(),\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/yourusername/super-analysis\",\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n    ],\n    python_requires='>=3.6',\n)\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/convert.py\nimport os\nimport argparse\nimport pandas as pd\nimport hashlib\nfrom typing import List, Dict\n\ndef get_csv_files(directory: str) -> List[str]:\n    \"\"\"Recursively get all CSV files in the given directory.\"\"\"\n    csv_files = []\n    for root, _, files in os.walk(directory):\n        for file in files:\n            if file.endswith('.csv'):\n                csv_files.append(os.path.join(root, file))\n    return csv_files\n\ndef extract_schema(df: pd.DataFrame) -> Dict[str, str]:\n    \"\"\"Extract schema from pandas DataFrame.\"\"\"\n    schema = {}\n    for column in df.columns:\n        dtype = str(df[column].dtype)\n        if dtype == 'object':\n            schema[column] = 'STRING'\n        elif dtype.startswith('int'):\n            schema[column] = 'INTEGER'\n        elif dtype.startswith('float'):\n            schema[column] = 'FLOAT'\n        elif dtype == 'bool':\n            schema[column] = 'BOOLEAN'\n        elif dtype.startswith('datetime'):\n            schema[column] = 'TIMESTAMP'\n        else:\n            schema[column] = 'STRING'  # Default to STRING for unknown types\n    return schema\n\ndef generate_schema_text(schema: Dict[str, str], csv_file: str) -> str:\n    \"\"\"Generate schema text from the schema dictionary and include the CSV file name.\"\"\"\n    schema_lines = [f\"CSV File: {csv_file}\\n\"]\n    schema_lines.extend([f\"{field}: {dtype}\" for field, dtype in schema.items()])\n    return \"\\n\".join(schema_lines)\n\ndef save_schema(schema_text: str, csv_file: str, output_dir: str) -> None:\n    \"\"\"Save schema text to a file in the output directory.\"\"\"\n    # Generate MD5 hash of the schema text\n    file_name = hashlib.md5(schema_text.encode()).hexdigest() + '.md'\n    output_path = os.path.join(output_dir, file_name)\n    \n    with open(output_path, 'w') as f:\n        f.write(schema_text)\n    print(f\"Schema for {csv_file} saved to {output_path}\")\n\ndef convert(data_dir: str, doc_dir: str) -> None:\n    \"\"\"Main conversion function.\"\"\"\n    csv_files = get_csv_files(data_dir)\n    \n    for csv_file in csv_files:\n        print(f\"Processing {csv_file}\")\n        df = pd.read_csv(csv_file)\n        schema = extract_schema(df)\n        schema_text = generate_schema_text(schema, csv_file)\n        save_schema(schema_text, csv_file, doc_dir)\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Convert CSV files to schema documents\")\n    parser.add_argument(\"--data_dir\", required=True, help=\"Directory containing CSV files\")\n    parser.add_argument(\"--doc_dir\", required=True, help=\"Output directory for schema documents\")\n    \n    args = parser.parse_args()\n    \n    if not os.path.exists(args.doc_dir):\n        os.makedirs(args.doc_dir)\n    \n    convert(args.data_dir, args.doc_dir)\n\nif __name__ == \"__main__\":\n    main()\n\n     \n\n下面是用户的需求：\n\n使用 TagExtractor 抽取 <_byzer_sql_></_byzer_sql_> 里的内容,里面只会有一个 标签对,在 #MARK 标记处实现\n"}, {"role": "assistant", "content": "To implement the extraction of SQL code from the `<_byzer_sql_>` tags using TagExtractor at the #MARK location, we need to modify the `_stream_chat_oai` method in the `SQLGenerator` class. Here's how we can do that:\n\n```python\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/generator.py\n<<<<<<< SEARCH\n        logger.info(\"SQL string generated\")\n        logger.debug(f\"Generated SQL string: {sql_str}\")\n\n        #MARK\n        logger.info(\"SQL code extracted\")\n        logger.debug(f\"Extracted SQL: {sql}\")\n\n        logger.info(f\"Sending SQL to Byzer SQL URL: {self.byzer_sql_url}\")\n=======\n        logger.info(\"SQL string generated\")\n        logger.debug(f\"Generated SQL string: {sql_str}\")\n\n        #MARK\n        # Extract SQL code from tags\n        tag_extractor = TagExtractor()\n        extracted_tags = tag_extractor.extract(sql_str)\n        if not extracted_tags or '_byzer_sql_' not in extracted_tags:\n            raise ValueError(\"No valid Byzer SQL found in the generated string\")\n        sql = extracted_tags['_byzer_sql_'][0]\n        logger.info(\"SQL code extracted\")\n        logger.debug(f\"Extracted SQL: {sql}\")\n\n        logger.info(f\"Sending SQL to Byzer SQL URL: {self.byzer_sql_url}\")\n>>>>>>> REPLACE\n```\n\nThis change does the following:\n\n1. We create an instance of `TagExtractor()`.\n2. We use the `extract` method of `TagExtractor` to extract the content from the tags in `sql_str`.\n3. We check if any tags were extracted and if the '_byzer_sql_' tag is present.\n4. If the tag is not found, we raise a `ValueError` with an appropriate message.\n5. If the tag is found, we assign the content of the first (and presumably only) '_byzer_sql_' tag to the `sql` variable.\n\nThis implementation ensures that we correctly extract the SQL code from the `<_byzer_sql_>` tags, handling potential errors if the tags are not present or if multiple tag pairs are found (by taking the first one).\n\nWould you like me to explain any part of this implementation or make any further changes?"}], "model": "human", "yaml_file": "actions/124_chat_action.yml"}
{"instruction": "在 @types.py 中添加一个 ModelConfig 里面有llm_model, gen_sql_llm_model 两个参数. 接着在 @cli.py中增加这两个参数,默认没有配置则和served-model-name 保持一致.同步修改SQLGenerator,让他接受这个新参数", "conversations": [{"role": "user", "content": "\n如果你需要生成代码，对于每个需要更改的文件,你需要按 *SEARCH/REPLACE block* 的格式进行生成。\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The opening fence and code language, eg: ```python\n2. The file path alone on a line, starting with \"##File:\" and verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n*SEARCH/REPLACE* blocks will replace *all* matching occurrences.\nInclude enough lines to make the SEARCH blocks unique.\n\nInclude *ALL* the code being searched and replaced!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n下面我们来看一个例子：\n\n当前项目目录结构：\n1. 项目根目录： /tmp/projects/mathweb\n2. 项目子目录/文件列表(类似tree 命令输出)\nflask/\n    app.py\n    templates/\n        index.html\n    static/\n        style.css\n\n用户需求： Change get_factorial() to use math.factorial\n\n回答： To make this change we need to modify `/tmp/projects/mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n用户需求： Refactor hello() into its own file.\n\n回答：To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/hello.py\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/main.py\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n现在让我们开始一个新的任务:\n\n当前项目目录结构：\n1. 项目根目录： /Users/allwefantasy/projects/super_analysis\n2. 项目子目录/文件列表(类似tree 命令输出)：\nsetup.py\n.auto-coder/\n    libs/\n        llm_friendly_packages/\n            src/\n                example1/\n                    main.py\nsrc/\n    superanalysis/\n        serve/\n            convert.py\n            openai_serve.py\n            __init__.py\n            llm_wrapper.py\n            api_server.py\n            cli.py\n        sql/\n            __init__.py\n            types.py\n            generator.py\n下面是一些文件路径以及每个文件对应的源码：\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/types.py\nfrom pydantic import BaseModel\n\nclass RAGConfig(BaseModel):\n    schema_rag_base_url: str = \"\"\n    schema_rag_api_key: str = \"xxxx\"\n\n    context_rag_base_url: str = \"\"\n    context_rag_api_key: str = \"xxxx\"\n\n    \n\n    \n\nclass ByzerSQLConfig(BaseModel):\n    byzer_sql_url: str = \"\"\n    data_dir: str = \"\"\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/cli.py\nimport argparse\nimport byzerllm\nfrom byzerllm.utils.client import ByzerLLM\nfrom superanalysis.serve.api_server import serve, ServerArgs\nfrom superanalysis.sql.types import RAGConfig, ByzerSQLConfig\nfrom superanalysis.sql.generator import SQLGenerator\nfrom superanalysis.serve.llm_wrapper import LLWrapper\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"ByzerLLM API Server\")\n    parser.add_argument(\"--ray-address\", type=str, default=\"auto\", help=\"ray address\")\n    parser.add_argument(\"--host\", type=str, default=\"0.0.0.0\", help=\"Host to bind the server to\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"Port to bind the server to\")\n    parser.add_argument(\"--uvicorn-log-level\", type=str, default=\"info\", help=\"Uvicorn log level\")\n    parser.add_argument(\"--allow-credentials\", action=\"store_true\", help=\"Allow credentials\")\n    parser.add_argument(\"--allowed-origins\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed origins\")\n    parser.add_argument(\"--allowed-methods\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed methods\")\n    parser.add_argument(\"--allowed-headers\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed headers\")\n    parser.add_argument(\"--api-key\", type=str, default=\"\", help=\"API key for authentication\")\n    parser.add_argument(\"--served-model-name\", type=str, required=True, help=\"Name of the model to be served\")\n    parser.add_argument(\"--prompt-template\", type=str, default=\"\", help=\"Prompt template\")\n    parser.add_argument(\"--response-role\", type=str, default=\"assistant\", help=\"Response role\")\n    parser.add_argument(\"--ssl-keyfile\", type=str, default=\"\", help=\"SSL key file\")\n    parser.add_argument(\"--ssl-certfile\", type=str, default=\"\", help=\"SSL certificate file\")\n    \n    # Add new arguments for RAGConfig\n    parser.add_argument(\"--schema-rag-base-url\", type=str, default=\"\", help=\"RAG base URL\")\n    parser.add_argument(\"--schema-rag-api-key\", type=str, default=\"xxxx\", help=\"RAG API key\")\n\n    parser.add_argument(\"--context-rag-base-url\", type=str, default=\"\", help=\"RAG base URL\")\n    parser.add_argument(\"--context-rag-api-key\", type=str, default=\"xxxx\", help=\"RAG API key\")\n    \n    # Add new argument for ByzerSQLConfig\n    parser.add_argument(\"--byzer-sql-url\", type=str, default=\"\", help=\"Byzer SQL URL\")\n    parser.add_argument(\"--data-dir\", type=str, default=\"\", help=\"Data directory\")\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile\n    )\n\n    # Create RAGConfig instance\n    rag_config = RAGConfig(\n        schema_rag_base_url=args.schema_rag_base_url,\n        schema_rag_api_key=args.schema_rag_api_key,\n        context_rag_base_url=args.context_rag_base_url,\n        context_rag_api_key=args.context_rag_api_key\n    )\n\n    # Create ByzerSQLConfig instance\n    byzer_sql_config = ByzerSQLConfig(\n        byzer_sql_url=args.byzer_sql_url,\n        data_dir=args.data_dir\n    )\n\n    byzerllm.connect_cluster(address=args.ray_address)\n    llm = ByzerLLM()\n    llm.setup_default_model_name(args.served_model_name)        \n    llm = byzerllm.ByzerLLM()\n    if not server_args.served_model_name:\n        raise ValueError(\"served_model_name is required\")\n    llm.setup_default_model_name(server_args.served_model_name)\n\n    print(\"RAG Config: \", rag_config)\n    rag = SQLGenerator(llm=llm,rag_config=rag_config, byzer_sql_config=byzer_sql_config)    \n    llm_wrapper = LLWrapper(llm=llm, entry=rag)\n    serve(llm_wrapper, server_args)\n\nif __name__ == \"__main__\":\n    main()\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/__init__.py\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/__init__.py\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/llm_wrapper.py\nfrom typing import Any, Dict, List, Optional, Union,Callable\nfrom byzerllm.utils.client.types import (    \n    LLMFunctionCallResponse,\n    LLMClassResponse,LLMResponse    \n)\nimport pydantic\nfrom byzerllm import ByzerLLM\nfrom byzerllm.utils.client import LLMResponse\nfrom byzerllm.utils.types import SingleOutputMeta\nfrom byzerllm.utils.langutil import asyncfy_with_semaphore\nfrom superanalysis.sql.generator import SQLGenerator\nfrom loguru import logger\n\nclass LLWrapper:\n\n    def __init__(self,llm:ByzerLLM,entry:SQLGenerator):\n        self.entry = entry\n        self.llm = llm\n        \n\n    def chat_oai(self,\n                 conversations,\n                 tools:List[Union[Callable,str]]=[], \n                 tool_choice:Optional[Union[Callable,str]]=None,\n                 execute_tool:bool=False,  \n                 impl_func:Optional[Callable]=None,\n                 execute_impl_func:bool=False,\n                 impl_func_params:Optional[Dict[str,Any]]=None,\n                 func_params:Optional[Dict[str,Any]]=None,\n                 response_class:Optional[Union[pydantic.BaseModel,str]] = None, \n                 response_after_chat:Optional[Union[pydantic.BaseModel,str]] = False,\n                 enable_default_sys_message:bool=True,                 \n                 model:Optional[str] = None,\n                 role_mapping=None,llm_config:Dict[str,Any]={}\n                 )->Union[List[LLMResponse],List[LLMFunctionCallResponse],List[LLMClassResponse]]: \n        res,contexts = self.entry.stream_chat_oai(conversations)\n        s = \"\".join(res)        \n        return [LLMResponse(output=s,metadata={},input=\"\")]\n\n    def stream_chat_oai(self,conversations, \n                        model:Optional[str]=None, \n                        role_mapping=None,\n                        delta_mode=False,\n                        llm_config:Dict[str,Any]={}): \n        res,contexts = self.entry.stream_chat_oai(conversations)        \n        for t in res:                        \n            yield (t,SingleOutputMeta(0,0))\n\n    async def async_stream_chat_oai(self,conversations, \n                        model:Optional[str]=None, \n                        role_mapping=None,\n                        delta_mode=False,\n                        llm_config:Dict[str,Any]={}): \n        res,contexts = self.rag.stream_chat_oai(conversations)                \n        for t in res:                                    \n            yield (t,SingleOutputMeta(0,0))\n                         \n\n    def __getattr__(self, name):        \n        return getattr(self.llm, name)\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/openai_serve.py\nimport argparse\nfrom service_server.api_server import ServerArgs, serve\nfrom service_server.llm_wrapper import LLWrapper\nfrom service_server.summary_rag import SummaryRAG\nimport byzerllm\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"OpenAI Serve\")\n    parser.add_argument(\n        \"--model\",\n        default=\"\",\n    )\n    parser.add_argument(\"--emb_model\", default=\"\")\n    parser.add_argument(\n        \"--ray_address\",\n        default=\"auto\",\n    )\n    parser.add_argument(\"--source_dir\", default=\".\", help=\"\")\n    parser.add_argument(\"--host\", default=\"\", help=\"\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"\")\n    parser.add_argument(\"--uvicorn_log_level\", default=\"info\", help=\"\")\n    parser.add_argument(\"--allow_credentials\", action=\"store_true\", help=\"\")\n    parser.add_argument(\"--allowed_origins\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--allowed_methods\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--allowed_headers\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--api_key\", default=\"\", help=\"\")\n    parser.add_argument(\"--served_model_name\", default=\"\", help=\"\")\n    parser.add_argument(\"--prompt_template\", default=\"\", help=\"\")\n    parser.add_argument(\"--ssl_keyfile\", default=\"\", help=\"\")\n    parser.add_argument(\"--ssl_certfile\", default=\"\", help=\"\")\n    parser.add_argument(\"--response_role\", default=\"assistant\", help=\"\")\n    parser.add_argument(\n        \"--collections\", default=\"\", help=\"Collection name for indexing\"\n    )\n    parser.add_argument(\n        \"--base_dir\",\n        default=\"\",\n        help=\"Path where the processed text embeddings were stored\",\n    )\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name or args.model,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile,\n    )\n    byzerllm.connect_cluster(address=args.ray_address)\n    llm = byzerllm.ByzerLLM()\n    if not server_args.served_model_name:\n        raise ValueError(\"served_model_name is required\")\n    llm.setup_default_model_name(server_args.served_model_name)\n    rag = SummaryRAG(llm=llm)    \n    llm_wrapper = LLWrapper(llm=llm, rag=rag)\n    serve(llm=llm_wrapper, args=server_args)\n\nif __name__ == \"__main__\":\n    main()\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/api_server.py\nimport os\nimport time\nimport uvicorn\nfrom fastapi import Request\nfrom fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import JSONResponse, StreamingResponse, Response\n\nfrom byzerllm.log import init_logger\nfrom byzerllm.utils import random_uuid\nfrom byzerllm.version import __version__ as version\nfrom byzerllm.utils.client import ByzerLLM, LLMRequest\nfrom byzerllm.utils.client.entrypoints.openai.serving_chat import OpenAIServingChat\nfrom byzerllm.utils.client.entrypoints.openai.serving_completion import OpenAIServingCompletion\nfrom byzerllm.utils.client.entrypoints.openai.protocol import (\n    ModelList,\n    ModelCard,\n    ModelPermission,\n    ChatCompletionRequest,\n    ErrorResponse,\n    CompletionRequest,\n    Embeddings,\n    EmbeddingsOutput,\n    EmbeddingsData,\n    EmbeddingsUsage,\n)\nfrom pydantic import BaseModel\nfrom typing import List\n\nlogger = init_logger(__name__)\n\nllm_client: ByzerLLM = None\nopenai_serving_chat: OpenAIServingChat = None\nopenai_serving_completion: OpenAIServingCompletion = None\n\nTIMEOUT_KEEP_ALIVE = 5  # seconds\n# timeout in 10 minutes. Streaming can take longer than 3 min\nTIMEOUT = float(os.environ.get(\"BYZERLLM_APISERVER_HTTP_TIMEOUT\", 600))\n\nrouter_app = FastAPI()\n\n\n@router_app.get(\"/health\")\nasync def health() -> Response:\n    \"\"\"Health check.\"\"\"\n    return Response(status_code=200)\n\n\n@router_app.get(\"/v1/models\")\nasync def show_available_models():\n    models = await openai_serving_chat.show_available_models()\n    return JSONResponse(content=models.model_dump())\n\n\n@router_app.get(\"/version\")\nasync def show_version():\n    return JSONResponse(content={\"version\": version})\n\n\n@router_app.get(\"/v1/models\", response_model=ModelList)\nasync def models() -> ModelList:\n    \"\"\"Show available models. Right now we only have one model.\"\"\"\n    model_cards = [\n        ModelCard(\n            id=\"\",\n            root=\"\",\n            permission=[ModelPermission()]\n        )\n    ]\n    return ModelList(data=model_cards)\n\n\n@router_app.post(\"/v1/completions\")\nasync def create_completion(\n        body: CompletionRequest,\n        request: Request\n):\n    generator = await openai_serving_completion.create_completion(body, request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            status_code=generator.code\n        )\n    if body.stream:\n        return StreamingResponse(\n            content=generator,\n            media_type=\"text/event-stream\"\n        )\n    else:\n        return JSONResponse(content=generator.model_dump())\n\n\n@router_app.post(\"/v1/chat/completions\")\nasync def create_chat_completion(\n        body: ChatCompletionRequest,\n        request: Request,\n):\n    \"\"\"Completion API similar to OpenAI's API.\n\n    See  https://platform.openai.com/docs/api-reference/chat/create\n    for the API specification. This API mimics the OpenAI ChatCompletion API.\n\n    NOTE: Currently we do not support the following features:\n        - function_call (Users should implement this by themselves)\n        - logit_bias (to be supported by vLLM engine)\n    \"\"\"\n    # async with async_timeout.timeout(TIMEOUT):\n\n    generator = await openai_serving_chat.create_chat_completion(body, request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            status_code=generator.code\n        )\n    if body.stream:\n        return StreamingResponse(\n            content=generator,\n            media_type=\"text/event-stream\"\n        )\n    else:\n        return JSONResponse(content=generator.model_dump())\n\n\n@router_app.post(\"/v1/embeddings\")\nasync def embed(body: Embeddings):\n    \"\"\"Given a prompt, the model will return one embedding.\n\n    Returns:\n        A response object with an embedding.\n    \"\"\"\n    embedding_id = f\"embed-{random_uuid()}\"\n\n    results_list = llm_client.emb(body.model, request=LLMRequest(instruction=body.input))\n    tokens = 0\n\n    return EmbeddingsOutput(\n        data=[\n            EmbeddingsData(\n                embedding=results.output,\n                index=i,\n                object=\"embedding\",\n            )\n            for i, results in enumerate(results_list)\n        ],\n        id=embedding_id,\n        object=\"list\",\n        created=int(time.time()),\n        model=body.model,\n        usage=EmbeddingsUsage(\n            prompt_tokens=tokens,\n            total_tokens=tokens,\n        ),\n    )\n\nclass ServerArgs(BaseModel):\n    host: str = None\n    port: int = 8000\n    uvicorn_log_level: str = \"info\"\n    allow_credentials: bool = False\n    allowed_origins: List[str] = [\"*\"]  \n    allowed_methods: List[str] = [\"*\"]\n    allowed_headers: List[str] = [\"*\"]\n    api_key: str = None\n    served_model_name: str = None\n    prompt_template: str = None\n    response_role: str = \"assistant\"\n    ssl_keyfile: str = None\n    ssl_certfile: str = None    \n\ndef serve(llm:ByzerLLM, args: ServerArgs):\n    \n    logger.info(f\"ByzerLLM API server version {version}\")\n    logger.info(f\"args: {args}\")\n\n    router_app.add_middleware(\n        CORSMiddleware,\n        allow_origins=args.allowed_origins,\n        allow_credentials=args.allow_credentials,\n        allow_methods=args.allowed_methods,\n        allow_headers=args.allowed_headers,\n    )\n    \n    if token := os.environ.get(\"BYZERLLM_API_KEY\") or args.api_key:\n\n        @router_app.middleware(\"http\")\n        async def authentication(request: Request, call_next):\n            if not request.url.path.startswith(\"/v1\"):\n                return await call_next(request)\n            if request.headers.get(\"Authorization\") != \"Bearer \" + token:\n                return JSONResponse(\n                    content={\"error\": \"Unauthorized\"},\n                    status_code=401\n                )\n            return await call_next(request)\n\n    # Register labels for metrics\n    # add_global_metrics_labels(model_name=engine_args.model)\n    global llm_client\n    llm_client = llm\n    \n    global openai_serving_chat\n    openai_serving_chat = OpenAIServingChat(\n        llm_client=llm_client,\n        response_role=args.response_role,\n        server_model_name=args.served_model_name,\n        prompt_template=args.prompt_template\n    )\n    global openai_serving_completion\n    openai_serving_completion = OpenAIServingCompletion(\n        llm_client=llm_client,\n        server_model_name=args.served_model_name,\n        prompt_template=args.prompt_template\n    )\n\n    uvicorn.run(\n        router_app,\n        host=args.host,\n        port=args.port,\n        log_level=args.uvicorn_log_level,\n        timeout_keep_alive=TIMEOUT_KEEP_ALIVE,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile\n    )\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/generator.py\nfrom typing import Optional, Dict, Any, List\nfrom openai import OpenAI\nimport json\nfrom .types import RAGConfig, ByzerSQLConfig\nfrom byzerllm.utils.client import code_utils\nimport byzerllm\nimport requests\nfrom loguru import logger\nimport traceback\nfrom byzerllm.utils.nontext import TagExtractor\n\n\nclass SQLGenerator:\n\n    @byzerllm.prompt()\n    def gen_context(self, conversations: List[Dict[str, str]]) -> str:\n        \"\"\"\n        对话内容：\n        {% for msg in conversations %}\n        <{{ msg.role }}>: {{ msg.content }}\n        {% endfor %}\n\n        根据提供的对话内容，理解用户最后的问题，然后提取可能会用到的 Byzer-SQL 相关的知识。\n        \"\"\"\n\n    @byzerllm.prompt()\n    def gen_sql(\n        self,\n        conversations: List[Dict[str, str]],\n        context: str,\n        schema: str,\n        model: str = \"deepseek_chat\",\n    ) -> str:\n        \"\"\"\n        相关知识：\n        {{ context }}\n\n        Schema信息：\n        {{ schema }}\n\n        对话内容：\n        {% for msg in conversations %}\n        <{{ msg.role }}>: {{ msg.content }}\n        {% endfor %}\n\n        模型UDF函数：\n        {{ model }}\n\n        根据提供的相关知识以及对话内容，理解用户最后的问题，然后根据看到的schema 信息，生成 Byzer-SQL 语句。\n        Byzer-SQL 首先需要用 load 语法加载 schema 中的路径映射成表格，然后根据表格的信息生成对应的 Byzer-SQL 语句。\n\n        特别需要关注的是:\n        1. 用户的很多需求是要对表格的文本内容做自然语言处理，你需要使用 Byzer-SQL 中的大模型辅助函数来完成这个需求。\n        2. 使用 `llm_result` 函数处理 LLM 模型函数的响应,`llm_result` 函数得到的是一段文本，虽然我们要求模型函数只输出特定的文本，但考虑到大模型会有一些额外的字符输出，不会只是输出我们要求的文字，所以通过 `like` 来进行匹配。\n        3. 注意 Byzer-SQL 的语法，比如select 语句最后一定需要有 as <table_name>。\n        3. 输出的 Byzer-SQL 语句务必使用 <_byzer_sql_></_byzer_sql_> 标签包裹起来。\n        \"\"\"\n\n    @byzerllm.prompt()\n    def fetch_schema(self, context: str) -> str:\n        \"\"\"\n        下面是一些基础知识，包括名词术语等等：\n        {{ context }}\n\n        当理解对话内容的时候，需要参考上面的基础知识，然后更好的理解对应的表格schema信息是不是有用。\n        \"\"\"\n\n    def __init__(\n        self,\n        llm: byzerllm.ByzerLLM,\n        rag_config: RAGConfig,\n        byzer_sql_config: ByzerSQLConfig,\n    ) -> None:\n        self.llm = llm\n        self.schema_rag_client = OpenAI(\n            api_key=rag_config.schema_rag_api_key,\n            base_url=rag_config.schema_rag_base_url,\n        )\n        self.context_rag_client = OpenAI(\n            api_key=rag_config.context_rag_api_key,\n            base_url=rag_config.context_rag_base_url,\n        )\n        self.byzer_sql_url = byzer_sql_config.byzer_sql_url\n\n    def stream_chat_oai(\n        self,\n        conversations,\n        model: Optional[str] = None,\n        role_mapping=None,\n        delta_mode=False,\n        llm_config: Dict[str, Any] = {},\n    ):\n        try:\n            v, c = self._stream_chat_oai(\n                conversations=conversations,\n                model=model,\n                role_mapping=role_mapping,\n                delta_mode=delta_mode,\n                llm_config=llm_config,\n            )\n        except Exception as e:\n            error_msg = f\"Error in stream_chat_oai: {e}\\n{traceback.format_exc()}\"\n            logger.error(error_msg)\n            v, c = [str(e)], []\n\n        return v, c\n\n    def _stream_chat_oai(\n        self,\n        conversations,\n        model: Optional[str] = None,\n        role_mapping=None,\n        delta_mode=False,\n        llm_config: Dict[str, Any] = {},\n    ):\n        logger.info(f\"Starting stream_chat_oai with model: {model}\")\n        logger.debug(f\"Conversations: {conversations}\")\n        logger.debug(f\"LLM Config: {llm_config}\")\n\n        if not model:\n            model = \"deepseek_chat\"\n        logger.info(f\"Using model: {model}\")\n\n        query = conversations[-1][\"content\"]\n\n        if (\n            \"使用四到五个字直接返回这句话的简要主题，不要解释、不要标点、不要语气词、不要多余文本，不要加粗，如果没有主题\"\n            in query\n            or \"简要总结一下对话内容，用作后续的上下文提示 prompt，控制在 200 字以内\"\n            in query\n        ):\n            chunks = self.llm.stream_chat_oai(\n                conversations=conversations,\n                model=model,\n                role_mapping=role_mapping,\n                llm_config=llm_config,\n                delta_mode=True,\n            )\n            return (chunk[0] for chunk in chunks), []\n\n        logger.info(\"Fetching context from context RAG client\")\n        context_query = self.gen_context.prompt(conversations=conversations)\n        context_response = self.context_rag_client.chat.completions.create(\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": json.dumps(\n                        {\"query\": context_query, \"only_contexts\": True},\n                        ensure_ascii=False,\n                    ),\n                }\n            ],\n            model=model,\n        )\n\n        context = context_response.choices[0].message.content\n        new_context = \"\"\n        for msg in context.split(\"\\n\"):\n            if msg.strip():\n                v = json.loads(msg)\n                new_context += v[\"source_code\"]\n        logger.info(f\"context {new_context[0:100]}...\")\n        logger.info(\"Context retrieved successfully\")\n\n        logger.info(\"Fetching schemas from schema RAG client\")\n        schema_query = self.fetch_schema.prompt(context=new_context)\n\n        response = self.schema_rag_client.chat.completions.create(\n            messages=conversations[0:-1]\n            + [\n                {\n                    \"role\": \"user\",\n                    \"content\": json.dumps(\n                        {\"query\": schema_query, \"only_contexts\": True},\n                        ensure_ascii=False,\n                    ),\n                }\n            ],\n            model=model,\n        )\n        logger.debug(f\"Schema RAG response: {response}\")\n\n        schema_str = response.choices[0].message.content\n        logger.info(\"Schema retrieved successfully\")\n\n        logger.info(\"Generating SQL string\")\n        sql_str = self.gen_sql.with_llm(self.llm).run(\n            conversations=conversations, context=new_context, schema=schema_str\n        )\n\n        logger.info(\"SQL string generated\")\n        logger.debug(f\"Generated SQL string: {sql_str}\")\n\n        # Extract SQL code from tags\n        tag_extractor = TagExtractor(sql_str)\n        extracted_tags = tag_extractor.extract()\n        sql = extracted_tags.content[0].content\n        logger.info(f\"Extracted SQL: {sql}\")\n\n        logger.info(f\"Sending SQL to Byzer SQL URL: {self.byzer_sql_url}\")\n        try:\n            response = requests.post(\n                self.byzer_sql_url,\n                data={\n                    \"sql\": sql,\n                    \"owner\": \"hello\",\n                    \"sessionPerUser\": True,\n                    \"sessionPerRequest\": True,\n                    \"includeSchema\": True,\n                },                \n            )\n            response.raise_for_status()\n            result = response.content.decode(\"utf-8\")\n            logger.info(\"SQL execution result decoded\")\n            logger.debug(f\"SQL execution result: {result}\")\n            return (item for item in [result]), []\n        except requests.exceptions.RequestException as e:\n            logger.error(f\"Failed to send SQL to Byzer SQL URL: {e}\")\n            if hasattr(e.response, \"content\"):\n                error_content = e.response.content.decode(\"utf-8\")\n                logger.error(f\"Server response: {error_content}\")\n            else:\n                logger.error(\"No server response available\")\n            raise Exception(f\"Byzer SQL request failed: {str(e)}\") from e\n\n\n##File: /Users/allwefantasy/projects/super_analysis/setup.py\nfrom setuptools import setup, find_packages\n\nsetup(\n    name=\"super-analysis\",\n    version=\"0.1.0\",\n    packages=find_packages(where=\"src\"),\n    package_dir={\"\": \"src\"},\n    install_requires=[\n        \"byzerllm\",\n        \"fastapi\",\n        \"uvicorn\",\n        \"argparse\",\n    ],\n    entry_points={\n        \"console_scripts\": [\n            \"super-analysis.serve=superanalysis.serve.cli:main\",\n            \"super-analysis.convert=superanalysis.serve.convert:main\",\n        ],\n    },\n    author=\"allwefantasy\",\n    author_email=\"allwefantasy@gmail.com\",\n    description=\"A package for super analysis with ByzerLLM\",\n    long_description=open(\"README.md\").read(),\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/yourusername/super-analysis\",\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n    ],\n    python_requires='>=3.6',\n)\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/convert.py\nimport os\nimport argparse\nimport pandas as pd\nimport hashlib\nfrom typing import List, Dict\n\ndef get_csv_files(directory: str) -> List[str]:\n    \"\"\"Recursively get all CSV files in the given directory.\"\"\"\n    csv_files = []\n    for root, _, files in os.walk(directory):\n        for file in files:\n            if file.endswith('.csv'):\n                csv_files.append(os.path.join(root, file))\n    return csv_files\n\ndef extract_schema(df: pd.DataFrame) -> Dict[str, str]:\n    \"\"\"Extract schema from pandas DataFrame.\"\"\"\n    schema = {}\n    for column in df.columns:\n        dtype = str(df[column].dtype)\n        if dtype == 'object':\n            schema[column] = 'STRING'\n        elif dtype.startswith('int'):\n            schema[column] = 'INTEGER'\n        elif dtype.startswith('float'):\n            schema[column] = 'FLOAT'\n        elif dtype == 'bool':\n            schema[column] = 'BOOLEAN'\n        elif dtype.startswith('datetime'):\n            schema[column] = 'TIMESTAMP'\n        else:\n            schema[column] = 'STRING'  # Default to STRING for unknown types\n    return schema\n\ndef generate_schema_text(schema: Dict[str, str], csv_file: str) -> str:\n    \"\"\"Generate schema text from the schema dictionary and include the CSV file name.\"\"\"\n    schema_lines = [f\"CSV File: {csv_file}\\n\"]\n    schema_lines.extend([f\"{field}: {dtype}\" for field, dtype in schema.items()])\n    return \"\\n\".join(schema_lines)\n\ndef save_schema(schema_text: str, csv_file: str, output_dir: str) -> None:\n    \"\"\"Save schema text to a file in the output directory.\"\"\"\n    # Generate MD5 hash of the schema text\n    file_name = hashlib.md5(schema_text.encode()).hexdigest() + '.md'\n    output_path = os.path.join(output_dir, file_name)\n    \n    with open(output_path, 'w') as f:\n        f.write(schema_text)\n    print(f\"Schema for {csv_file} saved to {output_path}\")\n\ndef convert(data_dir: str, doc_dir: str) -> None:\n    \"\"\"Main conversion function.\"\"\"\n    csv_files = get_csv_files(data_dir)\n    \n    for csv_file in csv_files:\n        print(f\"Processing {csv_file}\")\n        df = pd.read_csv(csv_file)\n        schema = extract_schema(df)\n        schema_text = generate_schema_text(schema, csv_file)\n        save_schema(schema_text, csv_file, doc_dir)\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Convert CSV files to schema documents\")\n    parser.add_argument(\"--data_dir\", required=True, help=\"Directory containing CSV files\")\n    parser.add_argument(\"--doc_dir\", required=True, help=\"Output directory for schema documents\")\n    \n    args = parser.parse_args()\n    \n    if not os.path.exists(args.doc_dir):\n        os.makedirs(args.doc_dir)\n    \n    convert(args.data_dir, args.doc_dir)\n\nif __name__ == \"__main__\":\n    main()\n\n     \n\n下面是用户的需求：\n\n在 @types.py 中添加一个 ModelConfig 里面有llm_model, gen_sql_llm_model 两个参数. 接着在 @cli.py中增加这两个参数,默认没有配置则和served-model-name 保持一致.同步修改SQLGenerator,让他接受这个新参数\n"}, {"role": "assistant", "content": "根据您的需求，我们需要修改以下文件：\n\n1. `/Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/types.py`\n2. `/Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/cli.py`\n3. `/Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/generator.py`\n\n下面是相应的修改：\n\n```python\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/types.py\n<<<<<<< SEARCH\nfrom pydantic import BaseModel\n\nclass RAGConfig(BaseModel):\n    schema_rag_base_url: str = \"\"\n    schema_rag_api_key: str = \"xxxx\"\n\n    context_rag_base_url: str = \"\"\n    context_rag_api_key: str = \"xxxx\"\n\n    \n\n    \n\nclass ByzerSQLConfig(BaseModel):\n    byzer_sql_url: str = \"\"\n    data_dir: str = \"\"\n=======\nfrom pydantic import BaseModel\n\nclass RAGConfig(BaseModel):\n    schema_rag_base_url: str = \"\"\n    schema_rag_api_key: str = \"xxxx\"\n\n    context_rag_base_url: str = \"\"\n    context_rag_api_key: str = \"xxxx\"\n\nclass ByzerSQLConfig(BaseModel):\n    byzer_sql_url: str = \"\"\n    data_dir: str = \"\"\n\nclass ModelConfig(BaseModel):\n    llm_model: str = \"\"\n    gen_sql_llm_model: str = \"\"\n>>>>>>> REPLACE\n```\n\n```python\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/cli.py\n<<<<<<< SEARCH\nfrom superanalysis.sql.types import RAGConfig, ByzerSQLConfig\n=======\nfrom superanalysis.sql.types import RAGConfig, ByzerSQLConfig, ModelConfig\n>>>>>>> REPLACE\n```\n\n```python\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/cli.py\n<<<<<<< SEARCH\n    parser.add_argument(\"--data-dir\", type=str, default=\"\", help=\"Data directory\")\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n=======\n    parser.add_argument(\"--data-dir\", type=str, default=\"\", help=\"Data directory\")\n    parser.add_argument(\"--llm-model\", type=str, default=\"\", help=\"LLM model for general use\")\n    parser.add_argument(\"--gen-sql-llm-model\", type=str, default=\"\", help=\"LLM model for SQL generation\")\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n>>>>>>> REPLACE\n```\n\n```python\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/cli.py\n<<<<<<< SEARCH\n    byzer_sql_config = ByzerSQLConfig(\n        byzer_sql_url=args.byzer_sql_url,\n        data_dir=args.data_dir\n    )\n\n    byzerllm.connect_cluster(address=args.ray_address)\n    llm = ByzerLLM()\n    llm.setup_default_model_name(args.served_model_name)        \n    llm = byzerllm.ByzerLLM()\n    if not server_args.served_model_name:\n        raise ValueError(\"served_model_name is required\")\n    llm.setup_default_model_name(server_args.served_model_name)\n\n    print(\"RAG Config: \", rag_config)\n    rag = SQLGenerator(llm=llm,rag_config=rag_config, byzer_sql_config=byzer_sql_config)    \n=======\n    byzer_sql_config = ByzerSQLConfig(\n        byzer_sql_url=args.byzer_sql_url,\n        data_dir=args.data_dir\n    )\n\n    model_config = ModelConfig(\n        llm_model=args.llm_model or args.served_model_name,\n        gen_sql_llm_model=args.gen_sql_llm_model or args.served_model_name\n    )\n\n    byzerllm.connect_cluster(address=args.ray_address)\n    llm = ByzerLLM()\n    llm.setup_default_model_name(args.served_model_name)        \n    llm = byzerllm.ByzerLLM()\n    if not server_args.served_model_name:\n        raise ValueError(\"served_model_name is required\")\n    llm.setup_default_model_name(server_args.served_model_name)\n\n    print(\"RAG Config: \", rag_config)\n    print(\"Model Config: \", model_config)\n    rag = SQLGenerator(llm=llm, rag_config=rag_config, byzer_sql_config=byzer_sql_config, model_config=model_config)    \n>>>>>>> REPLACE\n```\n\n```python\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/generator.py\n<<<<<<< SEARCH\nfrom .types import RAGConfig, ByzerSQLConfig\n=======\nfrom .types import RAGConfig, ByzerSQLConfig, ModelConfig\n>>>>>>> REPLACE\n```\n\n```python\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/generator.py\n<<<<<<< SEARCH\n    def __init__(\n        self,\n        llm: byzerllm.ByzerLLM,\n        rag_config: RAGConfig,\n        byzer_sql_config: ByzerSQLConfig,\n    ) -> None:\n        self.llm = llm\n        self.schema_rag_client = OpenAI(\n            api_key=rag_config.schema_rag_api_key,\n            base_url=rag_config.schema_rag_base_url,\n        )\n        self.context_rag_client = OpenAI(\n            api_key=rag_config.context_rag_api_key,\n            base_url=rag_config.context_rag_base_url,\n        )\n        self.byzer_sql_url = byzer_sql_config.byzer_sql_url\n=======\n    def __init__(\n        self,\n        llm: byzerllm.ByzerLLM,\n        rag_config: RAGConfig,\n        byzer_sql_config: ByzerSQLConfig,\n        model_config: ModelConfig,\n    ) -> None:\n        self.llm = llm\n        self.schema_rag_client = OpenAI(\n            api_key=rag_config.schema_rag_api_key,\n            base_url=rag_config.schema_rag_base_url,\n        )\n        self.context_rag_client = OpenAI(\n            api_key=rag_config.context_rag_api_key,\n            base_url=rag_config.context_rag_base_url,\n        )\n        self.byzer_sql_url = byzer_sql_config.byzer_sql_url\n        self.llm_model = model_config.llm_model\n        self.gen_sql_llm_model = model_config.gen_sql_llm_model\n>>>>>>> REPLACE\n```\n\n```python\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/generator.py\n<<<<<<< SEARCH\n        if not model:\n            model = \"deepseek_chat\"\n        logger.info(f\"Using model: {model}\")\n=======\n        if not model:\n            model = self.llm_model or \"deepseek_chat\"\n        logger.info(f\"Using model: {model}\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/generator.py\n<<<<<<< SEARCH\n        sql_str = self.gen_sql.with_llm(self.llm).run(\n            conversations=conversations, context=new_context, schema=schema_str\n        )\n=======\n        sql_str = self.gen_sql.with_llm(self.llm).run(\n            conversations=conversations, context=new_context, schema=schema_str,\n            model=self.gen_sql_llm_model or model\n        )\n>>>>>>> REPLACE\n```\n\n这些修改添加了新的 `ModelConfig` 类，并在 CLI 中添加了相应的参数。同时，`SQLGenerator` 类也被更新以接受和使用这些新的配置。\n\n请注意，这些修改假设您希望在 `gen_sql` 函数中使用特定的模型。如果不是这种情况，您可能需要进一步调整代码以适应您的具体需求。"}], "model": "human", "yaml_file": "actions/125_chat_action.yml"}
{"instruction": "@convert.py 添加一个参数 --include-rows-num 默认为 0,如果用户设置了该值,则在生成schema 的带上相应行数的示例数据", "conversations": [{"role": "user", "content": "\n如果你需要生成代码，对于每个需要更改的文件,你需要按 *SEARCH/REPLACE block* 的格式进行生成。\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The opening fence and code language, eg: ```python\n2. The file path alone on a line, starting with \"##File:\" and verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n*SEARCH/REPLACE* blocks will replace *all* matching occurrences.\nInclude enough lines to make the SEARCH blocks unique.\n\nInclude *ALL* the code being searched and replaced!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n下面我们来看一个例子：\n\n当前项目目录结构：\n1. 项目根目录： /tmp/projects/mathweb\n2. 项目子目录/文件列表(类似tree 命令输出)\nflask/\n    app.py\n    templates/\n        index.html\n    static/\n        style.css\n\n用户需求： Change get_factorial() to use math.factorial\n\n回答： To make this change we need to modify `/tmp/projects/mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n用户需求： Refactor hello() into its own file.\n\n回答：To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/hello.py\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/main.py\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n现在让我们开始一个新的任务:\n\n当前项目目录结构：\n1. 项目根目录： /Users/allwefantasy/projects/super_analysis\n2. 项目子目录/文件列表(类似tree 命令输出)：\nsetup.py\n.auto-coder/\n    libs/\n        llm_friendly_packages/\n            src/\n                example1/\n                    main.py\nsrc/\n    superanalysis/\n        serve/\n            convert.py\n            openai_serve.py\n            __init__.py\n            llm_wrapper.py\n            api_server.py\n            cli.py\n        sql/\n            __init__.py\n            types.py\n            generator.py\n下面是一些文件路径以及每个文件对应的源码：\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/types.py\nfrom pydantic import BaseModel\n\nclass RAGConfig(BaseModel):\n    schema_rag_base_url: str = \"\"\n    schema_rag_api_key: str = \"xxxx\"\n\n    context_rag_base_url: str = \"\"\n    context_rag_api_key: str = \"xxxx\"\n\nclass ByzerSQLConfig(BaseModel):\n    byzer_sql_url: str = \"\"\n    data_dir: str = \"\"\n\nclass ModelConfig(BaseModel):\n    llm_model: str = \"\"\n    gen_sql_llm_model: str = \"\"\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/cli.py\nimport argparse\nimport byzerllm\nfrom byzerllm.utils.client import ByzerLLM\nfrom superanalysis.serve.api_server import serve, ServerArgs\nfrom superanalysis.sql.types import RAGConfig, ByzerSQLConfig, ModelConfig\nfrom superanalysis.sql.generator import SQLGenerator\nfrom superanalysis.serve.llm_wrapper import LLWrapper\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"ByzerLLM API Server\")\n    parser.add_argument(\"--ray-address\", type=str, default=\"auto\", help=\"ray address\")\n    parser.add_argument(\n        \"--host\", type=str, default=\"0.0.0.0\", help=\"Host to bind the server to\"\n    )\n    parser.add_argument(\n        \"--port\", type=int, default=8000, help=\"Port to bind the server to\"\n    )\n    parser.add_argument(\n        \"--uvicorn-log-level\", type=str, default=\"info\", help=\"Uvicorn log level\"\n    )\n    parser.add_argument(\n        \"--allow-credentials\", action=\"store_true\", help=\"Allow credentials\"\n    )\n    parser.add_argument(\n        \"--allowed-origins\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed origins\"\n    )\n    parser.add_argument(\n        \"--allowed-methods\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed methods\"\n    )\n    parser.add_argument(\n        \"--allowed-headers\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed headers\"\n    )\n    parser.add_argument(\n        \"--api-key\", type=str, default=\"\", help=\"API key for authentication\"\n    )\n    parser.add_argument(\n        \"--served-model-name\",\n        type=str,\n        required=True,\n        help=\"Name of the model to be served\",\n    )\n    parser.add_argument(\n        \"--prompt-template\", type=str, default=\"\", help=\"Prompt template\"\n    )\n    parser.add_argument(\n        \"--response-role\", type=str, default=\"assistant\", help=\"Response role\"\n    )\n    parser.add_argument(\"--ssl-keyfile\", type=str, default=\"\", help=\"SSL key file\")\n    parser.add_argument(\n        \"--ssl-certfile\", type=str, default=\"\", help=\"SSL certificate file\"\n    )\n\n    # Add new arguments for RAGConfig\n    parser.add_argument(\n        \"--schema-rag-base-url\", type=str, default=\"\", help=\"RAG base URL\"\n    )\n    parser.add_argument(\n        \"--schema-rag-api-key\", type=str, default=\"xxxx\", help=\"RAG API key\"\n    )\n\n    parser.add_argument(\n        \"--context-rag-base-url\", type=str, default=\"\", help=\"RAG base URL\"\n    )\n    parser.add_argument(\n        \"--context-rag-api-key\", type=str, default=\"xxxx\", help=\"RAG API key\"\n    )\n\n    # Add new argument for ByzerSQLConfig\n    parser.add_argument(\"--byzer-sql-url\", type=str, default=\"\", help=\"Byzer SQL URL\")\n    parser.add_argument(\"--data-dir\", type=str, default=\"\", help=\"Data directory\")\n    parser.add_argument(\n        \"--llm-model\", type=str, default=\"\", help=\"LLM model for general use\"\n    )\n    parser.add_argument(\n        \"--gen-sql-llm-model\", type=str, default=\"\", help=\"LLM model for SQL generation\"\n    )\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile,\n    )\n\n    # Create RAGConfig instance\n    rag_config = RAGConfig(\n        schema_rag_base_url=args.schema_rag_base_url,\n        schema_rag_api_key=args.schema_rag_api_key,\n        context_rag_base_url=args.context_rag_base_url,\n        context_rag_api_key=args.context_rag_api_key,\n    )\n\n    # Create ByzerSQLConfig instance\n    byzer_sql_config = ByzerSQLConfig(\n        byzer_sql_url=args.byzer_sql_url, data_dir=args.data_dir\n    )\n\n    model_config = ModelConfig(\n        llm_model=args.llm_model or args.served_model_name,\n        gen_sql_llm_model=args.gen_sql_llm_model or args.served_model_name,\n    )\n\n    byzerllm.connect_cluster(address=args.ray_address)\n    llm = ByzerLLM()\n    llm.setup_default_model_name(args.served_model_name)\n    llm = byzerllm.ByzerLLM()\n    if not server_args.served_model_name:\n        raise ValueError(\"served_model_name is required\")\n    llm.setup_default_model_name(server_args.served_model_name)\n\n    print(\"RAG Config: \", rag_config)\n    print(\"Model Config: \", model_config)\n    rag = SQLGenerator(\n        llm=llm,\n        rag_config=rag_config,\n        byzer_sql_config=byzer_sql_config,\n        model_config=model_config,\n    )\n    llm_wrapper = LLWrapper(llm=llm, entry=rag)\n    serve(llm_wrapper, server_args)\n\n\nif __name__ == \"__main__\":\n    main()\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/__init__.py\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/__init__.py\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/llm_wrapper.py\nfrom typing import Any, Dict, List, Optional, Union,Callable\nfrom byzerllm.utils.client.types import (    \n    LLMFunctionCallResponse,\n    LLMClassResponse,LLMResponse    \n)\nimport pydantic\nfrom byzerllm import ByzerLLM\nfrom byzerllm.utils.client import LLMResponse\nfrom byzerllm.utils.types import SingleOutputMeta\nfrom byzerllm.utils.langutil import asyncfy_with_semaphore\nfrom superanalysis.sql.generator import SQLGenerator\nfrom loguru import logger\n\nclass LLWrapper:\n\n    def __init__(self,llm:ByzerLLM,entry:SQLGenerator):\n        self.entry = entry\n        self.llm = llm\n        \n\n    def chat_oai(self,\n                 conversations,\n                 tools:List[Union[Callable,str]]=[], \n                 tool_choice:Optional[Union[Callable,str]]=None,\n                 execute_tool:bool=False,  \n                 impl_func:Optional[Callable]=None,\n                 execute_impl_func:bool=False,\n                 impl_func_params:Optional[Dict[str,Any]]=None,\n                 func_params:Optional[Dict[str,Any]]=None,\n                 response_class:Optional[Union[pydantic.BaseModel,str]] = None, \n                 response_after_chat:Optional[Union[pydantic.BaseModel,str]] = False,\n                 enable_default_sys_message:bool=True,                 \n                 model:Optional[str] = None,\n                 role_mapping=None,llm_config:Dict[str,Any]={}\n                 )->Union[List[LLMResponse],List[LLMFunctionCallResponse],List[LLMClassResponse]]: \n        res,contexts = self.entry.stream_chat_oai(conversations)\n        s = \"\".join(res)        \n        return [LLMResponse(output=s,metadata={},input=\"\")]\n\n    def stream_chat_oai(self,conversations, \n                        model:Optional[str]=None, \n                        role_mapping=None,\n                        delta_mode=False,\n                        llm_config:Dict[str,Any]={}): \n        res,contexts = self.entry.stream_chat_oai(conversations)        \n        for t in res:                        \n            yield (t,SingleOutputMeta(0,0))\n\n    async def async_stream_chat_oai(self,conversations, \n                        model:Optional[str]=None, \n                        role_mapping=None,\n                        delta_mode=False,\n                        llm_config:Dict[str,Any]={}): \n        res,contexts = self.rag.stream_chat_oai(conversations)                \n        for t in res:                                    \n            yield (t,SingleOutputMeta(0,0))\n                         \n\n    def __getattr__(self, name):        \n        return getattr(self.llm, name)\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/openai_serve.py\nimport argparse\nfrom service_server.api_server import ServerArgs, serve\nfrom service_server.llm_wrapper import LLWrapper\nfrom service_server.summary_rag import SummaryRAG\nimport byzerllm\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"OpenAI Serve\")\n    parser.add_argument(\n        \"--model\",\n        default=\"\",\n    )\n    parser.add_argument(\"--emb_model\", default=\"\")\n    parser.add_argument(\n        \"--ray_address\",\n        default=\"auto\",\n    )\n    parser.add_argument(\"--source_dir\", default=\".\", help=\"\")\n    parser.add_argument(\"--host\", default=\"\", help=\"\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"\")\n    parser.add_argument(\"--uvicorn_log_level\", default=\"info\", help=\"\")\n    parser.add_argument(\"--allow_credentials\", action=\"store_true\", help=\"\")\n    parser.add_argument(\"--allowed_origins\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--allowed_methods\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--allowed_headers\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--api_key\", default=\"\", help=\"\")\n    parser.add_argument(\"--served_model_name\", default=\"\", help=\"\")\n    parser.add_argument(\"--prompt_template\", default=\"\", help=\"\")\n    parser.add_argument(\"--ssl_keyfile\", default=\"\", help=\"\")\n    parser.add_argument(\"--ssl_certfile\", default=\"\", help=\"\")\n    parser.add_argument(\"--response_role\", default=\"assistant\", help=\"\")\n    parser.add_argument(\n        \"--collections\", default=\"\", help=\"Collection name for indexing\"\n    )\n    parser.add_argument(\n        \"--base_dir\",\n        default=\"\",\n        help=\"Path where the processed text embeddings were stored\",\n    )\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name or args.model,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile,\n    )\n    byzerllm.connect_cluster(address=args.ray_address)\n    llm = byzerllm.ByzerLLM()\n    if not server_args.served_model_name:\n        raise ValueError(\"served_model_name is required\")\n    llm.setup_default_model_name(server_args.served_model_name)\n    rag = SummaryRAG(llm=llm)    \n    llm_wrapper = LLWrapper(llm=llm, rag=rag)\n    serve(llm=llm_wrapper, args=server_args)\n\nif __name__ == \"__main__\":\n    main()\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/api_server.py\nimport os\nimport time\nimport uvicorn\nfrom fastapi import Request\nfrom fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import JSONResponse, StreamingResponse, Response\n\nfrom byzerllm.log import init_logger\nfrom byzerllm.utils import random_uuid\nfrom byzerllm.version import __version__ as version\nfrom byzerllm.utils.client import ByzerLLM, LLMRequest\nfrom byzerllm.utils.client.entrypoints.openai.serving_chat import OpenAIServingChat\nfrom byzerllm.utils.client.entrypoints.openai.serving_completion import OpenAIServingCompletion\nfrom byzerllm.utils.client.entrypoints.openai.protocol import (\n    ModelList,\n    ModelCard,\n    ModelPermission,\n    ChatCompletionRequest,\n    ErrorResponse,\n    CompletionRequest,\n    Embeddings,\n    EmbeddingsOutput,\n    EmbeddingsData,\n    EmbeddingsUsage,\n)\nfrom pydantic import BaseModel\nfrom typing import List\n\nlogger = init_logger(__name__)\n\nllm_client: ByzerLLM = None\nopenai_serving_chat: OpenAIServingChat = None\nopenai_serving_completion: OpenAIServingCompletion = None\n\nTIMEOUT_KEEP_ALIVE = 5  # seconds\n# timeout in 10 minutes. Streaming can take longer than 3 min\nTIMEOUT = float(os.environ.get(\"BYZERLLM_APISERVER_HTTP_TIMEOUT\", 600))\n\nrouter_app = FastAPI()\n\n\n@router_app.get(\"/health\")\nasync def health() -> Response:\n    \"\"\"Health check.\"\"\"\n    return Response(status_code=200)\n\n\n@router_app.get(\"/v1/models\")\nasync def show_available_models():\n    models = await openai_serving_chat.show_available_models()\n    return JSONResponse(content=models.model_dump())\n\n\n@router_app.get(\"/version\")\nasync def show_version():\n    return JSONResponse(content={\"version\": version})\n\n\n@router_app.get(\"/v1/models\", response_model=ModelList)\nasync def models() -> ModelList:\n    \"\"\"Show available models. Right now we only have one model.\"\"\"\n    model_cards = [\n        ModelCard(\n            id=\"\",\n            root=\"\",\n            permission=[ModelPermission()]\n        )\n    ]\n    return ModelList(data=model_cards)\n\n\n@router_app.post(\"/v1/completions\")\nasync def create_completion(\n        body: CompletionRequest,\n        request: Request\n):\n    generator = await openai_serving_completion.create_completion(body, request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            status_code=generator.code\n        )\n    if body.stream:\n        return StreamingResponse(\n            content=generator,\n            media_type=\"text/event-stream\"\n        )\n    else:\n        return JSONResponse(content=generator.model_dump())\n\n\n@router_app.post(\"/v1/chat/completions\")\nasync def create_chat_completion(\n        body: ChatCompletionRequest,\n        request: Request,\n):\n    \"\"\"Completion API similar to OpenAI's API.\n\n    See  https://platform.openai.com/docs/api-reference/chat/create\n    for the API specification. This API mimics the OpenAI ChatCompletion API.\n\n    NOTE: Currently we do not support the following features:\n        - function_call (Users should implement this by themselves)\n        - logit_bias (to be supported by vLLM engine)\n    \"\"\"\n    # async with async_timeout.timeout(TIMEOUT):\n\n    generator = await openai_serving_chat.create_chat_completion(body, request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            status_code=generator.code\n        )\n    if body.stream:\n        return StreamingResponse(\n            content=generator,\n            media_type=\"text/event-stream\"\n        )\n    else:\n        return JSONResponse(content=generator.model_dump())\n\n\n@router_app.post(\"/v1/embeddings\")\nasync def embed(body: Embeddings):\n    \"\"\"Given a prompt, the model will return one embedding.\n\n    Returns:\n        A response object with an embedding.\n    \"\"\"\n    embedding_id = f\"embed-{random_uuid()}\"\n\n    results_list = llm_client.emb(body.model, request=LLMRequest(instruction=body.input))\n    tokens = 0\n\n    return EmbeddingsOutput(\n        data=[\n            EmbeddingsData(\n                embedding=results.output,\n                index=i,\n                object=\"embedding\",\n            )\n            for i, results in enumerate(results_list)\n        ],\n        id=embedding_id,\n        object=\"list\",\n        created=int(time.time()),\n        model=body.model,\n        usage=EmbeddingsUsage(\n            prompt_tokens=tokens,\n            total_tokens=tokens,\n        ),\n    )\n\nclass ServerArgs(BaseModel):\n    host: str = None\n    port: int = 8000\n    uvicorn_log_level: str = \"info\"\n    allow_credentials: bool = False\n    allowed_origins: List[str] = [\"*\"]  \n    allowed_methods: List[str] = [\"*\"]\n    allowed_headers: List[str] = [\"*\"]\n    api_key: str = None\n    served_model_name: str = None\n    prompt_template: str = None\n    response_role: str = \"assistant\"\n    ssl_keyfile: str = None\n    ssl_certfile: str = None    \n\ndef serve(llm:ByzerLLM, args: ServerArgs):\n    \n    logger.info(f\"ByzerLLM API server version {version}\")\n    logger.info(f\"args: {args}\")\n\n    router_app.add_middleware(\n        CORSMiddleware,\n        allow_origins=args.allowed_origins,\n        allow_credentials=args.allow_credentials,\n        allow_methods=args.allowed_methods,\n        allow_headers=args.allowed_headers,\n    )\n    \n    if token := os.environ.get(\"BYZERLLM_API_KEY\") or args.api_key:\n\n        @router_app.middleware(\"http\")\n        async def authentication(request: Request, call_next):\n            if not request.url.path.startswith(\"/v1\"):\n                return await call_next(request)\n            if request.headers.get(\"Authorization\") != \"Bearer \" + token:\n                return JSONResponse(\n                    content={\"error\": \"Unauthorized\"},\n                    status_code=401\n                )\n            return await call_next(request)\n\n    # Register labels for metrics\n    # add_global_metrics_labels(model_name=engine_args.model)\n    global llm_client\n    llm_client = llm\n    \n    global openai_serving_chat\n    openai_serving_chat = OpenAIServingChat(\n        llm_client=llm_client,\n        response_role=args.response_role,\n        server_model_name=args.served_model_name,\n        prompt_template=args.prompt_template\n    )\n    global openai_serving_completion\n    openai_serving_completion = OpenAIServingCompletion(\n        llm_client=llm_client,\n        server_model_name=args.served_model_name,\n        prompt_template=args.prompt_template\n    )\n\n    uvicorn.run(\n        router_app,\n        host=args.host,\n        port=args.port,\n        log_level=args.uvicorn_log_level,\n        timeout_keep_alive=TIMEOUT_KEEP_ALIVE,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile\n    )\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/generator.py\nfrom typing import Optional, Dict, Any, List\nfrom openai import OpenAI\nimport json\nfrom .types import RAGConfig, ByzerSQLConfig, ModelConfig\nfrom byzerllm.utils.client import code_utils\nimport byzerllm\nimport requests\nfrom loguru import logger\nimport traceback\nfrom byzerllm.utils.nontext import TagExtractor\n\n\nclass SQLGenerator:\n\n    @byzerllm.prompt()\n    def gen_context(self, conversations: List[Dict[str, str]]) -> str:\n        \"\"\"\n        对话内容：\n        {% for msg in conversations %}\n        <{{ msg.role }}>: {{ msg.content }}\n        {% endfor %}\n\n        根据提供的对话内容，理解用户最后的问题，然后提取可能会用到的 Byzer-SQL 相关的知识。\n        \"\"\"\n\n    @byzerllm.prompt()\n    def gen_sql(\n        self,\n        conversations: List[Dict[str, str]],\n        context: str,\n        schema: str,\n        model: str = \"deepseek_chat\",\n    ) -> str:\n        \"\"\"\n        相关知识：\n        {{ context }}\n\n        Schema信息：\n        {{ schema }}\n\n        对话内容：\n        {% for msg in conversations %}\n        <{{ msg.role }}>: {{ msg.content }}\n        {% endfor %}\n\n        模型UDF函数：\n        {{ model }}\n\n        根据提供的相关知识以及对话内容，理解用户最后的问题，然后根据看到的schema 信息，生成 Byzer-SQL 语句。\n        Byzer-SQL 首先需要用 load 语法加载 schema 中的路径映射成表格，然后根据表格的信息生成对应的 Byzer-SQL 语句。\n\n        特别需要关注的是:\n        1. 用户的很多需求是要对表格的文本内容做自然语言处理，你需要使用 Byzer-SQL 中的大模型辅助函数来完成这个需求。\n        2. 使用 `llm_result` 函数处理 LLM 模型函数的响应,`llm_result` 函数得到的是一段文本，虽然我们要求模型函数只输出特定的文本，但考虑到大模型会有一些额外的字符输出，不会只是输出我们要求的文字，所以通过 `like` 来进行匹配。\n        3. 注意 Byzer-SQL 的语法，比如select 语句最后一定需要有 as <table_name>。\n        3. 输出的 Byzer-SQL 语句务必使用 <_byzer_sql_></_byzer_sql_> 标签包裹起来。\n        \"\"\"\n\n    @byzerllm.prompt()\n    def fetch_schema(self, context: str) -> str:\n        \"\"\"\n        下面是一些基础知识，包括名词术语等等：\n        {{ context }}\n\n        当理解对话内容的时候，需要参考上面的基础知识，然后更好的理解对应的表格schema信息是不是有用。\n        \"\"\"\n\n    def __init__(\n        self,\n        llm: byzerllm.ByzerLLM,\n        rag_config: RAGConfig,\n        byzer_sql_config: ByzerSQLConfig,\n        model_config: ModelConfig,\n    ) -> None:\n        self.llm = llm\n        self.schema_rag_client = OpenAI(\n            api_key=rag_config.schema_rag_api_key,\n            base_url=rag_config.schema_rag_base_url,\n        )\n        self.context_rag_client = OpenAI(\n            api_key=rag_config.context_rag_api_key,\n            base_url=rag_config.context_rag_base_url,\n        )\n        self.byzer_sql_url = byzer_sql_config.byzer_sql_url\n\n        self.llm_model = byzerllm.ByzerLLM()\n        self.llm_model.setup_default_model_name(model_config.llm_model)\n        self.gen_sql_llm_model = byzerllm.ByzerLLM()\n        self.gen_sql_llm_model.setup_default_model_name(model_config.gen_sql_llm_model)\n\n    def stream_chat_oai(\n        self,\n        conversations,\n        model: Optional[str] = None,\n        role_mapping=None,\n        delta_mode=False,\n        llm_config: Dict[str, Any] = {},\n    ):\n        try:\n            v, c = self._stream_chat_oai(\n                conversations=conversations,\n                model=model,\n                role_mapping=role_mapping,\n                delta_mode=delta_mode,\n                llm_config=llm_config,\n            )\n        except Exception as e:\n            error_msg = f\"Error in stream_chat_oai: {e}\\n{traceback.format_exc()}\"\n            logger.error(error_msg)\n            v, c = [str(e)], []\n\n        return v, c\n\n    def _stream_chat_oai(\n        self,\n        conversations,\n        model: Optional[str] = None,\n        role_mapping=None,\n        delta_mode=False,\n        llm_config: Dict[str, Any] = {},\n    ):\n        logger.info(f\"Starting stream_chat_oai with model: {model}\")\n        logger.debug(f\"Conversations: {conversations}\")\n        logger.debug(f\"LLM Config: {llm_config}\")\n\n        if not model:\n            model = self.llm_model.default_model_name\n        logger.info(f\"Using model: {model}\")\n\n        query = conversations[-1][\"content\"]\n\n        if (\n            \"使用四到五个字直接返回这句话的简要主题，不要解释、不要标点、不要语气词、不要多余文本，不要加粗，如果没有主题\"\n            in query\n            or \"简要总结一下对话内容，用作后续的上下文提示 prompt，控制在 200 字以内\"\n            in query\n        ):\n            chunks = self.llm.stream_chat_oai(\n                conversations=conversations,\n                model=model,\n                role_mapping=role_mapping,\n                llm_config=llm_config,\n                delta_mode=True,\n            )\n            return (chunk[0] for chunk in chunks), []\n\n        logger.info(\"Fetching context from context RAG client\")\n        context_query = self.gen_context.prompt(conversations=conversations)\n        context_response = self.context_rag_client.chat.completions.create(\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": json.dumps(\n                        {\"query\": context_query, \"only_contexts\": True},\n                        ensure_ascii=False,\n                    ),\n                }\n            ],\n            model=model,\n        )\n\n        context = context_response.choices[0].message.content\n        new_context = \"\"\n        for msg in context.split(\"\\n\"):\n            if msg.strip():\n                v = json.loads(msg)\n                new_context += v[\"source_code\"]\n        logger.info(f\"context {new_context[0:100]}...\")\n        logger.info(\"Context retrieved successfully\")\n\n        logger.info(\"Fetching schemas from schema RAG client\")\n        schema_query = self.fetch_schema.prompt(context=new_context)\n\n        response = self.schema_rag_client.chat.completions.create(\n            messages=conversations[0:-1]\n            + [\n                {\n                    \"role\": \"user\",\n                    \"content\": json.dumps(\n                        {\"query\": schema_query, \"only_contexts\": True},\n                        ensure_ascii=False,\n                    ),\n                }\n            ],\n            model=model,\n        )\n        logger.debug(f\"Schema RAG response: {response}\")\n\n        schema_str = response.choices[0].message.content\n        logger.info(\"Schema retrieved successfully\")\n\n        logger.info(\"Generating SQL string\")\n        sql_str = self.gen_sql.with_llm(self.gen_sql_llm_model).run(\n            conversations=conversations,\n            context=new_context,\n            schema=schema_str,\n            model=model,\n        )\n\n        logger.info(\"SQL string generated\")\n        logger.debug(f\"Generated SQL string: {sql_str}\")\n\n        # Extract SQL code from tags\n        tag_extractor = TagExtractor(sql_str)\n        extracted_tags = tag_extractor.extract()\n        sql = extracted_tags.content[0].content\n        logger.info(f\"Extracted SQL: {sql}\")\n\n        logger.info(f\"Sending SQL to Byzer SQL URL: {self.byzer_sql_url}\")\n        try:\n            response = requests.post(\n                self.byzer_sql_url,\n                data={\n                    \"sql\": sql,\n                    \"owner\": \"hello\",\n                    \"sessionPerUser\": True,\n                    \"sessionPerRequest\": True,\n                    \"includeSchema\": True,\n                },\n            )\n            response.raise_for_status()\n            result = response.content.decode(\"utf-8\")\n            logger.info(\"SQL execution result decoded\")\n            logger.debug(f\"SQL execution result: {result}\")\n            return (item for item in [result]), []\n        except requests.exceptions.RequestException as e:\n            logger.error(f\"Failed to send SQL to Byzer SQL URL: {e}\")\n            if hasattr(e.response, \"content\"):\n                error_content = e.response.content.decode(\"utf-8\")\n                logger.error(f\"Server response: {error_content}\")\n            else:\n                logger.error(\"No server response available\")\n            raise Exception(f\"Byzer SQL request failed: {str(e)}\") from e\n\n\n##File: /Users/allwefantasy/projects/super_analysis/setup.py\nfrom setuptools import setup, find_packages\n\nsetup(\n    name=\"super-analysis\",\n    version=\"0.1.0\",\n    packages=find_packages(where=\"src\"),\n    package_dir={\"\": \"src\"},\n    install_requires=[\n        \"byzerllm\",\n        \"fastapi\",\n        \"uvicorn\",\n        \"argparse\",\n    ],\n    entry_points={\n        \"console_scripts\": [\n            \"super-analysis.serve=superanalysis.serve.cli:main\",\n            \"super-analysis.convert=superanalysis.serve.convert:main\",\n        ],\n    },\n    author=\"allwefantasy\",\n    author_email=\"allwefantasy@gmail.com\",\n    description=\"A package for super analysis with ByzerLLM\",\n    long_description=open(\"README.md\").read(),\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/yourusername/super-analysis\",\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n    ],\n    python_requires='>=3.6',\n)\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/convert.py\nimport os\nimport argparse\nimport pandas as pd\nimport hashlib\nfrom typing import List, Dict\n\ndef get_csv_files(directory: str) -> List[str]:\n    \"\"\"Recursively get all CSV files in the given directory.\"\"\"\n    csv_files = []\n    for root, _, files in os.walk(directory):\n        for file in files:\n            if file.endswith('.csv'):\n                csv_files.append(os.path.join(root, file))\n    return csv_files\n\ndef extract_schema(df: pd.DataFrame) -> Dict[str, str]:\n    \"\"\"Extract schema from pandas DataFrame.\"\"\"\n    schema = {}\n    for column in df.columns:\n        dtype = str(df[column].dtype)\n        if dtype == 'object':\n            schema[column] = 'STRING'\n        elif dtype.startswith('int'):\n            schema[column] = 'INTEGER'\n        elif dtype.startswith('float'):\n            schema[column] = 'FLOAT'\n        elif dtype == 'bool':\n            schema[column] = 'BOOLEAN'\n        elif dtype.startswith('datetime'):\n            schema[column] = 'TIMESTAMP'\n        else:\n            schema[column] = 'STRING'  # Default to STRING for unknown types\n    return schema\n\ndef generate_schema_text(schema: Dict[str, str], csv_file: str) -> str:\n    \"\"\"Generate schema text from the schema dictionary and include the CSV file name.\"\"\"\n    schema_lines = [f\"CSV File: {csv_file}\\n\"]\n    schema_lines.extend([f\"{field}: {dtype}\" for field, dtype in schema.items()])\n    return \"\\n\".join(schema_lines)\n\ndef save_schema(schema_text: str, csv_file: str, output_dir: str) -> None:\n    \"\"\"Save schema text to a file in the output directory.\"\"\"\n    # Generate MD5 hash of the schema text\n    file_name = hashlib.md5(schema_text.encode()).hexdigest() + '.md'\n    output_path = os.path.join(output_dir, file_name)\n    \n    with open(output_path, 'w') as f:\n        f.write(schema_text)\n    print(f\"Schema for {csv_file} saved to {output_path}\")\n\ndef convert(data_dir: str, doc_dir: str) -> None:\n    \"\"\"Main conversion function.\"\"\"\n    csv_files = get_csv_files(data_dir)\n    \n    for csv_file in csv_files:\n        print(f\"Processing {csv_file}\")\n        df = pd.read_csv(csv_file)\n        schema = extract_schema(df)\n        schema_text = generate_schema_text(schema, csv_file)\n        save_schema(schema_text, csv_file, doc_dir)\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Convert CSV files to schema documents\")\n    parser.add_argument(\"--data_dir\", required=True, help=\"Directory containing CSV files\")\n    parser.add_argument(\"--doc_dir\", required=True, help=\"Output directory for schema documents\")\n    \n    args = parser.parse_args()\n    \n    if not os.path.exists(args.doc_dir):\n        os.makedirs(args.doc_dir)\n    \n    convert(args.data_dir, args.doc_dir)\n\nif __name__ == \"__main__\":\n    main()\n\n     \n\n下面是用户的需求：\n\n@convert.py 添加一个参数 --include-rows-num 默认为 0,如果用户设置了该值,则在生成schema 的带上相应行数的示例数据\n"}, {"role": "assistant", "content": "To add the `--include-rows-num` parameter and include sample data in the schema output, we need to modify the `convert.py` file. Here are the changes:\n\n```python\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/convert.py\n<<<<<<< SEARCH\nimport os\nimport argparse\nimport pandas as pd\nimport hashlib\nfrom typing import List, Dict\n\ndef get_csv_files(directory: str) -> List[str]:\n    \"\"\"Recursively get all CSV files in the given directory.\"\"\"\n    csv_files = []\n    for root, _, files in os.walk(directory):\n        for file in files:\n            if file.endswith('.csv'):\n                csv_files.append(os.path.join(root, file))\n    return csv_files\n\ndef extract_schema(df: pd.DataFrame) -> Dict[str, str]:\n    \"\"\"Extract schema from pandas DataFrame.\"\"\"\n    schema = {}\n    for column in df.columns:\n        dtype = str(df[column].dtype)\n        if dtype == 'object':\n            schema[column] = 'STRING'\n        elif dtype.startswith('int'):\n            schema[column] = 'INTEGER'\n        elif dtype.startswith('float'):\n            schema[column] = 'FLOAT'\n        elif dtype == 'bool':\n            schema[column] = 'BOOLEAN'\n        elif dtype.startswith('datetime'):\n            schema[column] = 'TIMESTAMP'\n        else:\n            schema[column] = 'STRING'  # Default to STRING for unknown types\n    return schema\n\ndef generate_schema_text(schema: Dict[str, str], csv_file: str) -> str:\n    \"\"\"Generate schema text from the schema dictionary and include the CSV file name.\"\"\"\n    schema_lines = [f\"CSV File: {csv_file}\\n\"]\n    schema_lines.extend([f\"{field}: {dtype}\" for field, dtype in schema.items()])\n    return \"\\n\".join(schema_lines)\n\ndef save_schema(schema_text: str, csv_file: str, output_dir: str) -> None:\n    \"\"\"Save schema text to a file in the output directory.\"\"\"\n    # Generate MD5 hash of the schema text\n    file_name = hashlib.md5(schema_text.encode()).hexdigest() + '.md'\n    output_path = os.path.join(output_dir, file_name)\n    \n    with open(output_path, 'w') as f:\n        f.write(schema_text)\n    print(f\"Schema for {csv_file} saved to {output_path}\")\n\ndef convert(data_dir: str, doc_dir: str) -> None:\n    \"\"\"Main conversion function.\"\"\"\n    csv_files = get_csv_files(data_dir)\n    \n    for csv_file in csv_files:\n        print(f\"Processing {csv_file}\")\n        df = pd.read_csv(csv_file)\n        schema = extract_schema(df)\n        schema_text = generate_schema_text(schema, csv_file)\n        save_schema(schema_text, csv_file, doc_dir)\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Convert CSV files to schema documents\")\n    parser.add_argument(\"--data_dir\", required=True, help=\"Directory containing CSV files\")\n    parser.add_argument(\"--doc_dir\", required=True, help=\"Output directory for schema documents\")\n    \n    args = parser.parse_args()\n    \n    if not os.path.exists(args.doc_dir):\n        os.makedirs(args.doc_dir)\n    \n    convert(args.data_dir, args.doc_dir)\n\nif __name__ == \"__main__\":\n    main()\n=======\nimport os\nimport argparse\nimport pandas as pd\nimport hashlib\nfrom typing import List, Dict\n\ndef get_csv_files(directory: str) -> List[str]:\n    \"\"\"Recursively get all CSV files in the given directory.\"\"\"\n    csv_files = []\n    for root, _, files in os.walk(directory):\n        for file in files:\n            if file.endswith('.csv'):\n                csv_files.append(os.path.join(root, file))\n    return csv_files\n\ndef extract_schema(df: pd.DataFrame) -> Dict[str, str]:\n    \"\"\"Extract schema from pandas DataFrame.\"\"\"\n    schema = {}\n    for column in df.columns:\n        dtype = str(df[column].dtype)\n        if dtype == 'object':\n            schema[column] = 'STRING'\n        elif dtype.startswith('int'):\n            schema[column] = 'INTEGER'\n        elif dtype.startswith('float'):\n            schema[column] = 'FLOAT'\n        elif dtype == 'bool':\n            schema[column] = 'BOOLEAN'\n        elif dtype.startswith('datetime'):\n            schema[column] = 'TIMESTAMP'\n        else:\n            schema[column] = 'STRING'  # Default to STRING for unknown types\n    return schema\n\ndef generate_schema_text(schema: Dict[str, str], csv_file: str, df: pd.DataFrame, include_rows_num: int) -> str:\n    \"\"\"Generate schema text from the schema dictionary and include the CSV file name and sample data.\"\"\"\n    schema_lines = [f\"CSV File: {csv_file}\\n\"]\n    schema_lines.extend([f\"{field}: {dtype}\" for field, dtype in schema.items()])\n    \n    if include_rows_num > 0:\n        schema_lines.append(f\"\\nSample Data (first {include_rows_num} rows):\")\n        sample_data = df.head(include_rows_num).to_string(index=False)\n        schema_lines.append(sample_data)\n    \n    return \"\\n\".join(schema_lines)\n\ndef save_schema(schema_text: str, csv_file: str, output_dir: str) -> None:\n    \"\"\"Save schema text to a file in the output directory.\"\"\"\n    # Generate MD5 hash of the schema text\n    file_name = hashlib.md5(schema_text.encode()).hexdigest() + '.md'\n    output_path = os.path.join(output_dir, file_name)\n    \n    with open(output_path, 'w') as f:\n        f.write(schema_text)\n    print(f\"Schema for {csv_file} saved to {output_path}\")\n\ndef convert(data_dir: str, doc_dir: str, include_rows_num: int) -> None:\n    \"\"\"Main conversion function.\"\"\"\n    csv_files = get_csv_files(data_dir)\n    \n    for csv_file in csv_files:\n        print(f\"Processing {csv_file}\")\n        df = pd.read_csv(csv_file)\n        schema = extract_schema(df)\n        schema_text = generate_schema_text(schema, csv_file, df, include_rows_num)\n        save_schema(schema_text, csv_file, doc_dir)\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Convert CSV files to schema documents\")\n    parser.add_argument(\"--data_dir\", required=True, help=\"Directory containing CSV files\")\n    parser.add_argument(\"--doc_dir\", required=True, help=\"Output directory for schema documents\")\n    parser.add_argument(\"--include-rows-num\", type=int, default=0, help=\"Number of sample rows to include in the schema\")\n    \n    args = parser.parse_args()\n    \n    if not os.path.exists(args.doc_dir):\n        os.makedirs(args.doc_dir)\n    \n    convert(args.data_dir, args.doc_dir, args.include_rows_num)\n\nif __name__ == \"__main__\":\n    main()\n>>>>>>> REPLACE\n```\n\nThese changes add the `--include-rows-num` parameter and modify the code to include sample data in the schema output when the parameter is set. Here's a breakdown of the modifications:\n\n1. Added the `--include-rows-num` argument to the `ArgumentParser` in the `main()` function.\n2. Modified the `generate_schema_text()` function to accept the DataFrame and the `include_rows_num` parameter.\n3. Updated the `generate_schema_text()` function to include sample data when `include_rows_num` is greater than 0.\n4. Modified the `convert()` function to pass the `include_rows_num` parameter to `generate_schema_text()`.\n5. Updated the `main()` function to pass the `include_rows_num` argument to the `convert()` function.\n\nWith these changes, users can now specify the number of sample rows to include in the schema output using the `--include-rows-num` parameter. If not specified, it defaults to 0, which means no sample data will be included."}], "model": "human", "yaml_file": "actions/126_chat_action.yml"}
{"instruction": "根据 @convert.py 更新 @README.md", "conversations": [{"role": "user", "content": "\n如果你需要生成代码，对于每个需要更改的文件,你需要按 *SEARCH/REPLACE block* 的格式进行生成。\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The opening fence and code language, eg: ```python\n2. The file path alone on a line, starting with \"##File:\" and verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n*SEARCH/REPLACE* blocks will replace *all* matching occurrences.\nInclude enough lines to make the SEARCH blocks unique.\n\nInclude *ALL* the code being searched and replaced!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n下面我们来看一个例子：\n\n当前项目目录结构：\n1. 项目根目录： /tmp/projects/mathweb\n2. 项目子目录/文件列表(类似tree 命令输出)\nflask/\n    app.py\n    templates/\n        index.html\n    static/\n        style.css\n\n用户需求： Change get_factorial() to use math.factorial\n\n回答： To make this change we need to modify `/tmp/projects/mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n用户需求： Refactor hello() into its own file.\n\n回答：To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/hello.py\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/main.py\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n现在让我们开始一个新的任务:\n\n当前项目目录结构：\n1. 项目根目录： /Users/allwefantasy/projects/super_analysis\n2. 项目子目录/文件列表(类似tree 命令输出)：\nsetup.py\n.auto-coder/\n    libs/\n        llm_friendly_packages/\n            src/\n                example1/\n                    main.py\nsrc/\n    superanalysis/\n        serve/\n            convert.py\n            openai_serve.py\n            __init__.py\n            llm_wrapper.py\n            api_server.py\n            cli.py\n        sql/\n            __init__.py\n            types.py\n            generator.py\n下面是一些文件路径以及每个文件对应的源码：\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/types.py\nfrom pydantic import BaseModel\n\nclass RAGConfig(BaseModel):\n    schema_rag_base_url: str = \"\"\n    schema_rag_api_key: str = \"xxxx\"\n\n    context_rag_base_url: str = \"\"\n    context_rag_api_key: str = \"xxxx\"\n\nclass ByzerSQLConfig(BaseModel):\n    byzer_sql_url: str = \"\"\n    data_dir: str = \"\"\n\nclass ModelConfig(BaseModel):\n    llm_model: str = \"\"\n    gen_sql_llm_model: str = \"\"\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/cli.py\nimport argparse\nimport byzerllm\nfrom byzerllm.utils.client import ByzerLLM\nfrom superanalysis.serve.api_server import serve, ServerArgs\nfrom superanalysis.sql.types import RAGConfig, ByzerSQLConfig, ModelConfig\nfrom superanalysis.sql.generator import SQLGenerator\nfrom superanalysis.serve.llm_wrapper import LLWrapper\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"ByzerLLM API Server\")\n    parser.add_argument(\"--ray-address\", type=str, default=\"auto\", help=\"ray address\")\n    parser.add_argument(\n        \"--host\", type=str, default=\"0.0.0.0\", help=\"Host to bind the server to\"\n    )\n    parser.add_argument(\n        \"--port\", type=int, default=8000, help=\"Port to bind the server to\"\n    )\n    parser.add_argument(\n        \"--uvicorn-log-level\", type=str, default=\"info\", help=\"Uvicorn log level\"\n    )\n    parser.add_argument(\n        \"--allow-credentials\", action=\"store_true\", help=\"Allow credentials\"\n    )\n    parser.add_argument(\n        \"--allowed-origins\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed origins\"\n    )\n    parser.add_argument(\n        \"--allowed-methods\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed methods\"\n    )\n    parser.add_argument(\n        \"--allowed-headers\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed headers\"\n    )\n    parser.add_argument(\n        \"--api-key\", type=str, default=\"\", help=\"API key for authentication\"\n    )\n    parser.add_argument(\n        \"--served-model-name\",\n        type=str,\n        required=True,\n        help=\"Name of the model to be served\",\n    )\n    parser.add_argument(\n        \"--prompt-template\", type=str, default=\"\", help=\"Prompt template\"\n    )\n    parser.add_argument(\n        \"--response-role\", type=str, default=\"assistant\", help=\"Response role\"\n    )\n    parser.add_argument(\"--ssl-keyfile\", type=str, default=\"\", help=\"SSL key file\")\n    parser.add_argument(\n        \"--ssl-certfile\", type=str, default=\"\", help=\"SSL certificate file\"\n    )\n\n    # Add new arguments for RAGConfig\n    parser.add_argument(\n        \"--schema-rag-base-url\", type=str, default=\"\", help=\"RAG base URL\"\n    )\n    parser.add_argument(\n        \"--schema-rag-api-key\", type=str, default=\"xxxx\", help=\"RAG API key\"\n    )\n\n    parser.add_argument(\n        \"--context-rag-base-url\", type=str, default=\"\", help=\"RAG base URL\"\n    )\n    parser.add_argument(\n        \"--context-rag-api-key\", type=str, default=\"xxxx\", help=\"RAG API key\"\n    )\n\n    # Add new argument for ByzerSQLConfig\n    parser.add_argument(\"--byzer-sql-url\", type=str, default=\"\", help=\"Byzer SQL URL\")\n    parser.add_argument(\"--data-dir\", type=str, default=\"\", help=\"Data directory\")\n    parser.add_argument(\n        \"--llm-model\", type=str, default=\"\", help=\"LLM model for general use\"\n    )\n    parser.add_argument(\n        \"--gen-sql-llm-model\", type=str, default=\"\", help=\"LLM model for SQL generation\"\n    )\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile,\n    )\n\n    # Create RAGConfig instance\n    rag_config = RAGConfig(\n        schema_rag_base_url=args.schema_rag_base_url,\n        schema_rag_api_key=args.schema_rag_api_key,\n        context_rag_base_url=args.context_rag_base_url,\n        context_rag_api_key=args.context_rag_api_key,\n    )\n\n    # Create ByzerSQLConfig instance\n    byzer_sql_config = ByzerSQLConfig(\n        byzer_sql_url=args.byzer_sql_url, data_dir=args.data_dir\n    )\n\n    model_config = ModelConfig(\n        llm_model=args.llm_model or args.served_model_name,\n        gen_sql_llm_model=args.gen_sql_llm_model or args.served_model_name,\n    )\n\n    byzerllm.connect_cluster(address=args.ray_address)\n    llm = ByzerLLM()\n    llm.setup_default_model_name(args.served_model_name)\n    llm = byzerllm.ByzerLLM()\n    if not server_args.served_model_name:\n        raise ValueError(\"served_model_name is required\")\n    llm.setup_default_model_name(server_args.served_model_name)\n\n    print(\"RAG Config: \", rag_config)\n    print(\"Model Config: \", model_config)\n    rag = SQLGenerator(\n        llm=llm,\n        rag_config=rag_config,\n        byzer_sql_config=byzer_sql_config,\n        model_config=model_config,\n    )\n    llm_wrapper = LLWrapper(llm=llm, entry=rag)\n    serve(llm_wrapper, server_args)\n\n\nif __name__ == \"__main__\":\n    main()\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/__init__.py\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/__init__.py\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/llm_wrapper.py\nfrom typing import Any, Dict, List, Optional, Union,Callable\nfrom byzerllm.utils.client.types import (    \n    LLMFunctionCallResponse,\n    LLMClassResponse,LLMResponse    \n)\nimport pydantic\nfrom byzerllm import ByzerLLM\nfrom byzerllm.utils.client import LLMResponse\nfrom byzerllm.utils.types import SingleOutputMeta\nfrom byzerllm.utils.langutil import asyncfy_with_semaphore\nfrom superanalysis.sql.generator import SQLGenerator\nfrom loguru import logger\n\nclass LLWrapper:\n\n    def __init__(self,llm:ByzerLLM,entry:SQLGenerator):\n        self.entry = entry\n        self.llm = llm\n        \n\n    def chat_oai(self,\n                 conversations,\n                 tools:List[Union[Callable,str]]=[], \n                 tool_choice:Optional[Union[Callable,str]]=None,\n                 execute_tool:bool=False,  \n                 impl_func:Optional[Callable]=None,\n                 execute_impl_func:bool=False,\n                 impl_func_params:Optional[Dict[str,Any]]=None,\n                 func_params:Optional[Dict[str,Any]]=None,\n                 response_class:Optional[Union[pydantic.BaseModel,str]] = None, \n                 response_after_chat:Optional[Union[pydantic.BaseModel,str]] = False,\n                 enable_default_sys_message:bool=True,                 \n                 model:Optional[str] = None,\n                 role_mapping=None,llm_config:Dict[str,Any]={}\n                 )->Union[List[LLMResponse],List[LLMFunctionCallResponse],List[LLMClassResponse]]: \n        res,contexts = self.entry.stream_chat_oai(conversations)\n        s = \"\".join(res)        \n        return [LLMResponse(output=s,metadata={},input=\"\")]\n\n    def stream_chat_oai(self,conversations, \n                        model:Optional[str]=None, \n                        role_mapping=None,\n                        delta_mode=False,\n                        llm_config:Dict[str,Any]={}): \n        res,contexts = self.entry.stream_chat_oai(conversations)        \n        for t in res:                        \n            yield (t,SingleOutputMeta(0,0))\n\n    async def async_stream_chat_oai(self,conversations, \n                        model:Optional[str]=None, \n                        role_mapping=None,\n                        delta_mode=False,\n                        llm_config:Dict[str,Any]={}): \n        res,contexts = self.rag.stream_chat_oai(conversations)                \n        for t in res:                                    \n            yield (t,SingleOutputMeta(0,0))\n                         \n\n    def __getattr__(self, name):        \n        return getattr(self.llm, name)\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/openai_serve.py\nimport argparse\nfrom service_server.api_server import ServerArgs, serve\nfrom service_server.llm_wrapper import LLWrapper\nfrom service_server.summary_rag import SummaryRAG\nimport byzerllm\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"OpenAI Serve\")\n    parser.add_argument(\n        \"--model\",\n        default=\"\",\n    )\n    parser.add_argument(\"--emb_model\", default=\"\")\n    parser.add_argument(\n        \"--ray_address\",\n        default=\"auto\",\n    )\n    parser.add_argument(\"--source_dir\", default=\".\", help=\"\")\n    parser.add_argument(\"--host\", default=\"\", help=\"\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"\")\n    parser.add_argument(\"--uvicorn_log_level\", default=\"info\", help=\"\")\n    parser.add_argument(\"--allow_credentials\", action=\"store_true\", help=\"\")\n    parser.add_argument(\"--allowed_origins\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--allowed_methods\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--allowed_headers\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--api_key\", default=\"\", help=\"\")\n    parser.add_argument(\"--served_model_name\", default=\"\", help=\"\")\n    parser.add_argument(\"--prompt_template\", default=\"\", help=\"\")\n    parser.add_argument(\"--ssl_keyfile\", default=\"\", help=\"\")\n    parser.add_argument(\"--ssl_certfile\", default=\"\", help=\"\")\n    parser.add_argument(\"--response_role\", default=\"assistant\", help=\"\")\n    parser.add_argument(\n        \"--collections\", default=\"\", help=\"Collection name for indexing\"\n    )\n    parser.add_argument(\n        \"--base_dir\",\n        default=\"\",\n        help=\"Path where the processed text embeddings were stored\",\n    )\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name or args.model,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile,\n    )\n    byzerllm.connect_cluster(address=args.ray_address)\n    llm = byzerllm.ByzerLLM()\n    if not server_args.served_model_name:\n        raise ValueError(\"served_model_name is required\")\n    llm.setup_default_model_name(server_args.served_model_name)\n    rag = SummaryRAG(llm=llm)    \n    llm_wrapper = LLWrapper(llm=llm, rag=rag)\n    serve(llm=llm_wrapper, args=server_args)\n\nif __name__ == \"__main__\":\n    main()\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/api_server.py\nimport os\nimport time\nimport uvicorn\nfrom fastapi import Request\nfrom fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import JSONResponse, StreamingResponse, Response\n\nfrom byzerllm.log import init_logger\nfrom byzerllm.utils import random_uuid\nfrom byzerllm.version import __version__ as version\nfrom byzerllm.utils.client import ByzerLLM, LLMRequest\nfrom byzerllm.utils.client.entrypoints.openai.serving_chat import OpenAIServingChat\nfrom byzerllm.utils.client.entrypoints.openai.serving_completion import OpenAIServingCompletion\nfrom byzerllm.utils.client.entrypoints.openai.protocol import (\n    ModelList,\n    ModelCard,\n    ModelPermission,\n    ChatCompletionRequest,\n    ErrorResponse,\n    CompletionRequest,\n    Embeddings,\n    EmbeddingsOutput,\n    EmbeddingsData,\n    EmbeddingsUsage,\n)\nfrom pydantic import BaseModel\nfrom typing import List\n\nlogger = init_logger(__name__)\n\nllm_client: ByzerLLM = None\nopenai_serving_chat: OpenAIServingChat = None\nopenai_serving_completion: OpenAIServingCompletion = None\n\nTIMEOUT_KEEP_ALIVE = 5  # seconds\n# timeout in 10 minutes. Streaming can take longer than 3 min\nTIMEOUT = float(os.environ.get(\"BYZERLLM_APISERVER_HTTP_TIMEOUT\", 600))\n\nrouter_app = FastAPI()\n\n\n@router_app.get(\"/health\")\nasync def health() -> Response:\n    \"\"\"Health check.\"\"\"\n    return Response(status_code=200)\n\n\n@router_app.get(\"/v1/models\")\nasync def show_available_models():\n    models = await openai_serving_chat.show_available_models()\n    return JSONResponse(content=models.model_dump())\n\n\n@router_app.get(\"/version\")\nasync def show_version():\n    return JSONResponse(content={\"version\": version})\n\n\n@router_app.get(\"/v1/models\", response_model=ModelList)\nasync def models() -> ModelList:\n    \"\"\"Show available models. Right now we only have one model.\"\"\"\n    model_cards = [\n        ModelCard(\n            id=\"\",\n            root=\"\",\n            permission=[ModelPermission()]\n        )\n    ]\n    return ModelList(data=model_cards)\n\n\n@router_app.post(\"/v1/completions\")\nasync def create_completion(\n        body: CompletionRequest,\n        request: Request\n):\n    generator = await openai_serving_completion.create_completion(body, request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            status_code=generator.code\n        )\n    if body.stream:\n        return StreamingResponse(\n            content=generator,\n            media_type=\"text/event-stream\"\n        )\n    else:\n        return JSONResponse(content=generator.model_dump())\n\n\n@router_app.post(\"/v1/chat/completions\")\nasync def create_chat_completion(\n        body: ChatCompletionRequest,\n        request: Request,\n):\n    \"\"\"Completion API similar to OpenAI's API.\n\n    See  https://platform.openai.com/docs/api-reference/chat/create\n    for the API specification. This API mimics the OpenAI ChatCompletion API.\n\n    NOTE: Currently we do not support the following features:\n        - function_call (Users should implement this by themselves)\n        - logit_bias (to be supported by vLLM engine)\n    \"\"\"\n    # async with async_timeout.timeout(TIMEOUT):\n\n    generator = await openai_serving_chat.create_chat_completion(body, request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            status_code=generator.code\n        )\n    if body.stream:\n        return StreamingResponse(\n            content=generator,\n            media_type=\"text/event-stream\"\n        )\n    else:\n        return JSONResponse(content=generator.model_dump())\n\n\n@router_app.post(\"/v1/embeddings\")\nasync def embed(body: Embeddings):\n    \"\"\"Given a prompt, the model will return one embedding.\n\n    Returns:\n        A response object with an embedding.\n    \"\"\"\n    embedding_id = f\"embed-{random_uuid()}\"\n\n    results_list = llm_client.emb(body.model, request=LLMRequest(instruction=body.input))\n    tokens = 0\n\n    return EmbeddingsOutput(\n        data=[\n            EmbeddingsData(\n                embedding=results.output,\n                index=i,\n                object=\"embedding\",\n            )\n            for i, results in enumerate(results_list)\n        ],\n        id=embedding_id,\n        object=\"list\",\n        created=int(time.time()),\n        model=body.model,\n        usage=EmbeddingsUsage(\n            prompt_tokens=tokens,\n            total_tokens=tokens,\n        ),\n    )\n\nclass ServerArgs(BaseModel):\n    host: str = None\n    port: int = 8000\n    uvicorn_log_level: str = \"info\"\n    allow_credentials: bool = False\n    allowed_origins: List[str] = [\"*\"]  \n    allowed_methods: List[str] = [\"*\"]\n    allowed_headers: List[str] = [\"*\"]\n    api_key: str = None\n    served_model_name: str = None\n    prompt_template: str = None\n    response_role: str = \"assistant\"\n    ssl_keyfile: str = None\n    ssl_certfile: str = None    \n\ndef serve(llm:ByzerLLM, args: ServerArgs):\n    \n    logger.info(f\"ByzerLLM API server version {version}\")\n    logger.info(f\"args: {args}\")\n\n    router_app.add_middleware(\n        CORSMiddleware,\n        allow_origins=args.allowed_origins,\n        allow_credentials=args.allow_credentials,\n        allow_methods=args.allowed_methods,\n        allow_headers=args.allowed_headers,\n    )\n    \n    if token := os.environ.get(\"BYZERLLM_API_KEY\") or args.api_key:\n\n        @router_app.middleware(\"http\")\n        async def authentication(request: Request, call_next):\n            if not request.url.path.startswith(\"/v1\"):\n                return await call_next(request)\n            if request.headers.get(\"Authorization\") != \"Bearer \" + token:\n                return JSONResponse(\n                    content={\"error\": \"Unauthorized\"},\n                    status_code=401\n                )\n            return await call_next(request)\n\n    # Register labels for metrics\n    # add_global_metrics_labels(model_name=engine_args.model)\n    global llm_client\n    llm_client = llm\n    \n    global openai_serving_chat\n    openai_serving_chat = OpenAIServingChat(\n        llm_client=llm_client,\n        response_role=args.response_role,\n        server_model_name=args.served_model_name,\n        prompt_template=args.prompt_template\n    )\n    global openai_serving_completion\n    openai_serving_completion = OpenAIServingCompletion(\n        llm_client=llm_client,\n        server_model_name=args.served_model_name,\n        prompt_template=args.prompt_template\n    )\n\n    uvicorn.run(\n        router_app,\n        host=args.host,\n        port=args.port,\n        log_level=args.uvicorn_log_level,\n        timeout_keep_alive=TIMEOUT_KEEP_ALIVE,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile\n    )\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/generator.py\nfrom typing import Optional, Dict, Any, List\nfrom openai import OpenAI\nimport json\nfrom .types import RAGConfig, ByzerSQLConfig, ModelConfig\nfrom byzerllm.utils.client import code_utils\nimport byzerllm\nimport requests\nfrom loguru import logger\nimport traceback\nfrom byzerllm.utils.nontext import TagExtractor\n\n\nclass SQLGenerator:\n\n    @byzerllm.prompt()\n    def gen_context(self, conversations: List[Dict[str, str]]) -> str:\n        \"\"\"\n        对话内容：\n        {% for msg in conversations %}\n        <{{ msg.role }}>: {{ msg.content }}\n        {% endfor %}\n\n        根据提供的对话内容，理解用户最后的问题，然后提取可能会用到的 Byzer-SQL 相关的知识。\n        \"\"\"\n\n    @byzerllm.prompt()\n    def gen_sql(\n        self,\n        conversations: List[Dict[str, str]],\n        context: str,\n        schema: str,\n        model: str = \"deepseek_chat\",\n    ) -> str:\n        \"\"\"\n        相关知识：\n        {{ context }}\n\n        Schema信息：\n        {{ schema }}\n\n        对话内容：\n        {% for msg in conversations %}\n        <{{ msg.role }}>: {{ msg.content }}\n        {% endfor %}\n\n        模型UDF函数：\n        {{ model }}\n\n        根据提供的相关知识以及对话内容，理解用户最后的问题，然后根据看到的schema 信息，生成 Byzer-SQL 语句。\n        Byzer-SQL 首先需要用 load 语法加载 schema 中的路径映射成表格，然后根据表格的信息生成对应的 Byzer-SQL 语句。\n\n        特别需要关注的是:\n        1. 用户的很多需求是要对表格的文本内容做自然语言处理，你需要使用 Byzer-SQL 中的大模型辅助函数来完成这个需求。\n        2. 使用 `llm_result` 函数处理 LLM 模型函数的响应,`llm_result` 函数得到的是一段文本，虽然我们要求模型函数只输出特定的文本，但考虑到大模型会有一些额外的字符输出，不会只是输出我们要求的文字，所以通过 `like` 来进行匹配。\n        3. 注意 Byzer-SQL 的语法，比如select 语句最后一定需要有 as <table_name>。\n        3. 输出的 Byzer-SQL 语句务必使用 <_byzer_sql_></_byzer_sql_> 标签包裹起来。\n        \"\"\"\n\n    @byzerllm.prompt()\n    def fetch_schema(self, context: str) -> str:\n        \"\"\"\n        下面是一些基础知识，包括名词术语等等：\n        {{ context }}\n\n        当理解对话内容的时候，需要参考上面的基础知识，然后更好的理解对应的表格schema信息是不是有用。\n        \"\"\"\n\n    def __init__(\n        self,\n        llm: byzerllm.ByzerLLM,\n        rag_config: RAGConfig,\n        byzer_sql_config: ByzerSQLConfig,\n        model_config: ModelConfig,\n    ) -> None:\n        self.llm = llm\n        self.schema_rag_client = OpenAI(\n            api_key=rag_config.schema_rag_api_key,\n            base_url=rag_config.schema_rag_base_url,\n        )\n        self.context_rag_client = OpenAI(\n            api_key=rag_config.context_rag_api_key,\n            base_url=rag_config.context_rag_base_url,\n        )\n        self.byzer_sql_url = byzer_sql_config.byzer_sql_url\n\n        self.llm_model = byzerllm.ByzerLLM()\n        self.llm_model.setup_default_model_name(model_config.llm_model)\n        self.gen_sql_llm_model = byzerllm.ByzerLLM()\n        self.gen_sql_llm_model.setup_default_model_name(model_config.gen_sql_llm_model)\n\n    def stream_chat_oai(\n        self,\n        conversations,\n        model: Optional[str] = None,\n        role_mapping=None,\n        delta_mode=False,\n        llm_config: Dict[str, Any] = {},\n    ):\n        try:\n            v, c = self._stream_chat_oai(\n                conversations=conversations,\n                model=model,\n                role_mapping=role_mapping,\n                delta_mode=delta_mode,\n                llm_config=llm_config,\n            )\n        except Exception as e:\n            error_msg = f\"Error in stream_chat_oai: {e}\\n{traceback.format_exc()}\"\n            logger.error(error_msg)\n            v, c = [str(e)], []\n\n        return v, c\n\n    def _stream_chat_oai(\n        self,\n        conversations,\n        model: Optional[str] = None,\n        role_mapping=None,\n        delta_mode=False,\n        llm_config: Dict[str, Any] = {},\n    ):\n        logger.info(f\"Starting stream_chat_oai with model: {model}\")\n        logger.debug(f\"Conversations: {conversations}\")\n        logger.debug(f\"LLM Config: {llm_config}\")\n\n        if not model:\n            model = self.llm_model.default_model_name\n        logger.info(f\"Using model: {model}\")\n\n        query = conversations[-1][\"content\"]\n\n        if (\n            \"使用四到五个字直接返回这句话的简要主题，不要解释、不要标点、不要语气词、不要多余文本，不要加粗，如果没有主题\"\n            in query\n            or \"简要总结一下对话内容，用作后续的上下文提示 prompt，控制在 200 字以内\"\n            in query\n        ):\n            chunks = self.llm.stream_chat_oai(\n                conversations=conversations,\n                model=model,\n                role_mapping=role_mapping,\n                llm_config=llm_config,\n                delta_mode=True,\n            )\n            return (chunk[0] for chunk in chunks), []\n\n        logger.info(\"Fetching context from context RAG client\")\n        context_query = self.gen_context.prompt(conversations=conversations)\n        context_response = self.context_rag_client.chat.completions.create(\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": json.dumps(\n                        {\"query\": context_query, \"only_contexts\": True},\n                        ensure_ascii=False,\n                    ),\n                }\n            ],\n            model=model,\n        )\n\n        context = context_response.choices[0].message.content\n        new_context = \"\"\n        for msg in context.split(\"\\n\"):\n            if msg.strip():\n                v = json.loads(msg)\n                new_context += v[\"source_code\"]\n        logger.info(f\"context {new_context[0:100]}...\")\n        logger.info(\"Context retrieved successfully\")\n\n        logger.info(\"Fetching schemas from schema RAG client\")\n        schema_query = self.fetch_schema.prompt(context=new_context)\n\n        response = self.schema_rag_client.chat.completions.create(\n            messages=conversations[0:-1]\n            + [\n                {\n                    \"role\": \"user\",\n                    \"content\": json.dumps(\n                        {\"query\": schema_query, \"only_contexts\": True},\n                        ensure_ascii=False,\n                    ),\n                }\n            ],\n            model=model,\n        )\n        logger.debug(f\"Schema RAG response: {response}\")\n\n        schema_str = response.choices[0].message.content\n        logger.info(\"Schema retrieved successfully\")\n\n        logger.info(\"Generating SQL string\")\n        sql_str = self.gen_sql.with_llm(self.gen_sql_llm_model).run(\n            conversations=conversations,\n            context=new_context,\n            schema=schema_str,\n            model=model,\n        )\n\n        logger.info(\"SQL string generated\")\n        logger.debug(f\"Generated SQL string: {sql_str}\")\n\n        # Extract SQL code from tags\n        tag_extractor = TagExtractor(sql_str)\n        extracted_tags = tag_extractor.extract()\n        sql = extracted_tags.content[0].content\n        logger.info(f\"Extracted SQL: {sql}\")\n\n        logger.info(f\"Sending SQL to Byzer SQL URL: {self.byzer_sql_url}\")\n        try:\n            response = requests.post(\n                self.byzer_sql_url,\n                data={\n                    \"sql\": sql,\n                    \"owner\": \"hello\",\n                    \"sessionPerUser\": True,\n                    \"sessionPerRequest\": True,\n                    \"includeSchema\": True,\n                },\n            )\n            response.raise_for_status()\n            result = response.content.decode(\"utf-8\")\n            logger.info(\"SQL execution result decoded\")\n            logger.debug(f\"SQL execution result: {result}\")\n            return (item for item in [result]), []\n        except requests.exceptions.RequestException as e:\n            logger.error(f\"Failed to send SQL to Byzer SQL URL: {e}\")\n            if hasattr(e.response, \"content\"):\n                error_content = e.response.content.decode(\"utf-8\")\n                logger.error(f\"Server response: {error_content}\")\n            else:\n                logger.error(\"No server response available\")\n            raise Exception(f\"Byzer SQL request failed: {str(e)}\") from e\n\n\n##File: /Users/allwefantasy/projects/super_analysis/setup.py\nfrom setuptools import setup, find_packages\n\nsetup(\n    name=\"super-analysis\",\n    version=\"0.1.0\",\n    packages=find_packages(where=\"src\"),\n    package_dir={\"\": \"src\"},\n    install_requires=[\n        \"byzerllm\",\n        \"fastapi\",\n        \"uvicorn\",\n        \"argparse\",\n    ],\n    entry_points={\n        \"console_scripts\": [\n            \"super-analysis.serve=superanalysis.serve.cli:main\",\n            \"super-analysis.convert=superanalysis.serve.convert:main\",\n        ],\n    },\n    author=\"allwefantasy\",\n    author_email=\"allwefantasy@gmail.com\",\n    description=\"A package for super analysis with ByzerLLM\",\n    long_description=open(\"README.md\").read(),\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/yourusername/super-analysis\",\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n    ],\n    python_requires='>=3.6',\n)\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/convert.py\nimport os\nimport argparse\nimport pandas as pd\nimport hashlib\nfrom typing import List, Dict\n\ndef get_csv_files(directory: str) -> List[str]:\n    \"\"\"Recursively get all CSV files in the given directory.\"\"\"\n    csv_files = []\n    for root, _, files in os.walk(directory):\n        for file in files:\n            if file.endswith('.csv'):\n                csv_files.append(os.path.join(root, file))\n    return csv_files\n\ndef extract_schema(df: pd.DataFrame) -> Dict[str, str]:\n    \"\"\"Extract schema from pandas DataFrame.\"\"\"\n    schema = {}\n    for column in df.columns:\n        dtype = str(df[column].dtype)\n        if dtype == 'object':\n            schema[column] = 'STRING'\n        elif dtype.startswith('int'):\n            schema[column] = 'INTEGER'\n        elif dtype.startswith('float'):\n            schema[column] = 'FLOAT'\n        elif dtype == 'bool':\n            schema[column] = 'BOOLEAN'\n        elif dtype.startswith('datetime'):\n            schema[column] = 'TIMESTAMP'\n        else:\n            schema[column] = 'STRING'  # Default to STRING for unknown types\n    return schema\n\ndef generate_schema_text(schema: Dict[str, str], csv_file: str, df: pd.DataFrame, include_rows_num: int) -> str:\n    \"\"\"Generate schema text from the schema dictionary and include the CSV file name and sample data.\"\"\"\n    schema_lines = [f\"CSV File: {csv_file}\\n\"]\n    schema_lines.extend([f\"{field}: {dtype}\" for field, dtype in schema.items()])\n    \n    if include_rows_num > 0:\n        schema_lines.append(f\"\\nSample Data (first {include_rows_num} rows):\")\n        sample_data = df.head(include_rows_num).to_string(index=False)\n        schema_lines.append(sample_data)\n    \n    return \"\\n\".join(schema_lines)\n\ndef save_schema(schema_text: str, csv_file: str, output_dir: str) -> None:\n    \"\"\"Save schema text to a file in the output directory.\"\"\"\n    # Generate MD5 hash of the schema text\n    file_name = hashlib.md5(schema_text.encode()).hexdigest() + '.md'\n    output_path = os.path.join(output_dir, file_name)\n    \n    with open(output_path, 'w') as f:\n        f.write(schema_text)\n    print(f\"Schema for {csv_file} saved to {output_path}\")\n\ndef convert(data_dir: str, doc_dir: str, include_rows_num: int) -> None:\n    \"\"\"Main conversion function.\"\"\"\n    csv_files = get_csv_files(data_dir)\n    \n    for csv_file in csv_files:\n        print(f\"Processing {csv_file}\")\n        df = pd.read_csv(csv_file)\n        schema = extract_schema(df)\n        schema_text = generate_schema_text(schema, csv_file, df, include_rows_num)\n        save_schema(schema_text, csv_file, doc_dir)\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Convert CSV files to schema documents\")\n    parser.add_argument(\"--data_dir\", required=True, help=\"Directory containing CSV files\")\n    parser.add_argument(\"--doc_dir\", required=True, help=\"Output directory for schema documents\")\n    parser.add_argument(\"--include-rows-num\", type=int, default=0, help=\"Number of sample rows to include in the schema\")\n    \n    args = parser.parse_args()\n    \n    if not os.path.exists(args.doc_dir):\n        os.makedirs(args.doc_dir)\n    \n    convert(args.data_dir, args.doc_dir, args.include_rows_num)\n\nif __name__ == \"__main__\":\n    main()\n\n     \n\n下面是用户的需求：\n\n根据 @convert.py 更新 @README.md\n"}, {"role": "assistant", "content": "好的，我会根据 `convert.py` 文件的内容来更新 `README.md` 文件。以下是相应的 *SEARCH/REPLACE* 块：\n\n```markdown\n##File: /Users/allwefantasy/projects/super_analysis/README.md\n<<<<<<< SEARCH\n=======\n# Super Analysis\n\nThis project provides tools for super analysis with ByzerLLM, including a data conversion utility and a serving application.\n\n## Installation\n\nYou can install this package using pip:\n\n```\npip install super-analysis\n```\n\n## Features\n\n### Data Conversion Utility\n\nThe data conversion utility allows you to convert CSV files to schema documents. It's useful for preparing your data for analysis.\n\n#### Usage\n\n```\nsuper-analysis.convert --data_dir <path_to_csv_files> --doc_dir <output_directory> [--include-rows-num <number_of_sample_rows>]\n```\n\nArguments:\n- `--data_dir`: Directory containing CSV files (required)\n- `--doc_dir`: Output directory for schema documents (required)\n- `--include-rows-num`: Number of sample rows to include in the schema (optional, default is 0)\n\nThe utility will:\n1. Recursively find all CSV files in the specified directory\n2. Extract the schema from each CSV file\n3. Generate a schema text that includes:\n   - The CSV file name\n   - Field names and their data types\n   - Sample data (if `--include-rows-num` is specified)\n4. Save the schema text as a markdown file in the output directory\n\n### Serving Application\n\n(Add information about the serving application here)\n\n## License\n\nThis project is licensed under the MIT License.\n>>>>>>> REPLACE\n```\n\n这个更新添加了一个基本的 README 结构，包括项目描述、安装说明、特性介绍（重点是数据转换工具的使用说明）以及许可证信息。您可能还需要添加有关服务应用程序的更多信息，这取决于 `cli.py` 和其他相关文件中的具体实现。"}], "model": "human", "yaml_file": "actions/127_chat_action.yml"}
{"instruction": "提供一个普通 fastapi 实现的接口,然后通过命令 superanalysis.api 启动", "conversations": [{"role": "user", "content": "\n如果你需要生成代码，对于每个需要更改的文件,你需要按 *SEARCH/REPLACE block* 的格式进行生成。\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The opening fence and code language, eg: ```python\n2. The file path alone on a line, starting with \"##File:\" and verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n*SEARCH/REPLACE* blocks will replace *all* matching occurrences.\nInclude enough lines to make the SEARCH blocks unique.\n\nInclude *ALL* the code being searched and replaced!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n下面我们来看一个例子：\n\n当前项目目录结构：\n1. 项目根目录： /tmp/projects/mathweb\n2. 项目子目录/文件列表(类似tree 命令输出)\nflask/\n    app.py\n    templates/\n        index.html\n    static/\n        style.css\n\n用户需求： Change get_factorial() to use math.factorial\n\n回答： To make this change we need to modify `/tmp/projects/mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n用户需求： Refactor hello() into its own file.\n\n回答：To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/hello.py\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/main.py\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n现在让我们开始一个新的任务:\n\n当前项目目录结构：\n1. 项目根目录： /Users/allwefantasy/projects/super_analysis\n2. 项目子目录/文件列表(类似tree 命令输出)：\nsetup.py\n.auto-coder/\n    libs/\n        llm_friendly_packages/\n            src/\n                example1/\n                    main.py\nsrc/\n    superanalysis/\n        serve/\n            convert.py\n            openai_serve.py\n            __init__.py\n            llm_wrapper.py\n            api_server.py\n            cli.py\n        sql/\n            __init__.py\n            types.py\n            generator.py\n下面是一些文件路径以及每个文件对应的源码：\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/types.py\nfrom pydantic import BaseModel\n\nclass RAGConfig(BaseModel):\n    schema_rag_base_url: str = \"\"\n    schema_rag_api_key: str = \"xxxx\"\n\n    context_rag_base_url: str = \"\"\n    context_rag_api_key: str = \"xxxx\"\n\nclass ByzerSQLConfig(BaseModel):\n    byzer_sql_url: str = \"\"\n    data_dir: str = \"\"\n\nclass ModelConfig(BaseModel):\n    llm_model: str = \"\"\n    gen_sql_llm_model: str = \"\"\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/cli.py\nimport argparse\nimport byzerllm\nfrom byzerllm.utils.client import ByzerLLM\nfrom superanalysis.serve.api_server import serve, ServerArgs\nfrom superanalysis.sql.types import RAGConfig, ByzerSQLConfig, ModelConfig\nfrom superanalysis.sql.generator import SQLGenerator\nfrom superanalysis.serve.llm_wrapper import LLWrapper\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"ByzerLLM API Server\")\n    parser.add_argument(\"--ray-address\", type=str, default=\"auto\", help=\"ray address\")\n    parser.add_argument(\n        \"--host\", type=str, default=\"0.0.0.0\", help=\"Host to bind the server to\"\n    )\n    parser.add_argument(\n        \"--port\", type=int, default=8000, help=\"Port to bind the server to\"\n    )\n    parser.add_argument(\n        \"--uvicorn-log-level\", type=str, default=\"info\", help=\"Uvicorn log level\"\n    )\n    parser.add_argument(\n        \"--allow-credentials\", action=\"store_true\", help=\"Allow credentials\"\n    )\n    parser.add_argument(\n        \"--allowed-origins\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed origins\"\n    )\n    parser.add_argument(\n        \"--allowed-methods\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed methods\"\n    )\n    parser.add_argument(\n        \"--allowed-headers\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed headers\"\n    )\n    parser.add_argument(\n        \"--api-key\", type=str, default=\"\", help=\"API key for authentication\"\n    )\n    parser.add_argument(\n        \"--served-model-name\",\n        type=str,\n        required=True,\n        help=\"Name of the model to be served\",\n    )\n    parser.add_argument(\n        \"--prompt-template\", type=str, default=\"\", help=\"Prompt template\"\n    )\n    parser.add_argument(\n        \"--response-role\", type=str, default=\"assistant\", help=\"Response role\"\n    )\n    parser.add_argument(\"--ssl-keyfile\", type=str, default=\"\", help=\"SSL key file\")\n    parser.add_argument(\n        \"--ssl-certfile\", type=str, default=\"\", help=\"SSL certificate file\"\n    )\n\n    # Add new arguments for RAGConfig\n    parser.add_argument(\n        \"--schema-rag-base-url\", type=str, default=\"\", help=\"RAG base URL\"\n    )\n    parser.add_argument(\n        \"--schema-rag-api-key\", type=str, default=\"xxxx\", help=\"RAG API key\"\n    )\n\n    parser.add_argument(\n        \"--context-rag-base-url\", type=str, default=\"\", help=\"RAG base URL\"\n    )\n    parser.add_argument(\n        \"--context-rag-api-key\", type=str, default=\"xxxx\", help=\"RAG API key\"\n    )\n\n    # Add new argument for ByzerSQLConfig\n    parser.add_argument(\"--byzer-sql-url\", type=str, default=\"\", help=\"Byzer SQL URL\")\n    parser.add_argument(\"--data-dir\", type=str, default=\"\", help=\"Data directory\")\n    parser.add_argument(\n        \"--llm-model\", type=str, default=\"\", help=\"LLM model for general use\"\n    )\n    parser.add_argument(\n        \"--gen-sql-llm-model\", type=str, default=\"\", help=\"LLM model for SQL generation\"\n    )\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile,\n    )\n\n    # Create RAGConfig instance\n    rag_config = RAGConfig(\n        schema_rag_base_url=args.schema_rag_base_url,\n        schema_rag_api_key=args.schema_rag_api_key,\n        context_rag_base_url=args.context_rag_base_url,\n        context_rag_api_key=args.context_rag_api_key,\n    )\n\n    # Create ByzerSQLConfig instance\n    byzer_sql_config = ByzerSQLConfig(\n        byzer_sql_url=args.byzer_sql_url, data_dir=args.data_dir\n    )\n\n    model_config = ModelConfig(\n        llm_model=args.llm_model or args.served_model_name,\n        gen_sql_llm_model=args.gen_sql_llm_model or args.served_model_name,\n    )\n\n    byzerllm.connect_cluster(address=args.ray_address)\n    llm = ByzerLLM()\n    llm.setup_default_model_name(args.served_model_name)\n    llm = byzerllm.ByzerLLM()\n    if not server_args.served_model_name:\n        raise ValueError(\"served_model_name is required\")\n    llm.setup_default_model_name(server_args.served_model_name)\n\n    print(\"RAG Config: \", rag_config)\n    print(\"Model Config: \", model_config)\n    rag = SQLGenerator(\n        llm=llm,\n        rag_config=rag_config,\n        byzer_sql_config=byzer_sql_config,\n        model_config=model_config,\n    )\n    llm_wrapper = LLWrapper(llm=llm, entry=rag)\n    serve(llm_wrapper, server_args)\n\n\nif __name__ == \"__main__\":\n    main()\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/__init__.py\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/__init__.py\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/llm_wrapper.py\nfrom typing import Any, Dict, List, Optional, Union,Callable\nfrom byzerllm.utils.client.types import (    \n    LLMFunctionCallResponse,\n    LLMClassResponse,LLMResponse    \n)\nimport pydantic\nfrom byzerllm import ByzerLLM\nfrom byzerllm.utils.client import LLMResponse\nfrom byzerllm.utils.types import SingleOutputMeta\nfrom byzerllm.utils.langutil import asyncfy_with_semaphore\nfrom superanalysis.sql.generator import SQLGenerator\nfrom loguru import logger\n\nclass LLWrapper:\n\n    def __init__(self,llm:ByzerLLM,entry:SQLGenerator):\n        self.entry = entry\n        self.llm = llm\n        \n\n    def chat_oai(self,\n                 conversations,\n                 tools:List[Union[Callable,str]]=[], \n                 tool_choice:Optional[Union[Callable,str]]=None,\n                 execute_tool:bool=False,  \n                 impl_func:Optional[Callable]=None,\n                 execute_impl_func:bool=False,\n                 impl_func_params:Optional[Dict[str,Any]]=None,\n                 func_params:Optional[Dict[str,Any]]=None,\n                 response_class:Optional[Union[pydantic.BaseModel,str]] = None, \n                 response_after_chat:Optional[Union[pydantic.BaseModel,str]] = False,\n                 enable_default_sys_message:bool=True,                 \n                 model:Optional[str] = None,\n                 role_mapping=None,llm_config:Dict[str,Any]={}\n                 )->Union[List[LLMResponse],List[LLMFunctionCallResponse],List[LLMClassResponse]]: \n        res,contexts = self.entry.stream_chat_oai(conversations)\n        s = \"\".join(res)        \n        return [LLMResponse(output=s,metadata={},input=\"\")]\n\n    def stream_chat_oai(self,conversations, \n                        model:Optional[str]=None, \n                        role_mapping=None,\n                        delta_mode=False,\n                        llm_config:Dict[str,Any]={}): \n        res,contexts = self.entry.stream_chat_oai(conversations)        \n        for t in res:                        \n            yield (t,SingleOutputMeta(0,0))\n\n    async def async_stream_chat_oai(self,conversations, \n                        model:Optional[str]=None, \n                        role_mapping=None,\n                        delta_mode=False,\n                        llm_config:Dict[str,Any]={}): \n        res,contexts = self.rag.stream_chat_oai(conversations)                \n        for t in res:                                    \n            yield (t,SingleOutputMeta(0,0))\n                         \n\n    def __getattr__(self, name):        \n        return getattr(self.llm, name)\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/openai_serve.py\nimport argparse\nfrom service_server.api_server import ServerArgs, serve\nfrom service_server.llm_wrapper import LLWrapper\nfrom service_server.summary_rag import SummaryRAG\nimport byzerllm\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"OpenAI Serve\")\n    parser.add_argument(\n        \"--model\",\n        default=\"\",\n    )\n    parser.add_argument(\"--emb_model\", default=\"\")\n    parser.add_argument(\n        \"--ray_address\",\n        default=\"auto\",\n    )\n    parser.add_argument(\"--source_dir\", default=\".\", help=\"\")\n    parser.add_argument(\"--host\", default=\"\", help=\"\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"\")\n    parser.add_argument(\"--uvicorn_log_level\", default=\"info\", help=\"\")\n    parser.add_argument(\"--allow_credentials\", action=\"store_true\", help=\"\")\n    parser.add_argument(\"--allowed_origins\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--allowed_methods\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--allowed_headers\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--api_key\", default=\"\", help=\"\")\n    parser.add_argument(\"--served_model_name\", default=\"\", help=\"\")\n    parser.add_argument(\"--prompt_template\", default=\"\", help=\"\")\n    parser.add_argument(\"--ssl_keyfile\", default=\"\", help=\"\")\n    parser.add_argument(\"--ssl_certfile\", default=\"\", help=\"\")\n    parser.add_argument(\"--response_role\", default=\"assistant\", help=\"\")\n    parser.add_argument(\n        \"--collections\", default=\"\", help=\"Collection name for indexing\"\n    )\n    parser.add_argument(\n        \"--base_dir\",\n        default=\"\",\n        help=\"Path where the processed text embeddings were stored\",\n    )\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name or args.model,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile,\n    )\n    byzerllm.connect_cluster(address=args.ray_address)\n    llm = byzerllm.ByzerLLM()\n    if not server_args.served_model_name:\n        raise ValueError(\"served_model_name is required\")\n    llm.setup_default_model_name(server_args.served_model_name)\n    rag = SummaryRAG(llm=llm)    \n    llm_wrapper = LLWrapper(llm=llm, rag=rag)\n    serve(llm=llm_wrapper, args=server_args)\n\nif __name__ == \"__main__\":\n    main()\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/api_server.py\nimport os\nimport time\nimport uvicorn\nfrom fastapi import Request\nfrom fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import JSONResponse, StreamingResponse, Response\n\nfrom byzerllm.log import init_logger\nfrom byzerllm.utils import random_uuid\nfrom byzerllm.version import __version__ as version\nfrom byzerllm.utils.client import ByzerLLM, LLMRequest\nfrom byzerllm.utils.client.entrypoints.openai.serving_chat import OpenAIServingChat\nfrom byzerllm.utils.client.entrypoints.openai.serving_completion import OpenAIServingCompletion\nfrom byzerllm.utils.client.entrypoints.openai.protocol import (\n    ModelList,\n    ModelCard,\n    ModelPermission,\n    ChatCompletionRequest,\n    ErrorResponse,\n    CompletionRequest,\n    Embeddings,\n    EmbeddingsOutput,\n    EmbeddingsData,\n    EmbeddingsUsage,\n)\nfrom pydantic import BaseModel\nfrom typing import List\n\nlogger = init_logger(__name__)\n\nllm_client: ByzerLLM = None\nopenai_serving_chat: OpenAIServingChat = None\nopenai_serving_completion: OpenAIServingCompletion = None\n\nTIMEOUT_KEEP_ALIVE = 5  # seconds\n# timeout in 10 minutes. Streaming can take longer than 3 min\nTIMEOUT = float(os.environ.get(\"BYZERLLM_APISERVER_HTTP_TIMEOUT\", 600))\n\nrouter_app = FastAPI()\n\n\n@router_app.get(\"/health\")\nasync def health() -> Response:\n    \"\"\"Health check.\"\"\"\n    return Response(status_code=200)\n\n\n@router_app.get(\"/v1/models\")\nasync def show_available_models():\n    models = await openai_serving_chat.show_available_models()\n    return JSONResponse(content=models.model_dump())\n\n\n@router_app.get(\"/version\")\nasync def show_version():\n    return JSONResponse(content={\"version\": version})\n\n\n@router_app.get(\"/v1/models\", response_model=ModelList)\nasync def models() -> ModelList:\n    \"\"\"Show available models. Right now we only have one model.\"\"\"\n    model_cards = [\n        ModelCard(\n            id=\"\",\n            root=\"\",\n            permission=[ModelPermission()]\n        )\n    ]\n    return ModelList(data=model_cards)\n\n\n@router_app.post(\"/v1/completions\")\nasync def create_completion(\n        body: CompletionRequest,\n        request: Request\n):\n    generator = await openai_serving_completion.create_completion(body, request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            status_code=generator.code\n        )\n    if body.stream:\n        return StreamingResponse(\n            content=generator,\n            media_type=\"text/event-stream\"\n        )\n    else:\n        return JSONResponse(content=generator.model_dump())\n\n\n@router_app.post(\"/v1/chat/completions\")\nasync def create_chat_completion(\n        body: ChatCompletionRequest,\n        request: Request,\n):\n    \"\"\"Completion API similar to OpenAI's API.\n\n    See  https://platform.openai.com/docs/api-reference/chat/create\n    for the API specification. This API mimics the OpenAI ChatCompletion API.\n\n    NOTE: Currently we do not support the following features:\n        - function_call (Users should implement this by themselves)\n        - logit_bias (to be supported by vLLM engine)\n    \"\"\"\n    # async with async_timeout.timeout(TIMEOUT):\n\n    generator = await openai_serving_chat.create_chat_completion(body, request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            status_code=generator.code\n        )\n    if body.stream:\n        return StreamingResponse(\n            content=generator,\n            media_type=\"text/event-stream\"\n        )\n    else:\n        return JSONResponse(content=generator.model_dump())\n\n\n@router_app.post(\"/v1/embeddings\")\nasync def embed(body: Embeddings):\n    \"\"\"Given a prompt, the model will return one embedding.\n\n    Returns:\n        A response object with an embedding.\n    \"\"\"\n    embedding_id = f\"embed-{random_uuid()}\"\n\n    results_list = llm_client.emb(body.model, request=LLMRequest(instruction=body.input))\n    tokens = 0\n\n    return EmbeddingsOutput(\n        data=[\n            EmbeddingsData(\n                embedding=results.output,\n                index=i,\n                object=\"embedding\",\n            )\n            for i, results in enumerate(results_list)\n        ],\n        id=embedding_id,\n        object=\"list\",\n        created=int(time.time()),\n        model=body.model,\n        usage=EmbeddingsUsage(\n            prompt_tokens=tokens,\n            total_tokens=tokens,\n        ),\n    )\n\nclass ServerArgs(BaseModel):\n    host: str = None\n    port: int = 8000\n    uvicorn_log_level: str = \"info\"\n    allow_credentials: bool = False\n    allowed_origins: List[str] = [\"*\"]  \n    allowed_methods: List[str] = [\"*\"]\n    allowed_headers: List[str] = [\"*\"]\n    api_key: str = None\n    served_model_name: str = None\n    prompt_template: str = None\n    response_role: str = \"assistant\"\n    ssl_keyfile: str = None\n    ssl_certfile: str = None    \n\ndef serve(llm:ByzerLLM, args: ServerArgs):\n    \n    logger.info(f\"ByzerLLM API server version {version}\")\n    logger.info(f\"args: {args}\")\n\n    router_app.add_middleware(\n        CORSMiddleware,\n        allow_origins=args.allowed_origins,\n        allow_credentials=args.allow_credentials,\n        allow_methods=args.allowed_methods,\n        allow_headers=args.allowed_headers,\n    )\n    \n    if token := os.environ.get(\"BYZERLLM_API_KEY\") or args.api_key:\n\n        @router_app.middleware(\"http\")\n        async def authentication(request: Request, call_next):\n            if not request.url.path.startswith(\"/v1\"):\n                return await call_next(request)\n            if request.headers.get(\"Authorization\") != \"Bearer \" + token:\n                return JSONResponse(\n                    content={\"error\": \"Unauthorized\"},\n                    status_code=401\n                )\n            return await call_next(request)\n\n    # Register labels for metrics\n    # add_global_metrics_labels(model_name=engine_args.model)\n    global llm_client\n    llm_client = llm\n    \n    global openai_serving_chat\n    openai_serving_chat = OpenAIServingChat(\n        llm_client=llm_client,\n        response_role=args.response_role,\n        server_model_name=args.served_model_name,\n        prompt_template=args.prompt_template\n    )\n    global openai_serving_completion\n    openai_serving_completion = OpenAIServingCompletion(\n        llm_client=llm_client,\n        server_model_name=args.served_model_name,\n        prompt_template=args.prompt_template\n    )\n\n    uvicorn.run(\n        router_app,\n        host=args.host,\n        port=args.port,\n        log_level=args.uvicorn_log_level,\n        timeout_keep_alive=TIMEOUT_KEEP_ALIVE,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile\n    )\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/generator.py\nfrom typing import Optional, Dict, Any, List\nfrom openai import OpenAI\nimport json\nfrom .types import RAGConfig, ByzerSQLConfig, ModelConfig\nfrom byzerllm.utils.client import code_utils\nimport byzerllm\nimport requests\nfrom loguru import logger\nimport traceback\nfrom byzerllm.utils.nontext import TagExtractor\n\n\nclass SQLGenerator:\n\n    @byzerllm.prompt()\n    def gen_context(self, conversations: List[Dict[str, str]]) -> str:\n        \"\"\"\n        对话内容：\n        {% for msg in conversations %}\n        <{{ msg.role }}>: {{ msg.content }}\n        {% endfor %}\n\n        根据提供的对话内容，理解用户最后的问题，然后提取可能会用到的 Byzer-SQL 相关的知识。\n        \"\"\"\n\n    @byzerllm.prompt()\n    def gen_sql(\n        self,\n        conversations: List[Dict[str, str]],\n        context: str,\n        schema: str,\n        model: str = \"deepseek_chat\",\n    ) -> str:\n        \"\"\"\n        相关知识：\n        {{ context }}\n\n        Schema信息：\n        {{ schema }}\n\n        对话内容：\n        {% for msg in conversations %}\n        <{{ msg.role }}>: {{ msg.content }}\n        {% endfor %}\n\n        模型UDF函数：\n        {{ model }}\n\n        根据提供的相关知识以及对话内容，理解用户最后的问题，然后根据看到的schema 信息，生成 Byzer-SQL 语句。\n        Byzer-SQL 首先需要用 load 语法加载 schema 中的路径映射成表格，然后根据表格的信息生成对应的 Byzer-SQL 语句。\n\n        特别需要关注的是:\n        1. 用户的很多需求是要对表格的文本内容做自然语言处理，你需要使用 Byzer-SQL 中的大模型辅助函数来完成这个需求。\n        2. 使用 `llm_result` 函数处理 LLM 模型函数的响应,`llm_result` 函数得到的是一段文本，虽然我们要求模型函数只输出特定的文本，但考虑到大模型会有一些额外的字符输出，不会只是输出我们要求的文字，所以通过 `like` 来进行匹配。\n        3. 注意 Byzer-SQL 的语法，比如select 语句最后一定需要有 as <table_name>。\n        3. 输出的 Byzer-SQL 语句务必使用 <_byzer_sql_></_byzer_sql_> 标签包裹起来。\n        \"\"\"\n\n    @byzerllm.prompt()\n    def fetch_schema(self, context: str) -> str:\n        \"\"\"\n        下面是一些基础知识，包括名词术语等等：\n        {{ context }}\n\n        当理解对话内容的时候，需要参考上面的基础知识，然后更好的理解对应的表格schema信息是不是有用。\n        \"\"\"\n\n    def __init__(\n        self,\n        llm: byzerllm.ByzerLLM,\n        rag_config: RAGConfig,\n        byzer_sql_config: ByzerSQLConfig,\n        model_config: ModelConfig,\n    ) -> None:\n        self.llm = llm\n        self.schema_rag_client = OpenAI(\n            api_key=rag_config.schema_rag_api_key,\n            base_url=rag_config.schema_rag_base_url,\n        )\n        self.context_rag_client = OpenAI(\n            api_key=rag_config.context_rag_api_key,\n            base_url=rag_config.context_rag_base_url,\n        )\n        self.byzer_sql_url = byzer_sql_config.byzer_sql_url\n\n        self.llm_model = byzerllm.ByzerLLM()\n        self.llm_model.setup_default_model_name(model_config.llm_model)\n        self.gen_sql_llm_model = byzerllm.ByzerLLM()\n        self.gen_sql_llm_model.setup_default_model_name(model_config.gen_sql_llm_model)\n\n    def stream_chat_oai(\n        self,\n        conversations,\n        model: Optional[str] = None,\n        role_mapping=None,\n        delta_mode=False,\n        llm_config: Dict[str, Any] = {},\n    ):\n        try:\n            v, c = self._stream_chat_oai(\n                conversations=conversations,\n                model=model,\n                role_mapping=role_mapping,\n                delta_mode=delta_mode,\n                llm_config=llm_config,\n            )\n        except Exception as e:\n            error_msg = f\"Error in stream_chat_oai: {e}\\n{traceback.format_exc()}\"\n            logger.error(error_msg)\n            v, c = [str(e)], []\n\n        return v, c\n\n    def _stream_chat_oai(\n        self,\n        conversations,\n        model: Optional[str] = None,\n        role_mapping=None,\n        delta_mode=False,\n        llm_config: Dict[str, Any] = {},\n    ):\n        logger.info(f\"Starting stream_chat_oai with model: {model}\")\n        logger.debug(f\"Conversations: {conversations}\")\n        logger.debug(f\"LLM Config: {llm_config}\")\n\n        if not model:\n            model = self.llm_model.default_model_name\n        logger.info(f\"Using model: {model}\")\n\n        query = conversations[-1][\"content\"]\n\n        if (\n            \"使用四到五个字直接返回这句话的简要主题，不要解释、不要标点、不要语气词、不要多余文本，不要加粗，如果没有主题\"\n            in query\n            or \"简要总结一下对话内容，用作后续的上下文提示 prompt，控制在 200 字以内\"\n            in query\n        ):\n            chunks = self.llm.stream_chat_oai(\n                conversations=conversations,\n                model=model,\n                role_mapping=role_mapping,\n                llm_config=llm_config,\n                delta_mode=True,\n            )\n            return (chunk[0] for chunk in chunks), []\n\n        logger.info(\"Fetching context from context RAG client\")\n        context_query = self.gen_context.prompt(conversations=conversations)\n        context_response = self.context_rag_client.chat.completions.create(\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": json.dumps(\n                        {\"query\": context_query, \"only_contexts\": True},\n                        ensure_ascii=False,\n                    ),\n                }\n            ],\n            model=model,\n        )\n\n        context = context_response.choices[0].message.content\n        new_context = \"\"\n        for msg in context.split(\"\\n\"):\n            if msg.strip():\n                v = json.loads(msg)\n                new_context += v[\"source_code\"]\n        logger.info(f\"context {new_context[0:100]}...\")\n        logger.info(\"Context retrieved successfully\")\n\n        logger.info(\"Fetching schemas from schema RAG client\")\n        schema_query = self.fetch_schema.prompt(context=new_context)\n\n        response = self.schema_rag_client.chat.completions.create(\n            messages=conversations[0:-1]\n            + [\n                {\n                    \"role\": \"user\",\n                    \"content\": json.dumps(\n                        {\"query\": schema_query, \"only_contexts\": True},\n                        ensure_ascii=False,\n                    ),\n                }\n            ],\n            model=model,\n        )\n        logger.debug(f\"Schema RAG response: {response}\")\n\n        schema_str = response.choices[0].message.content\n        logger.info(\"Schema retrieved successfully\")\n\n        logger.info(\"Generating SQL string\")\n        sql_str = self.gen_sql.with_llm(self.gen_sql_llm_model).run(\n            conversations=conversations,\n            context=new_context,\n            schema=schema_str,\n            model=model,\n        )\n\n        logger.info(\"SQL string generated\")\n        logger.debug(f\"Generated SQL string: {sql_str}\")\n\n        # Extract SQL code from tags\n        tag_extractor = TagExtractor(sql_str)\n        extracted_tags = tag_extractor.extract()\n        sql = extracted_tags.content[0].content\n        logger.info(f\"Extracted SQL: {sql}\")\n\n        logger.info(f\"Sending SQL to Byzer SQL URL: {self.byzer_sql_url}\")\n        try:\n            response = requests.post(\n                self.byzer_sql_url,\n                data={\n                    \"sql\": sql,\n                    \"owner\": \"hello\",\n                    \"sessionPerUser\": True,\n                    \"sessionPerRequest\": True,\n                    \"includeSchema\": True,\n                },\n            )\n            response.raise_for_status()\n            result = response.content.decode(\"utf-8\")\n            logger.info(\"SQL execution result decoded\")\n            logger.debug(f\"SQL execution result: {result}\")\n            return (item for item in [result]), []\n        except requests.exceptions.RequestException as e:\n            logger.error(f\"Failed to send SQL to Byzer SQL URL: {e}\")\n            if hasattr(e.response, \"content\"):\n                error_content = e.response.content.decode(\"utf-8\")\n                logger.error(f\"Server response: {error_content}\")\n            else:\n                logger.error(\"No server response available\")\n            raise Exception(f\"Byzer SQL request failed: {str(e)}\") from e\n\n\n##File: /Users/allwefantasy/projects/super_analysis/setup.py\nfrom setuptools import setup, find_packages\n\nsetup(\n    name=\"super-analysis\",\n    version=\"0.1.0\",\n    packages=find_packages(where=\"src\"),\n    package_dir={\"\": \"src\"},\n    install_requires=[\n        \"byzerllm\",\n        \"fastapi\",\n        \"uvicorn\",\n        \"argparse\",\n    ],\n    entry_points={\n        \"console_scripts\": [\n            \"super-analysis.serve=superanalysis.serve.cli:main\",\n            \"super-analysis.convert=superanalysis.serve.convert:main\",\n        ],\n    },\n    author=\"allwefantasy\",\n    author_email=\"allwefantasy@gmail.com\",\n    description=\"A package for super analysis with ByzerLLM\",\n    long_description=open(\"README.md\").read(),\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/yourusername/super-analysis\",\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n    ],\n    python_requires='>=3.6',\n)\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/convert.py\nimport os\nimport argparse\nimport pandas as pd\nimport hashlib\nfrom typing import List, Dict\n\ndef get_csv_files(directory: str) -> List[str]:\n    \"\"\"Recursively get all CSV files in the given directory.\"\"\"\n    csv_files = []\n    for root, _, files in os.walk(directory):\n        for file in files:\n            if file.endswith('.csv'):\n                csv_files.append(os.path.join(root, file))\n    return csv_files\n\ndef extract_schema(df: pd.DataFrame) -> Dict[str, str]:\n    \"\"\"Extract schema from pandas DataFrame.\"\"\"\n    schema = {}\n    for column in df.columns:\n        dtype = str(df[column].dtype)\n        if dtype == 'object':\n            schema[column] = 'STRING'\n        elif dtype.startswith('int'):\n            schema[column] = 'INTEGER'\n        elif dtype.startswith('float'):\n            schema[column] = 'FLOAT'\n        elif dtype == 'bool':\n            schema[column] = 'BOOLEAN'\n        elif dtype.startswith('datetime'):\n            schema[column] = 'TIMESTAMP'\n        else:\n            schema[column] = 'STRING'  # Default to STRING for unknown types\n    return schema\n\ndef generate_schema_text(schema: Dict[str, str], csv_file: str, df: pd.DataFrame, include_rows_num: int) -> str:\n    \"\"\"Generate schema text from the schema dictionary and include the CSV file name and sample data.\"\"\"\n    schema_lines = [f\"CSV File: {csv_file}\\n\"]\n    schema_lines.extend([f\"{field}: {dtype}\" for field, dtype in schema.items()])\n    \n    if include_rows_num > 0:\n        schema_lines.append(f\"\\nSample Data (first {include_rows_num} rows):\")\n        sample_data = df.head(include_rows_num).to_string(index=False)\n        schema_lines.append(sample_data)\n    \n    return \"\\n\".join(schema_lines)\n\ndef save_schema(schema_text: str, csv_file: str, output_dir: str) -> None:\n    \"\"\"Save schema text to a file in the output directory.\"\"\"\n    # Generate MD5 hash of the schema text\n    file_name = hashlib.md5(schema_text.encode()).hexdigest() + '.md'\n    output_path = os.path.join(output_dir, file_name)\n    \n    with open(output_path, 'w') as f:\n        f.write(schema_text)\n    print(f\"Schema for {csv_file} saved to {output_path}\")\n\ndef convert(data_dir: str, doc_dir: str, include_rows_num: int) -> None:\n    \"\"\"Main conversion function.\"\"\"\n    csv_files = get_csv_files(data_dir)\n    \n    for csv_file in csv_files:\n        print(f\"Processing {csv_file}\")\n        df = pd.read_csv(csv_file)\n        schema = extract_schema(df)\n        schema_text = generate_schema_text(schema, csv_file, df, include_rows_num)\n        save_schema(schema_text, csv_file, doc_dir)\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Convert CSV files to schema documents\")\n    parser.add_argument(\"--data_dir\", required=True, help=\"Directory containing CSV files\")\n    parser.add_argument(\"--doc_dir\", required=True, help=\"Output directory for schema documents\")\n    parser.add_argument(\"--include-rows-num\", type=int, default=0, help=\"Number of sample rows to include in the schema\")\n    \n    args = parser.parse_args()\n    \n    if not os.path.exists(args.doc_dir):\n        os.makedirs(args.doc_dir)\n    \n    convert(args.data_dir, args.doc_dir, args.include_rows_num)\n\nif __name__ == \"__main__\":\n    main()\n\n     \n\n下面是用户的需求：\n\n提供一个普通 fastapi 实现的接口,然后通过命令 superanalysis.api 启动\n"}, {"role": "assistant", "content": "为了实现这个需求，我们需要创建一个新的 Python 文件来定义 FastAPI 接口，并修改 `setup.py` 文件以添加新的命令行入口点。让我们开始吧：\n\n首先，我们创建一个新的文件 `api.py`：\n\n```python\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/api.py\n<<<<<<< SEARCH\n=======\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nfrom typing import List, Optional\nimport uvicorn\nimport argparse\nfrom superanalysis.sql.generator import SQLGenerator\nfrom superanalysis.sql.types import RAGConfig, ByzerSQLConfig, ModelConfig\nimport byzerllm\n\napp = FastAPI()\n\nclass ChatMessage(BaseModel):\n    role: str\n    content: str\n\nclass ChatRequest(BaseModel):\n    conversations: List[ChatMessage]\n    model: Optional[str] = None\n\nclass ChatResponse(BaseModel):\n    response: str\n\n@app.post(\"/chat\", response_model=ChatResponse)\nasync def chat(request: ChatRequest):\n    try:\n        result, _ = sql_generator.stream_chat_oai(\n            conversations=[{\"role\": msg.role, \"content\": msg.content} for msg in request.conversations],\n            model=request.model\n        )\n        return ChatResponse(response=\"\".join(result))\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"SuperAnalysis API Server\")\n    parser.add_argument(\"--host\", type=str, default=\"0.0.0.0\", help=\"Host to bind the server to\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"Port to bind the server to\")\n    parser.add_argument(\"--schema-rag-base-url\", type=str, required=True, help=\"Schema RAG base URL\")\n    parser.add_argument(\"--schema-rag-api-key\", type=str, required=True, help=\"Schema RAG API key\")\n    parser.add_argument(\"--context-rag-base-url\", type=str, required=True, help=\"Context RAG base URL\")\n    parser.add_argument(\"--context-rag-api-key\", type=str, required=True, help=\"Context RAG API key\")\n    parser.add_argument(\"--byzer-sql-url\", type=str, required=True, help=\"Byzer SQL URL\")\n    parser.add_argument(\"--data-dir\", type=str, default=\"\", help=\"Data directory\")\n    parser.add_argument(\"--llm-model\", type=str, required=True, help=\"LLM model for general use\")\n    parser.add_argument(\"--gen-sql-llm-model\", type=str, required=True, help=\"LLM model for SQL generation\")\n    \n    args = parser.parse_args()\n\n    rag_config = RAGConfig(\n        schema_rag_base_url=args.schema_rag_base_url,\n        schema_rag_api_key=args.schema_rag_api_key,\n        context_rag_base_url=args.context_rag_base_url,\n        context_rag_api_key=args.context_rag_api_key\n    )\n\n    byzer_sql_config = ByzerSQLConfig(\n        byzer_sql_url=args.byzer_sql_url,\n        data_dir=args.data_dir\n    )\n\n    model_config = ModelConfig(\n        llm_model=args.llm_model,\n        gen_sql_llm_model=args.gen_sql_llm_model\n    )\n\n    llm = byzerllm.ByzerLLM()\n    llm.setup_default_model_name(args.llm_model)\n\n    global sql_generator\n    sql_generator = SQLGenerator(\n        llm=llm,\n        rag_config=rag_config,\n        byzer_sql_config=byzer_sql_config,\n        model_config=model_config\n    )\n\n    uvicorn.run(app, host=args.host, port=args.port)\n\nif __name__ == \"__main__\":\n    main()\n>>>>>>> REPLACE\n```\n\n现在，我们需要修改 `setup.py` 文件以添加新的命令行入口点：\n\n```python\n##File: /Users/allwefantasy/projects/super_analysis/setup.py\n<<<<<<< SEARCH\n    entry_points={\n        \"console_scripts\": [\n            \"super-analysis.serve=superanalysis.serve.cli:main\",\n            \"super-analysis.convert=superanalysis.serve.convert:main\",\n        ],\n    },\n=======\n    entry_points={\n        \"console_scripts\": [\n            \"super-analysis.serve=superanalysis.serve.cli:main\",\n            \"super-analysis.convert=superanalysis.serve.convert:main\",\n            \"superanalysis.api=superanalysis.serve.api:main\",\n        ],\n    },\n>>>>>>> REPLACE\n```\n\n这些更改将创建一个新的 FastAPI 应用程序，它提供了一个 `/chat` 端点来处理聊天请求。用户可以通过发送包含对话历史的 POST 请求来获取响应。\n\n新的命令行入口点 `superanalysis.api` 将允许用户通过命令行启动 API 服务器。用户可以使用以下命令启动服务器：\n\n```\nsuperanalysis.api --schema-rag-base-url <url> --schema-rag-api-key <key> --context-rag-base-url <url> --context-rag-api-key <key> --byzer-sql-url <url> --llm-model <model> --gen-sql-llm-model <model> [--host <host>] [--port <port>] [--data-dir <dir>]\n```\n\n这个实现提供了一个简单的 FastAPI 接口，它使用 `SQLGenerator` 类来处理聊天请求。用户可以通过 HTTP POST 请求与 API 交互，发送对话历史并接收生成的响应。\n\n请注意，您可能需要安装额外的依赖，如 `fastapi` 和 `uvicorn`。您可以将它们添加到 `setup.py` 文件的 `install_requires` 列表中：\n\n```python\n##File: /Users/allwefantasy/projects/super_analysis/setup.py\n<<<<<<< SEARCH\n    install_requires=[\n        \"byzerllm\",\n        \"fastapi\",\n        \"uvicorn\",\n        \"argparse\",\n    ],\n=======\n    install_requires=[\n        \"byzerllm\",\n        \"fastapi\",\n        \"uvicorn\",\n        \"argparse\",\n        \"pydantic\",\n        \"requests\",\n    ],\n>>>>>>> REPLACE\n```\n\n这个实现应该满足您的需求，提供了一个通过 `superanalysis.api` 命令启动的普通 FastAPI 实现的接口。"}], "model": "human", "yaml_file": "actions/128_chat_action.yml"}
{"instruction": "@setup.py 改成从 requirements.txt 里读取", "conversations": [{"role": "user", "content": "\n如果你需要生成代码，对于每个需要更改的文件,你需要按 *SEARCH/REPLACE block* 的格式进行生成。\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The opening fence and code language, eg: ```python\n2. The file path alone on a line, starting with \"##File:\" and verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n*SEARCH/REPLACE* blocks will replace *all* matching occurrences.\nInclude enough lines to make the SEARCH blocks unique.\n\nInclude *ALL* the code being searched and replaced!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n下面我们来看一个例子：\n\n当前项目目录结构：\n1. 项目根目录： /tmp/projects/mathweb\n2. 项目子目录/文件列表(类似tree 命令输出)\nflask/\n    app.py\n    templates/\n        index.html\n    static/\n        style.css\n\n用户需求： Change get_factorial() to use math.factorial\n\n回答： To make this change we need to modify `/tmp/projects/mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n用户需求： Refactor hello() into its own file.\n\n回答：To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/hello.py\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/main.py\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n现在让我们开始一个新的任务:\n\n当前项目目录结构：\n1. 项目根目录： /Users/allwefantasy/projects/super_analysis\n2. 项目子目录/文件列表(类似tree 命令输出)：\nsetup.py\n.auto-coder/\n    libs/\n        llm_friendly_packages/\n            src/\n                example1/\n                    main.py\nsrc/\n    superanalysis/\n        serve/\n            convert.py\n            openai_serve.py\n            __init__.py\n            api.py\n            llm_wrapper.py\n            api_server.py\n            cli.py\n        sql/\n            __init__.py\n            types.py\n            generator.py\n下面是一些文件路径以及每个文件对应的源码：\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/types.py\nfrom pydantic import BaseModel\n\nclass RAGConfig(BaseModel):\n    schema_rag_base_url: str = \"\"\n    schema_rag_api_key: str = \"xxxx\"\n\n    context_rag_base_url: str = \"\"\n    context_rag_api_key: str = \"xxxx\"\n\nclass ByzerSQLConfig(BaseModel):\n    byzer_sql_url: str = \"\"\n    data_dir: str = \"\"\n\nclass ModelConfig(BaseModel):\n    llm_model: str = \"\"\n    gen_sql_llm_model: str = \"\"\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/cli.py\nimport argparse\nimport byzerllm\nfrom byzerllm.utils.client import ByzerLLM\nfrom superanalysis.serve.api_server import serve, ServerArgs\nfrom superanalysis.sql.types import RAGConfig, ByzerSQLConfig, ModelConfig\nfrom superanalysis.sql.generator import SQLGenerator\nfrom superanalysis.serve.llm_wrapper import LLWrapper\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"ByzerLLM API Server\")\n    parser.add_argument(\"--ray-address\", type=str, default=\"auto\", help=\"ray address\")\n    parser.add_argument(\n        \"--host\", type=str, default=\"0.0.0.0\", help=\"Host to bind the server to\"\n    )\n    parser.add_argument(\n        \"--port\", type=int, default=8000, help=\"Port to bind the server to\"\n    )\n    parser.add_argument(\n        \"--uvicorn-log-level\", type=str, default=\"info\", help=\"Uvicorn log level\"\n    )\n    parser.add_argument(\n        \"--allow-credentials\", action=\"store_true\", help=\"Allow credentials\"\n    )\n    parser.add_argument(\n        \"--allowed-origins\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed origins\"\n    )\n    parser.add_argument(\n        \"--allowed-methods\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed methods\"\n    )\n    parser.add_argument(\n        \"--allowed-headers\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed headers\"\n    )\n    parser.add_argument(\n        \"--api-key\", type=str, default=\"\", help=\"API key for authentication\"\n    )\n    parser.add_argument(\n        \"--served-model-name\",\n        type=str,\n        required=True,\n        help=\"Name of the model to be served\",\n    )\n    parser.add_argument(\n        \"--prompt-template\", type=str, default=\"\", help=\"Prompt template\"\n    )\n    parser.add_argument(\n        \"--response-role\", type=str, default=\"assistant\", help=\"Response role\"\n    )\n    parser.add_argument(\"--ssl-keyfile\", type=str, default=\"\", help=\"SSL key file\")\n    parser.add_argument(\n        \"--ssl-certfile\", type=str, default=\"\", help=\"SSL certificate file\"\n    )\n\n    # Add new arguments for RAGConfig\n    parser.add_argument(\n        \"--schema-rag-base-url\", type=str, default=\"\", help=\"RAG base URL\"\n    )\n    parser.add_argument(\n        \"--schema-rag-api-key\", type=str, default=\"xxxx\", help=\"RAG API key\"\n    )\n\n    parser.add_argument(\n        \"--context-rag-base-url\", type=str, default=\"\", help=\"RAG base URL\"\n    )\n    parser.add_argument(\n        \"--context-rag-api-key\", type=str, default=\"xxxx\", help=\"RAG API key\"\n    )\n\n    # Add new argument for ByzerSQLConfig\n    parser.add_argument(\"--byzer-sql-url\", type=str, default=\"\", help=\"Byzer SQL URL\")\n    parser.add_argument(\"--data-dir\", type=str, default=\"\", help=\"Data directory\")\n    parser.add_argument(\n        \"--llm-model\", type=str, default=\"\", help=\"LLM model for general use\"\n    )\n    parser.add_argument(\n        \"--gen-sql-llm-model\", type=str, default=\"\", help=\"LLM model for SQL generation\"\n    )\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile,\n    )\n\n    # Create RAGConfig instance\n    rag_config = RAGConfig(\n        schema_rag_base_url=args.schema_rag_base_url,\n        schema_rag_api_key=args.schema_rag_api_key,\n        context_rag_base_url=args.context_rag_base_url,\n        context_rag_api_key=args.context_rag_api_key,\n    )\n\n    # Create ByzerSQLConfig instance\n    byzer_sql_config = ByzerSQLConfig(\n        byzer_sql_url=args.byzer_sql_url, data_dir=args.data_dir\n    )\n\n    model_config = ModelConfig(\n        llm_model=args.llm_model or args.served_model_name,\n        gen_sql_llm_model=args.gen_sql_llm_model or args.served_model_name,\n    )\n\n    byzerllm.connect_cluster(address=args.ray_address)\n    llm = ByzerLLM()\n    llm.setup_default_model_name(args.served_model_name)\n    llm = byzerllm.ByzerLLM()\n    if not server_args.served_model_name:\n        raise ValueError(\"served_model_name is required\")\n    llm.setup_default_model_name(server_args.served_model_name)\n\n    print(\"RAG Config: \", rag_config)\n    print(\"Model Config: \", model_config)\n    rag = SQLGenerator(\n        llm=llm,\n        rag_config=rag_config,\n        byzer_sql_config=byzer_sql_config,\n        model_config=model_config,\n    )\n    llm_wrapper = LLWrapper(llm=llm, entry=rag)\n    serve(llm_wrapper, server_args)\n\n\nif __name__ == \"__main__\":\n    main()\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/__init__.py\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/__init__.py\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/llm_wrapper.py\nfrom typing import Any, Dict, List, Optional, Union,Callable\nfrom byzerllm.utils.client.types import (    \n    LLMFunctionCallResponse,\n    LLMClassResponse,LLMResponse    \n)\nimport pydantic\nfrom byzerllm import ByzerLLM\nfrom byzerllm.utils.client import LLMResponse\nfrom byzerllm.utils.types import SingleOutputMeta\nfrom byzerllm.utils.langutil import asyncfy_with_semaphore\nfrom superanalysis.sql.generator import SQLGenerator\nfrom loguru import logger\n\nclass LLWrapper:\n\n    def __init__(self,llm:ByzerLLM,entry:SQLGenerator):\n        self.entry = entry\n        self.llm = llm\n        \n\n    def chat_oai(self,\n                 conversations,\n                 tools:List[Union[Callable,str]]=[], \n                 tool_choice:Optional[Union[Callable,str]]=None,\n                 execute_tool:bool=False,  \n                 impl_func:Optional[Callable]=None,\n                 execute_impl_func:bool=False,\n                 impl_func_params:Optional[Dict[str,Any]]=None,\n                 func_params:Optional[Dict[str,Any]]=None,\n                 response_class:Optional[Union[pydantic.BaseModel,str]] = None, \n                 response_after_chat:Optional[Union[pydantic.BaseModel,str]] = False,\n                 enable_default_sys_message:bool=True,                 \n                 model:Optional[str] = None,\n                 role_mapping=None,llm_config:Dict[str,Any]={}\n                 )->Union[List[LLMResponse],List[LLMFunctionCallResponse],List[LLMClassResponse]]: \n        res,contexts = self.entry.stream_chat_oai(conversations)\n        s = \"\".join(res)        \n        return [LLMResponse(output=s,metadata={},input=\"\")]\n\n    def stream_chat_oai(self,conversations, \n                        model:Optional[str]=None, \n                        role_mapping=None,\n                        delta_mode=False,\n                        llm_config:Dict[str,Any]={}): \n        res,contexts = self.entry.stream_chat_oai(conversations)        \n        for t in res:                        \n            yield (t,SingleOutputMeta(0,0))\n\n    async def async_stream_chat_oai(self,conversations, \n                        model:Optional[str]=None, \n                        role_mapping=None,\n                        delta_mode=False,\n                        llm_config:Dict[str,Any]={}): \n        res,contexts = self.rag.stream_chat_oai(conversations)                \n        for t in res:                                    \n            yield (t,SingleOutputMeta(0,0))\n                         \n\n    def __getattr__(self, name):        \n        return getattr(self.llm, name)\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/openai_serve.py\nimport argparse\nfrom service_server.api_server import ServerArgs, serve\nfrom service_server.llm_wrapper import LLWrapper\nfrom service_server.summary_rag import SummaryRAG\nimport byzerllm\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"OpenAI Serve\")\n    parser.add_argument(\n        \"--model\",\n        default=\"\",\n    )\n    parser.add_argument(\"--emb_model\", default=\"\")\n    parser.add_argument(\n        \"--ray_address\",\n        default=\"auto\",\n    )\n    parser.add_argument(\"--source_dir\", default=\".\", help=\"\")\n    parser.add_argument(\"--host\", default=\"\", help=\"\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"\")\n    parser.add_argument(\"--uvicorn_log_level\", default=\"info\", help=\"\")\n    parser.add_argument(\"--allow_credentials\", action=\"store_true\", help=\"\")\n    parser.add_argument(\"--allowed_origins\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--allowed_methods\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--allowed_headers\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--api_key\", default=\"\", help=\"\")\n    parser.add_argument(\"--served_model_name\", default=\"\", help=\"\")\n    parser.add_argument(\"--prompt_template\", default=\"\", help=\"\")\n    parser.add_argument(\"--ssl_keyfile\", default=\"\", help=\"\")\n    parser.add_argument(\"--ssl_certfile\", default=\"\", help=\"\")\n    parser.add_argument(\"--response_role\", default=\"assistant\", help=\"\")\n    parser.add_argument(\n        \"--collections\", default=\"\", help=\"Collection name for indexing\"\n    )\n    parser.add_argument(\n        \"--base_dir\",\n        default=\"\",\n        help=\"Path where the processed text embeddings were stored\",\n    )\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name or args.model,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile,\n    )\n    byzerllm.connect_cluster(address=args.ray_address)\n    llm = byzerllm.ByzerLLM()\n    if not server_args.served_model_name:\n        raise ValueError(\"served_model_name is required\")\n    llm.setup_default_model_name(server_args.served_model_name)\n    rag = SummaryRAG(llm=llm)    \n    llm_wrapper = LLWrapper(llm=llm, rag=rag)\n    serve(llm=llm_wrapper, args=server_args)\n\nif __name__ == \"__main__\":\n    main()\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/api_server.py\nimport os\nimport time\nimport uvicorn\nfrom fastapi import Request\nfrom fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import JSONResponse, StreamingResponse, Response\n\nfrom byzerllm.log import init_logger\nfrom byzerllm.utils import random_uuid\nfrom byzerllm.version import __version__ as version\nfrom byzerllm.utils.client import ByzerLLM, LLMRequest\nfrom byzerllm.utils.client.entrypoints.openai.serving_chat import OpenAIServingChat\nfrom byzerllm.utils.client.entrypoints.openai.serving_completion import OpenAIServingCompletion\nfrom byzerllm.utils.client.entrypoints.openai.protocol import (\n    ModelList,\n    ModelCard,\n    ModelPermission,\n    ChatCompletionRequest,\n    ErrorResponse,\n    CompletionRequest,\n    Embeddings,\n    EmbeddingsOutput,\n    EmbeddingsData,\n    EmbeddingsUsage,\n)\nfrom pydantic import BaseModel\nfrom typing import List\n\nlogger = init_logger(__name__)\n\nllm_client: ByzerLLM = None\nopenai_serving_chat: OpenAIServingChat = None\nopenai_serving_completion: OpenAIServingCompletion = None\n\nTIMEOUT_KEEP_ALIVE = 5  # seconds\n# timeout in 10 minutes. Streaming can take longer than 3 min\nTIMEOUT = float(os.environ.get(\"BYZERLLM_APISERVER_HTTP_TIMEOUT\", 600))\n\nrouter_app = FastAPI()\n\n\n@router_app.get(\"/health\")\nasync def health() -> Response:\n    \"\"\"Health check.\"\"\"\n    return Response(status_code=200)\n\n\n@router_app.get(\"/v1/models\")\nasync def show_available_models():\n    models = await openai_serving_chat.show_available_models()\n    return JSONResponse(content=models.model_dump())\n\n\n@router_app.get(\"/version\")\nasync def show_version():\n    return JSONResponse(content={\"version\": version})\n\n\n@router_app.get(\"/v1/models\", response_model=ModelList)\nasync def models() -> ModelList:\n    \"\"\"Show available models. Right now we only have one model.\"\"\"\n    model_cards = [\n        ModelCard(\n            id=\"\",\n            root=\"\",\n            permission=[ModelPermission()]\n        )\n    ]\n    return ModelList(data=model_cards)\n\n\n@router_app.post(\"/v1/completions\")\nasync def create_completion(\n        body: CompletionRequest,\n        request: Request\n):\n    generator = await openai_serving_completion.create_completion(body, request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            status_code=generator.code\n        )\n    if body.stream:\n        return StreamingResponse(\n            content=generator,\n            media_type=\"text/event-stream\"\n        )\n    else:\n        return JSONResponse(content=generator.model_dump())\n\n\n@router_app.post(\"/v1/chat/completions\")\nasync def create_chat_completion(\n        body: ChatCompletionRequest,\n        request: Request,\n):\n    \"\"\"Completion API similar to OpenAI's API.\n\n    See  https://platform.openai.com/docs/api-reference/chat/create\n    for the API specification. This API mimics the OpenAI ChatCompletion API.\n\n    NOTE: Currently we do not support the following features:\n        - function_call (Users should implement this by themselves)\n        - logit_bias (to be supported by vLLM engine)\n    \"\"\"\n    # async with async_timeout.timeout(TIMEOUT):\n\n    generator = await openai_serving_chat.create_chat_completion(body, request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            status_code=generator.code\n        )\n    if body.stream:\n        return StreamingResponse(\n            content=generator,\n            media_type=\"text/event-stream\"\n        )\n    else:\n        return JSONResponse(content=generator.model_dump())\n\n\n@router_app.post(\"/v1/embeddings\")\nasync def embed(body: Embeddings):\n    \"\"\"Given a prompt, the model will return one embedding.\n\n    Returns:\n        A response object with an embedding.\n    \"\"\"\n    embedding_id = f\"embed-{random_uuid()}\"\n\n    results_list = llm_client.emb(body.model, request=LLMRequest(instruction=body.input))\n    tokens = 0\n\n    return EmbeddingsOutput(\n        data=[\n            EmbeddingsData(\n                embedding=results.output,\n                index=i,\n                object=\"embedding\",\n            )\n            for i, results in enumerate(results_list)\n        ],\n        id=embedding_id,\n        object=\"list\",\n        created=int(time.time()),\n        model=body.model,\n        usage=EmbeddingsUsage(\n            prompt_tokens=tokens,\n            total_tokens=tokens,\n        ),\n    )\n\nclass ServerArgs(BaseModel):\n    host: str = None\n    port: int = 8000\n    uvicorn_log_level: str = \"info\"\n    allow_credentials: bool = False\n    allowed_origins: List[str] = [\"*\"]  \n    allowed_methods: List[str] = [\"*\"]\n    allowed_headers: List[str] = [\"*\"]\n    api_key: str = None\n    served_model_name: str = None\n    prompt_template: str = None\n    response_role: str = \"assistant\"\n    ssl_keyfile: str = None\n    ssl_certfile: str = None    \n\ndef serve(llm:ByzerLLM, args: ServerArgs):\n    \n    logger.info(f\"ByzerLLM API server version {version}\")\n    logger.info(f\"args: {args}\")\n\n    router_app.add_middleware(\n        CORSMiddleware,\n        allow_origins=args.allowed_origins,\n        allow_credentials=args.allow_credentials,\n        allow_methods=args.allowed_methods,\n        allow_headers=args.allowed_headers,\n    )\n    \n    if token := os.environ.get(\"BYZERLLM_API_KEY\") or args.api_key:\n\n        @router_app.middleware(\"http\")\n        async def authentication(request: Request, call_next):\n            if not request.url.path.startswith(\"/v1\"):\n                return await call_next(request)\n            if request.headers.get(\"Authorization\") != \"Bearer \" + token:\n                return JSONResponse(\n                    content={\"error\": \"Unauthorized\"},\n                    status_code=401\n                )\n            return await call_next(request)\n\n    # Register labels for metrics\n    # add_global_metrics_labels(model_name=engine_args.model)\n    global llm_client\n    llm_client = llm\n    \n    global openai_serving_chat\n    openai_serving_chat = OpenAIServingChat(\n        llm_client=llm_client,\n        response_role=args.response_role,\n        server_model_name=args.served_model_name,\n        prompt_template=args.prompt_template\n    )\n    global openai_serving_completion\n    openai_serving_completion = OpenAIServingCompletion(\n        llm_client=llm_client,\n        server_model_name=args.served_model_name,\n        prompt_template=args.prompt_template\n    )\n\n    uvicorn.run(\n        router_app,\n        host=args.host,\n        port=args.port,\n        log_level=args.uvicorn_log_level,\n        timeout_keep_alive=TIMEOUT_KEEP_ALIVE,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile\n    )\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/generator.py\nfrom typing import Optional, Dict, Any, List\nfrom openai import OpenAI\nimport json\nfrom .types import RAGConfig, ByzerSQLConfig, ModelConfig\nfrom byzerllm.utils.client import code_utils\nimport byzerllm\nimport requests\nfrom loguru import logger\nimport traceback\nfrom byzerllm.utils.nontext import TagExtractor\n\n\nclass SQLGenerator:\n\n    @byzerllm.prompt()\n    def gen_context(self, conversations: List[Dict[str, str]]) -> str:\n        \"\"\"\n        对话内容：\n        {% for msg in conversations %}\n        <{{ msg.role }}>: {{ msg.content }}\n        {% endfor %}\n\n        根据提供的对话内容，理解用户最后的问题，然后提取可能会用到的 Byzer-SQL 相关的知识。\n        \"\"\"\n\n    @byzerllm.prompt()\n    def gen_sql(\n        self,\n        conversations: List[Dict[str, str]],\n        context: str,\n        schema: str,\n        model: str = \"deepseek_chat\",\n    ) -> str:\n        \"\"\"\n        相关知识：\n        {{ context }}\n\n        Schema信息：\n        {{ schema }}\n\n        对话内容：\n        {% for msg in conversations %}\n        <{{ msg.role }}>: {{ msg.content }}\n        {% endfor %}\n\n        模型UDF函数：\n        {{ model }}\n\n        根据提供的相关知识以及对话内容，理解用户最后的问题，然后根据看到的schema 信息，生成 Byzer-SQL 语句。\n        Byzer-SQL 首先需要用 load 语法加载 schema 中的路径映射成表格，然后根据表格的信息生成对应的 Byzer-SQL 语句。\n\n        特别需要关注的是:\n        1. 用户的很多需求是要对表格的文本内容做自然语言处理，你需要使用 Byzer-SQL 中的大模型辅助函数来完成这个需求。\n        2. 使用 `llm_result` 函数处理 LLM 模型函数的响应,`llm_result` 函数得到的是一段文本，虽然我们要求模型函数只输出特定的文本，但考虑到大模型会有一些额外的字符输出，不会只是输出我们要求的文字，所以通过 `like` 来进行匹配。\n        3. 注意 Byzer-SQL 的语法，比如select 语句最后一定需要有 as <table_name>。\n        3. 输出的 Byzer-SQL 语句务必使用 <_byzer_sql_></_byzer_sql_> 标签包裹起来。\n        \"\"\"\n\n    @byzerllm.prompt()\n    def fetch_schema(self, context: str) -> str:\n        \"\"\"\n        下面是一些基础知识，包括名词术语等等：\n        {{ context }}\n\n        当理解对话内容的时候，需要参考上面的基础知识，然后更好的理解对应的表格schema信息是不是有用。\n        \"\"\"\n\n    def __init__(\n        self,\n        llm: byzerllm.ByzerLLM,\n        rag_config: RAGConfig,\n        byzer_sql_config: ByzerSQLConfig,\n        model_config: ModelConfig,\n    ) -> None:\n        self.llm = llm\n        self.schema_rag_client = OpenAI(\n            api_key=rag_config.schema_rag_api_key,\n            base_url=rag_config.schema_rag_base_url,\n        )\n        self.context_rag_client = OpenAI(\n            api_key=rag_config.context_rag_api_key,\n            base_url=rag_config.context_rag_base_url,\n        )\n        self.byzer_sql_url = byzer_sql_config.byzer_sql_url\n\n        self.llm_model = byzerllm.ByzerLLM()\n        self.llm_model.setup_default_model_name(model_config.llm_model)\n        self.gen_sql_llm_model = byzerllm.ByzerLLM()\n        self.gen_sql_llm_model.setup_default_model_name(model_config.gen_sql_llm_model)\n\n    def stream_chat_oai(\n        self,\n        conversations,\n        model: Optional[str] = None,\n        role_mapping=None,\n        delta_mode=False,\n        llm_config: Dict[str, Any] = {},\n    ):\n        try:\n            v, c = self._stream_chat_oai(\n                conversations=conversations,\n                model=model,\n                role_mapping=role_mapping,\n                delta_mode=delta_mode,\n                llm_config=llm_config,\n            )\n        except Exception as e:\n            error_msg = f\"Error in stream_chat_oai: {e}\\n{traceback.format_exc()}\"\n            logger.error(error_msg)\n            v, c = [str(e)], []\n\n        return v, c\n\n    def _stream_chat_oai(\n        self,\n        conversations,\n        model: Optional[str] = None,\n        role_mapping=None,\n        delta_mode=False,\n        llm_config: Dict[str, Any] = {},\n    ):\n        logger.info(f\"Starting stream_chat_oai with model: {model}\")\n        logger.debug(f\"Conversations: {conversations}\")\n        logger.debug(f\"LLM Config: {llm_config}\")\n\n        if not model:\n            model = self.llm_model.default_model_name\n        logger.info(f\"Using model: {model}\")\n\n        query = conversations[-1][\"content\"]\n\n        if (\n            \"使用四到五个字直接返回这句话的简要主题，不要解释、不要标点、不要语气词、不要多余文本，不要加粗，如果没有主题\"\n            in query\n            or \"简要总结一下对话内容，用作后续的上下文提示 prompt，控制在 200 字以内\"\n            in query\n        ):\n            chunks = self.llm.stream_chat_oai(\n                conversations=conversations,\n                model=model,\n                role_mapping=role_mapping,\n                llm_config=llm_config,\n                delta_mode=True,\n            )\n            return (chunk[0] for chunk in chunks), []\n\n        logger.info(\"Fetching context from context RAG client\")\n        context_query = self.gen_context.prompt(conversations=conversations)\n        context_response = self.context_rag_client.chat.completions.create(\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": json.dumps(\n                        {\"query\": context_query, \"only_contexts\": True},\n                        ensure_ascii=False,\n                    ),\n                }\n            ],\n            model=model,\n        )\n\n        context = context_response.choices[0].message.content\n        new_context = \"\"\n        for msg in context.split(\"\\n\"):\n            if msg.strip():\n                v = json.loads(msg)\n                new_context += v[\"source_code\"]\n        logger.info(f\"context {new_context[0:100]}...\")\n        logger.info(\"Context retrieved successfully\")\n\n        logger.info(\"Fetching schemas from schema RAG client\")\n        schema_query = self.fetch_schema.prompt(context=new_context)\n\n        response = self.schema_rag_client.chat.completions.create(\n            messages=conversations[0:-1]\n            + [\n                {\n                    \"role\": \"user\",\n                    \"content\": json.dumps(\n                        {\"query\": schema_query, \"only_contexts\": True},\n                        ensure_ascii=False,\n                    ),\n                }\n            ],\n            model=model,\n        )\n        logger.debug(f\"Schema RAG response: {response}\")\n\n        schema_str = response.choices[0].message.content\n        logger.info(\"Schema retrieved successfully\")\n\n        logger.info(\"Generating SQL string\")\n        sql_str = self.gen_sql.with_llm(self.gen_sql_llm_model).run(\n            conversations=conversations,\n            context=new_context,\n            schema=schema_str,\n            model=model,\n        )\n\n        logger.info(\"SQL string generated\")\n        logger.debug(f\"Generated SQL string: {sql_str}\")\n\n        # Extract SQL code from tags\n        tag_extractor = TagExtractor(sql_str)\n        extracted_tags = tag_extractor.extract()\n        sql = extracted_tags.content[0].content\n        logger.info(f\"Extracted SQL: {sql}\")\n\n        logger.info(f\"Sending SQL to Byzer SQL URL: {self.byzer_sql_url}\")\n        try:\n            response = requests.post(\n                self.byzer_sql_url,\n                data={\n                    \"sql\": sql,\n                    \"owner\": \"hello\",\n                    \"sessionPerUser\": True,\n                    \"sessionPerRequest\": True,\n                    \"includeSchema\": True,\n                },\n            )\n            response.raise_for_status()\n            result = response.content.decode(\"utf-8\")\n            logger.info(\"SQL execution result decoded\")\n            logger.debug(f\"SQL execution result: {result}\")\n            return (item for item in [result]), []\n        except requests.exceptions.RequestException as e:\n            logger.error(f\"Failed to send SQL to Byzer SQL URL: {e}\")\n            if hasattr(e.response, \"content\"):\n                error_content = e.response.content.decode(\"utf-8\")\n                logger.error(f\"Server response: {error_content}\")\n            else:\n                logger.error(\"No server response available\")\n            raise Exception(f\"Byzer SQL request failed: {str(e)}\") from e\n\n\n##File: /Users/allwefantasy/projects/super_analysis/setup.py\nfrom setuptools import setup, find_packages\n\nsetup(\n    name=\"super-analysis\",\n    version=\"0.1.0\",\n    packages=find_packages(where=\"src\"),\n    package_dir={\"\": \"src\"},\n    install_requires=[\n        \"byzerllm\",\n        \"fastapi\",\n        \"uvicorn\",\n        \"argparse\",\n        \"pydantic\",\n        \"requests\",\n    ],\n    entry_points={\n        \"console_scripts\": [\n            \"super-analysis.serve=superanalysis.serve.cli:main\",\n            \"super-analysis.convert=superanalysis.serve.convert:main\",\n            \"superanalysis.api=superanalysis.serve.api:main\",\n        ],\n    },\n    author=\"allwefantasy\",\n    author_email=\"allwefantasy@gmail.com\",\n    description=\"A package for super analysis with ByzerLLM\",\n    long_description=open(\"README.md\").read(),\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/yourusername/super-analysis\",\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n    ],\n    python_requires='>=3.6',\n)\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/convert.py\nimport os\nimport argparse\nimport pandas as pd\nimport hashlib\nfrom typing import List, Dict\n\ndef get_csv_files(directory: str) -> List[str]:\n    \"\"\"Recursively get all CSV files in the given directory.\"\"\"\n    csv_files = []\n    for root, _, files in os.walk(directory):\n        for file in files:\n            if file.endswith('.csv'):\n                csv_files.append(os.path.join(root, file))\n    return csv_files\n\ndef extract_schema(df: pd.DataFrame) -> Dict[str, str]:\n    \"\"\"Extract schema from pandas DataFrame.\"\"\"\n    schema = {}\n    for column in df.columns:\n        dtype = str(df[column].dtype)\n        if dtype == 'object':\n            schema[column] = 'STRING'\n        elif dtype.startswith('int'):\n            schema[column] = 'INTEGER'\n        elif dtype.startswith('float'):\n            schema[column] = 'FLOAT'\n        elif dtype == 'bool':\n            schema[column] = 'BOOLEAN'\n        elif dtype.startswith('datetime'):\n            schema[column] = 'TIMESTAMP'\n        else:\n            schema[column] = 'STRING'  # Default to STRING for unknown types\n    return schema\n\ndef generate_schema_text(schema: Dict[str, str], csv_file: str, df: pd.DataFrame, include_rows_num: int) -> str:\n    \"\"\"Generate schema text from the schema dictionary and include the CSV file name and sample data.\"\"\"\n    schema_lines = [f\"CSV File: {csv_file}\\n\"]\n    schema_lines.extend([f\"{field}: {dtype}\" for field, dtype in schema.items()])\n    \n    if include_rows_num > 0:\n        schema_lines.append(f\"\\nSample Data (first {include_rows_num} rows):\")\n        sample_data = df.head(include_rows_num).to_string(index=False)\n        schema_lines.append(sample_data)\n    \n    return \"\\n\".join(schema_lines)\n\ndef save_schema(schema_text: str, csv_file: str, output_dir: str) -> None:\n    \"\"\"Save schema text to a file in the output directory.\"\"\"\n    # Generate MD5 hash of the schema text\n    file_name = hashlib.md5(schema_text.encode()).hexdigest() + '.md'\n    output_path = os.path.join(output_dir, file_name)\n    \n    with open(output_path, 'w') as f:\n        f.write(schema_text)\n    print(f\"Schema for {csv_file} saved to {output_path}\")\n\ndef convert(data_dir: str, doc_dir: str, include_rows_num: int) -> None:\n    \"\"\"Main conversion function.\"\"\"\n    csv_files = get_csv_files(data_dir)\n    \n    for csv_file in csv_files:\n        print(f\"Processing {csv_file}\")\n        df = pd.read_csv(csv_file)\n        schema = extract_schema(df)\n        schema_text = generate_schema_text(schema, csv_file, df, include_rows_num)\n        save_schema(schema_text, csv_file, doc_dir)\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Convert CSV files to schema documents\")\n    parser.add_argument(\"--data_dir\", required=True, help=\"Directory containing CSV files\")\n    parser.add_argument(\"--doc_dir\", required=True, help=\"Output directory for schema documents\")\n    parser.add_argument(\"--include-rows-num\", type=int, default=0, help=\"Number of sample rows to include in the schema\")\n    \n    args = parser.parse_args()\n    \n    if not os.path.exists(args.doc_dir):\n        os.makedirs(args.doc_dir)\n    \n    convert(args.data_dir, args.doc_dir, args.include_rows_num)\n\nif __name__ == \"__main__\":\n    main()\n\n     \n\n下面是用户的需求：\n\n@setup.py 改成从 requirements.txt 里读取\n"}, {"role": "assistant", "content": "To modify the `setup.py` file to read dependencies from `requirements.txt`, we need to make a few changes. Here's how we can update the `setup.py` file:\n\n```python\n##File: /Users/allwefantasy/projects/super_analysis/setup.py\n<<<<<<< SEARCH\nfrom setuptools import setup, find_packages\n\nsetup(\n    name=\"super-analysis\",\n    version=\"0.1.0\",\n    packages=find_packages(where=\"src\"),\n    package_dir={\"\": \"src\"},\n    install_requires=[\n        \"byzerllm\",\n        \"fastapi\",\n        \"uvicorn\",\n        \"argparse\",\n        \"pydantic\",\n        \"requests\",\n    ],\n    entry_points={\n        \"console_scripts\": [\n            \"super-analysis.serve=superanalysis.serve.cli:main\",\n            \"super-analysis.convert=superanalysis.serve.convert:main\",\n            \"superanalysis.api=superanalysis.serve.api:main\",\n        ],\n    },\n    author=\"allwefantasy\",\n    author_email=\"allwefantasy@gmail.com\",\n    description=\"A package for super analysis with ByzerLLM\",\n    long_description=open(\"README.md\").read(),\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/yourusername/super-analysis\",\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n    ],\n    python_requires='>=3.6',\n)\n=======\nfrom setuptools import setup, find_packages\n\n# Read requirements from requirements.txt\nwith open('requirements.txt') as f:\n    requirements = f.read().splitlines()\n\nsetup(\n    name=\"super-analysis\",\n    version=\"0.1.0\",\n    packages=find_packages(where=\"src\"),\n    package_dir={\"\": \"src\"},\n    install_requires=requirements,\n    entry_points={\n        \"console_scripts\": [\n            \"super-analysis.serve=superanalysis.serve.cli:main\",\n            \"super-analysis.convert=superanalysis.serve.convert:main\",\n            \"superanalysis.api=superanalysis.serve.api:main\",\n        ],\n    },\n    author=\"allwefantasy\",\n    author_email=\"allwefantasy@gmail.com\",\n    description=\"A package for super analysis with ByzerLLM\",\n    long_description=open(\"README.md\").read(),\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/yourusername/super-analysis\",\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n    ],\n    python_requires='>=3.6',\n)\n>>>>>>> REPLACE\n```\n\nThis change does the following:\n\n1. We've added code to read the requirements from `requirements.txt`:\n   ```python\n   with open('requirements.txt') as f:\n       requirements = f.read().splitlines()\n   ```\n\n2. We've replaced the hardcoded `install_requires` list with the `requirements` variable:\n   ```python\n   install_requires=requirements,\n   ```\n\nNow, the `setup.py` file will read the dependencies from `requirements.txt`. Make sure that you have a `requirements.txt` file in the same directory as `setup.py`, and that it contains all the necessary dependencies.\n\nWould you like me to explain or break down this code change?"}], "model": "human", "yaml_file": "actions/129_chat_action.yml"}
{"instruction": "根据 @api.py 更新 @README.md 中## 非 OpenAi 兼容接口", "conversations": [{"role": "user", "content": "\n如果你需要生成代码，对于每个需要更改的文件,你需要按 *SEARCH/REPLACE block* 的格式进行生成。\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The opening fence and code language, eg: ```python\n2. The file path alone on a line, starting with \"##File:\" and verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n*SEARCH/REPLACE* blocks will replace *all* matching occurrences.\nInclude enough lines to make the SEARCH blocks unique.\n\nInclude *ALL* the code being searched and replaced!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n下面我们来看一个例子：\n\n当前项目目录结构：\n1. 项目根目录： /tmp/projects/mathweb\n2. 项目子目录/文件列表(类似tree 命令输出)\nflask/\n    app.py\n    templates/\n        index.html\n    static/\n        style.css\n\n用户需求： Change get_factorial() to use math.factorial\n\n回答： To make this change we need to modify `/tmp/projects/mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n用户需求： Refactor hello() into its own file.\n\n回答：To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/hello.py\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/main.py\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n现在让我们开始一个新的任务:\n\n当前项目目录结构：\n1. 项目根目录： /Users/allwefantasy/projects/super_analysis\n2. 项目子目录/文件列表(类似tree 命令输出)：\nsetup.py\n.auto-coder/\n    libs/\n        llm_friendly_packages/\n            src/\n                example1/\n                    main.py\nsrc/\n    superanalysis/\n        serve/\n            convert.py\n            openai_serve.py\n            __init__.py\n            api.py\n            llm_wrapper.py\n            api_server.py\n            cli.py\n        sql/\n            __init__.py\n            types.py\n            generator.py\n下面是一些文件路径以及每个文件对应的源码：\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/types.py\nfrom pydantic import BaseModel\n\nclass RAGConfig(BaseModel):\n    schema_rag_base_url: str = \"\"\n    schema_rag_api_key: str = \"xxxx\"\n\n    context_rag_base_url: str = \"\"\n    context_rag_api_key: str = \"xxxx\"\n\nclass ByzerSQLConfig(BaseModel):\n    byzer_sql_url: str = \"\"\n    data_dir: str = \"\"\n\nclass ModelConfig(BaseModel):\n    llm_model: str = \"\"\n    gen_sql_llm_model: str = \"\"\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/cli.py\nimport argparse\nimport byzerllm\nfrom byzerllm.utils.client import ByzerLLM\nfrom superanalysis.serve.api_server import serve, ServerArgs\nfrom superanalysis.sql.types import RAGConfig, ByzerSQLConfig, ModelConfig\nfrom superanalysis.sql.generator import SQLGenerator\nfrom superanalysis.serve.llm_wrapper import LLWrapper\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"ByzerLLM API Server\")\n    parser.add_argument(\"--ray-address\", type=str, default=\"auto\", help=\"ray address\")\n    parser.add_argument(\n        \"--host\", type=str, default=\"0.0.0.0\", help=\"Host to bind the server to\"\n    )\n    parser.add_argument(\n        \"--port\", type=int, default=8000, help=\"Port to bind the server to\"\n    )\n    parser.add_argument(\n        \"--uvicorn-log-level\", type=str, default=\"info\", help=\"Uvicorn log level\"\n    )\n    parser.add_argument(\n        \"--allow-credentials\", action=\"store_true\", help=\"Allow credentials\"\n    )\n    parser.add_argument(\n        \"--allowed-origins\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed origins\"\n    )\n    parser.add_argument(\n        \"--allowed-methods\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed methods\"\n    )\n    parser.add_argument(\n        \"--allowed-headers\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed headers\"\n    )\n    parser.add_argument(\n        \"--api-key\", type=str, default=\"\", help=\"API key for authentication\"\n    )\n    parser.add_argument(\n        \"--served-model-name\",\n        type=str,\n        required=True,\n        help=\"Name of the model to be served\",\n    )\n    parser.add_argument(\n        \"--prompt-template\", type=str, default=\"\", help=\"Prompt template\"\n    )\n    parser.add_argument(\n        \"--response-role\", type=str, default=\"assistant\", help=\"Response role\"\n    )\n    parser.add_argument(\"--ssl-keyfile\", type=str, default=\"\", help=\"SSL key file\")\n    parser.add_argument(\n        \"--ssl-certfile\", type=str, default=\"\", help=\"SSL certificate file\"\n    )\n\n    # Add new arguments for RAGConfig\n    parser.add_argument(\n        \"--schema-rag-base-url\", type=str, default=\"\", help=\"RAG base URL\"\n    )\n    parser.add_argument(\n        \"--schema-rag-api-key\", type=str, default=\"xxxx\", help=\"RAG API key\"\n    )\n\n    parser.add_argument(\n        \"--context-rag-base-url\", type=str, default=\"\", help=\"RAG base URL\"\n    )\n    parser.add_argument(\n        \"--context-rag-api-key\", type=str, default=\"xxxx\", help=\"RAG API key\"\n    )\n\n    # Add new argument for ByzerSQLConfig\n    parser.add_argument(\"--byzer-sql-url\", type=str, default=\"\", help=\"Byzer SQL URL\")\n    parser.add_argument(\"--data-dir\", type=str, default=\"\", help=\"Data directory\")\n    parser.add_argument(\n        \"--llm-model\", type=str, default=\"\", help=\"LLM model for general use\"\n    )\n    parser.add_argument(\n        \"--gen-sql-llm-model\", type=str, default=\"\", help=\"LLM model for SQL generation\"\n    )\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile,\n    )\n\n    # Create RAGConfig instance\n    rag_config = RAGConfig(\n        schema_rag_base_url=args.schema_rag_base_url,\n        schema_rag_api_key=args.schema_rag_api_key,\n        context_rag_base_url=args.context_rag_base_url,\n        context_rag_api_key=args.context_rag_api_key,\n    )\n\n    # Create ByzerSQLConfig instance\n    byzer_sql_config = ByzerSQLConfig(\n        byzer_sql_url=args.byzer_sql_url, data_dir=args.data_dir\n    )\n\n    model_config = ModelConfig(\n        llm_model=args.llm_model or args.served_model_name,\n        gen_sql_llm_model=args.gen_sql_llm_model or args.served_model_name,\n    )\n\n    byzerllm.connect_cluster(address=args.ray_address)\n    llm = ByzerLLM()\n    llm.setup_default_model_name(args.served_model_name)\n    llm = byzerllm.ByzerLLM()\n    if not server_args.served_model_name:\n        raise ValueError(\"served_model_name is required\")\n    llm.setup_default_model_name(server_args.served_model_name)\n\n    print(\"RAG Config: \", rag_config)\n    print(\"Model Config: \", model_config)\n    rag = SQLGenerator(\n        llm=llm,\n        rag_config=rag_config,\n        byzer_sql_config=byzer_sql_config,\n        model_config=model_config,\n    )\n    llm_wrapper = LLWrapper(llm=llm, entry=rag)\n    serve(llm_wrapper, server_args)\n\n\nif __name__ == \"__main__\":\n    main()\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/__init__.py\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/__init__.py\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/llm_wrapper.py\nfrom typing import Any, Dict, List, Optional, Union,Callable\nfrom byzerllm.utils.client.types import (    \n    LLMFunctionCallResponse,\n    LLMClassResponse,LLMResponse    \n)\nimport pydantic\nfrom byzerllm import ByzerLLM\nfrom byzerllm.utils.client import LLMResponse\nfrom byzerllm.utils.types import SingleOutputMeta\nfrom byzerllm.utils.langutil import asyncfy_with_semaphore\nfrom superanalysis.sql.generator import SQLGenerator\nfrom loguru import logger\n\nclass LLWrapper:\n\n    def __init__(self,llm:ByzerLLM,entry:SQLGenerator):\n        self.entry = entry\n        self.llm = llm\n        \n\n    def chat_oai(self,\n                 conversations,\n                 tools:List[Union[Callable,str]]=[], \n                 tool_choice:Optional[Union[Callable,str]]=None,\n                 execute_tool:bool=False,  \n                 impl_func:Optional[Callable]=None,\n                 execute_impl_func:bool=False,\n                 impl_func_params:Optional[Dict[str,Any]]=None,\n                 func_params:Optional[Dict[str,Any]]=None,\n                 response_class:Optional[Union[pydantic.BaseModel,str]] = None, \n                 response_after_chat:Optional[Union[pydantic.BaseModel,str]] = False,\n                 enable_default_sys_message:bool=True,                 \n                 model:Optional[str] = None,\n                 role_mapping=None,llm_config:Dict[str,Any]={}\n                 )->Union[List[LLMResponse],List[LLMFunctionCallResponse],List[LLMClassResponse]]: \n        res,contexts = self.entry.stream_chat_oai(conversations)\n        s = \"\".join(res)        \n        return [LLMResponse(output=s,metadata={},input=\"\")]\n\n    def stream_chat_oai(self,conversations, \n                        model:Optional[str]=None, \n                        role_mapping=None,\n                        delta_mode=False,\n                        llm_config:Dict[str,Any]={}): \n        res,contexts = self.entry.stream_chat_oai(conversations)        \n        for t in res:                        \n            yield (t,SingleOutputMeta(0,0))\n\n    async def async_stream_chat_oai(self,conversations, \n                        model:Optional[str]=None, \n                        role_mapping=None,\n                        delta_mode=False,\n                        llm_config:Dict[str,Any]={}): \n        res,contexts = self.rag.stream_chat_oai(conversations)                \n        for t in res:                                    \n            yield (t,SingleOutputMeta(0,0))\n                         \n\n    def __getattr__(self, name):        \n        return getattr(self.llm, name)\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/openai_serve.py\nimport argparse\nfrom service_server.api_server import ServerArgs, serve\nfrom service_server.llm_wrapper import LLWrapper\nfrom service_server.summary_rag import SummaryRAG\nimport byzerllm\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"OpenAI Serve\")\n    parser.add_argument(\n        \"--model\",\n        default=\"\",\n    )\n    parser.add_argument(\"--emb_model\", default=\"\")\n    parser.add_argument(\n        \"--ray_address\",\n        default=\"auto\",\n    )\n    parser.add_argument(\"--source_dir\", default=\".\", help=\"\")\n    parser.add_argument(\"--host\", default=\"\", help=\"\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"\")\n    parser.add_argument(\"--uvicorn_log_level\", default=\"info\", help=\"\")\n    parser.add_argument(\"--allow_credentials\", action=\"store_true\", help=\"\")\n    parser.add_argument(\"--allowed_origins\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--allowed_methods\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--allowed_headers\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--api_key\", default=\"\", help=\"\")\n    parser.add_argument(\"--served_model_name\", default=\"\", help=\"\")\n    parser.add_argument(\"--prompt_template\", default=\"\", help=\"\")\n    parser.add_argument(\"--ssl_keyfile\", default=\"\", help=\"\")\n    parser.add_argument(\"--ssl_certfile\", default=\"\", help=\"\")\n    parser.add_argument(\"--response_role\", default=\"assistant\", help=\"\")\n    parser.add_argument(\n        \"--collections\", default=\"\", help=\"Collection name for indexing\"\n    )\n    parser.add_argument(\n        \"--base_dir\",\n        default=\"\",\n        help=\"Path where the processed text embeddings were stored\",\n    )\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name or args.model,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile,\n    )\n    byzerllm.connect_cluster(address=args.ray_address)\n    llm = byzerllm.ByzerLLM()\n    if not server_args.served_model_name:\n        raise ValueError(\"served_model_name is required\")\n    llm.setup_default_model_name(server_args.served_model_name)\n    rag = SummaryRAG(llm=llm)    \n    llm_wrapper = LLWrapper(llm=llm, rag=rag)\n    serve(llm=llm_wrapper, args=server_args)\n\nif __name__ == \"__main__\":\n    main()\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/api_server.py\nimport os\nimport time\nimport uvicorn\nfrom fastapi import Request\nfrom fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import JSONResponse, StreamingResponse, Response\n\nfrom byzerllm.log import init_logger\nfrom byzerllm.utils import random_uuid\nfrom byzerllm.version import __version__ as version\nfrom byzerllm.utils.client import ByzerLLM, LLMRequest\nfrom byzerllm.utils.client.entrypoints.openai.serving_chat import OpenAIServingChat\nfrom byzerllm.utils.client.entrypoints.openai.serving_completion import OpenAIServingCompletion\nfrom byzerllm.utils.client.entrypoints.openai.protocol import (\n    ModelList,\n    ModelCard,\n    ModelPermission,\n    ChatCompletionRequest,\n    ErrorResponse,\n    CompletionRequest,\n    Embeddings,\n    EmbeddingsOutput,\n    EmbeddingsData,\n    EmbeddingsUsage,\n)\nfrom pydantic import BaseModel\nfrom typing import List\n\nlogger = init_logger(__name__)\n\nllm_client: ByzerLLM = None\nopenai_serving_chat: OpenAIServingChat = None\nopenai_serving_completion: OpenAIServingCompletion = None\n\nTIMEOUT_KEEP_ALIVE = 5  # seconds\n# timeout in 10 minutes. Streaming can take longer than 3 min\nTIMEOUT = float(os.environ.get(\"BYZERLLM_APISERVER_HTTP_TIMEOUT\", 600))\n\nrouter_app = FastAPI()\n\n\n@router_app.get(\"/health\")\nasync def health() -> Response:\n    \"\"\"Health check.\"\"\"\n    return Response(status_code=200)\n\n\n@router_app.get(\"/v1/models\")\nasync def show_available_models():\n    models = await openai_serving_chat.show_available_models()\n    return JSONResponse(content=models.model_dump())\n\n\n@router_app.get(\"/version\")\nasync def show_version():\n    return JSONResponse(content={\"version\": version})\n\n\n@router_app.get(\"/v1/models\", response_model=ModelList)\nasync def models() -> ModelList:\n    \"\"\"Show available models. Right now we only have one model.\"\"\"\n    model_cards = [\n        ModelCard(\n            id=\"\",\n            root=\"\",\n            permission=[ModelPermission()]\n        )\n    ]\n    return ModelList(data=model_cards)\n\n\n@router_app.post(\"/v1/completions\")\nasync def create_completion(\n        body: CompletionRequest,\n        request: Request\n):\n    generator = await openai_serving_completion.create_completion(body, request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            status_code=generator.code\n        )\n    if body.stream:\n        return StreamingResponse(\n            content=generator,\n            media_type=\"text/event-stream\"\n        )\n    else:\n        return JSONResponse(content=generator.model_dump())\n\n\n@router_app.post(\"/v1/chat/completions\")\nasync def create_chat_completion(\n        body: ChatCompletionRequest,\n        request: Request,\n):\n    \"\"\"Completion API similar to OpenAI's API.\n\n    See  https://platform.openai.com/docs/api-reference/chat/create\n    for the API specification. This API mimics the OpenAI ChatCompletion API.\n\n    NOTE: Currently we do not support the following features:\n        - function_call (Users should implement this by themselves)\n        - logit_bias (to be supported by vLLM engine)\n    \"\"\"\n    # async with async_timeout.timeout(TIMEOUT):\n\n    generator = await openai_serving_chat.create_chat_completion(body, request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            status_code=generator.code\n        )\n    if body.stream:\n        return StreamingResponse(\n            content=generator,\n            media_type=\"text/event-stream\"\n        )\n    else:\n        return JSONResponse(content=generator.model_dump())\n\n\n@router_app.post(\"/v1/embeddings\")\nasync def embed(body: Embeddings):\n    \"\"\"Given a prompt, the model will return one embedding.\n\n    Returns:\n        A response object with an embedding.\n    \"\"\"\n    embedding_id = f\"embed-{random_uuid()}\"\n\n    results_list = llm_client.emb(body.model, request=LLMRequest(instruction=body.input))\n    tokens = 0\n\n    return EmbeddingsOutput(\n        data=[\n            EmbeddingsData(\n                embedding=results.output,\n                index=i,\n                object=\"embedding\",\n            )\n            for i, results in enumerate(results_list)\n        ],\n        id=embedding_id,\n        object=\"list\",\n        created=int(time.time()),\n        model=body.model,\n        usage=EmbeddingsUsage(\n            prompt_tokens=tokens,\n            total_tokens=tokens,\n        ),\n    )\n\nclass ServerArgs(BaseModel):\n    host: str = None\n    port: int = 8000\n    uvicorn_log_level: str = \"info\"\n    allow_credentials: bool = False\n    allowed_origins: List[str] = [\"*\"]  \n    allowed_methods: List[str] = [\"*\"]\n    allowed_headers: List[str] = [\"*\"]\n    api_key: str = None\n    served_model_name: str = None\n    prompt_template: str = None\n    response_role: str = \"assistant\"\n    ssl_keyfile: str = None\n    ssl_certfile: str = None    \n\ndef serve(llm:ByzerLLM, args: ServerArgs):\n    \n    logger.info(f\"ByzerLLM API server version {version}\")\n    logger.info(f\"args: {args}\")\n\n    router_app.add_middleware(\n        CORSMiddleware,\n        allow_origins=args.allowed_origins,\n        allow_credentials=args.allow_credentials,\n        allow_methods=args.allowed_methods,\n        allow_headers=args.allowed_headers,\n    )\n    \n    if token := os.environ.get(\"BYZERLLM_API_KEY\") or args.api_key:\n\n        @router_app.middleware(\"http\")\n        async def authentication(request: Request, call_next):\n            if not request.url.path.startswith(\"/v1\"):\n                return await call_next(request)\n            if request.headers.get(\"Authorization\") != \"Bearer \" + token:\n                return JSONResponse(\n                    content={\"error\": \"Unauthorized\"},\n                    status_code=401\n                )\n            return await call_next(request)\n\n    # Register labels for metrics\n    # add_global_metrics_labels(model_name=engine_args.model)\n    global llm_client\n    llm_client = llm\n    \n    global openai_serving_chat\n    openai_serving_chat = OpenAIServingChat(\n        llm_client=llm_client,\n        response_role=args.response_role,\n        server_model_name=args.served_model_name,\n        prompt_template=args.prompt_template\n    )\n    global openai_serving_completion\n    openai_serving_completion = OpenAIServingCompletion(\n        llm_client=llm_client,\n        server_model_name=args.served_model_name,\n        prompt_template=args.prompt_template\n    )\n\n    uvicorn.run(\n        router_app,\n        host=args.host,\n        port=args.port,\n        log_level=args.uvicorn_log_level,\n        timeout_keep_alive=TIMEOUT_KEEP_ALIVE,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile\n    )\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/generator.py\nfrom typing import Optional, Dict, Any, List\nfrom openai import OpenAI\nimport json\nfrom .types import RAGConfig, ByzerSQLConfig, ModelConfig\nfrom byzerllm.utils.client import code_utils\nimport byzerllm\nimport requests\nfrom loguru import logger\nimport traceback\nfrom byzerllm.utils.nontext import TagExtractor\n\n\nclass SQLGenerator:\n\n    @byzerllm.prompt()\n    def gen_context(self, conversations: List[Dict[str, str]]) -> str:\n        \"\"\"\n        对话内容：\n        {% for msg in conversations %}\n        <{{ msg.role }}>: {{ msg.content }}\n        {% endfor %}\n\n        根据提供的对话内容，理解用户最后的问题，然后提取可能会用到的 Byzer-SQL 相关的知识。\n        \"\"\"\n\n    @byzerllm.prompt()\n    def gen_sql(\n        self,\n        conversations: List[Dict[str, str]],\n        context: str,\n        schema: str,\n        model: str = \"deepseek_chat\",\n    ) -> str:\n        \"\"\"\n        相关知识：\n        {{ context }}\n\n        Schema信息：\n        {{ schema }}\n\n        对话内容：\n        {% for msg in conversations %}\n        <{{ msg.role }}>: {{ msg.content }}\n        {% endfor %}\n\n        模型UDF函数：\n        {{ model }}\n\n        根据提供的相关知识以及对话内容，理解用户最后的问题，然后根据看到的schema 信息，生成 Byzer-SQL 语句。\n        Byzer-SQL 首先需要用 load 语法加载 schema 中的路径映射成表格，然后根据表格的信息生成对应的 Byzer-SQL 语句。\n\n        特别需要关注的是:\n        1. 用户的很多需求是要对表格的文本内容做自然语言处理，你需要使用 Byzer-SQL 中的大模型辅助函数来完成这个需求。\n        2. 使用 `llm_result` 函数处理 LLM 模型函数的响应,`llm_result` 函数得到的是一段文本，虽然我们要求模型函数只输出特定的文本，但考虑到大模型会有一些额外的字符输出，不会只是输出我们要求的文字，所以通过 `like` 来进行匹配。\n        3. 注意 Byzer-SQL 的语法，比如select 语句最后一定需要有 as <table_name>。\n        3. 输出的 Byzer-SQL 语句务必使用 <_byzer_sql_></_byzer_sql_> 标签包裹起来。\n        \"\"\"\n\n    @byzerllm.prompt()\n    def fetch_schema(self, context: str) -> str:\n        \"\"\"\n        下面是一些基础知识，包括名词术语等等：\n        {{ context }}\n\n        当理解对话内容的时候，需要参考上面的基础知识，然后更好的理解对应的表格schema信息是不是有用。\n        \"\"\"\n\n    def __init__(\n        self,\n        llm: byzerllm.ByzerLLM,\n        rag_config: RAGConfig,\n        byzer_sql_config: ByzerSQLConfig,\n        model_config: ModelConfig,\n    ) -> None:\n        self.llm = llm\n        self.schema_rag_client = OpenAI(\n            api_key=rag_config.schema_rag_api_key,\n            base_url=rag_config.schema_rag_base_url,\n        )\n        self.context_rag_client = OpenAI(\n            api_key=rag_config.context_rag_api_key,\n            base_url=rag_config.context_rag_base_url,\n        )\n        self.byzer_sql_url = byzer_sql_config.byzer_sql_url\n\n        self.llm_model = byzerllm.ByzerLLM()\n        self.llm_model.setup_default_model_name(model_config.llm_model)\n        self.gen_sql_llm_model = byzerllm.ByzerLLM()\n        self.gen_sql_llm_model.setup_default_model_name(model_config.gen_sql_llm_model)\n\n    def stream_chat_oai(\n        self,\n        conversations,\n        model: Optional[str] = None,\n        role_mapping=None,\n        delta_mode=False,\n        llm_config: Dict[str, Any] = {},\n    ):\n        try:\n            v, c = self._stream_chat_oai(\n                conversations=conversations,\n                model=model,\n                role_mapping=role_mapping,\n                delta_mode=delta_mode,\n                llm_config=llm_config,\n            )\n        except Exception as e:\n            error_msg = f\"Error in stream_chat_oai: {e}\\n{traceback.format_exc()}\"\n            logger.error(error_msg)\n            v, c = [str(e)], []\n\n        return v, c\n\n    def _stream_chat_oai(\n        self,\n        conversations,\n        model: Optional[str] = None,\n        role_mapping=None,\n        delta_mode=False,\n        llm_config: Dict[str, Any] = {},\n    ):\n        logger.info(f\"Starting stream_chat_oai with model: {model}\")\n        logger.debug(f\"Conversations: {conversations}\")\n        logger.debug(f\"LLM Config: {llm_config}\")\n\n        if not model:\n            model = self.llm_model.default_model_name\n        logger.info(f\"Using model: {model}\")\n\n        query = conversations[-1][\"content\"]\n\n        if (\n            \"使用四到五个字直接返回这句话的简要主题，不要解释、不要标点、不要语气词、不要多余文本，不要加粗，如果没有主题\"\n            in query\n            or \"简要总结一下对话内容，用作后续的上下文提示 prompt，控制在 200 字以内\"\n            in query\n        ):\n            chunks = self.llm.stream_chat_oai(\n                conversations=conversations,\n                model=model,\n                role_mapping=role_mapping,\n                llm_config=llm_config,\n                delta_mode=True,\n            )\n            return (chunk[0] for chunk in chunks), []\n\n        logger.info(\"Fetching context from context RAG client\")\n        context_query = self.gen_context.prompt(conversations=conversations)\n        context_response = self.context_rag_client.chat.completions.create(\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": json.dumps(\n                        {\"query\": context_query, \"only_contexts\": True},\n                        ensure_ascii=False,\n                    ),\n                }\n            ],\n            model=model,\n        )\n\n        context = context_response.choices[0].message.content\n        new_context = \"\"\n        for msg in context.split(\"\\n\"):\n            if msg.strip():\n                v = json.loads(msg)\n                new_context += v[\"source_code\"]\n        logger.info(f\"context {new_context[0:100]}...\")\n        logger.info(\"Context retrieved successfully\")\n\n        logger.info(\"Fetching schemas from schema RAG client\")\n        schema_query = self.fetch_schema.prompt(context=new_context)\n\n        response = self.schema_rag_client.chat.completions.create(\n            messages=conversations[0:-1]\n            + [\n                {\n                    \"role\": \"user\",\n                    \"content\": json.dumps(\n                        {\"query\": schema_query, \"only_contexts\": True},\n                        ensure_ascii=False,\n                    ),\n                }\n            ],\n            model=model,\n        )\n        logger.debug(f\"Schema RAG response: {response}\")\n\n        schema_str = response.choices[0].message.content\n        logger.info(\"Schema retrieved successfully\")\n\n        logger.info(\"Generating SQL string\")\n        sql_str = self.gen_sql.with_llm(self.gen_sql_llm_model).run(\n            conversations=conversations,\n            context=new_context,\n            schema=schema_str,\n            model=model,\n        )\n\n        logger.info(\"SQL string generated\")\n        logger.debug(f\"Generated SQL string: {sql_str}\")\n\n        # Extract SQL code from tags\n        tag_extractor = TagExtractor(sql_str)\n        extracted_tags = tag_extractor.extract()\n        sql = extracted_tags.content[0].content\n        logger.info(f\"Extracted SQL: {sql}\")\n\n        logger.info(f\"Sending SQL to Byzer SQL URL: {self.byzer_sql_url}\")\n        try:\n            response = requests.post(\n                self.byzer_sql_url,\n                data={\n                    \"sql\": sql,\n                    \"owner\": \"hello\",\n                    \"sessionPerUser\": True,\n                    \"sessionPerRequest\": True,\n                    \"includeSchema\": True,\n                },\n            )\n            response.raise_for_status()\n            result = response.content.decode(\"utf-8\")\n            logger.info(\"SQL execution result decoded\")\n            logger.debug(f\"SQL execution result: {result}\")\n            return (item for item in [result]), []\n        except requests.exceptions.RequestException as e:\n            logger.error(f\"Failed to send SQL to Byzer SQL URL: {e}\")\n            if hasattr(e.response, \"content\"):\n                error_content = e.response.content.decode(\"utf-8\")\n                logger.error(f\"Server response: {error_content}\")\n            else:\n                logger.error(\"No server response available\")\n            raise Exception(f\"Byzer SQL request failed: {str(e)}\") from e\n\n\n##File: /Users/allwefantasy/projects/super_analysis/setup.py\nfrom setuptools import setup, find_packages\n\n# Read requirements from requirements.txt\nwith open('requirements.txt') as f:\n    requirements = f.read().splitlines()\n\nsetup(\n    name=\"super-analysis\",\n    version=\"0.1.0\",\n    packages=find_packages(where=\"src\"),\n    package_dir={\"\": \"src\"},\n    install_requires=requirements,\n    entry_points={\n        \"console_scripts\": [\n            \"super-analysis.serve=superanalysis.serve.cli:main\",\n            \"super-analysis.convert=superanalysis.serve.convert:main\",\n            \"superanalysis.api=superanalysis.serve.api:main\",\n        ],\n    },\n    author=\"allwefantasy\",\n    author_email=\"allwefantasy@gmail.com\",\n    description=\"A package for super analysis with ByzerLLM\",\n    long_description=open(\"README.md\").read(),\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/yourusername/super-analysis\",\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n    ],\n    python_requires='>=3.6',\n)\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/convert.py\nimport os\nimport argparse\nimport pandas as pd\nimport hashlib\nfrom typing import List, Dict\n\ndef get_csv_files(directory: str) -> List[str]:\n    \"\"\"Recursively get all CSV files in the given directory.\"\"\"\n    csv_files = []\n    for root, _, files in os.walk(directory):\n        for file in files:\n            if file.endswith('.csv'):\n                csv_files.append(os.path.join(root, file))\n    return csv_files\n\ndef extract_schema(df: pd.DataFrame) -> Dict[str, str]:\n    \"\"\"Extract schema from pandas DataFrame.\"\"\"\n    schema = {}\n    for column in df.columns:\n        dtype = str(df[column].dtype)\n        if dtype == 'object':\n            schema[column] = 'STRING'\n        elif dtype.startswith('int'):\n            schema[column] = 'INTEGER'\n        elif dtype.startswith('float'):\n            schema[column] = 'FLOAT'\n        elif dtype == 'bool':\n            schema[column] = 'BOOLEAN'\n        elif dtype.startswith('datetime'):\n            schema[column] = 'TIMESTAMP'\n        else:\n            schema[column] = 'STRING'  # Default to STRING for unknown types\n    return schema\n\ndef generate_schema_text(schema: Dict[str, str], csv_file: str, df: pd.DataFrame, include_rows_num: int) -> str:\n    \"\"\"Generate schema text from the schema dictionary and include the CSV file name and sample data.\"\"\"\n    schema_lines = [f\"CSV File: {csv_file}\\n\"]\n    schema_lines.extend([f\"{field}: {dtype}\" for field, dtype in schema.items()])\n    \n    if include_rows_num > 0:\n        schema_lines.append(f\"\\nSample Data (first {include_rows_num} rows):\")\n        sample_data = df.head(include_rows_num).to_string(index=False)\n        schema_lines.append(sample_data)\n    \n    return \"\\n\".join(schema_lines)\n\ndef save_schema(schema_text: str, csv_file: str, output_dir: str) -> None:\n    \"\"\"Save schema text to a file in the output directory.\"\"\"\n    # Generate MD5 hash of the schema text\n    file_name = hashlib.md5(schema_text.encode()).hexdigest() + '.md'\n    output_path = os.path.join(output_dir, file_name)\n    \n    with open(output_path, 'w') as f:\n        f.write(schema_text)\n    print(f\"Schema for {csv_file} saved to {output_path}\")\n\ndef convert(data_dir: str, doc_dir: str, include_rows_num: int) -> None:\n    \"\"\"Main conversion function.\"\"\"\n    csv_files = get_csv_files(data_dir)\n    \n    for csv_file in csv_files:\n        print(f\"Processing {csv_file}\")\n        df = pd.read_csv(csv_file)\n        schema = extract_schema(df)\n        schema_text = generate_schema_text(schema, csv_file, df, include_rows_num)\n        save_schema(schema_text, csv_file, doc_dir)\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Convert CSV files to schema documents\")\n    parser.add_argument(\"--data_dir\", required=True, help=\"Directory containing CSV files\")\n    parser.add_argument(\"--doc_dir\", required=True, help=\"Output directory for schema documents\")\n    parser.add_argument(\"--include-rows-num\", type=int, default=0, help=\"Number of sample rows to include in the schema\")\n    \n    args = parser.parse_args()\n    \n    if not os.path.exists(args.doc_dir):\n        os.makedirs(args.doc_dir)\n    \n    convert(args.data_dir, args.doc_dir, args.include_rows_num)\n\nif __name__ == \"__main__\":\n    main()\n\n     \n\n下面是用户的需求：\n\n根据 @api.py 更新 @README.md 中## 非 OpenAi 兼容接口\n"}, {"role": "assistant", "content": "好的，我将根据 `api.py` 文件的内容来更新 `README.md` 中的 \"非 OpenAI 兼容接口\" 部分。首先，我需要创建 `api.py` 文件，然后更新 `README.md` 文件。让我们开始：\n\n1. 创建 `api.py` 文件：\n\n```python\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/api.py\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nfrom typing import List, Optional\nimport argparse\nimport uvicorn\nfrom superanalysis.sql.generator import SQLGenerator\nfrom superanalysis.sql.types import RAGConfig, ByzerSQLConfig, ModelConfig\nimport byzerllm\n\napp = FastAPI()\n\nclass ChatMessage(BaseModel):\n    role: str\n    content: str\n\nclass ChatRequest(BaseModel):\n    messages: List[ChatMessage]\n    model: Optional[str] = None\n\nclass ChatResponse(BaseModel):\n    content: str\n\n@app.post(\"/chat\", response_model=ChatResponse)\nasync def chat(request: ChatRequest):\n    try:\n        conversations = [{\"role\": msg.role, \"content\": msg.content} for msg in request.messages]\n        result, _ = sql_generator.stream_chat_oai(conversations, model=request.model)\n        return ChatResponse(content=\"\".join(result))\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Super Analysis API Server\")\n    parser.add_argument(\"--host\", type=str, default=\"0.0.0.0\", help=\"Host to bind the server to\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"Port to bind the server to\")\n    parser.add_argument(\"--schema-rag-base-url\", type=str, required=True, help=\"Schema RAG base URL\")\n    parser.add_argument(\"--schema-rag-api-key\", type=str, required=True, help=\"Schema RAG API key\")\n    parser.add_argument(\"--context-rag-base-url\", type=str, required=True, help=\"Context RAG base URL\")\n    parser.add_argument(\"--context-rag-api-key\", type=str, required=True, help=\"Context RAG API key\")\n    parser.add_argument(\"--byzer-sql-url\", type=str, required=True, help=\"Byzer SQL URL\")\n    parser.add_argument(\"--data-dir\", type=str, default=\"\", help=\"Data directory\")\n    parser.add_argument(\"--llm-model\", type=str, required=True, help=\"LLM model for general use\")\n    parser.add_argument(\"--gen-sql-llm-model\", type=str, required=True, help=\"LLM model for SQL generation\")\n    \n    args = parser.parse_args()\n\n    rag_config = RAGConfig(\n        schema_rag_base_url=args.schema_rag_base_url,\n        schema_rag_api_key=args.schema_rag_api_key,\n        context_rag_base_url=args.context_rag_base_url,\n        context_rag_api_key=args.context_rag_api_key,\n    )\n\n    byzer_sql_config = ByzerSQLConfig(\n        byzer_sql_url=args.byzer_sql_url,\n        data_dir=args.data_dir\n    )\n\n    model_config = ModelConfig(\n        llm_model=args.llm_model,\n        gen_sql_llm_model=args.gen_sql_llm_model\n    )\n\n    llm = byzerllm.ByzerLLM()\n    \n    global sql_generator\n    sql_generator = SQLGenerator(\n        llm=llm,\n        rag_config=rag_config,\n        byzer_sql_config=byzer_sql_config,\n        model_config=model_config\n    )\n\n    uvicorn.run(app, host=args.host, port=args.port)\n\nif __name__ == \"__main__\":\n    main()\n```\n\n2. 现在，让我们更新 `README.md` 文件中的 \"非 OpenAI 兼容接口\" 部分：\n\n```markdown\n##File: /Users/allwefantasy/projects/super_analysis/README.md\n<<<<<<< SEARCH\n## 非 OpenAi 兼容接口\n\n=======\n## 非 OpenAI 兼容接口\n\nSuper Analysis 提供了一个非 OpenAI 兼容的聊天接口，用于与 SQL 生成器进行交互。以下是接口的详细说明：\n\n### 聊天接口\n\n**Endpoint**: `/chat`\n\n**Method**: POST\n\n**Request Body**:\n\n```json\n{\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"你好，我想查询销售数据\"\n    }\n  ],\n  \"model\": \"gpt-3.5-turbo\"\n}\n```\n\n- `messages`: 包含聊天消息的数组。每条消息都有 `role` 和 `content` 字段。\n- `model`: (可选) 指定要使用的模型。如果不指定，将使用默认模型。\n\n**Response**:\n\n```json\n{\n  \"content\": \"好的，我可以帮您查询销售数据。请问您想查询哪个时间段的销售数据？或者您有具体的查询需求吗？\"\n}\n```\n\n- `content`: 包含 AI 助手的回复内容。\n\n**错误处理**:\n\n如果发生错误，API 将返回一个 HTTP 500 错误，并包含错误详情。\n\n### 启动服务器\n\n要启动 Super Analysis API 服务器，请使用以下命令：\n\n```bash\npython -m superanalysis.serve.api \\\n  --host 0.0.0.0 \\\n  --port 8000 \\\n  --schema-rag-base-url <schema_rag_base_url> \\\n  --schema-rag-api-key <schema_rag_api_key> \\\n  --context-rag-base-url <context_rag_base_url> \\\n  --context-rag-api-key <context_rag_api_key> \\\n  --byzer-sql-url <byzer_sql_url> \\\n  --llm-model <llm_model> \\\n  --gen-sql-llm-model <gen_sql_llm_model>\n```\n\n请确保替换尖括号中的参数为实际的值。\n\n这个接口允许您与 SQL 生成器进行交互，发送聊天消息并接收响应。它将使用配置的 RAG（检索增强生成）系统和 Byzer SQL 来处理查询并生成适当的 SQL 语句。\n>>>>>>> REPLACE\n```\n\n这些更改将更新 README.md 文件，添加有关非 OpenAI 兼容接口的详细信息，包括如何使用聊天接口和如何启动服务器。"}], "model": "human", "yaml_file": "actions/130_chat_action.yml"}
{"instruction": "根据 @rest.py 更新 @README.md 中的## 非 OpenAi 兼容接口 部分", "conversations": [{"role": "user", "content": "\n如果你需要生成代码，对于每个需要更改的文件,你需要按 *SEARCH/REPLACE block* 的格式进行生成。\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The opening fence and code language, eg: ```python\n2. The file path alone on a line, starting with \"##File:\" and verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n*SEARCH/REPLACE* blocks will replace *all* matching occurrences.\nInclude enough lines to make the SEARCH blocks unique.\n\nInclude *ALL* the code being searched and replaced!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n下面我们来看一个例子：\n\n当前项目目录结构：\n1. 项目根目录： /tmp/projects/mathweb\n2. 项目子目录/文件列表(类似tree 命令输出)\nflask/\n    app.py\n    templates/\n        index.html\n    static/\n        style.css\n\n用户需求： Change get_factorial() to use math.factorial\n\n回答： To make this change we need to modify `/tmp/projects/mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n用户需求： Refactor hello() into its own file.\n\n回答：To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/hello.py\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/main.py\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n现在让我们开始一个新的任务:\n\n当前项目目录结构：\n1. 项目根目录： /Users/allwefantasy/projects/super_analysis\n2. 项目子目录/文件列表(类似tree 命令输出)：\nsetup.py\n.auto-coder/\n    libs/\n        llm_friendly_packages/\n            src/\n                example1/\n                    main.py\nsrc/\n    superanalysis/\n        serve/\n            rest.py\n            convert.py\n            openai_serve.py\n            __init__.py\n            llm_wrapper.py\n            api_server.py\n            cli.py\n        sql/\n            __init__.py\n            types.py\n            generator.py\n下面是一些文件路径以及每个文件对应的源码：\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/types.py\nfrom pydantic import BaseModel\n\nclass RAGConfig(BaseModel):\n    schema_rag_base_url: str = \"\"\n    schema_rag_api_key: str = \"xxxx\"\n\n    context_rag_base_url: str = \"\"\n    context_rag_api_key: str = \"xxxx\"\n\nclass ByzerSQLConfig(BaseModel):\n    byzer_sql_url: str = \"\"\n    data_dir: str = \"\"\n\nclass ModelConfig(BaseModel):\n    llm_model: str = \"\"\n    gen_sql_llm_model: str = \"\"\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/cli.py\nimport argparse\nimport byzerllm\nfrom byzerllm.utils.client import ByzerLLM\nfrom superanalysis.serve.api_server import serve, ServerArgs\nfrom superanalysis.sql.types import RAGConfig, ByzerSQLConfig, ModelConfig\nfrom superanalysis.sql.generator import SQLGenerator\nfrom superanalysis.serve.llm_wrapper import LLWrapper\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"ByzerLLM API Server\")\n    parser.add_argument(\"--ray-address\", type=str, default=\"auto\", help=\"ray address\")\n    parser.add_argument(\n        \"--host\", type=str, default=\"0.0.0.0\", help=\"Host to bind the server to\"\n    )\n    parser.add_argument(\n        \"--port\", type=int, default=8000, help=\"Port to bind the server to\"\n    )\n    parser.add_argument(\n        \"--uvicorn-log-level\", type=str, default=\"info\", help=\"Uvicorn log level\"\n    )\n    parser.add_argument(\n        \"--allow-credentials\", action=\"store_true\", help=\"Allow credentials\"\n    )\n    parser.add_argument(\n        \"--allowed-origins\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed origins\"\n    )\n    parser.add_argument(\n        \"--allowed-methods\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed methods\"\n    )\n    parser.add_argument(\n        \"--allowed-headers\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed headers\"\n    )\n    parser.add_argument(\n        \"--api-key\", type=str, default=\"\", help=\"API key for authentication\"\n    )\n    parser.add_argument(\n        \"--served-model-name\",\n        type=str,\n        required=True,\n        help=\"Name of the model to be served\",\n    )\n    parser.add_argument(\n        \"--prompt-template\", type=str, default=\"\", help=\"Prompt template\"\n    )\n    parser.add_argument(\n        \"--response-role\", type=str, default=\"assistant\", help=\"Response role\"\n    )\n    parser.add_argument(\"--ssl-keyfile\", type=str, default=\"\", help=\"SSL key file\")\n    parser.add_argument(\n        \"--ssl-certfile\", type=str, default=\"\", help=\"SSL certificate file\"\n    )\n\n    # Add new arguments for RAGConfig\n    parser.add_argument(\n        \"--schema-rag-base-url\", type=str, default=\"\", help=\"RAG base URL\"\n    )\n    parser.add_argument(\n        \"--schema-rag-api-key\", type=str, default=\"xxxx\", help=\"RAG API key\"\n    )\n\n    parser.add_argument(\n        \"--context-rag-base-url\", type=str, default=\"\", help=\"RAG base URL\"\n    )\n    parser.add_argument(\n        \"--context-rag-api-key\", type=str, default=\"xxxx\", help=\"RAG API key\"\n    )\n\n    # Add new argument for ByzerSQLConfig\n    parser.add_argument(\"--byzer-sql-url\", type=str, default=\"\", help=\"Byzer SQL URL\")\n    parser.add_argument(\"--data-dir\", type=str, default=\"\", help=\"Data directory\")\n    parser.add_argument(\n        \"--llm-model\", type=str, default=\"\", help=\"LLM model for general use\"\n    )\n    parser.add_argument(\n        \"--gen-sql-llm-model\", type=str, default=\"\", help=\"LLM model for SQL generation\"\n    )\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile,\n    )\n\n    # Create RAGConfig instance\n    rag_config = RAGConfig(\n        schema_rag_base_url=args.schema_rag_base_url,\n        schema_rag_api_key=args.schema_rag_api_key,\n        context_rag_base_url=args.context_rag_base_url,\n        context_rag_api_key=args.context_rag_api_key,\n    )\n\n    # Create ByzerSQLConfig instance\n    byzer_sql_config = ByzerSQLConfig(\n        byzer_sql_url=args.byzer_sql_url, data_dir=args.data_dir\n    )\n\n    model_config = ModelConfig(\n        llm_model=args.llm_model or args.served_model_name,\n        gen_sql_llm_model=args.gen_sql_llm_model or args.served_model_name,\n    )\n\n    byzerllm.connect_cluster(address=args.ray_address)\n    llm = ByzerLLM()\n    llm.setup_default_model_name(args.served_model_name)\n    llm = byzerllm.ByzerLLM()\n    if not server_args.served_model_name:\n        raise ValueError(\"served_model_name is required\")\n    llm.setup_default_model_name(server_args.served_model_name)\n\n    print(\"RAG Config: \", rag_config)\n    print(\"Model Config: \", model_config)\n    rag = SQLGenerator(\n        llm=llm,\n        rag_config=rag_config,\n        byzer_sql_config=byzer_sql_config,\n        model_config=model_config,\n    )\n    llm_wrapper = LLWrapper(llm=llm, entry=rag)\n    serve(llm_wrapper, server_args)\n\n\nif __name__ == \"__main__\":\n    main()\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/__init__.py\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/__init__.py\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/llm_wrapper.py\nfrom typing import Any, Dict, List, Optional, Union,Callable\nfrom byzerllm.utils.client.types import (    \n    LLMFunctionCallResponse,\n    LLMClassResponse,LLMResponse    \n)\nimport pydantic\nfrom byzerllm import ByzerLLM\nfrom byzerllm.utils.client import LLMResponse\nfrom byzerllm.utils.types import SingleOutputMeta\nfrom byzerllm.utils.langutil import asyncfy_with_semaphore\nfrom superanalysis.sql.generator import SQLGenerator\nfrom loguru import logger\n\nclass LLWrapper:\n\n    def __init__(self,llm:ByzerLLM,entry:SQLGenerator):\n        self.entry = entry\n        self.llm = llm\n        \n\n    def chat_oai(self,\n                 conversations,\n                 tools:List[Union[Callable,str]]=[], \n                 tool_choice:Optional[Union[Callable,str]]=None,\n                 execute_tool:bool=False,  \n                 impl_func:Optional[Callable]=None,\n                 execute_impl_func:bool=False,\n                 impl_func_params:Optional[Dict[str,Any]]=None,\n                 func_params:Optional[Dict[str,Any]]=None,\n                 response_class:Optional[Union[pydantic.BaseModel,str]] = None, \n                 response_after_chat:Optional[Union[pydantic.BaseModel,str]] = False,\n                 enable_default_sys_message:bool=True,                 \n                 model:Optional[str] = None,\n                 role_mapping=None,llm_config:Dict[str,Any]={}\n                 )->Union[List[LLMResponse],List[LLMFunctionCallResponse],List[LLMClassResponse]]: \n        res,contexts = self.entry.stream_chat_oai(conversations)\n        s = \"\".join(res)        \n        return [LLMResponse(output=s,metadata={},input=\"\")]\n\n    def stream_chat_oai(self,conversations, \n                        model:Optional[str]=None, \n                        role_mapping=None,\n                        delta_mode=False,\n                        llm_config:Dict[str,Any]={}): \n        res,contexts = self.entry.stream_chat_oai(conversations)        \n        for t in res:                        \n            yield (t,SingleOutputMeta(0,0))\n\n    async def async_stream_chat_oai(self,conversations, \n                        model:Optional[str]=None, \n                        role_mapping=None,\n                        delta_mode=False,\n                        llm_config:Dict[str,Any]={}): \n        res,contexts = self.rag.stream_chat_oai(conversations)                \n        for t in res:                                    \n            yield (t,SingleOutputMeta(0,0))\n                         \n\n    def __getattr__(self, name):        \n        return getattr(self.llm, name)\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/openai_serve.py\nimport argparse\nfrom service_server.api_server import ServerArgs, serve\nfrom service_server.llm_wrapper import LLWrapper\nfrom service_server.summary_rag import SummaryRAG\nimport byzerllm\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"OpenAI Serve\")\n    parser.add_argument(\n        \"--model\",\n        default=\"\",\n    )\n    parser.add_argument(\"--emb_model\", default=\"\")\n    parser.add_argument(\n        \"--ray_address\",\n        default=\"auto\",\n    )\n    parser.add_argument(\"--source_dir\", default=\".\", help=\"\")\n    parser.add_argument(\"--host\", default=\"\", help=\"\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"\")\n    parser.add_argument(\"--uvicorn_log_level\", default=\"info\", help=\"\")\n    parser.add_argument(\"--allow_credentials\", action=\"store_true\", help=\"\")\n    parser.add_argument(\"--allowed_origins\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--allowed_methods\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--allowed_headers\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--api_key\", default=\"\", help=\"\")\n    parser.add_argument(\"--served_model_name\", default=\"\", help=\"\")\n    parser.add_argument(\"--prompt_template\", default=\"\", help=\"\")\n    parser.add_argument(\"--ssl_keyfile\", default=\"\", help=\"\")\n    parser.add_argument(\"--ssl_certfile\", default=\"\", help=\"\")\n    parser.add_argument(\"--response_role\", default=\"assistant\", help=\"\")\n    parser.add_argument(\n        \"--collections\", default=\"\", help=\"Collection name for indexing\"\n    )\n    parser.add_argument(\n        \"--base_dir\",\n        default=\"\",\n        help=\"Path where the processed text embeddings were stored\",\n    )\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name or args.model,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile,\n    )\n    byzerllm.connect_cluster(address=args.ray_address)\n    llm = byzerllm.ByzerLLM()\n    if not server_args.served_model_name:\n        raise ValueError(\"served_model_name is required\")\n    llm.setup_default_model_name(server_args.served_model_name)\n    rag = SummaryRAG(llm=llm)    \n    llm_wrapper = LLWrapper(llm=llm, rag=rag)\n    serve(llm=llm_wrapper, args=server_args)\n\nif __name__ == \"__main__\":\n    main()\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/api_server.py\nimport os\nimport time\nimport uvicorn\nfrom fastapi import Request\nfrom fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import JSONResponse, StreamingResponse, Response\n\nfrom byzerllm.log import init_logger\nfrom byzerllm.utils import random_uuid\nfrom byzerllm.version import __version__ as version\nfrom byzerllm.utils.client import ByzerLLM, LLMRequest\nfrom byzerllm.utils.client.entrypoints.openai.serving_chat import OpenAIServingChat\nfrom byzerllm.utils.client.entrypoints.openai.serving_completion import OpenAIServingCompletion\nfrom byzerllm.utils.client.entrypoints.openai.protocol import (\n    ModelList,\n    ModelCard,\n    ModelPermission,\n    ChatCompletionRequest,\n    ErrorResponse,\n    CompletionRequest,\n    Embeddings,\n    EmbeddingsOutput,\n    EmbeddingsData,\n    EmbeddingsUsage,\n)\nfrom pydantic import BaseModel\nfrom typing import List\n\nlogger = init_logger(__name__)\n\nllm_client: ByzerLLM = None\nopenai_serving_chat: OpenAIServingChat = None\nopenai_serving_completion: OpenAIServingCompletion = None\n\nTIMEOUT_KEEP_ALIVE = 5  # seconds\n# timeout in 10 minutes. Streaming can take longer than 3 min\nTIMEOUT = float(os.environ.get(\"BYZERLLM_APISERVER_HTTP_TIMEOUT\", 600))\n\nrouter_app = FastAPI()\n\n\n@router_app.get(\"/health\")\nasync def health() -> Response:\n    \"\"\"Health check.\"\"\"\n    return Response(status_code=200)\n\n\n@router_app.get(\"/v1/models\")\nasync def show_available_models():\n    models = await openai_serving_chat.show_available_models()\n    return JSONResponse(content=models.model_dump())\n\n\n@router_app.get(\"/version\")\nasync def show_version():\n    return JSONResponse(content={\"version\": version})\n\n\n@router_app.get(\"/v1/models\", response_model=ModelList)\nasync def models() -> ModelList:\n    \"\"\"Show available models. Right now we only have one model.\"\"\"\n    model_cards = [\n        ModelCard(\n            id=\"\",\n            root=\"\",\n            permission=[ModelPermission()]\n        )\n    ]\n    return ModelList(data=model_cards)\n\n\n@router_app.post(\"/v1/completions\")\nasync def create_completion(\n        body: CompletionRequest,\n        request: Request\n):\n    generator = await openai_serving_completion.create_completion(body, request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            status_code=generator.code\n        )\n    if body.stream:\n        return StreamingResponse(\n            content=generator,\n            media_type=\"text/event-stream\"\n        )\n    else:\n        return JSONResponse(content=generator.model_dump())\n\n\n@router_app.post(\"/v1/chat/completions\")\nasync def create_chat_completion(\n        body: ChatCompletionRequest,\n        request: Request,\n):\n    \"\"\"Completion API similar to OpenAI's API.\n\n    See  https://platform.openai.com/docs/api-reference/chat/create\n    for the API specification. This API mimics the OpenAI ChatCompletion API.\n\n    NOTE: Currently we do not support the following features:\n        - function_call (Users should implement this by themselves)\n        - logit_bias (to be supported by vLLM engine)\n    \"\"\"\n    # async with async_timeout.timeout(TIMEOUT):\n\n    generator = await openai_serving_chat.create_chat_completion(body, request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            status_code=generator.code\n        )\n    if body.stream:\n        return StreamingResponse(\n            content=generator,\n            media_type=\"text/event-stream\"\n        )\n    else:\n        return JSONResponse(content=generator.model_dump())\n\n\n@router_app.post(\"/v1/embeddings\")\nasync def embed(body: Embeddings):\n    \"\"\"Given a prompt, the model will return one embedding.\n\n    Returns:\n        A response object with an embedding.\n    \"\"\"\n    embedding_id = f\"embed-{random_uuid()}\"\n\n    results_list = llm_client.emb(body.model, request=LLMRequest(instruction=body.input))\n    tokens = 0\n\n    return EmbeddingsOutput(\n        data=[\n            EmbeddingsData(\n                embedding=results.output,\n                index=i,\n                object=\"embedding\",\n            )\n            for i, results in enumerate(results_list)\n        ],\n        id=embedding_id,\n        object=\"list\",\n        created=int(time.time()),\n        model=body.model,\n        usage=EmbeddingsUsage(\n            prompt_tokens=tokens,\n            total_tokens=tokens,\n        ),\n    )\n\nclass ServerArgs(BaseModel):\n    host: str = None\n    port: int = 8000\n    uvicorn_log_level: str = \"info\"\n    allow_credentials: bool = False\n    allowed_origins: List[str] = [\"*\"]  \n    allowed_methods: List[str] = [\"*\"]\n    allowed_headers: List[str] = [\"*\"]\n    api_key: str = None\n    served_model_name: str = None\n    prompt_template: str = None\n    response_role: str = \"assistant\"\n    ssl_keyfile: str = None\n    ssl_certfile: str = None    \n\ndef serve(llm:ByzerLLM, args: ServerArgs):\n    \n    logger.info(f\"ByzerLLM API server version {version}\")\n    logger.info(f\"args: {args}\")\n\n    router_app.add_middleware(\n        CORSMiddleware,\n        allow_origins=args.allowed_origins,\n        allow_credentials=args.allow_credentials,\n        allow_methods=args.allowed_methods,\n        allow_headers=args.allowed_headers,\n    )\n    \n    if token := os.environ.get(\"BYZERLLM_API_KEY\") or args.api_key:\n\n        @router_app.middleware(\"http\")\n        async def authentication(request: Request, call_next):\n            if not request.url.path.startswith(\"/v1\"):\n                return await call_next(request)\n            if request.headers.get(\"Authorization\") != \"Bearer \" + token:\n                return JSONResponse(\n                    content={\"error\": \"Unauthorized\"},\n                    status_code=401\n                )\n            return await call_next(request)\n\n    # Register labels for metrics\n    # add_global_metrics_labels(model_name=engine_args.model)\n    global llm_client\n    llm_client = llm\n    \n    global openai_serving_chat\n    openai_serving_chat = OpenAIServingChat(\n        llm_client=llm_client,\n        response_role=args.response_role,\n        server_model_name=args.served_model_name,\n        prompt_template=args.prompt_template\n    )\n    global openai_serving_completion\n    openai_serving_completion = OpenAIServingCompletion(\n        llm_client=llm_client,\n        server_model_name=args.served_model_name,\n        prompt_template=args.prompt_template\n    )\n\n    uvicorn.run(\n        router_app,\n        host=args.host,\n        port=args.port,\n        log_level=args.uvicorn_log_level,\n        timeout_keep_alive=TIMEOUT_KEEP_ALIVE,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile\n    )\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/generator.py\nfrom typing import Optional, Dict, Any, List\nfrom openai import OpenAI\nimport json\nfrom .types import RAGConfig, ByzerSQLConfig, ModelConfig\nfrom byzerllm.utils.client import code_utils\nimport byzerllm\nimport requests\nfrom loguru import logger\nimport traceback\nfrom byzerllm.utils.nontext import TagExtractor\n\n\nclass SQLGenerator:\n\n    @byzerllm.prompt()\n    def gen_context(self, conversations: List[Dict[str, str]]) -> str:\n        \"\"\"\n        对话内容：\n        {% for msg in conversations %}\n        <{{ msg.role }}>: {{ msg.content }}\n        {% endfor %}\n\n        根据提供的对话内容，理解用户最后的问题，然后提取可能会用到的 Byzer-SQL 相关的知识。\n        \"\"\"\n\n    @byzerllm.prompt()\n    def gen_sql(\n        self,\n        conversations: List[Dict[str, str]],\n        context: str,\n        schema: str,\n        model: str = \"deepseek_chat\",\n    ) -> str:\n        \"\"\"\n        相关知识：\n        {{ context }}\n\n        Schema信息：\n        {{ schema }}\n\n        对话内容：\n        {% for msg in conversations %}\n        <{{ msg.role }}>: {{ msg.content }}\n        {% endfor %}\n\n        模型UDF函数：\n        {{ model }}\n\n        根据提供的相关知识以及对话内容，理解用户最后的问题，然后根据看到的schema 信息，生成 Byzer-SQL 语句。\n        Byzer-SQL 首先需要用 load 语法加载 schema 中的路径映射成表格，然后根据表格的信息生成对应的 Byzer-SQL 语句。\n\n        特别需要关注的是:\n        1. 用户的很多需求是要对表格的文本内容做自然语言处理，你需要使用 Byzer-SQL 中的大模型辅助函数来完成这个需求。\n        2. 使用 `llm_result` 函数处理 LLM 模型函数的响应,`llm_result` 函数得到的是一段文本，虽然我们要求模型函数只输出特定的文本，但考虑到大模型会有一些额外的字符输出，不会只是输出我们要求的文字，所以通过 `like` 来进行匹配。\n        3. 注意 Byzer-SQL 的语法，比如select 语句最后一定需要有 as <table_name>。\n        3. 输出的 Byzer-SQL 语句务必使用 <_byzer_sql_></_byzer_sql_> 标签包裹起来。\n        \"\"\"\n\n    @byzerllm.prompt()\n    def fetch_schema(self, context: str) -> str:\n        \"\"\"\n        下面是一些基础知识，包括名词术语等等：\n        {{ context }}\n\n        当理解对话内容的时候，需要参考上面的基础知识，然后更好的理解对应的表格schema信息是不是有用。\n        \"\"\"\n\n    def __init__(\n        self,\n        llm: byzerllm.ByzerLLM,\n        rag_config: RAGConfig,\n        byzer_sql_config: ByzerSQLConfig,\n        model_config: ModelConfig,\n    ) -> None:\n        self.llm = llm\n        self.schema_rag_client = OpenAI(\n            api_key=rag_config.schema_rag_api_key,\n            base_url=rag_config.schema_rag_base_url,\n        )\n        self.context_rag_client = OpenAI(\n            api_key=rag_config.context_rag_api_key,\n            base_url=rag_config.context_rag_base_url,\n        )\n        self.byzer_sql_url = byzer_sql_config.byzer_sql_url\n\n        self.llm_model = byzerllm.ByzerLLM()\n        self.llm_model.setup_default_model_name(model_config.llm_model)\n        self.gen_sql_llm_model = byzerllm.ByzerLLM()\n        self.gen_sql_llm_model.setup_default_model_name(model_config.gen_sql_llm_model)\n\n    def stream_chat_oai(\n        self,\n        conversations,\n        model: Optional[str] = None,\n        role_mapping=None,\n        delta_mode=False,\n        llm_config: Dict[str, Any] = {},\n    ):\n        try:\n            v, c = self._stream_chat_oai(\n                conversations=conversations,\n                model=model,\n                role_mapping=role_mapping,\n                delta_mode=delta_mode,\n                llm_config=llm_config,\n            )\n        except Exception as e:\n            error_msg = f\"Error in stream_chat_oai: {e}\\n{traceback.format_exc()}\"\n            logger.error(error_msg)\n            v, c = [str(e)], []\n\n        return v, c\n\n    def _stream_chat_oai(\n        self,\n        conversations,\n        model: Optional[str] = None,\n        role_mapping=None,\n        delta_mode=False,\n        llm_config: Dict[str, Any] = {},\n    ):\n        logger.info(f\"Starting stream_chat_oai with model: {model}\")\n        logger.debug(f\"Conversations: {conversations}\")\n        logger.debug(f\"LLM Config: {llm_config}\")\n\n        if not model:\n            model = self.llm_model.default_model_name\n        logger.info(f\"Using model: {model}\")\n\n        query = conversations[-1][\"content\"]\n\n        if (\n            \"使用四到五个字直接返回这句话的简要主题，不要解释、不要标点、不要语气词、不要多余文本，不要加粗，如果没有主题\"\n            in query\n            or \"简要总结一下对话内容，用作后续的上下文提示 prompt，控制在 200 字以内\"\n            in query\n        ):\n            chunks = self.llm.stream_chat_oai(\n                conversations=conversations,\n                model=model,\n                role_mapping=role_mapping,\n                llm_config=llm_config,\n                delta_mode=True,\n            )\n            return (chunk[0] for chunk in chunks), []\n\n        logger.info(\"Fetching context from context RAG client\")\n        context_query = self.gen_context.prompt(conversations=conversations)\n        context_response = self.context_rag_client.chat.completions.create(\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": json.dumps(\n                        {\"query\": context_query, \"only_contexts\": True},\n                        ensure_ascii=False,\n                    ),\n                }\n            ],\n            model=model,\n        )\n\n        context = context_response.choices[0].message.content\n        new_context = \"\"\n        for msg in context.split(\"\\n\"):\n            if msg.strip():\n                v = json.loads(msg)\n                new_context += v[\"source_code\"]\n        logger.info(f\"context {new_context[0:100]}...\")\n        logger.info(\"Context retrieved successfully\")\n\n        logger.info(\"Fetching schemas from schema RAG client\")\n        schema_query = self.fetch_schema.prompt(context=new_context)\n\n        response = self.schema_rag_client.chat.completions.create(\n            messages=conversations[0:-1]\n            + [\n                {\n                    \"role\": \"user\",\n                    \"content\": json.dumps(\n                        {\"query\": schema_query, \"only_contexts\": True},\n                        ensure_ascii=False,\n                    ),\n                }\n            ],\n            model=model,\n        )\n        logger.debug(f\"Schema RAG response: {response}\")\n\n        schema_str = response.choices[0].message.content\n        logger.info(\"Schema retrieved successfully\")\n\n        logger.info(\"Generating SQL string\")\n        sql_str = self.gen_sql.with_llm(self.gen_sql_llm_model).run(\n            conversations=conversations,\n            context=new_context,\n            schema=schema_str,\n            model=model,\n        )\n\n        logger.info(\"SQL string generated\")\n        logger.debug(f\"Generated SQL string: {sql_str}\")\n\n        # Extract SQL code from tags\n        tag_extractor = TagExtractor(sql_str)\n        extracted_tags = tag_extractor.extract()\n        sql = extracted_tags.content[0].content\n        logger.info(f\"Extracted SQL: {sql}\")\n\n        logger.info(f\"Sending SQL to Byzer SQL URL: {self.byzer_sql_url}\")\n        try:\n            response = requests.post(\n                self.byzer_sql_url,\n                data={\n                    \"sql\": sql,\n                    \"owner\": \"hello\",\n                    \"sessionPerUser\": True,\n                    \"sessionPerRequest\": True,\n                    \"includeSchema\": True,\n                },\n            )\n            response.raise_for_status()\n            result = response.content.decode(\"utf-8\")\n            logger.info(\"SQL execution result decoded\")\n            logger.debug(f\"SQL execution result: {result}\")\n            return (item for item in [result]), []\n        except requests.exceptions.RequestException as e:\n            logger.error(f\"Failed to send SQL to Byzer SQL URL: {e}\")\n            if hasattr(e.response, \"content\"):\n                error_content = e.response.content.decode(\"utf-8\")\n                logger.error(f\"Server response: {error_content}\")\n            else:\n                logger.error(\"No server response available\")\n            raise Exception(f\"Byzer SQL request failed: {str(e)}\") from e\n\n\n##File: /Users/allwefantasy/projects/super_analysis/setup.py\nfrom setuptools import setup, find_packages\n\n# Read requirements from requirements.txt\nwith open('requirements.txt') as f:\n    requirements = f.read().splitlines()\n\nsetup(\n    name=\"super-analysis\",\n    version=\"0.1.0\",\n    packages=find_packages(where=\"src\"),\n    package_dir={\"\": \"src\"},\n    install_requires=requirements,\n    entry_points={\n        \"console_scripts\": [\n            \"super-analysis.serve=superanalysis.serve.cli:main\",\n            \"super-analysis.convert=superanalysis.serve.convert:main\",\n            \"superanalysis.api=superanalysis.serve.rest:main\",\n        ],\n    },\n    author=\"allwefantasy\",\n    author_email=\"allwefantasy@gmail.com\",\n    description=\"A package for super analysis with ByzerLLM\",\n    long_description=open(\"README.md\").read(),\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/yourusername/super-analysis\",\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n    ],\n    python_requires='>=3.6',\n)\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/convert.py\nimport os\nimport argparse\nimport pandas as pd\nimport hashlib\nfrom typing import List, Dict\n\ndef get_csv_files(directory: str) -> List[str]:\n    \"\"\"Recursively get all CSV files in the given directory.\"\"\"\n    csv_files = []\n    for root, _, files in os.walk(directory):\n        for file in files:\n            if file.endswith('.csv'):\n                csv_files.append(os.path.join(root, file))\n    return csv_files\n\ndef extract_schema(df: pd.DataFrame) -> Dict[str, str]:\n    \"\"\"Extract schema from pandas DataFrame.\"\"\"\n    schema = {}\n    for column in df.columns:\n        dtype = str(df[column].dtype)\n        if dtype == 'object':\n            schema[column] = 'STRING'\n        elif dtype.startswith('int'):\n            schema[column] = 'INTEGER'\n        elif dtype.startswith('float'):\n            schema[column] = 'FLOAT'\n        elif dtype == 'bool':\n            schema[column] = 'BOOLEAN'\n        elif dtype.startswith('datetime'):\n            schema[column] = 'TIMESTAMP'\n        else:\n            schema[column] = 'STRING'  # Default to STRING for unknown types\n    return schema\n\ndef generate_schema_text(schema: Dict[str, str], csv_file: str, df: pd.DataFrame, include_rows_num: int) -> str:\n    \"\"\"Generate schema text from the schema dictionary and include the CSV file name and sample data.\"\"\"\n    schema_lines = [f\"CSV File: {csv_file}\\n\"]\n    schema_lines.extend([f\"{field}: {dtype}\" for field, dtype in schema.items()])\n    \n    if include_rows_num > 0:\n        schema_lines.append(f\"\\nSample Data (first {include_rows_num} rows):\")\n        sample_data = df.head(include_rows_num).to_string(index=False)\n        schema_lines.append(sample_data)\n    \n    return \"\\n\".join(schema_lines)\n\ndef save_schema(schema_text: str, csv_file: str, output_dir: str) -> None:\n    \"\"\"Save schema text to a file in the output directory.\"\"\"\n    # Generate MD5 hash of the schema text\n    file_name = hashlib.md5(schema_text.encode()).hexdigest() + '.md'\n    output_path = os.path.join(output_dir, file_name)\n    \n    with open(output_path, 'w') as f:\n        f.write(schema_text)\n    print(f\"Schema for {csv_file} saved to {output_path}\")\n\ndef convert(data_dir: str, doc_dir: str, include_rows_num: int) -> None:\n    \"\"\"Main conversion function.\"\"\"\n    csv_files = get_csv_files(data_dir)\n    \n    for csv_file in csv_files:\n        print(f\"Processing {csv_file}\")\n        df = pd.read_csv(csv_file)\n        schema = extract_schema(df)\n        schema_text = generate_schema_text(schema, csv_file, df, include_rows_num)\n        save_schema(schema_text, csv_file, doc_dir)\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Convert CSV files to schema documents\")\n    parser.add_argument(\"--data_dir\", required=True, help=\"Directory containing CSV files\")\n    parser.add_argument(\"--doc_dir\", required=True, help=\"Output directory for schema documents\")\n    parser.add_argument(\"--include-rows-num\", type=int, default=0, help=\"Number of sample rows to include in the schema\")\n    \n    args = parser.parse_args()\n    \n    if not os.path.exists(args.doc_dir):\n        os.makedirs(args.doc_dir)\n    \n    convert(args.data_dir, args.doc_dir, args.include_rows_num)\n\nif __name__ == \"__main__\":\n    main()\n\n##File: /Users/allwefantasy/projects/super_analysis/README.md\n# Super Analysis\n\nSuper Analysis 支持 nlp2sql 以及 使用 SQL 对表中的文本字段做自然语言分析。\n\n## 安装\n\n你可以使用 pip 安装 Super Analysis：\n\n```bash\npip install super-analysis\n```\n\n## 使用方法\n\n安装完成后，你可以使用 `super-analysis.serve` 命令来启动 API 服务器：\n\n```bash\nsuper-analysis.serve --served-model-name your_model_name\n```\n\n### 命令行参数\n\n以下是可用的命令行参数：\n\n- `--host`: 服务器绑定的主机地址（默认：0.0.0.0）\n- `--port`: 服务器绑定的端口（默认：8000）\n- `--uvicorn-log-level`: Uvicorn 日志级别（默认：info）\n- `--allow-credentials`: 允许凭证（默认：False）\n- `--allowed-origins`: 允许的源（默认：[\"*\"]）\n- `--allowed-methods`: 允许的方法（默认：[\"*\"]）\n- `--allowed-headers`: 允许的头部（默认：[\"*\"]）\n- `--api-key`: 用于认证的 API 密钥\n- `--served-model-name`: 要服务的模型名称（必需）\n- `--prompt-template`: 提示模板\n- `--response-role`: 响应角色（默认：assistant）\n- `--ssl-keyfile`: SSL 密钥文件路径\n- `--ssl-certfile`: SSL 证书文件路径\n\n例如：\n\n```bash\nsuper-analysis.serve --served-model-name deepseek_chat --port 8000\n```\n\n## 开发\n\n如果你想参与开发，可以克隆这个仓库并安装依赖：\n\n```bash\ngit clone https://github.com/yourusername/super-analysis.git\ncd super-analysis\npip install -e .\n```\n\n\n## 抽取csv文件schema\n\nThe `super-analysis.convert` command allows you to convert CSV files to schema documents. This is useful for generating metadata about your CSV files that can be used for further analysis or documentation.\n\n#### Usage\n\n```\nsuper-analysis.convert --data_dir <path_to_csv_files> --doc_dir <output_directory> [--include-rows-num <number_of_sample_rows>]\n```\n\nArguments:\n- `--data_dir`: Directory containing CSV files (required)\n- `--doc_dir`: Output directory for schema documents (required)\n- `--include-rows-num`: Number of sample rows to include in the schema (optional, default is 0)\n\nThe utility will:\n1. Recursively find all CSV files in the specified directory\n2. Extract the schema from each CSV file\n3. Generate a schema text that includes:\n   - The CSV file name\n   - Field names and their data types\n   - Sample data (if `--include-rows-num` is specified)\n4. Save the schema text as a markdown file in the output directory\n\n## 启动分析服务\n\n```bash\n## 将csv文件转换为schema文档\nsuper-analysis.convert --data_dir /Users/allwefantasy/data/movice --doc_dir /Users/allwefantasy/data/movice/schemas/\n\n# 启动一个schema文档知识库\nauto-coder doc serve \\\n--model deepseek_chat --index_filter_workers 100 \\\n--tokenizer_path /Users/allwefantasy/Downloads/tokenizer.json \\\n--doc_dir /Users/allwefantasy/data/movice/schemas/ \\\n--port 8001\n\n# 启动一个 Byzer-SQL 以及其他需要的文档知识库\nauto-coder doc serve \\\n--model deepseek_chat --index_filter_workers 100 \\\n--tokenizer_path /Users/allwefantasy/Downloads/tokenizer.json \\\n--doc_dir  /Users/allwefantasy/projects/llm_friendly_packages/github.com/allwefantasy \\\n--port 8002\n\n# 启动兼容 OpenAI Server 的分析服务\nsuper-analysis.serve --served-model-name deepseek_chat --port 8000 \\\n--schema-rag-base-url http://127.0.0.1:8001/v1 \\\n--context-rag-base-url http://127.0.0.1:8002/v1 \\\n--byzer-sql-url http://127.0.0.1:9003/run/script\n```\n\n现在你可以用 OpenAI SDK 去使用了。 参考 [openai_local_api.ipynb](./notebooks/openai_local_api.ipynb)\n\n## 非 OpenAi 兼容接口\n\n## 联系方式\n\n如有任何问题或建议，请联系 allwefantasy@gmail.com。\n\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/rest.py\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nfrom typing import List, Optional\nimport uvicorn\nimport argparse\nfrom superanalysis.sql.generator import SQLGenerator\nfrom superanalysis.sql.types import RAGConfig, ByzerSQLConfig, ModelConfig\nimport byzerllm\n\napp = FastAPI()\n\nclass ChatMessage(BaseModel):\n    role: str\n    content: str\n\nclass ChatRequest(BaseModel):\n    conversations: List[ChatMessage]\n    model: Optional[str] = None\n\nclass ChatResponse(BaseModel):\n    response: str\n\n@app.post(\"/chat\", response_model=ChatResponse)\nasync def chat(request: ChatRequest):\n    try:\n        result, _ = sql_generator.stream_chat_oai(\n            conversations=[{\"role\": msg.role, \"content\": msg.content} for msg in request.conversations],\n            model=request.model\n        )\n        return ChatResponse(response=\"\".join(result))\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"SuperAnalysis API Server\")\n    parser.add_argument(\"--host\", type=str, default=\"0.0.0.0\", help=\"Host to bind the server to\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"Port to bind the server to\")\n    parser.add_argument(\"--schema-rag-base-url\", type=str, required=True, help=\"Schema RAG base URL\")\n    parser.add_argument(\"--schema-rag-api-key\", type=str, required=True, help=\"Schema RAG API key\")\n    parser.add_argument(\"--context-rag-base-url\", type=str, required=True, help=\"Context RAG base URL\")\n    parser.add_argument(\"--context-rag-api-key\", type=str, required=True, help=\"Context RAG API key\")\n    parser.add_argument(\"--byzer-sql-url\", type=str, required=True, help=\"Byzer SQL URL\")\n    parser.add_argument(\"--data-dir\", type=str, default=\"\", help=\"Data directory\")\n    parser.add_argument(\"--llm-model\", type=str, required=True, help=\"LLM model for general use\")\n    parser.add_argument(\"--gen-sql-llm-model\", type=str, required=True, help=\"LLM model for SQL generation\")\n    \n    args = parser.parse_args()\n\n    rag_config = RAGConfig(\n        schema_rag_base_url=args.schema_rag_base_url,\n        schema_rag_api_key=args.schema_rag_api_key,\n        context_rag_base_url=args.context_rag_base_url,\n        context_rag_api_key=args.context_rag_api_key\n    )\n\n    byzer_sql_config = ByzerSQLConfig(\n        byzer_sql_url=args.byzer_sql_url,\n        data_dir=args.data_dir\n    )\n\n    model_config = ModelConfig(\n        llm_model=args.llm_model,\n        gen_sql_llm_model=args.gen_sql_llm_model\n    )\n\n    llm = byzerllm.ByzerLLM()\n    llm.setup_default_model_name(args.llm_model)\n\n    global sql_generator\n    sql_generator = SQLGenerator(\n        llm=llm,\n        rag_config=rag_config,\n        byzer_sql_config=byzer_sql_config,\n        model_config=model_config\n    )\n\n    uvicorn.run(app, host=args.host, port=args.port)\n\nif __name__ == \"__main__\":\n    main()\n\n##File: .auto-coder/libs/llm_friendly_packages/github.com/allwefantasy/byzer-llm/README.md\n# byzerllm 大模型编程快速指南\n\n本文示例 [Notebook](../../notebooks/003_byzerllm_大模型编程快速指南.ipynb)\n\n## 安装\n\n```bash\npip install -U byzerllm\nray start --head\n```\n## 启动一个模型代理\n\nbyzerllm 支持私有化模型或者SaaS模型的部署。\n\n这里以 deepseek 官方API 为例：\n\n```bash\neasy-byzerllm deploy deepseek-chat --token xxxxx --alias deepseek_chat\n```\n\n或者跬基流动API:\n\n```bash\neasy-byzerllm deploy alibaba/Qwen1.5-110B-Chat --token xxxxx --alias qwen110b_chat\n```\n\n将上面的 API KEY 替换成你们自己的。\n\n如果你想部署私有化模型或者对接 Ollama 等更多需求，参考 [002_使用byzerllm进行模型部署.md](./002_使用byzerllm进行模型部署.md) 或者 [README.md](../../README.md)。\n\n之后，你就可以在代码里使用  deepseek_chat 或者 qwen110b_chat  访问模型了。\n\n## hello world\n\n来和我们的大模型打个招呼:\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\n@byzerllm.prompt(llm=llm)\ndef hello(q:str) ->str:\n    '''\n    你好, {{ q }}\n    '''\n\ns = hello(\"你是谁\")    \nprint(s)\n\n## 输出:\n## '你好！我是一个人工智能助手，专门设计来回答问题、提供信息和帮助解决问题。如果你有任何疑问或需要帮助，请随时告诉我。'\n```\n\n恭喜，你和大模型成功打了招呼！\n\n可以看到，我们通过 `@byzerllm.prompt` 装饰器，将一个方法转换成了一个大模型的调用，然后这个方法的主题是一段文本，文本中\n使用了 jinja2 模板语法，来获得方法的参数。当正常调用该方法时，实际上就发起了和大模型的交互，并且返回了大模型的结果。\n\n在 byzerllm 中，我们把这种方法称之为 prompt 函数。\n\n## 查看发送给大模型的prompt\n\n很多情况你可能需要调试，查看自己的 prompt 渲染后到底是什么样子的，这个时候你可以通过如下方式\n获取渲染后的prompt:\n\n```python\nhello.prompt(\"你是谁\")\n## '你好, 你是谁'\n```            \n\n## 动态换一个模型\n\n前面的 hello 方法在初始化的时候，我们使用了默认的模型 deepseek_chat，如果我们想换一个模型，可以这样做：\n\n```python\nhello.with_llm(llm).run(\"你是谁\")\n## '你好！我是一个人工智能助手，专门设计来回答问题、提供信息和帮助解决问题。如果你有任何疑问或需要帮助，请随时告诉我。'\n```\n\n通过 with_llm 你可以设置一个新的 llm 对象，然后调用 run 方法，就可以使用新的模型了。\n\n## 超长文本生成\n\n我们知道，大模型一次生成的长度其实是有限的，如果你想生成超长文本，你可能需手动的不断获得\n生成结果，然后把他转化为输入，然后再次生成，这样的方式是比较麻烦的。\n\nbyzerllm 提供了更加易用的 API :\n\n```python\nimport byzerllm\nfrom byzerllm import ByzerLLM\n\nllm = ByzerLLM.from_default_model(\"deepseek_chat\")\n\n@byzerllm.prompt()\ndef tell_story() -> str:\n    \"\"\"\n    讲一个100字的故事。    \n    \"\"\"\n\n\ns = (\n    tell_story.with_llm(llm)\n    .with_response_markers()\n    .options({\"llm_config\": {\"max_length\": 10}})\n    .run()\n)\nprint(s)\n\n## 从前，森林里住着一只聪明的小狐狸。一天，它发现了一块闪闪发光的宝石。小狐狸决定用这块宝石帮助森林里的动物们。它用宝石的光芒指引迷路的小鹿找到了回家的路，用宝石的温暖治愈了受伤的小鸟。从此，小狐狸成了森林里的英雄，动物们都感激它的善良和智慧。\n```\n\n实际核心部分就是这一行：\n\n```python\ntell_story.with_llm(llm)\n    .with_response_markers()    \n    .run()\n```\n\n我们只需要调用 `with_response_markers` 方法，系统就会自动的帮我们生成超长文本。\n在上面的案例中，我们通过\n\n```python\n.options({\"llm_config\": {\"max_length\": 10}})\n```\n\n认为的限制大模型一次交互最多只能输出10个字符，但是系统依然自动完成了远超过10个字符的文本生成。\n\n## 对象输出\n\n前面我们的例子都是返回字符串，但是我们也可以返回对象，这样我们就可以更加灵活的处理返回结果。\n\n```python\nimport pydantic \n\nclass Story(pydantic.BaseModel):\n    '''\n    故事\n    '''\n\n    title: str = pydantic.Field(description=\"故事的标题\")\n    body: str = pydantic.Field(description=\"故事主体\")\n\n@byzerllm.prompt()\ndef tell_story()->Story:\n    '''\n    讲一个100字的故事。    \n    '''\n\ns = tell_story.with_llm(llm).run()\nprint(isinstance(s, Story))\nprint(s.title)\n\n## True\n## 勇敢的小鸟\n```\n\n可以看到，我们很轻松的将输出转化为格式化输出。\n\n## 自定义字段抽取\n\n前面的结构化输出，其实会消耗更多token,还有一种更加精准的结构化输出方式。\n比如让大模型生成一个正则表达式，但实际上大模型很难准确只输出一个正则表达式，这个时候我们可以通过自定义抽取函数来获取我们想要的结果。\n\n\n```python\nfrom loguru import logger\nimport re\n\n@byzerllm.prompt()\ndef generate_regex_pattern(desc: str) -> str:\n    \"\"\"\n    根据下面的描述生成一个正则表达式，要符合python re.compile 库的要求。\n\n    {{ desc }}\n\n    最后生成的正则表达式要在<REGEX></REGEX>标签对里。\n    \"\"\"    \n\ndef extract_regex_pattern(regex_block: str) -> str:    \n    pattern = re.search(r\"<REGEX>(.*)</REGEX>\", regex_block, re.DOTALL)\n    if pattern is None:\n        logger.warning(\"No regex pattern found in the generated block:\\n {regex_block}\")\n        raise None\n    return pattern.group(1)\n\npattern = \"匹配一个邮箱地址\"\nv = generate_regex_pattern.with_llm(llm).with_extractor(extract_regex_pattern).run(desc=pattern)\nprint(v)\n## ^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\n```\n\n在上面的例子里，我们根据一句话生成一个正则表达式。我们通过 `with_extractor` 方法，传入了一个自定义的抽取函数，这个函数会在大模型生成结果后，对结果进行处理，然后返回我们想要的结果。\n\n我们在 prompt 明确说了，生成的结果要放到 `<REGEX></REGEX>` 标签对里，然后我们通过 extract_regex_pattern 函数，从结果中提取出了我们想要的正则表达式。\n\n## 在实例方法中使用大模型\n\n```python\nimport byzerllm\ndata = {\n    'name': 'Jane Doe',\n    'task_count': 3,\n    'tasks': [\n        {'name': 'Submit report', 'due_date': '2024-03-10'},\n        {'name': 'Finish project', 'due_date': '2024-03-15'},\n        {'name': 'Reply to emails', 'due_date': '2024-03-08'}\n    ]\n}\n\n\nclass RAG():\n    def __init__(self):        \n        self.llm = byzerllm.ByzerLLM()\n        self.llm.setup_template(model=\"deepseek_chat\",template=\"auto\")\n        self.llm.setup_default_model_name(\"deepseek_chat\")        \n    \n    @byzerllm.prompt(lambda self:self.llm)\n    def generate_answer(self,name,task_count,tasks)->str:\n        '''\n        Hello {{ name }},\n\n        This is a reminder that you have {{ task_count }} pending tasks:\n        {% for task in tasks %}\n        - Task: {{ task.name }} | Due: {{ task.due_date }}\n        {% endfor %}\n\n        Best regards,\n        Your Reminder System\n        '''        \n\nt = RAG()\n\nresponse = t.generate_answer(**data)\nprint(response)\n\n## 输出:\n## Hello Jane Doe,\n##I hope this message finds you well. I wanted to remind you of your 3 pending tasks to ensure you stay on track:\n## 1. **Submit report** - This task is due on **2024-03-10**. Please ensure that you allocat\n```\n\n这里我们给了个比较复杂的例子，但我们可以看到，给一个实例prompt方法和普通prompt 方法差异不大。\n唯一的区别是如果你希望在定义的时候就指定大模型，使用一个lambda函数返回实例的 llm 对象即可。\n\n```python\n@byzerllm.prompt(lambda self:self.llm)\n```\n\n你也可以不返回，在调用的时候通过 `with_llm` 方法指定 llm 对象。\n\n此外，这个例子也展示了如何通过jinja2模板语法，来处理复杂的结构化数据。\n\n## 通过 Python 代码处理复杂入参\n\n上面的一个例子中，我们通过 jinja2 模板语法，来处理复杂的结构化数据，但是有时候我们可能需要更加复杂的处理，这个时候我们可以通过 Python 代码来处理。\n\n```python\nimport byzerllm\n\ndata = {\n    'name': 'Jane Doe',\n    'task_count': 3,\n    'tasks': [\n        {'name': 'Submit report', 'due_date': '2024-03-10'},\n        {'name': 'Finish project', 'due_date': '2024-03-15'},\n        {'name': 'Reply to emails', 'due_date': '2024-03-08'}\n    ]\n}\n\n\nclass RAG():\n    def __init__(self):        \n        self.llm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n    \n    @byzerllm.prompt()\n    def generate_answer(self,name,task_count,tasks)->str:\n        '''\n        Hello {{ name }},\n\n        This is a reminder that you have {{ task_count }} pending tasks:\n            \n        {{ tasks }}\n\n        Best regards,\n        Your Reminder System\n        '''\n        \n        tasks_str = \"\\n\".join([f\"- Task: {task['name']} | Due: { task['due_date'] }\" for task in tasks])\n        return {\"tasks\": tasks_str}\n\nt = RAG()\n\nresponse = t.generate_answer.with_llm(t.llm).run(**data)\nprint(response)\n\n## Just a gentle nudge to keep you on track with your pending tasks. Here's a quick recap:....\n```\n\n在这个例子里，我们直接把 tasks 在方法体里进行处理，然后作为一个字符串返回，最够构建一个字典，字典的key为 tasks,然后\n你就可以在 docstring 里使用 `{{ tasks }}` 来引用这个字符串。\n\n这样对于很复杂的入参，就不用谢繁琐的 jinja2 模板语法了。\n\n## 如何自动实现一个方法\n\n比如我定义一个签名，但是我不想自己实现里面的逻辑，让大模型来实现。这个在 byzerllm 中叫 function impl。我们来看看怎么\n实现:\n\n```python\nimport pydantic\nclass Time(pydantic.BaseModel):\n    time: str = pydantic.Field(...,description=\"时间，时间格式为 yyyy-MM-dd\")\n\n\n@llm.impl()\ndef calculate_current_time()->Time:\n    '''\n    计算当前时间\n    '''\n    pass \n\n\ncalculate_current_time()\n#output: Time(time='2024-06-14')\n```\n\n在这个例子里，我们定义了一个 calculate_current_time 方法，但是我们没有实现里面的逻辑，我们通过 `@llm.impl()` 装饰器，让大模型来实现这个方法。\n为了避免每次都要“生成”这个方法，导致无法适用，我们提供了缓存，用户可以按如下方式打印速度：\n\n```python\nstart = time.monotonic()\ncalculate_current_time()\nprint(f\"first time cost: {time.monotonic()-start}\")\n\nstart = time.monotonic()\ncalculate_current_time()\nprint(f\"second time cost: {time.monotonic()-start}\")\n\n# output:\n# first time cost: 6.067266260739416\n# second time cost: 4.347506910562515e-05\n```\n可以看到，第一次执行花费了6s,第二次几乎是瞬间完成的，这是因为第一次执行的时候，我们实际上是在生成这个方法，第二次执行的时候，我们是执行已经生成好的代码，所以速度会非常快。你可以显示的调用 `llm.clear_impl_cache()` 清理掉函数缓存。\n\n## Stream 模式\n\n前面的例子都是一次性生成结果，但是有时候我们可能需要一个流式的输出，这个时候我们可能需要用底层一点的API来完成了：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\nv = llm.stream_chat_oai(model=\"deepseek_chat\",conversations=[{\n    \"role\":\"user\",\n    \"content\":\"你好，你是谁\",\n}],delta_mode=True)\n\nfor t in v:\n    print(t,flush=True)  \n\n# 你好\n# ！\n# 我\n# 是一个\n# 人工智能\n# 助手\n# ，\n# 旨在\n# 提供\n# 信息\n# 、\n# 解答\n# 问题....\n```\n\n如果你不想要流式输出，但是想用底层一点的API，你可以使用 `llm.chat_oai` 方法：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\nv = llm.chat_oai(model=\"deepseek_chat\",conversations=[{\n    \"role\":\"user\",\n    \"content\":\"你好，你是谁\",\n}])\n\nprint(v[0].output)\n## 你好！我是一个人工智能助手，旨在提供信息、解答问题和帮助用户解决问题。如果你有任何问题或需要帮助，请随时告诉我。\n```\n\n## Function Calling \n\nbyzerllm 可以不依赖模型自身就能提供 function calling 支持，我们来看个例子：\n\n\n```python\nfrom typing import List,Dict,Any,Annotated\nimport pydantic \nimport datetime\nfrom dateutil.relativedelta import relativedelta\n\ndef compute_date_range(count:Annotated[int,\"时间跨度，数值类型\"],\n                       unit:Annotated[str,\"时间单位，字符串类型\",{\"enum\":[\"day\",\"week\",\"month\",\"year\"]}])->List[str]:\n    '''\n    计算日期范围\n\n    Args:\n        count: 时间跨度，数值类型\n        unit: 时间单位，字符串类型，可选值为 day,week,month,year\n    '''        \n    now = datetime.datetime.now()\n    now_str = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n    if unit == \"day\":\n        return [(now - relativedelta(days=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"week\":\n        return [(now - relativedelta(weeks=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"month\":\n        return [(now - relativedelta(months=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"year\":\n        return [(now - relativedelta(years=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    return [\"\",\"\"]\n\ndef compute_now()->str:\n    '''\n    计算当前时间\n    '''\n    return datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n```\n\n我们定义了两个方法，一个是计算日期范围，一个是计算当前时间。\n\n现在我么可以来测试下，系统如何根据自然语言决定调用哪个方法：\n\n```python\nt = llm.chat_oai([{\n    \"content\":'''计算当前时间''',\n    \"role\":\"user\"    \n}],tools=[compute_date_range,compute_now],execute_tool=True)\n\nt[0].values\n\n## output: ['2024-06-14 15:18:02']\n```\n\n我们可以看到，他正确的选择了 compute_now 方法。\n\n接着我们再试一个：\n\n```python\nt = llm.chat_oai([{\n    \"content\":'''最近三个月趋势''',\n    \"role\":\"user\"    \n}],tools=[compute_date_range,compute_now],execute_tool=True)\n\nt[0].values\n\n## output: [['2024-03-14 15:19:13', '2024-06-14 15:19:13']]\n```\n\n模型正确的选择了 compute_date_range 方法。\n\n## 多模态\n\nbyerllm 也能很好的支持多模态的交互，而且统一了多模态大模型的接口，比如你可以用一样的方式使用 openai 或者 claude 的图片转文字能力， 或者一致的方式使用火山，azuer, openai的语音合成接口。\n\n### 图生文\n\n```python\nimport byzerllm\nfrom byzerllm.types import ImagePath\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\n\n\n@byzerllm.prompt()\ndef what_in_image(image_path: ImagePath) -> str:\n    \"\"\"\n    {{ image_path }}\n    这个图片里有什么？\n    \"\"\"    \n\n\nv = what_in_image.with_llm(vl_llm).run(\n    ImagePath(value=\"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\")\n)\nv\n## OUTPUT: 这张图片展示了多只可爱的猫咪，采用了艺术风格的绘画。猫咪们有不同的颜色和花纹，背景是浅棕色，上面还点缀着一些红色的花朵。整体画面给人一种温馨和谐的感觉\n```\n\n可以看到，我们只需要把 prompt 函数的图片地址入参使用 byzerllm.types.ImagePath里进行包装，就可以直接在 prompt 函数体里\n带上图片。\n\n或者你可以这样：\n\n```python\nimport byzerllm\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\n\n\n@byzerllm.prompt()\ndef what_in_image(image_path: str) -> str:\n    \"\"\"\n    {{ image }}\n    这个图片里有什么？\n    \"\"\"\n    return {\"image\": byzerllm.Image.load_image_from_path(image_path)}\n\n\nv = what_in_image.with_llm(vl_llm).run(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv\n```\n\n通过 `image_path` 参数，然后通过 `byzerllm.Image.load_image_from_path` 方法，转化为一个图片对象 image，最后在 prompt 函数体里\n使用 `{{ image }}` 引用这个图片对象。\n\n另外我们也是可以支持配置多张图片的。\n\n另外，我们也可以使用基础的 `llm.chat_oai` 方法来实现：\n\n```python\nimport byzerllm\nimport json\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\nimage = byzerllm.Image.load_image_from_path(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv = vl_llm.chat_oai(\n    conversations=[\n        {\n            \"role\": \"user\",\n            \"content\": json.dumps(\n                [{\"image\": image, \"text\": \"这个图片里有什么？\"}], ensure_ascii=False\n            ),\n        }\n    ]\n)\nv[0].output\n```\n\n还可以这么写：\n\n```python\nimport byzerllm\nimport json\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\nimage = byzerllm.Image.load_image_from_path(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv = vl_llm.chat_oai(\n    conversations=[\n        {\n            \"role\": \"user\",\n            \"content\": json.dumps(\n                [\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\"url\": image, \"detail\": \"high\"},\n                    },\n                    {\"text\": \"这个图片里有什么？\", \"type\": \"text\"},\n                ],\n                ensure_ascii=False,\n            ),\n        }\n    ]\n)\nv[0].output\n```\n\n### 语音合成\n\n这里以 openai 的 tts 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=tts-1 \\\n--model openai_tts\n```\n\n此外，byzerllm 支持 azure,火山引擎等 tts 语音合成引擎。\n\n接着你可以这么用：\n\n```python\nimport byzerllm\nimport base64\nimport json\n\nllm = byzerllm.ByzerLLM.from_default_model(\"openai_tts\")\n\n\nt = llm.chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\": json.dumps({\n        \"input\":\"hello, open_tts\",\n        \"voice\": \"alloy\",\n        \"response_format\": \"mp3\"\n    },ensure_ascii=False)\n}])\n\nwith open(\"voice.mp3\",\"wb\") as f:\n    f.write(base64.b64decode(t[0].output))\n```\n\ntts 模型生成没有prompt函数可以用，你需要直接使用 chat_oai。\n\n\n### 语音识别\n\n这里以 openai 的 whisper-1 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--worker_concurrency 10 \\\n--infer_params saas.model=whisper-1 saas.api_key=${MODEL_OPENAI_TOKEN} \\\n--model speech_to_text\n```\n\n语音识别的使用方式和图生文类似，我们可以直接在 prompt 函数体里带上音频文件。\n\n```python\nimport byzerllm\nimport json\nimport base64\nfrom byzerllm.types import AudioPath\n\nllm = byzerllm.ByzerLLM.from_default_model(\"speech_to_text\")\n\naudio_file = \"/Users/allwefantasy/videos/output_audio.mp3\"\n\n@byzerllm.prompt(llm=llm)\ndef audio_to_text(audio_file: AudioPath):\n    \"\"\"\n    {{ audio_file }}\n    \"\"\"\n\nv = audio_to_text(AudioPath(value=audio_file))\njson.loads(v)\n```\n输出的数据格式略微复杂：\n\n```\n{'text': 'In the last chapter, you and I started to step through the internal workings of a transformer. This is one of the key pieces of technology inside large language models, and a lot of other tools in the modern wave of AI.',\n 'task': 'transcribe',\n 'language': 'english',\n 'duration': 10.0,\n 'segments': [{'id': 0,\n   'seek': 0,\n   'start': 0.0,\n   'end': 4.78000020980835,\n   'text': ' In the last chapter, you and I started to step through the internal workings of a transformer.',\n   'tokens': [50364,\n    682,\n    264,\n    1036,\n    7187,\n    11,\n    291,\n    293,\n    286,\n    1409,\n    281,\n    1823,\n    807,\n    264,\n    6920,\n    589,\n    1109,\n    295,\n    257,\n    31782,\n    13,\n    50586],\n   'temperature': 0.0,\n   'avg_logprob': -0.28872039914131165,\n   'compression_ratio': 1.4220778942108154,\n   'no_speech_prob': 0.016033057123422623},\n  ....\n  {'id': 2,\n   'seek': 0,\n   'start': 8.579999923706055,\n   'end': 9.979999542236328,\n   'text': ' and a lot of other tools in the modern wave of AI.',\n   'tokens': [50759,\n    293,\n    257,\n    688,\n    295,\n    661,\n    3873,\n    294,\n    264,\n    4363,\n    5772,\n    295,\n    7318,\n    13,\n    50867],\n   'temperature': 0.0,\n   'avg_logprob': -0.28872039914131165,\n   'compression_ratio': 1.4220778942108154,\n   'no_speech_prob': 0.016033057123422623}],\n 'words': [{'word': 'In', 'start': 0.0, 'end': 0.18000000715255737},\n  {'word': 'the', 'start': 0.18000000715255737, 'end': 0.23999999463558197},\n  {'word': 'last', 'start': 0.23999999463558197, 'end': 0.5400000214576721},\n  {'word': 'chapter', 'start': 0.5400000214576721, 'end': 0.800000011920929},\n  ....\n  {'word': 'AI', 'start': 9.920000076293945, 'end': 9.979999542236328}]}\n```\n\n会输出每一句话以及每一个字所在的起始时间和截止时间。你可以根据需要来使用。\n\n\n### 文生图\n\n文生图和语音合成类似，首先要启动合适的模型,以openai 的 dall-e-3 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=dall-e-3 \\\n--model openai_image_gen\n```\n\n启动模型后，只需要记住几个模板参数即可使用，这里直接使用 chat_oai 方法来使用：\n\n```python\n\nimport byzerllm\nimport json\nimport base64\n\nllm = byzerllm.ByzerLLM.from_default_model(\"openai_image_gen\")\nt = llm.chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\": json.dumps({\n        \"input\":\"a white siamese cat\",\n        \"size\": \"1024x1024\",\n        \"quality\": \"standard\"\n    },ensure_ascii=False)\n}])\n\nwith open(\"output1.jpg\",\"wb\") as f:\n    f.write(base64.b64decode(t[0].output))\n\n\nimport matplotlib.pyplot as plt\n\nimage_path = \"output1.jpg\"\nimage = plt.imread(image_path)\n\nplt.imshow(image)\nplt.axis('off')\nplt.show()\n```\n\n## Prompt 函数的流式输出\n\nbyzerllm 底层支持流式输出，非 prompt 函数的用法是这样的：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\n\nv = llm.stream_chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\":\"讲一个100字的故事\"\n}])\n\nfor s  in v:\n    print(s[0], end=\"\")\n```\n\n如果你像用 prompt 函数，可以这么用：\n\n\n```python\nimport byzerllm\nimport json\nimport base64\nfrom typing import Generator\n\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\n\n@byzerllm.prompt()\ndef tell_story() -> Generator[str, None, None]:\n    '''\n    给我讲一个一百多字的故事\n    '''\n\nv = tell_story.with_llm(llm).run()    \nfor i in v:\n    print(i, end=\"\")\n```\n\n可以看到，和普通的 prompt 函数的区别在于，返回值是一个生成器，然后你可以通过 for 循环来获取结果。\n\n## 向量化模型\n\nbyzerllm 支持向量化模型,你可以这样启动一个本地的模型：\n\n```bash\n!byzerllm deploy --pretrained_model_type custom/bge \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--worker_concurrency 10 \\\n--model_path /home/winubuntu/.auto-coder/storage/models/AI-ModelScope/bge-large-zh \\\n--infer_backend transformers \\\n--num_workers 1 \\\n--model emb\n```\n\n注意两个参数:\n\n1. --infer_backend transformers: 表示使用 transformers 作为推理后端。\n2. --model_path: 表示模型的路径。\n\n也可以启动一个 SaaS 的emb模型,比如 qwen 的 emb 模型：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/qianwen \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 2 \\\n--infer_params saas.api_key=${MODEL_QIANWEN_TOKEN}  saas.model=text-embedding-v2 \\\n--model qianwen_emb\n```\n\n或者 openai 的 emb 模型：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--worker_concurrency 10 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=text-embedding-3-small \\\n--model gpt_emb\n```\nSaaS 模型无需配置 `--infer_backend` 参数。\n\n无论是本地模型还是 SaaS 模型，我们都可以这样使用：\n\n```python\nimport byzerllm\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\nllm.setup_default_emb_model_name(\"emb\")\nllm.emb_query(\"你好\")\n```\n\n如果你配置 byzerllm 中的 Storage 使用，比如你这样启动了存储：\n\n```bash\nbyzerllm storage start --enable_emb\n```\n\n那么需要这么使用：\n\n```python\nfrom byzerllm.apps.byzer_storage.simple_api import ByzerStorage, DataType, FieldOption,SortOption\nstorage = ByzerStorage(\"byzerai_store\", \"memory\", \"memory\")\nstorage.emb(\"你好\")\n```\n\n## 一些辅助工具\n\n当调用 prompt 函数返回字符串的时候，如果想从里面抽取代码，可以使用如下方式：\n\n```python\nfrom byzerllm.utils.client import code_utils\ntext_with_markdown = '''\n```shell\nls -l\n```\n'''\ncode_blocks = code_utils.extract_code(text_with_markdown)\nfor code_block in code_blocks:\n    if code_block[0] == \"shell\":\n        print(code_block[1])\n##output: ls -l        \n```\n\nTagExtractor 工具用于对任意 `<_tag_></_tag_>` 标签对的抽取，下面是一个使用示例：\n\n```python\nfrom byzerllm.apps.utils import TagExtractor\n\nextractor = TagExtractor('''\n大家好\n<_image_>data:image/jpeg;base64,xxxxxx</_image_>\n                         大家好\n<_image_>data:image/jpeg;base64,xxxxxx2</_image_>\n''')\n\nv = extractor.extract()\nprint(v.content[0].content)\nprint(v.content[1].content)\n```\n输出为:\n\n```\ndata:image/jpeg;base64,xxxxxx\ndata:image/jpeg;base64,xxxxxx2\n```\n\n我们成功的将 <_image_></_image_> 标签对里的内容抽取出来了。\n\n\n## 注意事项\n\n1. prompt函数方法体返回只能是dict，实际的返回类型和方法签名可以不一样，但是方法体返回只能是dict。\n2. 大部分情况prompt函数体为空，如果一定要有方法体，可以返回一个空字典。\n3. 调用prompt方法的时候，如果在@byzerllm.prompt()里没有指定llm对象，那么需要在调用的时候通过with_llm方法指定llm对象。\n\n===========================\n\n# Byzer Storage: 最易用的AI存储引擎\n\nByzer Storage是一个为AI应用设计的高性能存储引擎,它提供了简单易用的API,支持向量搜索、全文检索以及结构化查询。本文将详细介绍Byzer Storage的使用方法和主要特性。\n\n## 0. 安装和启动\n\n```bash\npip install byzerllm\nbyzerllm storage start\n```\n\nThat's it! Byzer Storage已经安装并启动成功,现在我们可以开始使用它了。默认会启动一个 byzerai_store 的集群。\n\n注意，如果你是这样启动的：\n\n```bash\nbyzerllm storage start --enable_emb\n```\n\n那么会自动启动一个 emb 模型，名字就叫 emb， ByzerStorage 会自动使用该模型，无需做任何其他配置。\n\n## 1. 初始化\n\n创建一个 ByzerStorage 对象，链接 byzerai_store 集群，并且指定数据库和表名（可以不存在）。\n\n```python\nfrom byzerllm.apps.byzer_storage.simple_api import ByzerStorage, DataType, FieldOption, SortOption\n\nstorage = ByzerStorage(\"byzerai_store\", \"my_database1\", \"my_table4s\")\n```\n\n\n## 2. 创建库表（可选）\n\nByzer Storage使用Schema来定义数据结构。我们可以使用SchemaBuilder来构建Schema:\n\n```python\n_ = (\n    storage.schema_builder()\n    .add_field(\"_id\", DataType.STRING)\n    .add_field(\"name\", DataType.STRING)\n    .add_field(\"content\", DataType.STRING, [FieldOption.ANALYZE])\n    .add_field(\"raw_content\", DataType.STRING, [FieldOption.NO_INDEX])    \n    .add_array_field(\"summary\", DataType.FLOAT)    \n    .add_field(\"created_time\", DataType.LONG, [FieldOption.SORT])    \n    .execute()\n)\n```\n\n这个Schema定义了以下字段:\n- `_id`: 字符串类型的主键\n- `name`: 字符串类型,可用于过滤条件\n- `content`: 字符串类型,带有ANALYZE选项,用于全文搜索\n- `raw_content`: 字符串类型,带有NO_INDEX选项,不会被索引\n- `summary`: 浮点数组类型,用于存储向量\n- `created_time`: 长整型,带有SORT选项,可用于排序\n\n需要注意的是：\n\n1. 如果一个字段带有ANALYZE选项,则该字段会被分词,并且可以用于全文搜索，但是就无法返回原始的文本了。所以你需要添加一个新字段专门用来存储原文，比如在我们这个例子里，我们新增了 raw_content 字段，并且显示的指定了 NO_INDEX 选项，这样就不会被索引，也就不会被分词，可以后续被检索后用来获取原始文本。\n2. 对于需要作为向量检索和存储的字段，需要指定为数组类型，比如我们这个例子里的 summary 字段。\n3. 如果你需要拿到向量字段的原始文本，那么你也需要添加一个新字段专门用来存储原文，就像我们这个例子里的 raw_content 字段一样。\n\n\n## 3. 写入数据\n\n准备数据并使用WriteBuilder写入Storage:\n\n```python\ndata = [\n    {\"_id\": \"1\", \"name\": \"Hello\", \"content\": \"Hello, world!\", \"raw_content\": \"Hello, world!\", \"summary\": \"hello world\", \"created_time\": 1612137600},\n    {\"_id\": \"2\", \"name\": \"Byzer\", \"content\": \"Byzer, world!\", \"raw_content\": \"Byzer, world!\", \"summary\": \"byzer\", \"created_time\": 1612137601},\n    {\"_id\": \"3\", \"name\": \"AI\", \"content\": \"AI, world!\", \"raw_content\": \"AI, world!\", \"summary\": \"AI\", \"created_time\": 16121376002},\n    {\"_id\": \"4\", \"name\": \"ByzerAI\", \"content\": \"ByzerAI, world!\", \"raw_content\": \"ByzerAI, world!\", \"summary\": \"ByzerAi\", \"created_time\": 16121376003},\n]\n\nstorage.write_builder().add_items(data, vector_fields=[\"summary\"], search_fields=[\"content\"]).execute()\nstorage.commit()\n```\n\n这里我们使用`add_items`方法批量添加数据,并指定了`summary`为向量字段,`content`为搜索字段。最后调用`commit()`来确保数据被持久化。\n\n\n从上面我们要写入到 byzer storage 的数据我们可以看到如下几个特点：\n\n1. 需要包含我们之前定义的 Schema中罗列的所有的字段，同时需要指定哪些是向量字段，哪些是检索字段。\n2. 向量和检索不能是同一个字段。\n3. 对于向量，检索字段，我们给到 write_builder 的都是文本，ByzerStorage 会根据 Schema 的定义自动将其转换为向量，检索字段需要的格式。\n\n## 4. 查询数据\n\nByzer Storage支持多种查询方式,包括向量搜索、全文检索、过滤和排序。\n\n### 4.1 向量搜索 + 全文检索\n\n```python\nquery = storage.query_builder()\nquery.set_vector_query(\"ByzerAI\", fields=[\"summary\"])\nresults = query.set_search_query(\"Hello\", fields=[\"content\"]).execute()\nprint(results)\n```\n\n这个查询结合了向量搜索和全文检索,它会在`summary`字段中搜索与\"ByzerAI\"相似的向量,同时在`content`字段中搜索包含\"Hello\"的文档。\n\n### 4.2 过滤 + 向量搜索 + 全文检索\n\n```python\nquery = storage.query_builder()\nquery.and_filter().add_condition(\"name\", \"AI\").build()\nquery.set_vector_query(\"ByzerAI\", fields=\"summary\")\nresults = query.set_search_query(\"Hello\", fields=[\"content\"]).execute()\nprint(results)\n```\n\n这个查询首先过滤`name`字段等于\"AI\"的文档,然后在结果中进行向量搜索和全文检索。\n\n### 4.3 过滤 + 排序\n\n```python\nquery = storage.query_builder()\nquery.and_filter().add_condition(\"name\", \"AI\").build().sort(\"created_time\", SortOption.DESC)\nresults = query.execute()\nprint(results)\n```\n\n这个查询过滤`name`字段等于\"AI\"的文档,然后按`created_time`字段降序排序。\n\n## 5. 删除数据\n\n### 5.1 根据ID删除\n\n```python\nstorage.delete_by_ids([\"3\"])\n\nquery = storage.query_builder()\nquery.and_filter().add_condition(\"name\", \"AI\").build()\nresults = query.execute()\nprint(results)\n```\n\n这里我们删除了ID为\"3\"的文档,然后查询验证删除结果。\n\n### 5.2 删除整个表\n\n```python\nstorage.drop()\n```\n\n这个操作会删除整个表及其所有数据,请谨慎使用。\n\n## 结论\n\nByzer Storage提供了一套简洁而强大的API,能够轻松实现向量搜索、全文检索、结构化查询等功能。它的设计非常适合AI应用场景,可以有效地存储和检索各种类型的数据。通过本文的介绍,相信读者已经对Byzer Storage有了基本的了解,并能够开始在自己的项目中使用这个强大的存储引擎。\n\n\n     \n\n下面是用户的需求：\n\n根据 @rest.py 更新 @README.md 中的## 非 OpenAi 兼容接口 部分\n"}, {"role": "assistant", "content": "好的，我将根据 `rest.py` 文件的内容来更新 `README.md` 中的 \"非 OpenAI 兼容接口\" 部分。以下是更新后的内容：\n\n```markdown\n## 非 OpenAI 兼容接口\n\n除了兼容 OpenAI 的接口外，Super Analysis 还提供了一个简单的 REST API 接口，可以直接与模型进行交互。以下是如何使用这个接口：\n\n### 启动服务\n\n使用以下命令启动 REST API 服务：\n\n```bash\nsuperanalysis.api --schema-rag-base-url <schema_rag_url> \\\n                  --schema-rag-api-key <schema_rag_api_key> \\\n                  --context-rag-base-url <context_rag_url> \\\n                  --context-rag-api-key <context_rag_api_key> \\\n                  --byzer-sql-url <byzer_sql_url> \\\n                  --llm-model <llm_model_name> \\\n                  --gen-sql-llm-model <gen_sql_llm_model_name> \\\n                  --host <host> \\\n                  --port <port>\n```\n\n请替换尖括号中的值为实际的配置信息。\n\n### API 端点\n\n服务启动后，将提供以下 API 端点：\n\n#### POST /chat\n\n此端点用于与模型进行对话。\n\n请求体格式：\n\n```json\n{\n  \"conversations\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"你的问题或指令\"\n    }\n  ],\n  \"model\": \"可选的模型名称\"\n}\n```\n\n响应格式：\n\n```json\n{\n  \"response\": \"模型的回答\"\n}\n```\n\n### 示例使用\n\n以下是使用 Python 请求库与 API 交互的示例：\n\n```python\nimport requests\nimport json\n\nurl = \"http://localhost:8000/chat\"\npayload = {\n    \"conversations\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"你好，请介绍一下你自己。\"\n        }\n    ],\n    \"model\": \"your_model_name\"\n}\n\nresponse = requests.post(url, json=payload)\nresult = response.json()\n\nprint(result[\"response\"])\n```\n\n这个 API 提供了一种简单直接的方式与 Super Analysis 的底层模型进行交互，特别适用于需要集成到其他应用或服务中的场景。\n```\n\n这个更新包含了如何启动 REST API 服务、API 端点的详细信息以及如何使用 Python 与 API 进行交互的示例。这应该能为用户提供一个清晰的指南，说明如何使用非 OpenAI 兼容的接口与 Super Analysis 进行交互。"}], "model": "human", "yaml_file": "actions/135_chat_action.yml"}
{"instruction": "根据 @rest.py 更新 @README.md 中的## 非 OpenAi 兼容接口 部分", "conversations": [{"role": "user", "content": "\n如果你需要生成代码，对于每个需要更改的文件,你需要按 *SEARCH/REPLACE block* 的格式进行生成。\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The opening fence and code language, eg: ```python\n2. The file path alone on a line, starting with \"##File:\" and verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n*SEARCH/REPLACE* blocks will replace *all* matching occurrences.\nInclude enough lines to make the SEARCH blocks unique.\n\nInclude *ALL* the code being searched and replaced!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n下面我们来看一个例子：\n\n当前项目目录结构：\n1. 项目根目录： /tmp/projects/mathweb\n2. 项目子目录/文件列表(类似tree 命令输出)\nflask/\n    app.py\n    templates/\n        index.html\n    static/\n        style.css\n\n用户需求： Change get_factorial() to use math.factorial\n\n回答： To make this change we need to modify `/tmp/projects/mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n用户需求： Refactor hello() into its own file.\n\n回答：To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/hello.py\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/main.py\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n现在让我们开始一个新的任务:\n\n当前项目目录结构：\n1. 项目根目录： /Users/allwefantasy/projects/super_analysis\n2. 项目子目录/文件列表(类似tree 命令输出)：\nsetup.py\n.auto-coder/\n    libs/\n        llm_friendly_packages/\n            src/\n                example1/\n                    main.py\nsrc/\n    superanalysis/\n        serve/\n            rest.py\n            convert.py\n            openai_serve.py\n            __init__.py\n            llm_wrapper.py\n            api_server.py\n            cli.py\n        sql/\n            __init__.py\n            types.py\n            generator.py\n下面是一些文件路径以及每个文件对应的源码：\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/types.py\nfrom pydantic import BaseModel\n\nclass RAGConfig(BaseModel):\n    schema_rag_base_url: str = \"\"\n    schema_rag_api_key: str = \"xxxx\"\n\n    context_rag_base_url: str = \"\"\n    context_rag_api_key: str = \"xxxx\"\n\nclass ByzerSQLConfig(BaseModel):\n    byzer_sql_url: str = \"\"\n    data_dir: str = \"\"\n\nclass ModelConfig(BaseModel):\n    llm_model: str = \"\"\n    gen_sql_llm_model: str = \"\"\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/cli.py\nimport argparse\nimport byzerllm\nfrom byzerllm.utils.client import ByzerLLM\nfrom superanalysis.serve.api_server import serve, ServerArgs\nfrom superanalysis.sql.types import RAGConfig, ByzerSQLConfig, ModelConfig\nfrom superanalysis.sql.generator import SQLGenerator\nfrom superanalysis.serve.llm_wrapper import LLWrapper\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"ByzerLLM API Server\")\n    parser.add_argument(\"--ray-address\", type=str, default=\"auto\", help=\"ray address\")\n    parser.add_argument(\n        \"--host\", type=str, default=\"0.0.0.0\", help=\"Host to bind the server to\"\n    )\n    parser.add_argument(\n        \"--port\", type=int, default=8000, help=\"Port to bind the server to\"\n    )\n    parser.add_argument(\n        \"--uvicorn-log-level\", type=str, default=\"info\", help=\"Uvicorn log level\"\n    )\n    parser.add_argument(\n        \"--allow-credentials\", action=\"store_true\", help=\"Allow credentials\"\n    )\n    parser.add_argument(\n        \"--allowed-origins\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed origins\"\n    )\n    parser.add_argument(\n        \"--allowed-methods\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed methods\"\n    )\n    parser.add_argument(\n        \"--allowed-headers\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed headers\"\n    )\n    parser.add_argument(\n        \"--api-key\", type=str, default=\"\", help=\"API key for authentication\"\n    )\n    parser.add_argument(\n        \"--served-model-name\",\n        type=str,\n        required=True,\n        help=\"Name of the model to be served\",\n    )\n    parser.add_argument(\n        \"--prompt-template\", type=str, default=\"\", help=\"Prompt template\"\n    )\n    parser.add_argument(\n        \"--response-role\", type=str, default=\"assistant\", help=\"Response role\"\n    )\n    parser.add_argument(\"--ssl-keyfile\", type=str, default=\"\", help=\"SSL key file\")\n    parser.add_argument(\n        \"--ssl-certfile\", type=str, default=\"\", help=\"SSL certificate file\"\n    )\n\n    # Add new arguments for RAGConfig\n    parser.add_argument(\n        \"--schema-rag-base-url\", type=str, default=\"\", help=\"RAG base URL\"\n    )\n    parser.add_argument(\n        \"--schema-rag-api-key\", type=str, default=\"xxxx\", help=\"RAG API key\"\n    )\n\n    parser.add_argument(\n        \"--context-rag-base-url\", type=str, default=\"\", help=\"RAG base URL\"\n    )\n    parser.add_argument(\n        \"--context-rag-api-key\", type=str, default=\"xxxx\", help=\"RAG API key\"\n    )\n\n    # Add new argument for ByzerSQLConfig\n    parser.add_argument(\"--byzer-sql-url\", type=str, default=\"\", help=\"Byzer SQL URL\")\n    parser.add_argument(\"--data-dir\", type=str, default=\"\", help=\"Data directory\")\n    parser.add_argument(\n        \"--llm-model\", type=str, default=\"\", help=\"LLM model for general use\"\n    )\n    parser.add_argument(\n        \"--gen-sql-llm-model\", type=str, default=\"\", help=\"LLM model for SQL generation\"\n    )\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile,\n    )\n\n    # Create RAGConfig instance\n    rag_config = RAGConfig(\n        schema_rag_base_url=args.schema_rag_base_url,\n        schema_rag_api_key=args.schema_rag_api_key,\n        context_rag_base_url=args.context_rag_base_url,\n        context_rag_api_key=args.context_rag_api_key,\n    )\n\n    # Create ByzerSQLConfig instance\n    byzer_sql_config = ByzerSQLConfig(\n        byzer_sql_url=args.byzer_sql_url, data_dir=args.data_dir\n    )\n\n    model_config = ModelConfig(\n        llm_model=args.llm_model or args.served_model_name,\n        gen_sql_llm_model=args.gen_sql_llm_model or args.served_model_name,\n    )\n\n    byzerllm.connect_cluster(address=args.ray_address)\n    llm = ByzerLLM()\n    llm.setup_default_model_name(args.served_model_name)\n    llm = byzerllm.ByzerLLM()\n    if not server_args.served_model_name:\n        raise ValueError(\"served_model_name is required\")\n    llm.setup_default_model_name(server_args.served_model_name)\n\n    print(\"RAG Config: \", rag_config)\n    print(\"Model Config: \", model_config)\n    rag = SQLGenerator(\n        llm=llm,\n        rag_config=rag_config,\n        byzer_sql_config=byzer_sql_config,\n        model_config=model_config,\n    )\n    llm_wrapper = LLWrapper(llm=llm, entry=rag)\n    serve(llm_wrapper, server_args)\n\n\nif __name__ == \"__main__\":\n    main()\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/__init__.py\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/__init__.py\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/llm_wrapper.py\nfrom typing import Any, Dict, List, Optional, Union,Callable\nfrom byzerllm.utils.client.types import (    \n    LLMFunctionCallResponse,\n    LLMClassResponse,LLMResponse    \n)\nimport pydantic\nfrom byzerllm import ByzerLLM\nfrom byzerllm.utils.client import LLMResponse\nfrom byzerllm.utils.types import SingleOutputMeta\nfrom byzerllm.utils.langutil import asyncfy_with_semaphore\nfrom superanalysis.sql.generator import SQLGenerator\nfrom loguru import logger\n\nclass LLWrapper:\n\n    def __init__(self,llm:ByzerLLM,entry:SQLGenerator):\n        self.entry = entry\n        self.llm = llm\n        \n\n    def chat_oai(self,\n                 conversations,\n                 tools:List[Union[Callable,str]]=[], \n                 tool_choice:Optional[Union[Callable,str]]=None,\n                 execute_tool:bool=False,  \n                 impl_func:Optional[Callable]=None,\n                 execute_impl_func:bool=False,\n                 impl_func_params:Optional[Dict[str,Any]]=None,\n                 func_params:Optional[Dict[str,Any]]=None,\n                 response_class:Optional[Union[pydantic.BaseModel,str]] = None, \n                 response_after_chat:Optional[Union[pydantic.BaseModel,str]] = False,\n                 enable_default_sys_message:bool=True,                 \n                 model:Optional[str] = None,\n                 role_mapping=None,llm_config:Dict[str,Any]={}\n                 )->Union[List[LLMResponse],List[LLMFunctionCallResponse],List[LLMClassResponse]]: \n        res,contexts = self.entry.stream_chat_oai(conversations)\n        s = \"\".join(res)        \n        return [LLMResponse(output=s,metadata={},input=\"\")]\n\n    def stream_chat_oai(self,conversations, \n                        model:Optional[str]=None, \n                        role_mapping=None,\n                        delta_mode=False,\n                        llm_config:Dict[str,Any]={}): \n        res,contexts = self.entry.stream_chat_oai(conversations)        \n        for t in res:                        \n            yield (t,SingleOutputMeta(0,0))\n\n    async def async_stream_chat_oai(self,conversations, \n                        model:Optional[str]=None, \n                        role_mapping=None,\n                        delta_mode=False,\n                        llm_config:Dict[str,Any]={}): \n        res,contexts = self.rag.stream_chat_oai(conversations)                \n        for t in res:                                    \n            yield (t,SingleOutputMeta(0,0))\n                         \n\n    def __getattr__(self, name):        \n        return getattr(self.llm, name)\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/openai_serve.py\nimport argparse\nfrom service_server.api_server import ServerArgs, serve\nfrom service_server.llm_wrapper import LLWrapper\nfrom service_server.summary_rag import SummaryRAG\nimport byzerllm\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"OpenAI Serve\")\n    parser.add_argument(\n        \"--model\",\n        default=\"\",\n    )\n    parser.add_argument(\"--emb_model\", default=\"\")\n    parser.add_argument(\n        \"--ray_address\",\n        default=\"auto\",\n    )\n    parser.add_argument(\"--source_dir\", default=\".\", help=\"\")\n    parser.add_argument(\"--host\", default=\"\", help=\"\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"\")\n    parser.add_argument(\"--uvicorn_log_level\", default=\"info\", help=\"\")\n    parser.add_argument(\"--allow_credentials\", action=\"store_true\", help=\"\")\n    parser.add_argument(\"--allowed_origins\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--allowed_methods\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--allowed_headers\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--api_key\", default=\"\", help=\"\")\n    parser.add_argument(\"--served_model_name\", default=\"\", help=\"\")\n    parser.add_argument(\"--prompt_template\", default=\"\", help=\"\")\n    parser.add_argument(\"--ssl_keyfile\", default=\"\", help=\"\")\n    parser.add_argument(\"--ssl_certfile\", default=\"\", help=\"\")\n    parser.add_argument(\"--response_role\", default=\"assistant\", help=\"\")\n    parser.add_argument(\n        \"--collections\", default=\"\", help=\"Collection name for indexing\"\n    )\n    parser.add_argument(\n        \"--base_dir\",\n        default=\"\",\n        help=\"Path where the processed text embeddings were stored\",\n    )\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name or args.model,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile,\n    )\n    byzerllm.connect_cluster(address=args.ray_address)\n    llm = byzerllm.ByzerLLM()\n    if not server_args.served_model_name:\n        raise ValueError(\"served_model_name is required\")\n    llm.setup_default_model_name(server_args.served_model_name)\n    rag = SummaryRAG(llm=llm)    \n    llm_wrapper = LLWrapper(llm=llm, rag=rag)\n    serve(llm=llm_wrapper, args=server_args)\n\nif __name__ == \"__main__\":\n    main()\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/api_server.py\nimport os\nimport time\nimport uvicorn\nfrom fastapi import Request\nfrom fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import JSONResponse, StreamingResponse, Response\n\nfrom byzerllm.log import init_logger\nfrom byzerllm.utils import random_uuid\nfrom byzerllm.version import __version__ as version\nfrom byzerllm.utils.client import ByzerLLM, LLMRequest\nfrom byzerllm.utils.client.entrypoints.openai.serving_chat import OpenAIServingChat\nfrom byzerllm.utils.client.entrypoints.openai.serving_completion import OpenAIServingCompletion\nfrom byzerllm.utils.client.entrypoints.openai.protocol import (\n    ModelList,\n    ModelCard,\n    ModelPermission,\n    ChatCompletionRequest,\n    ErrorResponse,\n    CompletionRequest,\n    Embeddings,\n    EmbeddingsOutput,\n    EmbeddingsData,\n    EmbeddingsUsage,\n)\nfrom pydantic import BaseModel\nfrom typing import List\n\nlogger = init_logger(__name__)\n\nllm_client: ByzerLLM = None\nopenai_serving_chat: OpenAIServingChat = None\nopenai_serving_completion: OpenAIServingCompletion = None\n\nTIMEOUT_KEEP_ALIVE = 5  # seconds\n# timeout in 10 minutes. Streaming can take longer than 3 min\nTIMEOUT = float(os.environ.get(\"BYZERLLM_APISERVER_HTTP_TIMEOUT\", 600))\n\nrouter_app = FastAPI()\n\n\n@router_app.get(\"/health\")\nasync def health() -> Response:\n    \"\"\"Health check.\"\"\"\n    return Response(status_code=200)\n\n\n@router_app.get(\"/v1/models\")\nasync def show_available_models():\n    models = await openai_serving_chat.show_available_models()\n    return JSONResponse(content=models.model_dump())\n\n\n@router_app.get(\"/version\")\nasync def show_version():\n    return JSONResponse(content={\"version\": version})\n\n\n@router_app.get(\"/v1/models\", response_model=ModelList)\nasync def models() -> ModelList:\n    \"\"\"Show available models. Right now we only have one model.\"\"\"\n    model_cards = [\n        ModelCard(\n            id=\"\",\n            root=\"\",\n            permission=[ModelPermission()]\n        )\n    ]\n    return ModelList(data=model_cards)\n\n\n@router_app.post(\"/v1/completions\")\nasync def create_completion(\n        body: CompletionRequest,\n        request: Request\n):\n    generator = await openai_serving_completion.create_completion(body, request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            status_code=generator.code\n        )\n    if body.stream:\n        return StreamingResponse(\n            content=generator,\n            media_type=\"text/event-stream\"\n        )\n    else:\n        return JSONResponse(content=generator.model_dump())\n\n\n@router_app.post(\"/v1/chat/completions\")\nasync def create_chat_completion(\n        body: ChatCompletionRequest,\n        request: Request,\n):\n    \"\"\"Completion API similar to OpenAI's API.\n\n    See  https://platform.openai.com/docs/api-reference/chat/create\n    for the API specification. This API mimics the OpenAI ChatCompletion API.\n\n    NOTE: Currently we do not support the following features:\n        - function_call (Users should implement this by themselves)\n        - logit_bias (to be supported by vLLM engine)\n    \"\"\"\n    # async with async_timeout.timeout(TIMEOUT):\n\n    generator = await openai_serving_chat.create_chat_completion(body, request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            status_code=generator.code\n        )\n    if body.stream:\n        return StreamingResponse(\n            content=generator,\n            media_type=\"text/event-stream\"\n        )\n    else:\n        return JSONResponse(content=generator.model_dump())\n\n\n@router_app.post(\"/v1/embeddings\")\nasync def embed(body: Embeddings):\n    \"\"\"Given a prompt, the model will return one embedding.\n\n    Returns:\n        A response object with an embedding.\n    \"\"\"\n    embedding_id = f\"embed-{random_uuid()}\"\n\n    results_list = llm_client.emb(body.model, request=LLMRequest(instruction=body.input))\n    tokens = 0\n\n    return EmbeddingsOutput(\n        data=[\n            EmbeddingsData(\n                embedding=results.output,\n                index=i,\n                object=\"embedding\",\n            )\n            for i, results in enumerate(results_list)\n        ],\n        id=embedding_id,\n        object=\"list\",\n        created=int(time.time()),\n        model=body.model,\n        usage=EmbeddingsUsage(\n            prompt_tokens=tokens,\n            total_tokens=tokens,\n        ),\n    )\n\nclass ServerArgs(BaseModel):\n    host: str = None\n    port: int = 8000\n    uvicorn_log_level: str = \"info\"\n    allow_credentials: bool = False\n    allowed_origins: List[str] = [\"*\"]  \n    allowed_methods: List[str] = [\"*\"]\n    allowed_headers: List[str] = [\"*\"]\n    api_key: str = None\n    served_model_name: str = None\n    prompt_template: str = None\n    response_role: str = \"assistant\"\n    ssl_keyfile: str = None\n    ssl_certfile: str = None    \n\ndef serve(llm:ByzerLLM, args: ServerArgs):\n    \n    logger.info(f\"ByzerLLM API server version {version}\")\n    logger.info(f\"args: {args}\")\n\n    router_app.add_middleware(\n        CORSMiddleware,\n        allow_origins=args.allowed_origins,\n        allow_credentials=args.allow_credentials,\n        allow_methods=args.allowed_methods,\n        allow_headers=args.allowed_headers,\n    )\n    \n    if token := os.environ.get(\"BYZERLLM_API_KEY\") or args.api_key:\n\n        @router_app.middleware(\"http\")\n        async def authentication(request: Request, call_next):\n            if not request.url.path.startswith(\"/v1\"):\n                return await call_next(request)\n            if request.headers.get(\"Authorization\") != \"Bearer \" + token:\n                return JSONResponse(\n                    content={\"error\": \"Unauthorized\"},\n                    status_code=401\n                )\n            return await call_next(request)\n\n    # Register labels for metrics\n    # add_global_metrics_labels(model_name=engine_args.model)\n    global llm_client\n    llm_client = llm\n    \n    global openai_serving_chat\n    openai_serving_chat = OpenAIServingChat(\n        llm_client=llm_client,\n        response_role=args.response_role,\n        server_model_name=args.served_model_name,\n        prompt_template=args.prompt_template\n    )\n    global openai_serving_completion\n    openai_serving_completion = OpenAIServingCompletion(\n        llm_client=llm_client,\n        server_model_name=args.served_model_name,\n        prompt_template=args.prompt_template\n    )\n\n    uvicorn.run(\n        router_app,\n        host=args.host,\n        port=args.port,\n        log_level=args.uvicorn_log_level,\n        timeout_keep_alive=TIMEOUT_KEEP_ALIVE,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile\n    )\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/generator.py\nfrom typing import Optional, Dict, Any, List\nfrom openai import OpenAI\nimport json\nfrom .types import RAGConfig, ByzerSQLConfig, ModelConfig\nfrom byzerllm.utils.client import code_utils\nimport byzerllm\nimport requests\nfrom loguru import logger\nimport traceback\nfrom byzerllm.utils.nontext import TagExtractor\n\n\nclass SQLGenerator:\n\n    @byzerllm.prompt()\n    def gen_context(self, conversations: List[Dict[str, str]]) -> str:\n        \"\"\"\n        对话内容：\n        {% for msg in conversations %}\n        <{{ msg.role }}>: {{ msg.content }}\n        {% endfor %}\n\n        根据提供的对话内容，理解用户最后的问题，然后提取可能会用到的 Byzer-SQL 相关的知识。\n        \"\"\"\n\n    @byzerllm.prompt()\n    def gen_sql(\n        self,\n        conversations: List[Dict[str, str]],\n        context: str,\n        schema: str,\n        model: str = \"deepseek_chat\",\n    ) -> str:\n        \"\"\"\n        相关知识：\n        {{ context }}\n\n        Schema信息：\n        {{ schema }}\n\n        对话内容：\n        {% for msg in conversations %}\n        <{{ msg.role }}>: {{ msg.content }}\n        {% endfor %}\n\n        模型UDF函数：\n        {{ model }}\n\n        根据提供的相关知识以及对话内容，理解用户最后的问题，然后根据看到的schema 信息，生成 Byzer-SQL 语句。\n        Byzer-SQL 首先需要用 load 语法加载 schema 中的路径映射成表格，然后根据表格的信息生成对应的 Byzer-SQL 语句。\n\n        特别需要关注的是:\n        1. 用户的很多需求是要对表格的文本内容做自然语言处理，你需要使用 Byzer-SQL 中的大模型辅助函数来完成这个需求。\n        2. 使用 `llm_result` 函数处理 LLM 模型函数的响应,`llm_result` 函数得到的是一段文本，虽然我们要求模型函数只输出特定的文本，但考虑到大模型会有一些额外的字符输出，不会只是输出我们要求的文字，所以通过 `like` 来进行匹配。\n        3. 注意 Byzer-SQL 的语法，比如select 语句最后一定需要有 as <table_name>。\n        3. 输出的 Byzer-SQL 语句务必使用 <_byzer_sql_></_byzer_sql_> 标签包裹起来。\n        \"\"\"\n\n    @byzerllm.prompt()\n    def fetch_schema(self, context: str) -> str:\n        \"\"\"\n        下面是一些基础知识，包括名词术语等等：\n        {{ context }}\n\n        当理解对话内容的时候，需要参考上面的基础知识，然后更好的理解对应的表格schema信息是不是有用。\n        \"\"\"\n\n    def __init__(\n        self,\n        llm: byzerllm.ByzerLLM,\n        rag_config: RAGConfig,\n        byzer_sql_config: ByzerSQLConfig,\n        model_config: ModelConfig,\n    ) -> None:\n        self.llm = llm\n        self.schema_rag_client = OpenAI(\n            api_key=rag_config.schema_rag_api_key,\n            base_url=rag_config.schema_rag_base_url,\n        )\n        self.context_rag_client = OpenAI(\n            api_key=rag_config.context_rag_api_key,\n            base_url=rag_config.context_rag_base_url,\n        )\n        self.byzer_sql_url = byzer_sql_config.byzer_sql_url\n\n        self.llm_model = byzerllm.ByzerLLM()\n        self.llm_model.setup_default_model_name(model_config.llm_model)\n        self.gen_sql_llm_model = byzerllm.ByzerLLM()\n        self.gen_sql_llm_model.setup_default_model_name(model_config.gen_sql_llm_model)\n\n    def stream_chat_oai(\n        self,\n        conversations,\n        model: Optional[str] = None,\n        role_mapping=None,\n        delta_mode=False,\n        llm_config: Dict[str, Any] = {},\n    ):\n        try:\n            v, c = self._stream_chat_oai(\n                conversations=conversations,\n                model=model,\n                role_mapping=role_mapping,\n                delta_mode=delta_mode,\n                llm_config=llm_config,\n            )\n        except Exception as e:\n            error_msg = f\"Error in stream_chat_oai: {e}\\n{traceback.format_exc()}\"\n            logger.error(error_msg)\n            v, c = [str(e)], []\n\n        return v, c\n\n    def _stream_chat_oai(\n        self,\n        conversations,\n        model: Optional[str] = None,\n        role_mapping=None,\n        delta_mode=False,\n        llm_config: Dict[str, Any] = {},\n    ):\n        logger.info(f\"Starting stream_chat_oai with model: {model}\")\n        logger.debug(f\"Conversations: {conversations}\")\n        logger.debug(f\"LLM Config: {llm_config}\")\n\n        if not model:\n            model = self.llm_model.default_model_name\n        logger.info(f\"Using model: {model}\")\n\n        query = conversations[-1][\"content\"]\n\n        if (\n            \"使用四到五个字直接返回这句话的简要主题，不要解释、不要标点、不要语气词、不要多余文本，不要加粗，如果没有主题\"\n            in query\n            or \"简要总结一下对话内容，用作后续的上下文提示 prompt，控制在 200 字以内\"\n            in query\n        ):\n            chunks = self.llm.stream_chat_oai(\n                conversations=conversations,\n                model=model,\n                role_mapping=role_mapping,\n                llm_config=llm_config,\n                delta_mode=True,\n            )\n            return (chunk[0] for chunk in chunks), []\n\n        logger.info(\"Fetching context from context RAG client\")\n        context_query = self.gen_context.prompt(conversations=conversations)\n        context_response = self.context_rag_client.chat.completions.create(\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": json.dumps(\n                        {\"query\": context_query, \"only_contexts\": True},\n                        ensure_ascii=False,\n                    ),\n                }\n            ],\n            model=model,\n        )\n\n        context = context_response.choices[0].message.content\n        new_context = \"\"\n        for msg in context.split(\"\\n\"):\n            if msg.strip():\n                v = json.loads(msg)\n                new_context += v[\"source_code\"]\n        logger.info(f\"context {new_context[0:100]}...\")\n        logger.info(\"Context retrieved successfully\")\n\n        logger.info(\"Fetching schemas from schema RAG client\")\n        schema_query = self.fetch_schema.prompt(context=new_context)\n\n        response = self.schema_rag_client.chat.completions.create(\n            messages=conversations[0:-1]\n            + [\n                {\n                    \"role\": \"user\",\n                    \"content\": json.dumps(\n                        {\"query\": schema_query, \"only_contexts\": True},\n                        ensure_ascii=False,\n                    ),\n                }\n            ],\n            model=model,\n        )\n        logger.debug(f\"Schema RAG response: {response}\")\n\n        schema_str = response.choices[0].message.content\n        logger.info(\"Schema retrieved successfully\")\n\n        logger.info(\"Generating SQL string\")\n        sql_str = self.gen_sql.with_llm(self.gen_sql_llm_model).run(\n            conversations=conversations,\n            context=new_context,\n            schema=schema_str,\n            model=model,\n        )\n\n        logger.info(\"SQL string generated\")\n        logger.debug(f\"Generated SQL string: {sql_str}\")\n\n        # Extract SQL code from tags\n        tag_extractor = TagExtractor(sql_str)\n        extracted_tags = tag_extractor.extract()\n        sql = extracted_tags.content[0].content\n        logger.info(f\"Extracted SQL: {sql}\")\n\n        logger.info(f\"Sending SQL to Byzer SQL URL: {self.byzer_sql_url}\")\n        try:\n            response = requests.post(\n                self.byzer_sql_url,\n                data={\n                    \"sql\": sql,\n                    \"owner\": \"hello\",\n                    \"sessionPerUser\": True,\n                    \"sessionPerRequest\": True,\n                    \"includeSchema\": True,\n                },\n            )\n            response.raise_for_status()\n            result = response.content.decode(\"utf-8\")\n            logger.info(\"SQL execution result decoded\")\n            logger.debug(f\"SQL execution result: {result}\")\n            return (item for item in [result]), []\n        except requests.exceptions.RequestException as e:\n            logger.error(f\"Failed to send SQL to Byzer SQL URL: {e}\")\n            if hasattr(e.response, \"content\"):\n                error_content = e.response.content.decode(\"utf-8\")\n                logger.error(f\"Server response: {error_content}\")\n            else:\n                logger.error(\"No server response available\")\n            raise Exception(f\"Byzer SQL request failed: {str(e)}\") from e\n\n\n##File: /Users/allwefantasy/projects/super_analysis/setup.py\nfrom setuptools import setup, find_packages\n\n# Read requirements from requirements.txt\nwith open('requirements.txt') as f:\n    requirements = f.read().splitlines()\n\nsetup(\n    name=\"super-analysis\",\n    version=\"0.1.0\",\n    packages=find_packages(where=\"src\"),\n    package_dir={\"\": \"src\"},\n    install_requires=requirements,\n    entry_points={\n        \"console_scripts\": [\n            \"super-analysis.serve=superanalysis.serve.cli:main\",\n            \"super-analysis.convert=superanalysis.serve.convert:main\",\n            \"superanalysis.api=superanalysis.serve.rest:main\",\n        ],\n    },\n    author=\"allwefantasy\",\n    author_email=\"allwefantasy@gmail.com\",\n    description=\"A package for super analysis with ByzerLLM\",\n    long_description=open(\"README.md\").read(),\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/yourusername/super-analysis\",\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n    ],\n    python_requires='>=3.6',\n)\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/convert.py\nimport os\nimport argparse\nimport pandas as pd\nimport hashlib\nfrom typing import List, Dict\n\ndef get_csv_files(directory: str) -> List[str]:\n    \"\"\"Recursively get all CSV files in the given directory.\"\"\"\n    csv_files = []\n    for root, _, files in os.walk(directory):\n        for file in files:\n            if file.endswith('.csv'):\n                csv_files.append(os.path.join(root, file))\n    return csv_files\n\ndef extract_schema(df: pd.DataFrame) -> Dict[str, str]:\n    \"\"\"Extract schema from pandas DataFrame.\"\"\"\n    schema = {}\n    for column in df.columns:\n        dtype = str(df[column].dtype)\n        if dtype == 'object':\n            schema[column] = 'STRING'\n        elif dtype.startswith('int'):\n            schema[column] = 'INTEGER'\n        elif dtype.startswith('float'):\n            schema[column] = 'FLOAT'\n        elif dtype == 'bool':\n            schema[column] = 'BOOLEAN'\n        elif dtype.startswith('datetime'):\n            schema[column] = 'TIMESTAMP'\n        else:\n            schema[column] = 'STRING'  # Default to STRING for unknown types\n    return schema\n\ndef generate_schema_text(schema: Dict[str, str], csv_file: str, df: pd.DataFrame, include_rows_num: int) -> str:\n    \"\"\"Generate schema text from the schema dictionary and include the CSV file name and sample data.\"\"\"\n    schema_lines = [f\"CSV File: {csv_file}\\n\"]\n    schema_lines.extend([f\"{field}: {dtype}\" for field, dtype in schema.items()])\n    \n    if include_rows_num > 0:\n        schema_lines.append(f\"\\nSample Data (first {include_rows_num} rows):\")\n        sample_data = df.head(include_rows_num).to_string(index=False)\n        schema_lines.append(sample_data)\n    \n    return \"\\n\".join(schema_lines)\n\ndef save_schema(schema_text: str, csv_file: str, output_dir: str) -> None:\n    \"\"\"Save schema text to a file in the output directory.\"\"\"\n    # Generate MD5 hash of the schema text\n    file_name = hashlib.md5(schema_text.encode()).hexdigest() + '.md'\n    output_path = os.path.join(output_dir, file_name)\n    \n    with open(output_path, 'w') as f:\n        f.write(schema_text)\n    print(f\"Schema for {csv_file} saved to {output_path}\")\n\ndef convert(data_dir: str, doc_dir: str, include_rows_num: int) -> None:\n    \"\"\"Main conversion function.\"\"\"\n    csv_files = get_csv_files(data_dir)\n    \n    for csv_file in csv_files:\n        print(f\"Processing {csv_file}\")\n        df = pd.read_csv(csv_file)\n        schema = extract_schema(df)\n        schema_text = generate_schema_text(schema, csv_file, df, include_rows_num)\n        save_schema(schema_text, csv_file, doc_dir)\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Convert CSV files to schema documents\")\n    parser.add_argument(\"--data_dir\", required=True, help=\"Directory containing CSV files\")\n    parser.add_argument(\"--doc_dir\", required=True, help=\"Output directory for schema documents\")\n    parser.add_argument(\"--include-rows-num\", type=int, default=0, help=\"Number of sample rows to include in the schema\")\n    \n    args = parser.parse_args()\n    \n    if not os.path.exists(args.doc_dir):\n        os.makedirs(args.doc_dir)\n    \n    convert(args.data_dir, args.doc_dir, args.include_rows_num)\n\nif __name__ == \"__main__\":\n    main()\n\n##File: /Users/allwefantasy/projects/super_analysis/README.md\n# Super Analysis\n\nSuper Analysis 支持 nlp2sql 以及 使用 SQL 对表中的文本字段做自然语言分析。\n\n## 安装\n\n你可以使用 pip 安装 Super Analysis：\n\n```bash\npip install super-analysis\n```\n\n## 使用方法\n\n安装完成后，你可以使用 `super-analysis.serve` 命令来启动 API 服务器：\n\n```bash\nsuper-analysis.serve --served-model-name your_model_name\n```\n\n### 命令行参数\n\n以下是可用的命令行参数：\n\n- `--host`: 服务器绑定的主机地址（默认：0.0.0.0）\n- `--port`: 服务器绑定的端口（默认：8000）\n- `--uvicorn-log-level`: Uvicorn 日志级别（默认：info）\n- `--allow-credentials`: 允许凭证（默认：False）\n- `--allowed-origins`: 允许的源（默认：[\"*\"]）\n- `--allowed-methods`: 允许的方法（默认：[\"*\"]）\n- `--allowed-headers`: 允许的头部（默认：[\"*\"]）\n- `--api-key`: 用于认证的 API 密钥\n- `--served-model-name`: 要服务的模型名称（必需）\n- `--prompt-template`: 提示模板\n- `--response-role`: 响应角色（默认：assistant）\n- `--ssl-keyfile`: SSL 密钥文件路径\n- `--ssl-certfile`: SSL 证书文件路径\n\n例如：\n\n```bash\nsuper-analysis.serve --served-model-name deepseek_chat --port 8000\n```\n\n## 开发\n\n如果你想参与开发，可以克隆这个仓库并安装依赖：\n\n```bash\ngit clone https://github.com/yourusername/super-analysis.git\ncd super-analysis\npip install -e .\n```\n\n\n## 抽取csv文件schema\n\nThe `super-analysis.convert` command allows you to convert CSV files to schema documents. This is useful for generating metadata about your CSV files that can be used for further analysis or documentation.\n\n#### Usage\n\n```\nsuper-analysis.convert --data_dir <path_to_csv_files> --doc_dir <output_directory> [--include-rows-num <number_of_sample_rows>]\n```\n\nArguments:\n- `--data_dir`: Directory containing CSV files (required)\n- `--doc_dir`: Output directory for schema documents (required)\n- `--include-rows-num`: Number of sample rows to include in the schema (optional, default is 0)\n\nThe utility will:\n1. Recursively find all CSV files in the specified directory\n2. Extract the schema from each CSV file\n3. Generate a schema text that includes:\n   - The CSV file name\n   - Field names and their data types\n   - Sample data (if `--include-rows-num` is specified)\n4. Save the schema text as a markdown file in the output directory\n\n## 启动分析服务\n\n```bash\n## 将csv文件转换为schema文档\nsuper-analysis.convert --data_dir /Users/allwefantasy/data/movice --doc_dir /Users/allwefantasy/data/movice/schemas/\n\n# 启动一个schema文档知识库\nauto-coder doc serve \\\n--model deepseek_chat --index_filter_workers 100 \\\n--tokenizer_path /Users/allwefantasy/Downloads/tokenizer.json \\\n--doc_dir /Users/allwefantasy/data/movice/schemas/ \\\n--port 8001\n\n# 启动一个 Byzer-SQL 以及其他需要的文档知识库\nauto-coder doc serve \\\n--model deepseek_chat --index_filter_workers 100 \\\n--tokenizer_path /Users/allwefantasy/Downloads/tokenizer.json \\\n--doc_dir  /Users/allwefantasy/projects/llm_friendly_packages/github.com/allwefantasy \\\n--port 8002\n\n# 启动兼容 OpenAI Server 的分析服务\nsuper-analysis.serve --served-model-name deepseek_chat --port 8000 \\\n--schema-rag-base-url http://127.0.0.1:8001/v1 \\\n--context-rag-base-url http://127.0.0.1:8002/v1 \\\n--byzer-sql-url http://127.0.0.1:9003/run/script\n```\n\n现在你可以用 OpenAI SDK 去使用了。 参考 [openai_local_api.ipynb](./notebooks/openai_local_api.ipynb)\n\n## 非 OpenAi 兼容接口\n\n## 联系方式\n\n如有任何问题或建议，请联系 allwefantasy@gmail.com。\n\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/rest.py\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nfrom typing import List, Optional\nimport uvicorn\nimport argparse\nfrom superanalysis.sql.generator import SQLGenerator\nfrom superanalysis.sql.types import RAGConfig, ByzerSQLConfig, ModelConfig\nimport byzerllm\n\napp = FastAPI()\n\nclass ChatMessage(BaseModel):\n    role: str\n    content: str\n\nclass ChatRequest(BaseModel):\n    conversations: List[ChatMessage]\n    model: Optional[str] = None\n\nclass ChatResponse(BaseModel):\n    response: str\n\n@app.post(\"/chat\", response_model=ChatResponse)\nasync def chat(request: ChatRequest):\n    try:\n        result, _ = sql_generator.stream_chat_oai(\n            conversations=[{\"role\": msg.role, \"content\": msg.content} for msg in request.conversations],\n            model=request.model\n        )\n        return ChatResponse(response=\"\".join(result))\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"SuperAnalysis API Server\")\n    parser.add_argument(\"--host\", type=str, default=\"0.0.0.0\", help=\"Host to bind the server to\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"Port to bind the server to\")\n    parser.add_argument(\"--schema-rag-base-url\", type=str, required=True, help=\"Schema RAG base URL\")\n    parser.add_argument(\"--schema-rag-api-key\", type=str, required=True, help=\"Schema RAG API key\")\n    parser.add_argument(\"--context-rag-base-url\", type=str, required=True, help=\"Context RAG base URL\")\n    parser.add_argument(\"--context-rag-api-key\", type=str, required=True, help=\"Context RAG API key\")\n    parser.add_argument(\"--byzer-sql-url\", type=str, required=True, help=\"Byzer SQL URL\")\n    parser.add_argument(\"--data-dir\", type=str, default=\"\", help=\"Data directory\")\n    parser.add_argument(\"--llm-model\", type=str, required=True, help=\"LLM model for general use\")\n    parser.add_argument(\"--gen-sql-llm-model\", type=str, required=True, help=\"LLM model for SQL generation\")\n    \n    args = parser.parse_args()\n\n    rag_config = RAGConfig(\n        schema_rag_base_url=args.schema_rag_base_url,\n        schema_rag_api_key=args.schema_rag_api_key,\n        context_rag_base_url=args.context_rag_base_url,\n        context_rag_api_key=args.context_rag_api_key\n    )\n\n    byzer_sql_config = ByzerSQLConfig(\n        byzer_sql_url=args.byzer_sql_url,\n        data_dir=args.data_dir\n    )\n\n    model_config = ModelConfig(\n        llm_model=args.llm_model,\n        gen_sql_llm_model=args.gen_sql_llm_model\n    )\n\n    llm = byzerllm.ByzerLLM()\n    llm.setup_default_model_name(args.llm_model)\n\n    global sql_generator\n    sql_generator = SQLGenerator(\n        llm=llm,\n        rag_config=rag_config,\n        byzer_sql_config=byzer_sql_config,\n        model_config=model_config\n    )\n\n    uvicorn.run(app, host=args.host, port=args.port)\n\nif __name__ == \"__main__\":\n    main()\n\n##File: .auto-coder/libs/llm_friendly_packages/github.com/allwefantasy/byzer-llm/README.md\n# byzerllm 大模型编程快速指南\n\n本文示例 [Notebook](../../notebooks/003_byzerllm_大模型编程快速指南.ipynb)\n\n## 安装\n\n```bash\npip install -U byzerllm\nray start --head\n```\n## 启动一个模型代理\n\nbyzerllm 支持私有化模型或者SaaS模型的部署。\n\n这里以 deepseek 官方API 为例：\n\n```bash\neasy-byzerllm deploy deepseek-chat --token xxxxx --alias deepseek_chat\n```\n\n或者跬基流动API:\n\n```bash\neasy-byzerllm deploy alibaba/Qwen1.5-110B-Chat --token xxxxx --alias qwen110b_chat\n```\n\n将上面的 API KEY 替换成你们自己的。\n\n如果你想部署私有化模型或者对接 Ollama 等更多需求，参考 [002_使用byzerllm进行模型部署.md](./002_使用byzerllm进行模型部署.md) 或者 [README.md](../../README.md)。\n\n之后，你就可以在代码里使用  deepseek_chat 或者 qwen110b_chat  访问模型了。\n\n## hello world\n\n来和我们的大模型打个招呼:\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\n@byzerllm.prompt(llm=llm)\ndef hello(q:str) ->str:\n    '''\n    你好, {{ q }}\n    '''\n\ns = hello(\"你是谁\")    \nprint(s)\n\n## 输出:\n## '你好！我是一个人工智能助手，专门设计来回答问题、提供信息和帮助解决问题。如果你有任何疑问或需要帮助，请随时告诉我。'\n```\n\n恭喜，你和大模型成功打了招呼！\n\n可以看到，我们通过 `@byzerllm.prompt` 装饰器，将一个方法转换成了一个大模型的调用，然后这个方法的主题是一段文本，文本中\n使用了 jinja2 模板语法，来获得方法的参数。当正常调用该方法时，实际上就发起了和大模型的交互，并且返回了大模型的结果。\n\n在 byzerllm 中，我们把这种方法称之为 prompt 函数。\n\n## 查看发送给大模型的prompt\n\n很多情况你可能需要调试，查看自己的 prompt 渲染后到底是什么样子的，这个时候你可以通过如下方式\n获取渲染后的prompt:\n\n```python\nhello.prompt(\"你是谁\")\n## '你好, 你是谁'\n```            \n\n## 动态换一个模型\n\n前面的 hello 方法在初始化的时候，我们使用了默认的模型 deepseek_chat，如果我们想换一个模型，可以这样做：\n\n```python\nhello.with_llm(llm).run(\"你是谁\")\n## '你好！我是一个人工智能助手，专门设计来回答问题、提供信息和帮助解决问题。如果你有任何疑问或需要帮助，请随时告诉我。'\n```\n\n通过 with_llm 你可以设置一个新的 llm 对象，然后调用 run 方法，就可以使用新的模型了。\n\n## 超长文本生成\n\n我们知道，大模型一次生成的长度其实是有限的，如果你想生成超长文本，你可能需手动的不断获得\n生成结果，然后把他转化为输入，然后再次生成，这样的方式是比较麻烦的。\n\nbyzerllm 提供了更加易用的 API :\n\n```python\nimport byzerllm\nfrom byzerllm import ByzerLLM\n\nllm = ByzerLLM.from_default_model(\"deepseek_chat\")\n\n@byzerllm.prompt()\ndef tell_story() -> str:\n    \"\"\"\n    讲一个100字的故事。    \n    \"\"\"\n\n\ns = (\n    tell_story.with_llm(llm)\n    .with_response_markers()\n    .options({\"llm_config\": {\"max_length\": 10}})\n    .run()\n)\nprint(s)\n\n## 从前，森林里住着一只聪明的小狐狸。一天，它发现了一块闪闪发光的宝石。小狐狸决定用这块宝石帮助森林里的动物们。它用宝石的光芒指引迷路的小鹿找到了回家的路，用宝石的温暖治愈了受伤的小鸟。从此，小狐狸成了森林里的英雄，动物们都感激它的善良和智慧。\n```\n\n实际核心部分就是这一行：\n\n```python\ntell_story.with_llm(llm)\n    .with_response_markers()    \n    .run()\n```\n\n我们只需要调用 `with_response_markers` 方法，系统就会自动的帮我们生成超长文本。\n在上面的案例中，我们通过\n\n```python\n.options({\"llm_config\": {\"max_length\": 10}})\n```\n\n认为的限制大模型一次交互最多只能输出10个字符，但是系统依然自动完成了远超过10个字符的文本生成。\n\n## 对象输出\n\n前面我们的例子都是返回字符串，但是我们也可以返回对象，这样我们就可以更加灵活的处理返回结果。\n\n```python\nimport pydantic \n\nclass Story(pydantic.BaseModel):\n    '''\n    故事\n    '''\n\n    title: str = pydantic.Field(description=\"故事的标题\")\n    body: str = pydantic.Field(description=\"故事主体\")\n\n@byzerllm.prompt()\ndef tell_story()->Story:\n    '''\n    讲一个100字的故事。    \n    '''\n\ns = tell_story.with_llm(llm).run()\nprint(isinstance(s, Story))\nprint(s.title)\n\n## True\n## 勇敢的小鸟\n```\n\n可以看到，我们很轻松的将输出转化为格式化输出。\n\n## 自定义字段抽取\n\n前面的结构化输出，其实会消耗更多token,还有一种更加精准的结构化输出方式。\n比如让大模型生成一个正则表达式，但实际上大模型很难准确只输出一个正则表达式，这个时候我们可以通过自定义抽取函数来获取我们想要的结果。\n\n\n```python\nfrom loguru import logger\nimport re\n\n@byzerllm.prompt()\ndef generate_regex_pattern(desc: str) -> str:\n    \"\"\"\n    根据下面的描述生成一个正则表达式，要符合python re.compile 库的要求。\n\n    {{ desc }}\n\n    最后生成的正则表达式要在<REGEX></REGEX>标签对里。\n    \"\"\"    \n\ndef extract_regex_pattern(regex_block: str) -> str:    \n    pattern = re.search(r\"<REGEX>(.*)</REGEX>\", regex_block, re.DOTALL)\n    if pattern is None:\n        logger.warning(\"No regex pattern found in the generated block:\\n {regex_block}\")\n        raise None\n    return pattern.group(1)\n\npattern = \"匹配一个邮箱地址\"\nv = generate_regex_pattern.with_llm(llm).with_extractor(extract_regex_pattern).run(desc=pattern)\nprint(v)\n## ^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\n```\n\n在上面的例子里，我们根据一句话生成一个正则表达式。我们通过 `with_extractor` 方法，传入了一个自定义的抽取函数，这个函数会在大模型生成结果后，对结果进行处理，然后返回我们想要的结果。\n\n我们在 prompt 明确说了，生成的结果要放到 `<REGEX></REGEX>` 标签对里，然后我们通过 extract_regex_pattern 函数，从结果中提取出了我们想要的正则表达式。\n\n## 在实例方法中使用大模型\n\n```python\nimport byzerllm\ndata = {\n    'name': 'Jane Doe',\n    'task_count': 3,\n    'tasks': [\n        {'name': 'Submit report', 'due_date': '2024-03-10'},\n        {'name': 'Finish project', 'due_date': '2024-03-15'},\n        {'name': 'Reply to emails', 'due_date': '2024-03-08'}\n    ]\n}\n\n\nclass RAG():\n    def __init__(self):        \n        self.llm = byzerllm.ByzerLLM()\n        self.llm.setup_template(model=\"deepseek_chat\",template=\"auto\")\n        self.llm.setup_default_model_name(\"deepseek_chat\")        \n    \n    @byzerllm.prompt(lambda self:self.llm)\n    def generate_answer(self,name,task_count,tasks)->str:\n        '''\n        Hello {{ name }},\n\n        This is a reminder that you have {{ task_count }} pending tasks:\n        {% for task in tasks %}\n        - Task: {{ task.name }} | Due: {{ task.due_date }}\n        {% endfor %}\n\n        Best regards,\n        Your Reminder System\n        '''        \n\nt = RAG()\n\nresponse = t.generate_answer(**data)\nprint(response)\n\n## 输出:\n## Hello Jane Doe,\n##I hope this message finds you well. I wanted to remind you of your 3 pending tasks to ensure you stay on track:\n## 1. **Submit report** - This task is due on **2024-03-10**. Please ensure that you allocat\n```\n\n这里我们给了个比较复杂的例子，但我们可以看到，给一个实例prompt方法和普通prompt 方法差异不大。\n唯一的区别是如果你希望在定义的时候就指定大模型，使用一个lambda函数返回实例的 llm 对象即可。\n\n```python\n@byzerllm.prompt(lambda self:self.llm)\n```\n\n你也可以不返回，在调用的时候通过 `with_llm` 方法指定 llm 对象。\n\n此外，这个例子也展示了如何通过jinja2模板语法，来处理复杂的结构化数据。\n\n## 通过 Python 代码处理复杂入参\n\n上面的一个例子中，我们通过 jinja2 模板语法，来处理复杂的结构化数据，但是有时候我们可能需要更加复杂的处理，这个时候我们可以通过 Python 代码来处理。\n\n```python\nimport byzerllm\n\ndata = {\n    'name': 'Jane Doe',\n    'task_count': 3,\n    'tasks': [\n        {'name': 'Submit report', 'due_date': '2024-03-10'},\n        {'name': 'Finish project', 'due_date': '2024-03-15'},\n        {'name': 'Reply to emails', 'due_date': '2024-03-08'}\n    ]\n}\n\n\nclass RAG():\n    def __init__(self):        \n        self.llm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n    \n    @byzerllm.prompt()\n    def generate_answer(self,name,task_count,tasks)->str:\n        '''\n        Hello {{ name }},\n\n        This is a reminder that you have {{ task_count }} pending tasks:\n            \n        {{ tasks }}\n\n        Best regards,\n        Your Reminder System\n        '''\n        \n        tasks_str = \"\\n\".join([f\"- Task: {task['name']} | Due: { task['due_date'] }\" for task in tasks])\n        return {\"tasks\": tasks_str}\n\nt = RAG()\n\nresponse = t.generate_answer.with_llm(t.llm).run(**data)\nprint(response)\n\n## Just a gentle nudge to keep you on track with your pending tasks. Here's a quick recap:....\n```\n\n在这个例子里，我们直接把 tasks 在方法体里进行处理，然后作为一个字符串返回，最够构建一个字典，字典的key为 tasks,然后\n你就可以在 docstring 里使用 `{{ tasks }}` 来引用这个字符串。\n\n这样对于很复杂的入参，就不用谢繁琐的 jinja2 模板语法了。\n\n## 如何自动实现一个方法\n\n比如我定义一个签名，但是我不想自己实现里面的逻辑，让大模型来实现。这个在 byzerllm 中叫 function impl。我们来看看怎么\n实现:\n\n```python\nimport pydantic\nclass Time(pydantic.BaseModel):\n    time: str = pydantic.Field(...,description=\"时间，时间格式为 yyyy-MM-dd\")\n\n\n@llm.impl()\ndef calculate_current_time()->Time:\n    '''\n    计算当前时间\n    '''\n    pass \n\n\ncalculate_current_time()\n#output: Time(time='2024-06-14')\n```\n\n在这个例子里，我们定义了一个 calculate_current_time 方法，但是我们没有实现里面的逻辑，我们通过 `@llm.impl()` 装饰器，让大模型来实现这个方法。\n为了避免每次都要“生成”这个方法，导致无法适用，我们提供了缓存，用户可以按如下方式打印速度：\n\n```python\nstart = time.monotonic()\ncalculate_current_time()\nprint(f\"first time cost: {time.monotonic()-start}\")\n\nstart = time.monotonic()\ncalculate_current_time()\nprint(f\"second time cost: {time.monotonic()-start}\")\n\n# output:\n# first time cost: 6.067266260739416\n# second time cost: 4.347506910562515e-05\n```\n可以看到，第一次执行花费了6s,第二次几乎是瞬间完成的，这是因为第一次执行的时候，我们实际上是在生成这个方法，第二次执行的时候，我们是执行已经生成好的代码，所以速度会非常快。你可以显示的调用 `llm.clear_impl_cache()` 清理掉函数缓存。\n\n## Stream 模式\n\n前面的例子都是一次性生成结果，但是有时候我们可能需要一个流式的输出，这个时候我们可能需要用底层一点的API来完成了：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\nv = llm.stream_chat_oai(model=\"deepseek_chat\",conversations=[{\n    \"role\":\"user\",\n    \"content\":\"你好，你是谁\",\n}],delta_mode=True)\n\nfor t in v:\n    print(t,flush=True)  \n\n# 你好\n# ！\n# 我\n# 是一个\n# 人工智能\n# 助手\n# ，\n# 旨在\n# 提供\n# 信息\n# 、\n# 解答\n# 问题....\n```\n\n如果你不想要流式输出，但是想用底层一点的API，你可以使用 `llm.chat_oai` 方法：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\nv = llm.chat_oai(model=\"deepseek_chat\",conversations=[{\n    \"role\":\"user\",\n    \"content\":\"你好，你是谁\",\n}])\n\nprint(v[0].output)\n## 你好！我是一个人工智能助手，旨在提供信息、解答问题和帮助用户解决问题。如果你有任何问题或需要帮助，请随时告诉我。\n```\n\n## Function Calling \n\nbyzerllm 可以不依赖模型自身就能提供 function calling 支持，我们来看个例子：\n\n\n```python\nfrom typing import List,Dict,Any,Annotated\nimport pydantic \nimport datetime\nfrom dateutil.relativedelta import relativedelta\n\ndef compute_date_range(count:Annotated[int,\"时间跨度，数值类型\"],\n                       unit:Annotated[str,\"时间单位，字符串类型\",{\"enum\":[\"day\",\"week\",\"month\",\"year\"]}])->List[str]:\n    '''\n    计算日期范围\n\n    Args:\n        count: 时间跨度，数值类型\n        unit: 时间单位，字符串类型，可选值为 day,week,month,year\n    '''        \n    now = datetime.datetime.now()\n    now_str = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n    if unit == \"day\":\n        return [(now - relativedelta(days=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"week\":\n        return [(now - relativedelta(weeks=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"month\":\n        return [(now - relativedelta(months=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"year\":\n        return [(now - relativedelta(years=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    return [\"\",\"\"]\n\ndef compute_now()->str:\n    '''\n    计算当前时间\n    '''\n    return datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n```\n\n我们定义了两个方法，一个是计算日期范围，一个是计算当前时间。\n\n现在我么可以来测试下，系统如何根据自然语言决定调用哪个方法：\n\n```python\nt = llm.chat_oai([{\n    \"content\":'''计算当前时间''',\n    \"role\":\"user\"    \n}],tools=[compute_date_range,compute_now],execute_tool=True)\n\nt[0].values\n\n## output: ['2024-06-14 15:18:02']\n```\n\n我们可以看到，他正确的选择了 compute_now 方法。\n\n接着我们再试一个：\n\n```python\nt = llm.chat_oai([{\n    \"content\":'''最近三个月趋势''',\n    \"role\":\"user\"    \n}],tools=[compute_date_range,compute_now],execute_tool=True)\n\nt[0].values\n\n## output: [['2024-03-14 15:19:13', '2024-06-14 15:19:13']]\n```\n\n模型正确的选择了 compute_date_range 方法。\n\n## 多模态\n\nbyerllm 也能很好的支持多模态的交互，而且统一了多模态大模型的接口，比如你可以用一样的方式使用 openai 或者 claude 的图片转文字能力， 或者一致的方式使用火山，azuer, openai的语音合成接口。\n\n### 图生文\n\n```python\nimport byzerllm\nfrom byzerllm.types import ImagePath\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\n\n\n@byzerllm.prompt()\ndef what_in_image(image_path: ImagePath) -> str:\n    \"\"\"\n    {{ image_path }}\n    这个图片里有什么？\n    \"\"\"    \n\n\nv = what_in_image.with_llm(vl_llm).run(\n    ImagePath(value=\"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\")\n)\nv\n## OUTPUT: 这张图片展示了多只可爱的猫咪，采用了艺术风格的绘画。猫咪们有不同的颜色和花纹，背景是浅棕色，上面还点缀着一些红色的花朵。整体画面给人一种温馨和谐的感觉\n```\n\n可以看到，我们只需要把 prompt 函数的图片地址入参使用 byzerllm.types.ImagePath里进行包装，就可以直接在 prompt 函数体里\n带上图片。\n\n或者你可以这样：\n\n```python\nimport byzerllm\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\n\n\n@byzerllm.prompt()\ndef what_in_image(image_path: str) -> str:\n    \"\"\"\n    {{ image }}\n    这个图片里有什么？\n    \"\"\"\n    return {\"image\": byzerllm.Image.load_image_from_path(image_path)}\n\n\nv = what_in_image.with_llm(vl_llm).run(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv\n```\n\n通过 `image_path` 参数，然后通过 `byzerllm.Image.load_image_from_path` 方法，转化为一个图片对象 image，最后在 prompt 函数体里\n使用 `{{ image }}` 引用这个图片对象。\n\n另外我们也是可以支持配置多张图片的。\n\n另外，我们也可以使用基础的 `llm.chat_oai` 方法来实现：\n\n```python\nimport byzerllm\nimport json\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\nimage = byzerllm.Image.load_image_from_path(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv = vl_llm.chat_oai(\n    conversations=[\n        {\n            \"role\": \"user\",\n            \"content\": json.dumps(\n                [{\"image\": image, \"text\": \"这个图片里有什么？\"}], ensure_ascii=False\n            ),\n        }\n    ]\n)\nv[0].output\n```\n\n还可以这么写：\n\n```python\nimport byzerllm\nimport json\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\nimage = byzerllm.Image.load_image_from_path(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv = vl_llm.chat_oai(\n    conversations=[\n        {\n            \"role\": \"user\",\n            \"content\": json.dumps(\n                [\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\"url\": image, \"detail\": \"high\"},\n                    },\n                    {\"text\": \"这个图片里有什么？\", \"type\": \"text\"},\n                ],\n                ensure_ascii=False,\n            ),\n        }\n    ]\n)\nv[0].output\n```\n\n### 语音合成\n\n这里以 openai 的 tts 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=tts-1 \\\n--model openai_tts\n```\n\n此外，byzerllm 支持 azure,火山引擎等 tts 语音合成引擎。\n\n接着你可以这么用：\n\n```python\nimport byzerllm\nimport base64\nimport json\n\nllm = byzerllm.ByzerLLM.from_default_model(\"openai_tts\")\n\n\nt = llm.chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\": json.dumps({\n        \"input\":\"hello, open_tts\",\n        \"voice\": \"alloy\",\n        \"response_format\": \"mp3\"\n    },ensure_ascii=False)\n}])\n\nwith open(\"voice.mp3\",\"wb\") as f:\n    f.write(base64.b64decode(t[0].output))\n```\n\ntts 模型生成没有prompt函数可以用，你需要直接使用 chat_oai。\n\n\n### 语音识别\n\n这里以 openai 的 whisper-1 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--worker_concurrency 10 \\\n--infer_params saas.model=whisper-1 saas.api_key=${MODEL_OPENAI_TOKEN} \\\n--model speech_to_text\n```\n\n语音识别的使用方式和图生文类似，我们可以直接在 prompt 函数体里带上音频文件。\n\n```python\nimport byzerllm\nimport json\nimport base64\nfrom byzerllm.types import AudioPath\n\nllm = byzerllm.ByzerLLM.from_default_model(\"speech_to_text\")\n\naudio_file = \"/Users/allwefantasy/videos/output_audio.mp3\"\n\n@byzerllm.prompt(llm=llm)\ndef audio_to_text(audio_file: AudioPath):\n    \"\"\"\n    {{ audio_file }}\n    \"\"\"\n\nv = audio_to_text(AudioPath(value=audio_file))\njson.loads(v)\n```\n输出的数据格式略微复杂：\n\n```\n{'text': 'In the last chapter, you and I started to step through the internal workings of a transformer. This is one of the key pieces of technology inside large language models, and a lot of other tools in the modern wave of AI.',\n 'task': 'transcribe',\n 'language': 'english',\n 'duration': 10.0,\n 'segments': [{'id': 0,\n   'seek': 0,\n   'start': 0.0,\n   'end': 4.78000020980835,\n   'text': ' In the last chapter, you and I started to step through the internal workings of a transformer.',\n   'tokens': [50364,\n    682,\n    264,\n    1036,\n    7187,\n    11,\n    291,\n    293,\n    286,\n    1409,\n    281,\n    1823,\n    807,\n    264,\n    6920,\n    589,\n    1109,\n    295,\n    257,\n    31782,\n    13,\n    50586],\n   'temperature': 0.0,\n   'avg_logprob': -0.28872039914131165,\n   'compression_ratio': 1.4220778942108154,\n   'no_speech_prob': 0.016033057123422623},\n  ....\n  {'id': 2,\n   'seek': 0,\n   'start': 8.579999923706055,\n   'end': 9.979999542236328,\n   'text': ' and a lot of other tools in the modern wave of AI.',\n   'tokens': [50759,\n    293,\n    257,\n    688,\n    295,\n    661,\n    3873,\n    294,\n    264,\n    4363,\n    5772,\n    295,\n    7318,\n    13,\n    50867],\n   'temperature': 0.0,\n   'avg_logprob': -0.28872039914131165,\n   'compression_ratio': 1.4220778942108154,\n   'no_speech_prob': 0.016033057123422623}],\n 'words': [{'word': 'In', 'start': 0.0, 'end': 0.18000000715255737},\n  {'word': 'the', 'start': 0.18000000715255737, 'end': 0.23999999463558197},\n  {'word': 'last', 'start': 0.23999999463558197, 'end': 0.5400000214576721},\n  {'word': 'chapter', 'start': 0.5400000214576721, 'end': 0.800000011920929},\n  ....\n  {'word': 'AI', 'start': 9.920000076293945, 'end': 9.979999542236328}]}\n```\n\n会输出每一句话以及每一个字所在的起始时间和截止时间。你可以根据需要来使用。\n\n\n### 文生图\n\n文生图和语音合成类似，首先要启动合适的模型,以openai 的 dall-e-3 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=dall-e-3 \\\n--model openai_image_gen\n```\n\n启动模型后，只需要记住几个模板参数即可使用，这里直接使用 chat_oai 方法来使用：\n\n```python\n\nimport byzerllm\nimport json\nimport base64\n\nllm = byzerllm.ByzerLLM.from_default_model(\"openai_image_gen\")\nt = llm.chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\": json.dumps({\n        \"input\":\"a white siamese cat\",\n        \"size\": \"1024x1024\",\n        \"quality\": \"standard\"\n    },ensure_ascii=False)\n}])\n\nwith open(\"output1.jpg\",\"wb\") as f:\n    f.write(base64.b64decode(t[0].output))\n\n\nimport matplotlib.pyplot as plt\n\nimage_path = \"output1.jpg\"\nimage = plt.imread(image_path)\n\nplt.imshow(image)\nplt.axis('off')\nplt.show()\n```\n\n## Prompt 函数的流式输出\n\nbyzerllm 底层支持流式输出，非 prompt 函数的用法是这样的：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\n\nv = llm.stream_chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\":\"讲一个100字的故事\"\n}])\n\nfor s  in v:\n    print(s[0], end=\"\")\n```\n\n如果你像用 prompt 函数，可以这么用：\n\n\n```python\nimport byzerllm\nimport json\nimport base64\nfrom typing import Generator\n\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\n\n@byzerllm.prompt()\ndef tell_story() -> Generator[str, None, None]:\n    '''\n    给我讲一个一百多字的故事\n    '''\n\nv = tell_story.with_llm(llm).run()    \nfor i in v:\n    print(i, end=\"\")\n```\n\n可以看到，和普通的 prompt 函数的区别在于，返回值是一个生成器，然后你可以通过 for 循环来获取结果。\n\n## 向量化模型\n\nbyzerllm 支持向量化模型,你可以这样启动一个本地的模型：\n\n```bash\n!byzerllm deploy --pretrained_model_type custom/bge \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--worker_concurrency 10 \\\n--model_path /home/winubuntu/.auto-coder/storage/models/AI-ModelScope/bge-large-zh \\\n--infer_backend transformers \\\n--num_workers 1 \\\n--model emb\n```\n\n注意两个参数:\n\n1. --infer_backend transformers: 表示使用 transformers 作为推理后端。\n2. --model_path: 表示模型的路径。\n\n也可以启动一个 SaaS 的emb模型,比如 qwen 的 emb 模型：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/qianwen \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 2 \\\n--infer_params saas.api_key=${MODEL_QIANWEN_TOKEN}  saas.model=text-embedding-v2 \\\n--model qianwen_emb\n```\n\n或者 openai 的 emb 模型：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--worker_concurrency 10 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=text-embedding-3-small \\\n--model gpt_emb\n```\nSaaS 模型无需配置 `--infer_backend` 参数。\n\n无论是本地模型还是 SaaS 模型，我们都可以这样使用：\n\n```python\nimport byzerllm\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\nllm.setup_default_emb_model_name(\"emb\")\nllm.emb_query(\"你好\")\n```\n\n如果你配置 byzerllm 中的 Storage 使用，比如你这样启动了存储：\n\n```bash\nbyzerllm storage start --enable_emb\n```\n\n那么需要这么使用：\n\n```python\nfrom byzerllm.apps.byzer_storage.simple_api import ByzerStorage, DataType, FieldOption,SortOption\nstorage = ByzerStorage(\"byzerai_store\", \"memory\", \"memory\")\nstorage.emb(\"你好\")\n```\n\n## 一些辅助工具\n\n当调用 prompt 函数返回字符串的时候，如果想从里面抽取代码，可以使用如下方式：\n\n```python\nfrom byzerllm.utils.client import code_utils\ntext_with_markdown = '''\n```shell\nls -l\n```\n'''\ncode_blocks = code_utils.extract_code(text_with_markdown)\nfor code_block in code_blocks:\n    if code_block[0] == \"shell\":\n        print(code_block[1])\n##output: ls -l        \n```\n\nTagExtractor 工具用于对任意 `<_tag_></_tag_>` 标签对的抽取，下面是一个使用示例：\n\n```python\nfrom byzerllm.apps.utils import TagExtractor\n\nextractor = TagExtractor('''\n大家好\n<_image_>data:image/jpeg;base64,xxxxxx</_image_>\n                         大家好\n<_image_>data:image/jpeg;base64,xxxxxx2</_image_>\n''')\n\nv = extractor.extract()\nprint(v.content[0].content)\nprint(v.content[1].content)\n```\n输出为:\n\n```\ndata:image/jpeg;base64,xxxxxx\ndata:image/jpeg;base64,xxxxxx2\n```\n\n我们成功的将 <_image_></_image_> 标签对里的内容抽取出来了。\n\n\n## 注意事项\n\n1. prompt函数方法体返回只能是dict，实际的返回类型和方法签名可以不一样，但是方法体返回只能是dict。\n2. 大部分情况prompt函数体为空，如果一定要有方法体，可以返回一个空字典。\n3. 调用prompt方法的时候，如果在@byzerllm.prompt()里没有指定llm对象，那么需要在调用的时候通过with_llm方法指定llm对象。\n\n===========================\n\n# Byzer Storage: 最易用的AI存储引擎\n\nByzer Storage是一个为AI应用设计的高性能存储引擎,它提供了简单易用的API,支持向量搜索、全文检索以及结构化查询。本文将详细介绍Byzer Storage的使用方法和主要特性。\n\n## 0. 安装和启动\n\n```bash\npip install byzerllm\nbyzerllm storage start\n```\n\nThat's it! Byzer Storage已经安装并启动成功,现在我们可以开始使用它了。默认会启动一个 byzerai_store 的集群。\n\n注意，如果你是这样启动的：\n\n```bash\nbyzerllm storage start --enable_emb\n```\n\n那么会自动启动一个 emb 模型，名字就叫 emb， ByzerStorage 会自动使用该模型，无需做任何其他配置。\n\n## 1. 初始化\n\n创建一个 ByzerStorage 对象，链接 byzerai_store 集群，并且指定数据库和表名（可以不存在）。\n\n```python\nfrom byzerllm.apps.byzer_storage.simple_api import ByzerStorage, DataType, FieldOption, SortOption\n\nstorage = ByzerStorage(\"byzerai_store\", \"my_database1\", \"my_table4s\")\n```\n\n\n## 2. 创建库表（可选）\n\nByzer Storage使用Schema来定义数据结构。我们可以使用SchemaBuilder来构建Schema:\n\n```python\n_ = (\n    storage.schema_builder()\n    .add_field(\"_id\", DataType.STRING)\n    .add_field(\"name\", DataType.STRING)\n    .add_field(\"content\", DataType.STRING, [FieldOption.ANALYZE])\n    .add_field(\"raw_content\", DataType.STRING, [FieldOption.NO_INDEX])    \n    .add_array_field(\"summary\", DataType.FLOAT)    \n    .add_field(\"created_time\", DataType.LONG, [FieldOption.SORT])    \n    .execute()\n)\n```\n\n这个Schema定义了以下字段:\n- `_id`: 字符串类型的主键\n- `name`: 字符串类型,可用于过滤条件\n- `content`: 字符串类型,带有ANALYZE选项,用于全文搜索\n- `raw_content`: 字符串类型,带有NO_INDEX选项,不会被索引\n- `summary`: 浮点数组类型,用于存储向量\n- `created_time`: 长整型,带有SORT选项,可用于排序\n\n需要注意的是：\n\n1. 如果一个字段带有ANALYZE选项,则该字段会被分词,并且可以用于全文搜索，但是就无法返回原始的文本了。所以你需要添加一个新字段专门用来存储原文，比如在我们这个例子里，我们新增了 raw_content 字段，并且显示的指定了 NO_INDEX 选项，这样就不会被索引，也就不会被分词，可以后续被检索后用来获取原始文本。\n2. 对于需要作为向量检索和存储的字段，需要指定为数组类型，比如我们这个例子里的 summary 字段。\n3. 如果你需要拿到向量字段的原始文本，那么你也需要添加一个新字段专门用来存储原文，就像我们这个例子里的 raw_content 字段一样。\n\n\n## 3. 写入数据\n\n准备数据并使用WriteBuilder写入Storage:\n\n```python\ndata = [\n    {\"_id\": \"1\", \"name\": \"Hello\", \"content\": \"Hello, world!\", \"raw_content\": \"Hello, world!\", \"summary\": \"hello world\", \"created_time\": 1612137600},\n    {\"_id\": \"2\", \"name\": \"Byzer\", \"content\": \"Byzer, world!\", \"raw_content\": \"Byzer, world!\", \"summary\": \"byzer\", \"created_time\": 1612137601},\n    {\"_id\": \"3\", \"name\": \"AI\", \"content\": \"AI, world!\", \"raw_content\": \"AI, world!\", \"summary\": \"AI\", \"created_time\": 16121376002},\n    {\"_id\": \"4\", \"name\": \"ByzerAI\", \"content\": \"ByzerAI, world!\", \"raw_content\": \"ByzerAI, world!\", \"summary\": \"ByzerAi\", \"created_time\": 16121376003},\n]\n\nstorage.write_builder().add_items(data, vector_fields=[\"summary\"], search_fields=[\"content\"]).execute()\nstorage.commit()\n```\n\n这里我们使用`add_items`方法批量添加数据,并指定了`summary`为向量字段,`content`为搜索字段。最后调用`commit()`来确保数据被持久化。\n\n\n从上面我们要写入到 byzer storage 的数据我们可以看到如下几个特点：\n\n1. 需要包含我们之前定义的 Schema中罗列的所有的字段，同时需要指定哪些是向量字段，哪些是检索字段。\n2. 向量和检索不能是同一个字段。\n3. 对于向量，检索字段，我们给到 write_builder 的都是文本，ByzerStorage 会根据 Schema 的定义自动将其转换为向量，检索字段需要的格式。\n\n## 4. 查询数据\n\nByzer Storage支持多种查询方式,包括向量搜索、全文检索、过滤和排序。\n\n### 4.1 向量搜索 + 全文检索\n\n```python\nquery = storage.query_builder()\nquery.set_vector_query(\"ByzerAI\", fields=[\"summary\"])\nresults = query.set_search_query(\"Hello\", fields=[\"content\"]).execute()\nprint(results)\n```\n\n这个查询结合了向量搜索和全文检索,它会在`summary`字段中搜索与\"ByzerAI\"相似的向量,同时在`content`字段中搜索包含\"Hello\"的文档。\n\n### 4.2 过滤 + 向量搜索 + 全文检索\n\n```python\nquery = storage.query_builder()\nquery.and_filter().add_condition(\"name\", \"AI\").build()\nquery.set_vector_query(\"ByzerAI\", fields=\"summary\")\nresults = query.set_search_query(\"Hello\", fields=[\"content\"]).execute()\nprint(results)\n```\n\n这个查询首先过滤`name`字段等于\"AI\"的文档,然后在结果中进行向量搜索和全文检索。\n\n### 4.3 过滤 + 排序\n\n```python\nquery = storage.query_builder()\nquery.and_filter().add_condition(\"name\", \"AI\").build().sort(\"created_time\", SortOption.DESC)\nresults = query.execute()\nprint(results)\n```\n\n这个查询过滤`name`字段等于\"AI\"的文档,然后按`created_time`字段降序排序。\n\n## 5. 删除数据\n\n### 5.1 根据ID删除\n\n```python\nstorage.delete_by_ids([\"3\"])\n\nquery = storage.query_builder()\nquery.and_filter().add_condition(\"name\", \"AI\").build()\nresults = query.execute()\nprint(results)\n```\n\n这里我们删除了ID为\"3\"的文档,然后查询验证删除结果。\n\n### 5.2 删除整个表\n\n```python\nstorage.drop()\n```\n\n这个操作会删除整个表及其所有数据,请谨慎使用。\n\n## 结论\n\nByzer Storage提供了一套简洁而强大的API,能够轻松实现向量搜索、全文检索、结构化查询等功能。它的设计非常适合AI应用场景,可以有效地存储和检索各种类型的数据。通过本文的介绍,相信读者已经对Byzer Storage有了基本的了解,并能够开始在自己的项目中使用这个强大的存储引擎。\n\n\n     \n\n下面是用户的需求：\n\n根据 @rest.py 更新 @README.md 中的## 非 OpenAi 兼容接口 部分\n"}, {"role": "assistant", "content": "明白，我会按照 SEARCH/REPLACE 格式来提供更新。以下是更新 README.md 文件的 SEARCH/REPLACE 块：\n\n```markdown\n##File: /Users/allwefantasy/projects/super_analysis/README.md\n<<<<<<< SEARCH\n## 非 OpenAi 兼容接口\n=======\n## 非 OpenAI 兼容接口\n\n除了兼容 OpenAI 的接口外，Super Analysis 还提供了一个简单的 REST API 接口，可以直接与模型进行交互。以下是如何使用这个接口：\n\n### 启动服务\n\n使用以下命令启动 REST API 服务：\n\n```bash\nsuperanalysis.api --schema-rag-base-url <schema_rag_url> \\\n                  --schema-rag-api-key <schema_rag_api_key> \\\n                  --context-rag-base-url <context_rag_url> \\\n                  --context-rag-api-key <context_rag_api_key> \\\n                  --byzer-sql-url <byzer_sql_url> \\\n                  --llm-model <llm_model_name> \\\n                  --gen-sql-llm-model <gen_sql_llm_model_name> \\\n                  --host <host> \\\n                  --port <port>\n```\n\n请替换尖括号中的值为实际的配置信息。\n\n### API 端点\n\n服务启动后，将提供以下 API 端点：\n\n#### POST /chat\n\n此端点用于与模型进行对话。\n\n请求体格式：\n\n```json\n{\n  \"conversations\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"你的问题或指令\"\n    }\n  ],\n  \"model\": \"可选的模型名称\"\n}\n```\n\n响应格式：\n\n```json\n{\n  \"response\": \"模型的回答\"\n}\n```\n\n### 示例使用\n\n以下是使用 Python 请求库与 API 交互的示例：\n\n```python\nimport requests\nimport json\n\nurl = \"http://localhost:8000/chat\"\npayload = {\n    \"conversations\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"你好，请介绍一下你自己。\"\n        }\n    ],\n    \"model\": \"your_model_name\"\n}\n\nresponse = requests.post(url, json=payload)\nresult = response.json()\n\nprint(result[\"response\"])\n```\n\n这个 API 提供了一种简单直接的方式与 Super Analysis 的底层模型进行交互，特别适用于需要集成到其他应用或服务中的场景。\n>>>>>>> REPLACE\n```\n\n这个 SEARCH/REPLACE 块将替换 README.md 文件中的 \"非 OpenAI 兼容接口\" 部分，提供了新的详细说明，包括如何启动服务、API 端点的信息以及使用示例。"}], "model": "human", "yaml_file": "actions/136_chat_action.yml"}
{"instruction": "@@ByzerSQLConfig 新增一个 owner 参数, 同步在 rest.py 和 cli.py中新增对应的命令行参数", "conversations": [{"role": "user", "content": "\n如果你需要生成代码，对于每个需要更改的文件,你需要按 *SEARCH/REPLACE block* 的格式进行生成。\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The opening fence and code language, eg: ```python\n2. The file path alone on a line, starting with \"##File:\" and verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n*SEARCH/REPLACE* blocks will replace *all* matching occurrences.\nInclude enough lines to make the SEARCH blocks unique.\n\nInclude *ALL* the code being searched and replaced!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n下面我们来看一个例子：\n\n当前项目目录结构：\n1. 项目根目录： /tmp/projects/mathweb\n2. 项目子目录/文件列表(类似tree 命令输出)\nflask/\n    app.py\n    templates/\n        index.html\n    static/\n        style.css\n\n用户需求： Change get_factorial() to use math.factorial\n\n回答： To make this change we need to modify `/tmp/projects/mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n用户需求： Refactor hello() into its own file.\n\n回答：To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/hello.py\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/main.py\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n现在让我们开始一个新的任务:\n\n当前项目目录结构：\n1. 项目根目录： /Users/allwefantasy/projects/super_analysis\n2. 项目子目录/文件列表(类似tree 命令输出)：\nsetup.py\n.auto-coder/\n    libs/\n        llm_friendly_packages/\n            src/\n                example1/\n                    main.py\nsrc/\n    superanalysis/\n        serve/\n            rest.py\n            convert.py\n            openai_serve.py\n            __init__.py\n            llm_wrapper.py\n            api_server.py\n            cli.py\n        sql/\n            __init__.py\n            types.py\n            generator.py\n下面是一些文件路径以及每个文件对应的源码：\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/types.py\nfrom pydantic import BaseModel\n\nclass RAGConfig(BaseModel):\n    schema_rag_base_url: str = \"\"\n    schema_rag_api_key: str = \"xxxx\"\n\n    context_rag_base_url: str = \"\"\n    context_rag_api_key: str = \"xxxx\"\n\nclass ByzerSQLConfig(BaseModel):\n    byzer_sql_url: str = \"\"\n    data_dir: str = \"\"\n\nclass ModelConfig(BaseModel):\n    llm_model: str = \"\"\n    gen_sql_llm_model: str = \"\"\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/cli.py\nimport argparse\nimport byzerllm\nfrom byzerllm.utils.client import ByzerLLM\nfrom superanalysis.serve.api_server import serve, ServerArgs\nfrom superanalysis.sql.types import RAGConfig, ByzerSQLConfig, ModelConfig\nfrom superanalysis.sql.generator import SQLGenerator\nfrom superanalysis.serve.llm_wrapper import LLWrapper\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"ByzerLLM API Server\")\n    parser.add_argument(\"--ray-address\", type=str, default=\"auto\", help=\"ray address\")\n    parser.add_argument(\n        \"--host\", type=str, default=\"0.0.0.0\", help=\"Host to bind the server to\"\n    )\n    parser.add_argument(\n        \"--port\", type=int, default=8000, help=\"Port to bind the server to\"\n    )\n    parser.add_argument(\n        \"--uvicorn-log-level\", type=str, default=\"info\", help=\"Uvicorn log level\"\n    )\n    parser.add_argument(\n        \"--allow-credentials\", action=\"store_true\", help=\"Allow credentials\"\n    )\n    parser.add_argument(\n        \"--allowed-origins\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed origins\"\n    )\n    parser.add_argument(\n        \"--allowed-methods\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed methods\"\n    )\n    parser.add_argument(\n        \"--allowed-headers\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed headers\"\n    )\n    parser.add_argument(\n        \"--api-key\", type=str, default=\"\", help=\"API key for authentication\"\n    )\n    parser.add_argument(\n        \"--served-model-name\",\n        type=str,\n        required=True,\n        help=\"Name of the model to be served\",\n    )\n    parser.add_argument(\n        \"--prompt-template\", type=str, default=\"\", help=\"Prompt template\"\n    )\n    parser.add_argument(\n        \"--response-role\", type=str, default=\"assistant\", help=\"Response role\"\n    )\n    parser.add_argument(\"--ssl-keyfile\", type=str, default=\"\", help=\"SSL key file\")\n    parser.add_argument(\n        \"--ssl-certfile\", type=str, default=\"\", help=\"SSL certificate file\"\n    )\n\n    # Add new arguments for RAGConfig\n    parser.add_argument(\n        \"--schema-rag-base-url\", type=str, default=\"\", help=\"RAG base URL\"\n    )\n    parser.add_argument(\n        \"--schema-rag-api-key\", type=str, default=\"xxxx\", help=\"RAG API key\"\n    )\n\n    parser.add_argument(\n        \"--context-rag-base-url\", type=str, default=\"\", help=\"RAG base URL\"\n    )\n    parser.add_argument(\n        \"--context-rag-api-key\", type=str, default=\"xxxx\", help=\"RAG API key\"\n    )\n\n    # Add new argument for ByzerSQLConfig\n    parser.add_argument(\"--byzer-sql-url\", type=str, default=\"\", help=\"Byzer SQL URL\")\n    parser.add_argument(\"--data-dir\", type=str, default=\"\", help=\"Data directory\")\n    parser.add_argument(\n        \"--llm-model\", type=str, default=\"\", help=\"LLM model for general use\"\n    )\n    parser.add_argument(\n        \"--gen-sql-llm-model\", type=str, default=\"\", help=\"LLM model for SQL generation\"\n    )\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile,\n    )\n\n    # Create RAGConfig instance\n    rag_config = RAGConfig(\n        schema_rag_base_url=args.schema_rag_base_url,\n        schema_rag_api_key=args.schema_rag_api_key,\n        context_rag_base_url=args.context_rag_base_url,\n        context_rag_api_key=args.context_rag_api_key,\n    )\n\n    # Create ByzerSQLConfig instance\n    byzer_sql_config = ByzerSQLConfig(\n        byzer_sql_url=args.byzer_sql_url, data_dir=args.data_dir\n    )\n\n    model_config = ModelConfig(\n        llm_model=args.llm_model or args.served_model_name,\n        gen_sql_llm_model=args.gen_sql_llm_model or args.served_model_name,\n    )\n\n    byzerllm.connect_cluster(address=args.ray_address)\n    llm = ByzerLLM()\n    llm.setup_default_model_name(args.served_model_name)\n    llm = byzerllm.ByzerLLM()\n    if not server_args.served_model_name:\n        raise ValueError(\"served_model_name is required\")\n    llm.setup_default_model_name(server_args.served_model_name)\n\n    print(\"RAG Config: \", rag_config)\n    print(\"Model Config: \", model_config)\n    rag = SQLGenerator(\n        llm=llm,\n        rag_config=rag_config,\n        byzer_sql_config=byzer_sql_config,\n        model_config=model_config,\n    )\n    llm_wrapper = LLWrapper(llm=llm, entry=rag)\n    serve(llm_wrapper, server_args)\n\n\nif __name__ == \"__main__\":\n    main()\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/__init__.py\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/__init__.py\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/llm_wrapper.py\nfrom typing import Any, Dict, List, Optional, Union,Callable\nfrom byzerllm.utils.client.types import (    \n    LLMFunctionCallResponse,\n    LLMClassResponse,LLMResponse    \n)\nimport pydantic\nfrom byzerllm import ByzerLLM\nfrom byzerllm.utils.client import LLMResponse\nfrom byzerllm.utils.types import SingleOutputMeta\nfrom byzerllm.utils.langutil import asyncfy_with_semaphore\nfrom superanalysis.sql.generator import SQLGenerator\nfrom loguru import logger\n\nclass LLWrapper:\n\n    def __init__(self,llm:ByzerLLM,entry:SQLGenerator):\n        self.entry = entry\n        self.llm = llm\n        \n\n    def chat_oai(self,\n                 conversations,\n                 tools:List[Union[Callable,str]]=[], \n                 tool_choice:Optional[Union[Callable,str]]=None,\n                 execute_tool:bool=False,  \n                 impl_func:Optional[Callable]=None,\n                 execute_impl_func:bool=False,\n                 impl_func_params:Optional[Dict[str,Any]]=None,\n                 func_params:Optional[Dict[str,Any]]=None,\n                 response_class:Optional[Union[pydantic.BaseModel,str]] = None, \n                 response_after_chat:Optional[Union[pydantic.BaseModel,str]] = False,\n                 enable_default_sys_message:bool=True,                 \n                 model:Optional[str] = None,\n                 role_mapping=None,llm_config:Dict[str,Any]={}\n                 )->Union[List[LLMResponse],List[LLMFunctionCallResponse],List[LLMClassResponse]]: \n        res,contexts = self.entry.stream_chat_oai(conversations)\n        s = \"\".join(res)        \n        return [LLMResponse(output=s,metadata={},input=\"\")]\n\n    def stream_chat_oai(self,conversations, \n                        model:Optional[str]=None, \n                        role_mapping=None,\n                        delta_mode=False,\n                        llm_config:Dict[str,Any]={}): \n        res,contexts = self.entry.stream_chat_oai(conversations)        \n        for t in res:                        \n            yield (t,SingleOutputMeta(0,0))\n\n    async def async_stream_chat_oai(self,conversations, \n                        model:Optional[str]=None, \n                        role_mapping=None,\n                        delta_mode=False,\n                        llm_config:Dict[str,Any]={}): \n        res,contexts = self.rag.stream_chat_oai(conversations)                \n        for t in res:                                    \n            yield (t,SingleOutputMeta(0,0))\n                         \n\n    def __getattr__(self, name):        \n        return getattr(self.llm, name)\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/openai_serve.py\nimport argparse\nfrom service_server.api_server import ServerArgs, serve\nfrom service_server.llm_wrapper import LLWrapper\nfrom service_server.summary_rag import SummaryRAG\nimport byzerllm\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"OpenAI Serve\")\n    parser.add_argument(\n        \"--model\",\n        default=\"\",\n    )\n    parser.add_argument(\"--emb_model\", default=\"\")\n    parser.add_argument(\n        \"--ray_address\",\n        default=\"auto\",\n    )\n    parser.add_argument(\"--source_dir\", default=\".\", help=\"\")\n    parser.add_argument(\"--host\", default=\"\", help=\"\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"\")\n    parser.add_argument(\"--uvicorn_log_level\", default=\"info\", help=\"\")\n    parser.add_argument(\"--allow_credentials\", action=\"store_true\", help=\"\")\n    parser.add_argument(\"--allowed_origins\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--allowed_methods\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--allowed_headers\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--api_key\", default=\"\", help=\"\")\n    parser.add_argument(\"--served_model_name\", default=\"\", help=\"\")\n    parser.add_argument(\"--prompt_template\", default=\"\", help=\"\")\n    parser.add_argument(\"--ssl_keyfile\", default=\"\", help=\"\")\n    parser.add_argument(\"--ssl_certfile\", default=\"\", help=\"\")\n    parser.add_argument(\"--response_role\", default=\"assistant\", help=\"\")\n    parser.add_argument(\n        \"--collections\", default=\"\", help=\"Collection name for indexing\"\n    )\n    parser.add_argument(\n        \"--base_dir\",\n        default=\"\",\n        help=\"Path where the processed text embeddings were stored\",\n    )\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name or args.model,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile,\n    )\n    byzerllm.connect_cluster(address=args.ray_address)\n    llm = byzerllm.ByzerLLM()\n    if not server_args.served_model_name:\n        raise ValueError(\"served_model_name is required\")\n    llm.setup_default_model_name(server_args.served_model_name)\n    rag = SummaryRAG(llm=llm)    \n    llm_wrapper = LLWrapper(llm=llm, rag=rag)\n    serve(llm=llm_wrapper, args=server_args)\n\nif __name__ == \"__main__\":\n    main()\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/api_server.py\nimport os\nimport time\nimport uvicorn\nfrom fastapi import Request\nfrom fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import JSONResponse, StreamingResponse, Response\n\nfrom byzerllm.log import init_logger\nfrom byzerllm.utils import random_uuid\nfrom byzerllm.version import __version__ as version\nfrom byzerllm.utils.client import ByzerLLM, LLMRequest\nfrom byzerllm.utils.client.entrypoints.openai.serving_chat import OpenAIServingChat\nfrom byzerllm.utils.client.entrypoints.openai.serving_completion import OpenAIServingCompletion\nfrom byzerllm.utils.client.entrypoints.openai.protocol import (\n    ModelList,\n    ModelCard,\n    ModelPermission,\n    ChatCompletionRequest,\n    ErrorResponse,\n    CompletionRequest,\n    Embeddings,\n    EmbeddingsOutput,\n    EmbeddingsData,\n    EmbeddingsUsage,\n)\nfrom pydantic import BaseModel\nfrom typing import List\n\nlogger = init_logger(__name__)\n\nllm_client: ByzerLLM = None\nopenai_serving_chat: OpenAIServingChat = None\nopenai_serving_completion: OpenAIServingCompletion = None\n\nTIMEOUT_KEEP_ALIVE = 5  # seconds\n# timeout in 10 minutes. Streaming can take longer than 3 min\nTIMEOUT = float(os.environ.get(\"BYZERLLM_APISERVER_HTTP_TIMEOUT\", 600))\n\nrouter_app = FastAPI()\n\n\n@router_app.get(\"/health\")\nasync def health() -> Response:\n    \"\"\"Health check.\"\"\"\n    return Response(status_code=200)\n\n\n@router_app.get(\"/v1/models\")\nasync def show_available_models():\n    models = await openai_serving_chat.show_available_models()\n    return JSONResponse(content=models.model_dump())\n\n\n@router_app.get(\"/version\")\nasync def show_version():\n    return JSONResponse(content={\"version\": version})\n\n\n@router_app.get(\"/v1/models\", response_model=ModelList)\nasync def models() -> ModelList:\n    \"\"\"Show available models. Right now we only have one model.\"\"\"\n    model_cards = [\n        ModelCard(\n            id=\"\",\n            root=\"\",\n            permission=[ModelPermission()]\n        )\n    ]\n    return ModelList(data=model_cards)\n\n\n@router_app.post(\"/v1/completions\")\nasync def create_completion(\n        body: CompletionRequest,\n        request: Request\n):\n    generator = await openai_serving_completion.create_completion(body, request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            status_code=generator.code\n        )\n    if body.stream:\n        return StreamingResponse(\n            content=generator,\n            media_type=\"text/event-stream\"\n        )\n    else:\n        return JSONResponse(content=generator.model_dump())\n\n\n@router_app.post(\"/v1/chat/completions\")\nasync def create_chat_completion(\n        body: ChatCompletionRequest,\n        request: Request,\n):\n    \"\"\"Completion API similar to OpenAI's API.\n\n    See  https://platform.openai.com/docs/api-reference/chat/create\n    for the API specification. This API mimics the OpenAI ChatCompletion API.\n\n    NOTE: Currently we do not support the following features:\n        - function_call (Users should implement this by themselves)\n        - logit_bias (to be supported by vLLM engine)\n    \"\"\"\n    # async with async_timeout.timeout(TIMEOUT):\n\n    generator = await openai_serving_chat.create_chat_completion(body, request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            status_code=generator.code\n        )\n    if body.stream:\n        return StreamingResponse(\n            content=generator,\n            media_type=\"text/event-stream\"\n        )\n    else:\n        return JSONResponse(content=generator.model_dump())\n\n\n@router_app.post(\"/v1/embeddings\")\nasync def embed(body: Embeddings):\n    \"\"\"Given a prompt, the model will return one embedding.\n\n    Returns:\n        A response object with an embedding.\n    \"\"\"\n    embedding_id = f\"embed-{random_uuid()}\"\n\n    results_list = llm_client.emb(body.model, request=LLMRequest(instruction=body.input))\n    tokens = 0\n\n    return EmbeddingsOutput(\n        data=[\n            EmbeddingsData(\n                embedding=results.output,\n                index=i,\n                object=\"embedding\",\n            )\n            for i, results in enumerate(results_list)\n        ],\n        id=embedding_id,\n        object=\"list\",\n        created=int(time.time()),\n        model=body.model,\n        usage=EmbeddingsUsage(\n            prompt_tokens=tokens,\n            total_tokens=tokens,\n        ),\n    )\n\nclass ServerArgs(BaseModel):\n    host: str = None\n    port: int = 8000\n    uvicorn_log_level: str = \"info\"\n    allow_credentials: bool = False\n    allowed_origins: List[str] = [\"*\"]  \n    allowed_methods: List[str] = [\"*\"]\n    allowed_headers: List[str] = [\"*\"]\n    api_key: str = None\n    served_model_name: str = None\n    prompt_template: str = None\n    response_role: str = \"assistant\"\n    ssl_keyfile: str = None\n    ssl_certfile: str = None    \n\ndef serve(llm:ByzerLLM, args: ServerArgs):\n    \n    logger.info(f\"ByzerLLM API server version {version}\")\n    logger.info(f\"args: {args}\")\n\n    router_app.add_middleware(\n        CORSMiddleware,\n        allow_origins=args.allowed_origins,\n        allow_credentials=args.allow_credentials,\n        allow_methods=args.allowed_methods,\n        allow_headers=args.allowed_headers,\n    )\n    \n    if token := os.environ.get(\"BYZERLLM_API_KEY\") or args.api_key:\n\n        @router_app.middleware(\"http\")\n        async def authentication(request: Request, call_next):\n            if not request.url.path.startswith(\"/v1\"):\n                return await call_next(request)\n            if request.headers.get(\"Authorization\") != \"Bearer \" + token:\n                return JSONResponse(\n                    content={\"error\": \"Unauthorized\"},\n                    status_code=401\n                )\n            return await call_next(request)\n\n    # Register labels for metrics\n    # add_global_metrics_labels(model_name=engine_args.model)\n    global llm_client\n    llm_client = llm\n    \n    global openai_serving_chat\n    openai_serving_chat = OpenAIServingChat(\n        llm_client=llm_client,\n        response_role=args.response_role,\n        server_model_name=args.served_model_name,\n        prompt_template=args.prompt_template\n    )\n    global openai_serving_completion\n    openai_serving_completion = OpenAIServingCompletion(\n        llm_client=llm_client,\n        server_model_name=args.served_model_name,\n        prompt_template=args.prompt_template\n    )\n\n    uvicorn.run(\n        router_app,\n        host=args.host,\n        port=args.port,\n        log_level=args.uvicorn_log_level,\n        timeout_keep_alive=TIMEOUT_KEEP_ALIVE,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile\n    )\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/generator.py\nfrom typing import Optional, Dict, Any, List\nfrom openai import OpenAI\nimport json\nfrom .types import RAGConfig, ByzerSQLConfig, ModelConfig\nfrom byzerllm.utils.client import code_utils\nimport byzerllm\nimport requests\nfrom loguru import logger\nimport traceback\nfrom byzerllm.utils.nontext import TagExtractor\n\n\nclass SQLGenerator:\n\n    @byzerllm.prompt()\n    def gen_context(self, conversations: List[Dict[str, str]]) -> str:\n        \"\"\"\n        对话内容：\n        {% for msg in conversations %}\n        <{{ msg.role }}>: {{ msg.content }}\n        {% endfor %}\n\n        根据提供的对话内容，理解用户最后的问题，然后提取可能会用到的 Byzer-SQL 相关的知识。\n        \"\"\"\n\n    @byzerllm.prompt()\n    def gen_sql(\n        self,\n        conversations: List[Dict[str, str]],\n        context: str,\n        schema: str,\n        model: str = \"deepseek_chat\",\n    ) -> str:\n        \"\"\"\n        相关知识：\n        {{ context }}\n\n        Schema信息：\n        {{ schema }}\n\n        对话内容：\n        {% for msg in conversations %}\n        <{{ msg.role }}>: {{ msg.content }}\n        {% endfor %}\n\n        模型UDF函数：\n        {{ model }}\n\n        根据提供的相关知识以及对话内容，理解用户最后的问题，然后根据看到的schema 信息，生成 Byzer-SQL 语句。\n        Byzer-SQL 首先需要用 load 语法加载 schema 中的路径映射成表格，然后根据表格的信息生成对应的 Byzer-SQL 语句。\n\n        特别需要关注的是:\n        1. 用户的很多需求是要对表格的文本内容做自然语言处理，你需要使用 Byzer-SQL 中的大模型辅助函数来完成这个需求。\n        2. 使用 `llm_result` 函数处理 LLM 模型函数的响应,`llm_result` 函数得到的是一段文本，虽然我们要求模型函数只输出特定的文本，但考虑到大模型会有一些额外的字符输出，不会只是输出我们要求的文字，所以通过 `like` 来进行匹配。\n        3. 注意 Byzer-SQL 的语法，比如select 语句最后一定需要有 as <table_name>。\n        3. 输出的 Byzer-SQL 语句务必使用 <_byzer_sql_></_byzer_sql_> 标签包裹起来。\n        \"\"\"\n\n    @byzerllm.prompt()\n    def fetch_schema(self, context: str) -> str:\n        \"\"\"\n        下面是一些基础知识，包括名词术语等等：\n        {{ context }}\n\n        当理解对话内容的时候，需要参考上面的基础知识，然后更好的理解对应的表格schema信息是不是有用。\n        \"\"\"\n\n    def __init__(\n        self,\n        llm: byzerllm.ByzerLLM,\n        rag_config: RAGConfig,\n        byzer_sql_config: ByzerSQLConfig,\n        model_config: ModelConfig,\n    ) -> None:\n        self.llm = llm\n        self.schema_rag_client = OpenAI(\n            api_key=rag_config.schema_rag_api_key,\n            base_url=rag_config.schema_rag_base_url,\n        )\n        self.context_rag_client = OpenAI(\n            api_key=rag_config.context_rag_api_key,\n            base_url=rag_config.context_rag_base_url,\n        )\n        self.byzer_sql_url = byzer_sql_config.byzer_sql_url\n\n        self.llm_model = byzerllm.ByzerLLM()\n        self.llm_model.setup_default_model_name(model_config.llm_model)\n        self.gen_sql_llm_model = byzerllm.ByzerLLM()\n        self.gen_sql_llm_model.setup_default_model_name(model_config.gen_sql_llm_model)\n\n    def stream_chat_oai(\n        self,\n        conversations,\n        model: Optional[str] = None,\n        role_mapping=None,\n        delta_mode=False,\n        llm_config: Dict[str, Any] = {},\n    ):\n        try:\n            v, c = self._stream_chat_oai(\n                conversations=conversations,\n                model=model,\n                role_mapping=role_mapping,\n                delta_mode=delta_mode,\n                llm_config=llm_config,\n            )\n        except Exception as e:\n            error_msg = f\"Error in stream_chat_oai: {e}\\n{traceback.format_exc()}\"\n            logger.error(error_msg)\n            v, c = [str(e)], []\n\n        return v, c\n\n    def _stream_chat_oai(\n        self,\n        conversations,\n        model: Optional[str] = None,\n        role_mapping=None,\n        delta_mode=False,\n        llm_config: Dict[str, Any] = {},\n    ):\n        logger.info(f\"Starting stream_chat_oai with model: {model}\")\n        logger.debug(f\"Conversations: {conversations}\")\n        logger.debug(f\"LLM Config: {llm_config}\")\n\n        if not model:\n            model = self.llm_model.default_model_name\n        logger.info(f\"Using model: {model}\")\n\n        query = conversations[-1][\"content\"]\n\n        if (\n            \"使用四到五个字直接返回这句话的简要主题，不要解释、不要标点、不要语气词、不要多余文本，不要加粗，如果没有主题\"\n            in query\n            or \"简要总结一下对话内容，用作后续的上下文提示 prompt，控制在 200 字以内\"\n            in query\n        ):\n            chunks = self.llm.stream_chat_oai(\n                conversations=conversations,\n                model=model,\n                role_mapping=role_mapping,\n                llm_config=llm_config,\n                delta_mode=True,\n            )\n            return (chunk[0] for chunk in chunks), []\n\n        logger.info(\"Fetching context from context RAG client\")\n        context_query = self.gen_context.prompt(conversations=conversations)\n        context_response = self.context_rag_client.chat.completions.create(\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": json.dumps(\n                        {\"query\": context_query, \"only_contexts\": True},\n                        ensure_ascii=False,\n                    ),\n                }\n            ],\n            model=model,\n        )\n\n        context = context_response.choices[0].message.content\n        new_context = \"\"\n        for msg in context.split(\"\\n\"):\n            if msg.strip():\n                v = json.loads(msg)\n                new_context += v[\"source_code\"]\n        logger.info(f\"context {new_context[0:100]}...\")\n        logger.info(\"Context retrieved successfully\")\n\n        logger.info(\"Fetching schemas from schema RAG client\")\n        schema_query = self.fetch_schema.prompt(context=new_context)\n\n        response = self.schema_rag_client.chat.completions.create(\n            messages=conversations[0:-1]\n            + [\n                {\n                    \"role\": \"user\",\n                    \"content\": json.dumps(\n                        {\"query\": schema_query, \"only_contexts\": True},\n                        ensure_ascii=False,\n                    ),\n                }\n            ],\n            model=model,\n        )\n        logger.debug(f\"Schema RAG response: {response}\")\n\n        schema_str = response.choices[0].message.content\n        logger.info(\"Schema retrieved successfully\")\n\n        logger.info(\"Generating SQL string\")\n        sql_str = self.gen_sql.with_llm(self.gen_sql_llm_model).run(\n            conversations=conversations,\n            context=new_context,\n            schema=schema_str,\n            model=model,\n        )\n\n        logger.info(\"SQL string generated\")\n        logger.debug(f\"Generated SQL string: {sql_str}\")\n\n        # Extract SQL code from tags\n        tag_extractor = TagExtractor(sql_str)\n        extracted_tags = tag_extractor.extract()\n        sql = extracted_tags.content[0].content\n        logger.info(f\"Extracted SQL: {sql}\")\n\n        logger.info(f\"Sending SQL to Byzer SQL URL: {self.byzer_sql_url}\")\n        try:\n            response = requests.post(\n                self.byzer_sql_url,\n                data={\n                    \"sql\": sql,\n                    \"owner\": \"hello\",\n                    \"sessionPerUser\": True,\n                    \"sessionPerRequest\": True,\n                    \"includeSchema\": True,\n                },\n            )\n            response.raise_for_status()\n            result = response.content.decode(\"utf-8\")\n            logger.info(\"SQL execution result decoded\")\n            logger.debug(f\"SQL execution result: {result}\")\n            return (item for item in [result]), []\n        except requests.exceptions.RequestException as e:\n            logger.error(f\"Failed to send SQL to Byzer SQL URL: {e}\")\n            if hasattr(e.response, \"content\"):\n                error_content = e.response.content.decode(\"utf-8\")\n                logger.error(f\"Server response: {error_content}\")\n            else:\n                logger.error(\"No server response available\")\n            raise Exception(f\"Byzer SQL request failed: {str(e)}\") from e\n\n\n##File: /Users/allwefantasy/projects/super_analysis/setup.py\nfrom setuptools import setup, find_packages\n\n# Read requirements from requirements.txt\nwith open('requirements.txt') as f:\n    requirements = f.read().splitlines()\n\nsetup(\n    name=\"super-analysis\",\n    version=\"0.1.0\",\n    packages=find_packages(where=\"src\"),\n    package_dir={\"\": \"src\"},\n    install_requires=requirements,\n    entry_points={\n        \"console_scripts\": [\n            \"super-analysis.serve=superanalysis.serve.cli:main\",\n            \"super-analysis.convert=superanalysis.serve.convert:main\",\n            \"superanalysis.api=superanalysis.serve.rest:main\",\n        ],\n    },\n    author=\"allwefantasy\",\n    author_email=\"allwefantasy@gmail.com\",\n    description=\"A package for super analysis with ByzerLLM\",\n    long_description=open(\"README.md\").read(),\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/yourusername/super-analysis\",\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n    ],\n    python_requires='>=3.6',\n)\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/convert.py\nimport os\nimport argparse\nimport pandas as pd\nimport hashlib\nfrom typing import List, Dict\n\ndef get_csv_files(directory: str) -> List[str]:\n    \"\"\"Recursively get all CSV files in the given directory.\"\"\"\n    csv_files = []\n    for root, _, files in os.walk(directory):\n        for file in files:\n            if file.endswith('.csv'):\n                csv_files.append(os.path.join(root, file))\n    return csv_files\n\ndef extract_schema(df: pd.DataFrame) -> Dict[str, str]:\n    \"\"\"Extract schema from pandas DataFrame.\"\"\"\n    schema = {}\n    for column in df.columns:\n        dtype = str(df[column].dtype)\n        if dtype == 'object':\n            schema[column] = 'STRING'\n        elif dtype.startswith('int'):\n            schema[column] = 'INTEGER'\n        elif dtype.startswith('float'):\n            schema[column] = 'FLOAT'\n        elif dtype == 'bool':\n            schema[column] = 'BOOLEAN'\n        elif dtype.startswith('datetime'):\n            schema[column] = 'TIMESTAMP'\n        else:\n            schema[column] = 'STRING'  # Default to STRING for unknown types\n    return schema\n\ndef generate_schema_text(schema: Dict[str, str], csv_file: str, df: pd.DataFrame, include_rows_num: int) -> str:\n    \"\"\"Generate schema text from the schema dictionary and include the CSV file name and sample data.\"\"\"\n    schema_lines = [f\"CSV File: {csv_file}\\n\"]\n    schema_lines.extend([f\"{field}: {dtype}\" for field, dtype in schema.items()])\n    \n    if include_rows_num > 0:\n        schema_lines.append(f\"\\nSample Data (first {include_rows_num} rows):\")\n        sample_data = df.head(include_rows_num).to_string(index=False)\n        schema_lines.append(sample_data)\n    \n    return \"\\n\".join(schema_lines)\n\ndef save_schema(schema_text: str, csv_file: str, output_dir: str) -> None:\n    \"\"\"Save schema text to a file in the output directory.\"\"\"\n    # Generate MD5 hash of the schema text\n    file_name = hashlib.md5(schema_text.encode()).hexdigest() + '.md'\n    output_path = os.path.join(output_dir, file_name)\n    \n    with open(output_path, 'w') as f:\n        f.write(schema_text)\n    print(f\"Schema for {csv_file} saved to {output_path}\")\n\ndef convert(data_dir: str, doc_dir: str, include_rows_num: int) -> None:\n    \"\"\"Main conversion function.\"\"\"\n    csv_files = get_csv_files(data_dir)\n    \n    for csv_file in csv_files:\n        print(f\"Processing {csv_file}\")\n        df = pd.read_csv(csv_file)\n        schema = extract_schema(df)\n        schema_text = generate_schema_text(schema, csv_file, df, include_rows_num)\n        save_schema(schema_text, csv_file, doc_dir)\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Convert CSV files to schema documents\")\n    parser.add_argument(\"--data_dir\", required=True, help=\"Directory containing CSV files\")\n    parser.add_argument(\"--doc_dir\", required=True, help=\"Output directory for schema documents\")\n    parser.add_argument(\"--include-rows-num\", type=int, default=0, help=\"Number of sample rows to include in the schema\")\n    \n    args = parser.parse_args()\n    \n    if not os.path.exists(args.doc_dir):\n        os.makedirs(args.doc_dir)\n    \n    convert(args.data_dir, args.doc_dir, args.include_rows_num)\n\nif __name__ == \"__main__\":\n    main()\n\n##File: /Users/allwefantasy/projects/super_analysis/README.md\n# Super Analysis\n\nSuper Analysis 支持 NLP2SQL 以及 使用 SQL 对表中的文本字段做自然语言分析。\n\n## 安装\n\n你可以使用 pip 安装 Super Analysis：\n\n```bash\ngit clone https://github.com/allwefantasy/super_analysis\ncd super-analysis\npip install -e .\n```\n\n## 使用方法\n\n安装完成后，你可以使用 `super-analysis.serve` 命令来启动 API 服务器：\n\n```bash\nsuper-analysis.serve --served-model-name your_model_name\n```\n\n### 命令行参数\n\n以下是可用的命令行参数：\n\n- `--host`: 服务器绑定的主机地址（默认：0.0.0.0）\n- `--port`: 服务器绑定的端口（默认：8000）\n- `--uvicorn-log-level`: Uvicorn 日志级别（默认：info）\n- `--allow-credentials`: 允许凭证（默认：False）\n- `--allowed-origins`: 允许的源（默认：[\"*\"]）\n- `--allowed-methods`: 允许的方法（默认：[\"*\"]）\n- `--allowed-headers`: 允许的头部（默认：[\"*\"]）\n- `--api-key`: 用于认证的 API 密钥\n- `--served-model-name`: 要服务的模型名称（必需）\n- `--prompt-template`: 提示模板\n- `--response-role`: 响应角色（默认：assistant）\n- `--ssl-keyfile`: SSL 密钥文件路径\n- `--ssl-certfile`: SSL 证书文件路径\n\n例如：\n\n```bash\nsuper-analysis.serve --served-model-name deepseek_chat --port 8000\n```\n\n\n## 抽取csv文件schema\n\nThe `super-analysis.convert` command allows you to convert CSV files to schema documents. This is useful for generating metadata about your CSV files that can be used for further analysis or documentation.\n\n#### Usage\n\n```\nsuper-analysis.convert --data_dir <path_to_csv_files> --doc_dir <output_directory> [--include-rows-num <number_of_sample_rows>]\n```\n\nArguments:\n- `--data_dir`: Directory containing CSV files (required)\n- `--doc_dir`: Output directory for schema documents (required)\n- `--include-rows-num`: Number of sample rows to include in the schema (optional, default is 0)\n\nThe utility will:\n1. Recursively find all CSV files in the specified directory\n2. Extract the schema from each CSV file\n3. Generate a schema text that includes:\n   - The CSV file name\n   - Field names and their data types\n   - Sample data (if `--include-rows-num` is specified)\n4. Save the schema text as a markdown file in the output directory\n\n## 示例数据\n\n下载电影数据集： https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset/download?datasetVersionNumber=7\n\n## 启动分析服务\n\n整个 super-analysis 服务分为三个部分：\n\n1. schema 文档知识库\n2. context 文档知识库\n4. byzer-sql 分布式执行引擎 (参考 [这个文档](./docs/4.3.1%20安装与配置%20Byzer-SQL.pdf) 完成部署)\n5. super-analysis 自身\n\n在启动 super-analysis 之前，你需要启动 schema 文档知识库和 context 文档知识库以及 byzer-sql。\n\n当 byzer-sql 部署完成后，注册账号为hello, 然后进入在 byzer-sql 的控制台中执行以下脚本：\n\n```sql\n!byzerllm setup single;\n\nrun command as LLM.`` where \naction=\"infer\"\nand reconnect=\"true\"\nand pretrainedModelType=\"saas/openai\"\nand udfName=\"deepseek_chat\";\n```\n\n现在按如下方式一次启动相关服务：\n\n```bash\n## 将电影数据集的csv文件转换为schema文档\nsuper-analysis.convert --data_dir /Users/allwefantasy/data/movice --doc_dir /Users/allwefantasy/data/movice/schemas/\n\n# 启动一个schema文档知识库\nauto-coder doc serve \\\n--model deepseek_chat --index_filter_workers 100 \\\n--tokenizer_path /Users/allwefantasy/Downloads/tokenizer.json \\\n--doc_dir /Users/allwefantasy/data/movice/schemas/ \\\n--port 8001\n\n# 启动一个 Byzer-SQL 以及其他需要的文档知识库\n# 下载 Byzer-SQL 专门为大模型优化的文档\ngit clone https://github.com/allwefantasy/llm_friendly_packages\n\n## 启动一个文档知识库\nauto-coder doc serve \\\n--model deepseek_chat --index_filter_workers 100 \\\n--tokenizer_path /Users/allwefantasy/Downloads/tokenizer.json \\\n--doc_dir  /Users/allwefantasy/projects/llm_friendly_packages/github.com/allwefantasy \\\n--port 8002\n\n# 启动兼容 OpenAI Server 的分析服务\nsuper-analysis.serve --served-model-name deepseek_chat --port 8000 \\\n--schema-rag-base-url http://127.0.0.1:8001/v1 \\\n--context-rag-base-url http://127.0.0.1:8002/v1 \\\n--byzer-sql-url http://127.0.0.1:9003/run/script\n```\n\n现在你可以用 OpenAI SDK 去使用了。 参考 [openai_local_api.ipynb](./notebooks/openai_local_api.ipynb)\n\n## 非 OpenAI 兼容接口\n\n除了兼容 OpenAI 的接口外，Super Analysis 还提供了一个简单的 REST API 接口，可以直接与模型进行交互。以下是如何使用这个接口：\n\n### 启动服务\n\n使用以下命令启动 REST API 服务：\n\n```bash\nsuperanalysis.api --schema-rag-base-url <schema_rag_url> \\\n                  --schema-rag-api-key <schema_rag_api_key> \\\n                  --context-rag-base-url <context_rag_url> \\\n                  --context-rag-api-key <context_rag_api_key> \\\n                  --byzer-sql-url <byzer_sql_url> \\\n                  --llm-model <llm_model_name> \\\n                  --gen-sql-llm-model <gen_sql_llm_model_name> \\\n                  --host <host> \\\n                  --port <port>\n```\n\n请替换尖括号中的值为实际的配置信息。\n\n### API 端点\n\n服务启动后，将提供以下 API 端点：\n\n#### POST /chat\n\n此端点用于与模型进行对话。\n\n请求体格式：\n\n```json\n{\n  \"conversations\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"你的问题或指令\"\n    }\n  ],\n  \"model\": \"可选的模型名称\"\n}\n```\n\n响应格式：\n\n```json\n{\n  \"response\": \"模型的回答\"\n}\n```\n\n### 示例使用\n\n以下是使用 Python 请求库与 API 交互的示例：\n\n```python\nimport requests\nimport json\n\nurl = \"http://localhost:8000/chat\"\npayload = {\n    \"conversations\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"你好，请介绍一下你自己。\"\n        }\n    ],\n    \"model\": \"your_model_name\"\n}\n\nresponse = requests.post(url, json=payload)\nresult = response.json()\n\nprint(result[\"response\"])\n```\n\n这个 API 相比兼容 OpenAI 接口，可以在请求中提供提供更多可选参数。\n\n## 联系方式\n\n如有任何问题或建议，请联系 allwefantasy@gmail.com。\n\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/rest.py\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nfrom typing import List, Optional\nimport uvicorn\nimport argparse\nfrom superanalysis.sql.generator import SQLGenerator\nfrom superanalysis.sql.types import RAGConfig, ByzerSQLConfig, ModelConfig\nimport byzerllm\n\napp = FastAPI()\n\nclass ChatMessage(BaseModel):\n    role: str\n    content: str\n\nclass ChatRequest(BaseModel):\n    conversations: List[ChatMessage]\n    model: Optional[str] = None\n\nclass ChatResponse(BaseModel):\n    response: str\n\n@app.post(\"/chat\", response_model=ChatResponse)\nasync def chat(request: ChatRequest):\n    try:\n        result, _ = sql_generator.stream_chat_oai(\n            conversations=[{\"role\": msg.role, \"content\": msg.content} for msg in request.conversations],\n            model=request.model\n        )\n        return ChatResponse(response=\"\".join(result))\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"SuperAnalysis API Server\")\n    parser.add_argument(\"--host\", type=str, default=\"0.0.0.0\", help=\"Host to bind the server to\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"Port to bind the server to\")\n    parser.add_argument(\"--schema-rag-base-url\", type=str, required=True, help=\"Schema RAG base URL\")\n    parser.add_argument(\"--schema-rag-api-key\", type=str, required=True, help=\"Schema RAG API key\")\n    parser.add_argument(\"--context-rag-base-url\", type=str, required=True, help=\"Context RAG base URL\")\n    parser.add_argument(\"--context-rag-api-key\", type=str, required=True, help=\"Context RAG API key\")\n    parser.add_argument(\"--byzer-sql-url\", type=str, required=True, help=\"Byzer SQL URL\")\n    parser.add_argument(\"--data-dir\", type=str, default=\"\", help=\"Data directory\")\n    parser.add_argument(\"--llm-model\", type=str, required=True, help=\"LLM model for general use\")\n    parser.add_argument(\"--gen-sql-llm-model\", type=str, required=True, help=\"LLM model for SQL generation\")\n    \n    args = parser.parse_args()\n\n    rag_config = RAGConfig(\n        schema_rag_base_url=args.schema_rag_base_url,\n        schema_rag_api_key=args.schema_rag_api_key,\n        context_rag_base_url=args.context_rag_base_url,\n        context_rag_api_key=args.context_rag_api_key\n    )\n\n    byzer_sql_config = ByzerSQLConfig(\n        byzer_sql_url=args.byzer_sql_url,\n        data_dir=args.data_dir\n    )\n\n    model_config = ModelConfig(\n        llm_model=args.llm_model,\n        gen_sql_llm_model=args.gen_sql_llm_model\n    )\n\n    llm = byzerllm.ByzerLLM()\n    llm.setup_default_model_name(args.llm_model)\n\n    global sql_generator\n    sql_generator = SQLGenerator(\n        llm=llm,\n        rag_config=rag_config,\n        byzer_sql_config=byzer_sql_config,\n        model_config=model_config\n    )\n\n    uvicorn.run(app, host=args.host, port=args.port)\n\nif __name__ == \"__main__\":\n    main()\n\n##File: .auto-coder/libs/llm_friendly_packages/github.com/allwefantasy/byzer-llm/README.md\n# byzerllm 大模型编程快速指南\n\n本文示例 [Notebook](../../notebooks/003_byzerllm_大模型编程快速指南.ipynb)\n\n## 安装\n\n```bash\npip install -U byzerllm\nray start --head\n```\n## 启动一个模型代理\n\nbyzerllm 支持私有化模型或者SaaS模型的部署。\n\n这里以 deepseek 官方API 为例：\n\n```bash\neasy-byzerllm deploy deepseek-chat --token xxxxx --alias deepseek_chat\n```\n\n或者跬基流动API:\n\n```bash\neasy-byzerllm deploy alibaba/Qwen1.5-110B-Chat --token xxxxx --alias qwen110b_chat\n```\n\n将上面的 API KEY 替换成你们自己的。\n\n如果你想部署私有化模型或者对接 Ollama 等更多需求，参考 [002_使用byzerllm进行模型部署.md](./002_使用byzerllm进行模型部署.md) 或者 [README.md](../../README.md)。\n\n之后，你就可以在代码里使用  deepseek_chat 或者 qwen110b_chat  访问模型了。\n\n## hello world\n\n来和我们的大模型打个招呼:\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\n@byzerllm.prompt(llm=llm)\ndef hello(q:str) ->str:\n    '''\n    你好, {{ q }}\n    '''\n\ns = hello(\"你是谁\")    \nprint(s)\n\n## 输出:\n## '你好！我是一个人工智能助手，专门设计来回答问题、提供信息和帮助解决问题。如果你有任何疑问或需要帮助，请随时告诉我。'\n```\n\n恭喜，你和大模型成功打了招呼！\n\n可以看到，我们通过 `@byzerllm.prompt` 装饰器，将一个方法转换成了一个大模型的调用，然后这个方法的主题是一段文本，文本中\n使用了 jinja2 模板语法，来获得方法的参数。当正常调用该方法时，实际上就发起了和大模型的交互，并且返回了大模型的结果。\n\n在 byzerllm 中，我们把这种方法称之为 prompt 函数。\n\n## 查看发送给大模型的prompt\n\n很多情况你可能需要调试，查看自己的 prompt 渲染后到底是什么样子的，这个时候你可以通过如下方式\n获取渲染后的prompt:\n\n```python\nhello.prompt(\"你是谁\")\n## '你好, 你是谁'\n```            \n\n## 动态换一个模型\n\n前面的 hello 方法在初始化的时候，我们使用了默认的模型 deepseek_chat，如果我们想换一个模型，可以这样做：\n\n```python\nhello.with_llm(llm).run(\"你是谁\")\n## '你好！我是一个人工智能助手，专门设计来回答问题、提供信息和帮助解决问题。如果你有任何疑问或需要帮助，请随时告诉我。'\n```\n\n通过 with_llm 你可以设置一个新的 llm 对象，然后调用 run 方法，就可以使用新的模型了。\n\n## 超长文本生成\n\n我们知道，大模型一次生成的长度其实是有限的，如果你想生成超长文本，你可能需手动的不断获得\n生成结果，然后把他转化为输入，然后再次生成，这样的方式是比较麻烦的。\n\nbyzerllm 提供了更加易用的 API :\n\n```python\nimport byzerllm\nfrom byzerllm import ByzerLLM\n\nllm = ByzerLLM.from_default_model(\"deepseek_chat\")\n\n@byzerllm.prompt()\ndef tell_story() -> str:\n    \"\"\"\n    讲一个100字的故事。    \n    \"\"\"\n\n\ns = (\n    tell_story.with_llm(llm)\n    .with_response_markers()\n    .options({\"llm_config\": {\"max_length\": 10}})\n    .run()\n)\nprint(s)\n\n## 从前，森林里住着一只聪明的小狐狸。一天，它发现了一块闪闪发光的宝石。小狐狸决定用这块宝石帮助森林里的动物们。它用宝石的光芒指引迷路的小鹿找到了回家的路，用宝石的温暖治愈了受伤的小鸟。从此，小狐狸成了森林里的英雄，动物们都感激它的善良和智慧。\n```\n\n实际核心部分就是这一行：\n\n```python\ntell_story.with_llm(llm)\n    .with_response_markers()    \n    .run()\n```\n\n我们只需要调用 `with_response_markers` 方法，系统就会自动的帮我们生成超长文本。\n在上面的案例中，我们通过\n\n```python\n.options({\"llm_config\": {\"max_length\": 10}})\n```\n\n认为的限制大模型一次交互最多只能输出10个字符，但是系统依然自动完成了远超过10个字符的文本生成。\n\n## 对象输出\n\n前面我们的例子都是返回字符串，但是我们也可以返回对象，这样我们就可以更加灵活的处理返回结果。\n\n```python\nimport pydantic \n\nclass Story(pydantic.BaseModel):\n    '''\n    故事\n    '''\n\n    title: str = pydantic.Field(description=\"故事的标题\")\n    body: str = pydantic.Field(description=\"故事主体\")\n\n@byzerllm.prompt()\ndef tell_story()->Story:\n    '''\n    讲一个100字的故事。    \n    '''\n\ns = tell_story.with_llm(llm).run()\nprint(isinstance(s, Story))\nprint(s.title)\n\n## True\n## 勇敢的小鸟\n```\n\n可以看到，我们很轻松的将输出转化为格式化输出。\n\n## 自定义字段抽取\n\n前面的结构化输出，其实会消耗更多token,还有一种更加精准的结构化输出方式。\n比如让大模型生成一个正则表达式，但实际上大模型很难准确只输出一个正则表达式，这个时候我们可以通过自定义抽取函数来获取我们想要的结果。\n\n\n```python\nfrom loguru import logger\nimport re\n\n@byzerllm.prompt()\ndef generate_regex_pattern(desc: str) -> str:\n    \"\"\"\n    根据下面的描述生成一个正则表达式，要符合python re.compile 库的要求。\n\n    {{ desc }}\n\n    最后生成的正则表达式要在<REGEX></REGEX>标签对里。\n    \"\"\"    \n\ndef extract_regex_pattern(regex_block: str) -> str:    \n    pattern = re.search(r\"<REGEX>(.*)</REGEX>\", regex_block, re.DOTALL)\n    if pattern is None:\n        logger.warning(\"No regex pattern found in the generated block:\\n {regex_block}\")\n        raise None\n    return pattern.group(1)\n\npattern = \"匹配一个邮箱地址\"\nv = generate_regex_pattern.with_llm(llm).with_extractor(extract_regex_pattern).run(desc=pattern)\nprint(v)\n## ^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\n```\n\n在上面的例子里，我们根据一句话生成一个正则表达式。我们通过 `with_extractor` 方法，传入了一个自定义的抽取函数，这个函数会在大模型生成结果后，对结果进行处理，然后返回我们想要的结果。\n\n我们在 prompt 明确说了，生成的结果要放到 `<REGEX></REGEX>` 标签对里，然后我们通过 extract_regex_pattern 函数，从结果中提取出了我们想要的正则表达式。\n\n## 在实例方法中使用大模型\n\n```python\nimport byzerllm\ndata = {\n    'name': 'Jane Doe',\n    'task_count': 3,\n    'tasks': [\n        {'name': 'Submit report', 'due_date': '2024-03-10'},\n        {'name': 'Finish project', 'due_date': '2024-03-15'},\n        {'name': 'Reply to emails', 'due_date': '2024-03-08'}\n    ]\n}\n\n\nclass RAG():\n    def __init__(self):        \n        self.llm = byzerllm.ByzerLLM()\n        self.llm.setup_template(model=\"deepseek_chat\",template=\"auto\")\n        self.llm.setup_default_model_name(\"deepseek_chat\")        \n    \n    @byzerllm.prompt(lambda self:self.llm)\n    def generate_answer(self,name,task_count,tasks)->str:\n        '''\n        Hello {{ name }},\n\n        This is a reminder that you have {{ task_count }} pending tasks:\n        {% for task in tasks %}\n        - Task: {{ task.name }} | Due: {{ task.due_date }}\n        {% endfor %}\n\n        Best regards,\n        Your Reminder System\n        '''        \n\nt = RAG()\n\nresponse = t.generate_answer(**data)\nprint(response)\n\n## 输出:\n## Hello Jane Doe,\n##I hope this message finds you well. I wanted to remind you of your 3 pending tasks to ensure you stay on track:\n## 1. **Submit report** - This task is due on **2024-03-10**. Please ensure that you allocat\n```\n\n这里我们给了个比较复杂的例子，但我们可以看到，给一个实例prompt方法和普通prompt 方法差异不大。\n唯一的区别是如果你希望在定义的时候就指定大模型，使用一个lambda函数返回实例的 llm 对象即可。\n\n```python\n@byzerllm.prompt(lambda self:self.llm)\n```\n\n你也可以不返回，在调用的时候通过 `with_llm` 方法指定 llm 对象。\n\n此外，这个例子也展示了如何通过jinja2模板语法，来处理复杂的结构化数据。\n\n## 通过 Python 代码处理复杂入参\n\n上面的一个例子中，我们通过 jinja2 模板语法，来处理复杂的结构化数据，但是有时候我们可能需要更加复杂的处理，这个时候我们可以通过 Python 代码来处理。\n\n```python\nimport byzerllm\n\ndata = {\n    'name': 'Jane Doe',\n    'task_count': 3,\n    'tasks': [\n        {'name': 'Submit report', 'due_date': '2024-03-10'},\n        {'name': 'Finish project', 'due_date': '2024-03-15'},\n        {'name': 'Reply to emails', 'due_date': '2024-03-08'}\n    ]\n}\n\n\nclass RAG():\n    def __init__(self):        \n        self.llm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n    \n    @byzerllm.prompt()\n    def generate_answer(self,name,task_count,tasks)->str:\n        '''\n        Hello {{ name }},\n\n        This is a reminder that you have {{ task_count }} pending tasks:\n            \n        {{ tasks }}\n\n        Best regards,\n        Your Reminder System\n        '''\n        \n        tasks_str = \"\\n\".join([f\"- Task: {task['name']} | Due: { task['due_date'] }\" for task in tasks])\n        return {\"tasks\": tasks_str}\n\nt = RAG()\n\nresponse = t.generate_answer.with_llm(t.llm).run(**data)\nprint(response)\n\n## Just a gentle nudge to keep you on track with your pending tasks. Here's a quick recap:....\n```\n\n在这个例子里，我们直接把 tasks 在方法体里进行处理，然后作为一个字符串返回，最够构建一个字典，字典的key为 tasks,然后\n你就可以在 docstring 里使用 `{{ tasks }}` 来引用这个字符串。\n\n这样对于很复杂的入参，就不用谢繁琐的 jinja2 模板语法了。\n\n## 如何自动实现一个方法\n\n比如我定义一个签名，但是我不想自己实现里面的逻辑，让大模型来实现。这个在 byzerllm 中叫 function impl。我们来看看怎么\n实现:\n\n```python\nimport pydantic\nclass Time(pydantic.BaseModel):\n    time: str = pydantic.Field(...,description=\"时间，时间格式为 yyyy-MM-dd\")\n\n\n@llm.impl()\ndef calculate_current_time()->Time:\n    '''\n    计算当前时间\n    '''\n    pass \n\n\ncalculate_current_time()\n#output: Time(time='2024-06-14')\n```\n\n在这个例子里，我们定义了一个 calculate_current_time 方法，但是我们没有实现里面的逻辑，我们通过 `@llm.impl()` 装饰器，让大模型来实现这个方法。\n为了避免每次都要“生成”这个方法，导致无法适用，我们提供了缓存，用户可以按如下方式打印速度：\n\n```python\nstart = time.monotonic()\ncalculate_current_time()\nprint(f\"first time cost: {time.monotonic()-start}\")\n\nstart = time.monotonic()\ncalculate_current_time()\nprint(f\"second time cost: {time.monotonic()-start}\")\n\n# output:\n# first time cost: 6.067266260739416\n# second time cost: 4.347506910562515e-05\n```\n可以看到，第一次执行花费了6s,第二次几乎是瞬间完成的，这是因为第一次执行的时候，我们实际上是在生成这个方法，第二次执行的时候，我们是执行已经生成好的代码，所以速度会非常快。你可以显示的调用 `llm.clear_impl_cache()` 清理掉函数缓存。\n\n## Stream 模式\n\n前面的例子都是一次性生成结果，但是有时候我们可能需要一个流式的输出，这个时候我们可能需要用底层一点的API来完成了：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\nv = llm.stream_chat_oai(model=\"deepseek_chat\",conversations=[{\n    \"role\":\"user\",\n    \"content\":\"你好，你是谁\",\n}],delta_mode=True)\n\nfor t in v:\n    print(t,flush=True)  \n\n# 你好\n# ！\n# 我\n# 是一个\n# 人工智能\n# 助手\n# ，\n# 旨在\n# 提供\n# 信息\n# 、\n# 解答\n# 问题....\n```\n\n如果你不想要流式输出，但是想用底层一点的API，你可以使用 `llm.chat_oai` 方法：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\nv = llm.chat_oai(model=\"deepseek_chat\",conversations=[{\n    \"role\":\"user\",\n    \"content\":\"你好，你是谁\",\n}])\n\nprint(v[0].output)\n## 你好！我是一个人工智能助手，旨在提供信息、解答问题和帮助用户解决问题。如果你有任何问题或需要帮助，请随时告诉我。\n```\n\n## Function Calling \n\nbyzerllm 可以不依赖模型自身就能提供 function calling 支持，我们来看个例子：\n\n\n```python\nfrom typing import List,Dict,Any,Annotated\nimport pydantic \nimport datetime\nfrom dateutil.relativedelta import relativedelta\n\ndef compute_date_range(count:Annotated[int,\"时间跨度，数值类型\"],\n                       unit:Annotated[str,\"时间单位，字符串类型\",{\"enum\":[\"day\",\"week\",\"month\",\"year\"]}])->List[str]:\n    '''\n    计算日期范围\n\n    Args:\n        count: 时间跨度，数值类型\n        unit: 时间单位，字符串类型，可选值为 day,week,month,year\n    '''        \n    now = datetime.datetime.now()\n    now_str = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n    if unit == \"day\":\n        return [(now - relativedelta(days=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"week\":\n        return [(now - relativedelta(weeks=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"month\":\n        return [(now - relativedelta(months=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"year\":\n        return [(now - relativedelta(years=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    return [\"\",\"\"]\n\ndef compute_now()->str:\n    '''\n    计算当前时间\n    '''\n    return datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n```\n\n我们定义了两个方法，一个是计算日期范围，一个是计算当前时间。\n\n现在我么可以来测试下，系统如何根据自然语言决定调用哪个方法：\n\n```python\nt = llm.chat_oai([{\n    \"content\":'''计算当前时间''',\n    \"role\":\"user\"    \n}],tools=[compute_date_range,compute_now],execute_tool=True)\n\nt[0].values\n\n## output: ['2024-06-14 15:18:02']\n```\n\n我们可以看到，他正确的选择了 compute_now 方法。\n\n接着我们再试一个：\n\n```python\nt = llm.chat_oai([{\n    \"content\":'''最近三个月趋势''',\n    \"role\":\"user\"    \n}],tools=[compute_date_range,compute_now],execute_tool=True)\n\nt[0].values\n\n## output: [['2024-03-14 15:19:13', '2024-06-14 15:19:13']]\n```\n\n模型正确的选择了 compute_date_range 方法。\n\n## 多模态\n\nbyerllm 也能很好的支持多模态的交互，而且统一了多模态大模型的接口，比如你可以用一样的方式使用 openai 或者 claude 的图片转文字能力， 或者一致的方式使用火山，azuer, openai的语音合成接口。\n\n### 图生文\n\n```python\nimport byzerllm\nfrom byzerllm.types import ImagePath\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\n\n\n@byzerllm.prompt()\ndef what_in_image(image_path: ImagePath) -> str:\n    \"\"\"\n    {{ image_path }}\n    这个图片里有什么？\n    \"\"\"    \n\n\nv = what_in_image.with_llm(vl_llm).run(\n    ImagePath(value=\"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\")\n)\nv\n## OUTPUT: 这张图片展示了多只可爱的猫咪，采用了艺术风格的绘画。猫咪们有不同的颜色和花纹，背景是浅棕色，上面还点缀着一些红色的花朵。整体画面给人一种温馨和谐的感觉\n```\n\n可以看到，我们只需要把 prompt 函数的图片地址入参使用 byzerllm.types.ImagePath里进行包装，就可以直接在 prompt 函数体里\n带上图片。\n\n或者你可以这样：\n\n```python\nimport byzerllm\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\n\n\n@byzerllm.prompt()\ndef what_in_image(image_path: str) -> str:\n    \"\"\"\n    {{ image }}\n    这个图片里有什么？\n    \"\"\"\n    return {\"image\": byzerllm.Image.load_image_from_path(image_path)}\n\n\nv = what_in_image.with_llm(vl_llm).run(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv\n```\n\n通过 `image_path` 参数，然后通过 `byzerllm.Image.load_image_from_path` 方法，转化为一个图片对象 image，最后在 prompt 函数体里\n使用 `{{ image }}` 引用这个图片对象。\n\n另外我们也是可以支持配置多张图片的。\n\n另外，我们也可以使用基础的 `llm.chat_oai` 方法来实现：\n\n```python\nimport byzerllm\nimport json\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\nimage = byzerllm.Image.load_image_from_path(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv = vl_llm.chat_oai(\n    conversations=[\n        {\n            \"role\": \"user\",\n            \"content\": json.dumps(\n                [{\"image\": image, \"text\": \"这个图片里有什么？\"}], ensure_ascii=False\n            ),\n        }\n    ]\n)\nv[0].output\n```\n\n还可以这么写：\n\n```python\nimport byzerllm\nimport json\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\nimage = byzerllm.Image.load_image_from_path(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv = vl_llm.chat_oai(\n    conversations=[\n        {\n            \"role\": \"user\",\n            \"content\": json.dumps(\n                [\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\"url\": image, \"detail\": \"high\"},\n                    },\n                    {\"text\": \"这个图片里有什么？\", \"type\": \"text\"},\n                ],\n                ensure_ascii=False,\n            ),\n        }\n    ]\n)\nv[0].output\n```\n\n### 语音合成\n\n这里以 openai 的 tts 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=tts-1 \\\n--model openai_tts\n```\n\n此外，byzerllm 支持 azure,火山引擎等 tts 语音合成引擎。\n\n接着你可以这么用：\n\n```python\nimport byzerllm\nimport base64\nimport json\n\nllm = byzerllm.ByzerLLM.from_default_model(\"openai_tts\")\n\n\nt = llm.chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\": json.dumps({\n        \"input\":\"hello, open_tts\",\n        \"voice\": \"alloy\",\n        \"response_format\": \"mp3\"\n    },ensure_ascii=False)\n}])\n\nwith open(\"voice.mp3\",\"wb\") as f:\n    f.write(base64.b64decode(t[0].output))\n```\n\ntts 模型生成没有prompt函数可以用，你需要直接使用 chat_oai。\n\n\n### 语音识别\n\n这里以 openai 的 whisper-1 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--worker_concurrency 10 \\\n--infer_params saas.model=whisper-1 saas.api_key=${MODEL_OPENAI_TOKEN} \\\n--model speech_to_text\n```\n\n语音识别的使用方式和图生文类似，我们可以直接在 prompt 函数体里带上音频文件。\n\n```python\nimport byzerllm\nimport json\nimport base64\nfrom byzerllm.types import AudioPath\n\nllm = byzerllm.ByzerLLM.from_default_model(\"speech_to_text\")\n\naudio_file = \"/Users/allwefantasy/videos/output_audio.mp3\"\n\n@byzerllm.prompt(llm=llm)\ndef audio_to_text(audio_file: AudioPath):\n    \"\"\"\n    {{ audio_file }}\n    \"\"\"\n\nv = audio_to_text(AudioPath(value=audio_file))\njson.loads(v)\n```\n输出的数据格式略微复杂：\n\n```\n{'text': 'In the last chapter, you and I started to step through the internal workings of a transformer. This is one of the key pieces of technology inside large language models, and a lot of other tools in the modern wave of AI.',\n 'task': 'transcribe',\n 'language': 'english',\n 'duration': 10.0,\n 'segments': [{'id': 0,\n   'seek': 0,\n   'start': 0.0,\n   'end': 4.78000020980835,\n   'text': ' In the last chapter, you and I started to step through the internal workings of a transformer.',\n   'tokens': [50364,\n    682,\n    264,\n    1036,\n    7187,\n    11,\n    291,\n    293,\n    286,\n    1409,\n    281,\n    1823,\n    807,\n    264,\n    6920,\n    589,\n    1109,\n    295,\n    257,\n    31782,\n    13,\n    50586],\n   'temperature': 0.0,\n   'avg_logprob': -0.28872039914131165,\n   'compression_ratio': 1.4220778942108154,\n   'no_speech_prob': 0.016033057123422623},\n  ....\n  {'id': 2,\n   'seek': 0,\n   'start': 8.579999923706055,\n   'end': 9.979999542236328,\n   'text': ' and a lot of other tools in the modern wave of AI.',\n   'tokens': [50759,\n    293,\n    257,\n    688,\n    295,\n    661,\n    3873,\n    294,\n    264,\n    4363,\n    5772,\n    295,\n    7318,\n    13,\n    50867],\n   'temperature': 0.0,\n   'avg_logprob': -0.28872039914131165,\n   'compression_ratio': 1.4220778942108154,\n   'no_speech_prob': 0.016033057123422623}],\n 'words': [{'word': 'In', 'start': 0.0, 'end': 0.18000000715255737},\n  {'word': 'the', 'start': 0.18000000715255737, 'end': 0.23999999463558197},\n  {'word': 'last', 'start': 0.23999999463558197, 'end': 0.5400000214576721},\n  {'word': 'chapter', 'start': 0.5400000214576721, 'end': 0.800000011920929},\n  ....\n  {'word': 'AI', 'start': 9.920000076293945, 'end': 9.979999542236328}]}\n```\n\n会输出每一句话以及每一个字所在的起始时间和截止时间。你可以根据需要来使用。\n\n\n### 文生图\n\n文生图和语音合成类似，首先要启动合适的模型,以openai 的 dall-e-3 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=dall-e-3 \\\n--model openai_image_gen\n```\n\n启动模型后，只需要记住几个模板参数即可使用，这里直接使用 chat_oai 方法来使用：\n\n```python\n\nimport byzerllm\nimport json\nimport base64\n\nllm = byzerllm.ByzerLLM.from_default_model(\"openai_image_gen\")\nt = llm.chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\": json.dumps({\n        \"input\":\"a white siamese cat\",\n        \"size\": \"1024x1024\",\n        \"quality\": \"standard\"\n    },ensure_ascii=False)\n}])\n\nwith open(\"output1.jpg\",\"wb\") as f:\n    f.write(base64.b64decode(t[0].output))\n\n\nimport matplotlib.pyplot as plt\n\nimage_path = \"output1.jpg\"\nimage = plt.imread(image_path)\n\nplt.imshow(image)\nplt.axis('off')\nplt.show()\n```\n\n## Prompt 函数的流式输出\n\nbyzerllm 底层支持流式输出，非 prompt 函数的用法是这样的：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\n\nv = llm.stream_chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\":\"讲一个100字的故事\"\n}])\n\nfor s  in v:\n    print(s[0], end=\"\")\n```\n\n如果你像用 prompt 函数，可以这么用：\n\n\n```python\nimport byzerllm\nimport json\nimport base64\nfrom typing import Generator\n\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\n\n@byzerllm.prompt()\ndef tell_story() -> Generator[str, None, None]:\n    '''\n    给我讲一个一百多字的故事\n    '''\n\nv = tell_story.with_llm(llm).run()    \nfor i in v:\n    print(i, end=\"\")\n```\n\n可以看到，和普通的 prompt 函数的区别在于，返回值是一个生成器，然后你可以通过 for 循环来获取结果。\n\n## 向量化模型\n\nbyzerllm 支持向量化模型,你可以这样启动一个本地的模型：\n\n```bash\n!byzerllm deploy --pretrained_model_type custom/bge \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--worker_concurrency 10 \\\n--model_path /home/winubuntu/.auto-coder/storage/models/AI-ModelScope/bge-large-zh \\\n--infer_backend transformers \\\n--num_workers 1 \\\n--model emb\n```\n\n注意两个参数:\n\n1. --infer_backend transformers: 表示使用 transformers 作为推理后端。\n2. --model_path: 表示模型的路径。\n\n也可以启动一个 SaaS 的emb模型,比如 qwen 的 emb 模型：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/qianwen \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 2 \\\n--infer_params saas.api_key=${MODEL_QIANWEN_TOKEN}  saas.model=text-embedding-v2 \\\n--model qianwen_emb\n```\n\n或者 openai 的 emb 模型：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--worker_concurrency 10 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=text-embedding-3-small \\\n--model gpt_emb\n```\nSaaS 模型无需配置 `--infer_backend` 参数。\n\n无论是本地模型还是 SaaS 模型，我们都可以这样使用：\n\n```python\nimport byzerllm\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\nllm.setup_default_emb_model_name(\"emb\")\nllm.emb_query(\"你好\")\n```\n\n如果你配置 byzerllm 中的 Storage 使用，比如你这样启动了存储：\n\n```bash\nbyzerllm storage start --enable_emb\n```\n\n那么需要这么使用：\n\n```python\nfrom byzerllm.apps.byzer_storage.simple_api import ByzerStorage, DataType, FieldOption,SortOption\nstorage = ByzerStorage(\"byzerai_store\", \"memory\", \"memory\")\nstorage.emb(\"你好\")\n```\n\n## 一些辅助工具\n\n当调用 prompt 函数返回字符串的时候，如果想从里面抽取代码，可以使用如下方式：\n\n```python\nfrom byzerllm.utils.client import code_utils\ntext_with_markdown = '''\n```shell\nls -l\n```\n'''\ncode_blocks = code_utils.extract_code(text_with_markdown)\nfor code_block in code_blocks:\n    if code_block[0] == \"shell\":\n        print(code_block[1])\n##output: ls -l        \n```\n\nTagExtractor 工具用于对任意 `<_tag_></_tag_>` 标签对的抽取，下面是一个使用示例：\n\n```python\nfrom byzerllm.apps.utils import TagExtractor\n\nextractor = TagExtractor('''\n大家好\n<_image_>data:image/jpeg;base64,xxxxxx</_image_>\n                         大家好\n<_image_>data:image/jpeg;base64,xxxxxx2</_image_>\n''')\n\nv = extractor.extract()\nprint(v.content[0].content)\nprint(v.content[1].content)\n```\n输出为:\n\n```\ndata:image/jpeg;base64,xxxxxx\ndata:image/jpeg;base64,xxxxxx2\n```\n\n我们成功的将 <_image_></_image_> 标签对里的内容抽取出来了。\n\n\n## 注意事项\n\n1. prompt函数方法体返回只能是dict，实际的返回类型和方法签名可以不一样，但是方法体返回只能是dict。\n2. 大部分情况prompt函数体为空，如果一定要有方法体，可以返回一个空字典。\n3. 调用prompt方法的时候，如果在@byzerllm.prompt()里没有指定llm对象，那么需要在调用的时候通过with_llm方法指定llm对象。\n\n===========================\n\n# Byzer Storage: 最易用的AI存储引擎\n\nByzer Storage是一个为AI应用设计的高性能存储引擎,它提供了简单易用的API,支持向量搜索、全文检索以及结构化查询。本文将详细介绍Byzer Storage的使用方法和主要特性。\n\n## 0. 安装和启动\n\n```bash\npip install byzerllm\nbyzerllm storage start\n```\n\nThat's it! Byzer Storage已经安装并启动成功,现在我们可以开始使用它了。默认会启动一个 byzerai_store 的集群。\n\n注意，如果你是这样启动的：\n\n```bash\nbyzerllm storage start --enable_emb\n```\n\n那么会自动启动一个 emb 模型，名字就叫 emb， ByzerStorage 会自动使用该模型，无需做任何其他配置。\n\n## 1. 初始化\n\n创建一个 ByzerStorage 对象，链接 byzerai_store 集群，并且指定数据库和表名（可以不存在）。\n\n```python\nfrom byzerllm.apps.byzer_storage.simple_api import ByzerStorage, DataType, FieldOption, SortOption\n\nstorage = ByzerStorage(\"byzerai_store\", \"my_database1\", \"my_table4s\")\n```\n\n\n## 2. 创建库表（可选）\n\nByzer Storage使用Schema来定义数据结构。我们可以使用SchemaBuilder来构建Schema:\n\n```python\n_ = (\n    storage.schema_builder()\n    .add_field(\"_id\", DataType.STRING)\n    .add_field(\"name\", DataType.STRING)\n    .add_field(\"content\", DataType.STRING, [FieldOption.ANALYZE])\n    .add_field(\"raw_content\", DataType.STRING, [FieldOption.NO_INDEX])    \n    .add_array_field(\"summary\", DataType.FLOAT)    \n    .add_field(\"created_time\", DataType.LONG, [FieldOption.SORT])    \n    .execute()\n)\n```\n\n这个Schema定义了以下字段:\n- `_id`: 字符串类型的主键\n- `name`: 字符串类型,可用于过滤条件\n- `content`: 字符串类型,带有ANALYZE选项,用于全文搜索\n- `raw_content`: 字符串类型,带有NO_INDEX选项,不会被索引\n- `summary`: 浮点数组类型,用于存储向量\n- `created_time`: 长整型,带有SORT选项,可用于排序\n\n需要注意的是：\n\n1. 如果一个字段带有ANALYZE选项,则该字段会被分词,并且可以用于全文搜索，但是就无法返回原始的文本了。所以你需要添加一个新字段专门用来存储原文，比如在我们这个例子里，我们新增了 raw_content 字段，并且显示的指定了 NO_INDEX 选项，这样就不会被索引，也就不会被分词，可以后续被检索后用来获取原始文本。\n2. 对于需要作为向量检索和存储的字段，需要指定为数组类型，比如我们这个例子里的 summary 字段。\n3. 如果你需要拿到向量字段的原始文本，那么你也需要添加一个新字段专门用来存储原文，就像我们这个例子里的 raw_content 字段一样。\n\n\n## 3. 写入数据\n\n准备数据并使用WriteBuilder写入Storage:\n\n```python\ndata = [\n    {\"_id\": \"1\", \"name\": \"Hello\", \"content\": \"Hello, world!\", \"raw_content\": \"Hello, world!\", \"summary\": \"hello world\", \"created_time\": 1612137600},\n    {\"_id\": \"2\", \"name\": \"Byzer\", \"content\": \"Byzer, world!\", \"raw_content\": \"Byzer, world!\", \"summary\": \"byzer\", \"created_time\": 1612137601},\n    {\"_id\": \"3\", \"name\": \"AI\", \"content\": \"AI, world!\", \"raw_content\": \"AI, world!\", \"summary\": \"AI\", \"created_time\": 16121376002},\n    {\"_id\": \"4\", \"name\": \"ByzerAI\", \"content\": \"ByzerAI, world!\", \"raw_content\": \"ByzerAI, world!\", \"summary\": \"ByzerAi\", \"created_time\": 16121376003},\n]\n\nstorage.write_builder().add_items(data, vector_fields=[\"summary\"], search_fields=[\"content\"]).execute()\nstorage.commit()\n```\n\n这里我们使用`add_items`方法批量添加数据,并指定了`summary`为向量字段,`content`为搜索字段。最后调用`commit()`来确保数据被持久化。\n\n\n从上面我们要写入到 byzer storage 的数据我们可以看到如下几个特点：\n\n1. 需要包含我们之前定义的 Schema中罗列的所有的字段，同时需要指定哪些是向量字段，哪些是检索字段。\n2. 向量和检索不能是同一个字段。\n3. 对于向量，检索字段，我们给到 write_builder 的都是文本，ByzerStorage 会根据 Schema 的定义自动将其转换为向量，检索字段需要的格式。\n\n## 4. 查询数据\n\nByzer Storage支持多种查询方式,包括向量搜索、全文检索、过滤和排序。\n\n### 4.1 向量搜索 + 全文检索\n\n```python\nquery = storage.query_builder()\nquery.set_vector_query(\"ByzerAI\", fields=[\"summary\"])\nresults = query.set_search_query(\"Hello\", fields=[\"content\"]).execute()\nprint(results)\n```\n\n这个查询结合了向量搜索和全文检索,它会在`summary`字段中搜索与\"ByzerAI\"相似的向量,同时在`content`字段中搜索包含\"Hello\"的文档。\n\n### 4.2 过滤 + 向量搜索 + 全文检索\n\n```python\nquery = storage.query_builder()\nquery.and_filter().add_condition(\"name\", \"AI\").build()\nquery.set_vector_query(\"ByzerAI\", fields=\"summary\")\nresults = query.set_search_query(\"Hello\", fields=[\"content\"]).execute()\nprint(results)\n```\n\n这个查询首先过滤`name`字段等于\"AI\"的文档,然后在结果中进行向量搜索和全文检索。\n\n### 4.3 过滤 + 排序\n\n```python\nquery = storage.query_builder()\nquery.and_filter().add_condition(\"name\", \"AI\").build().sort(\"created_time\", SortOption.DESC)\nresults = query.execute()\nprint(results)\n```\n\n这个查询过滤`name`字段等于\"AI\"的文档,然后按`created_time`字段降序排序。\n\n## 5. 删除数据\n\n### 5.1 根据ID删除\n\n```python\nstorage.delete_by_ids([\"3\"])\n\nquery = storage.query_builder()\nquery.and_filter().add_condition(\"name\", \"AI\").build()\nresults = query.execute()\nprint(results)\n```\n\n这里我们删除了ID为\"3\"的文档,然后查询验证删除结果。\n\n### 5.2 删除整个表\n\n```python\nstorage.drop()\n```\n\n这个操作会删除整个表及其所有数据,请谨慎使用。\n\n## 结论\n\nByzer Storage提供了一套简洁而强大的API,能够轻松实现向量搜索、全文检索、结构化查询等功能。它的设计非常适合AI应用场景,可以有效地存储和检索各种类型的数据。通过本文的介绍,相信读者已经对Byzer Storage有了基本的了解,并能够开始在自己的项目中使用这个强大的存储引擎。\n\n\n     \n\n下面是用户的需求：\n\n@@ByzerSQLConfig 新增一个 owner 参数, 同步在 rest.py 和 cli.py中新增对应的命令行参数\n"}, {"role": "assistant", "content": "好的，我会按照您的要求修改 `ByzerSQLConfig`，并在 `rest.py` 和 `cli.py` 中添加相应的命令行参数。以下是需要修改的文件和对应的修改内容：\n\n1. 首先修改 `types.py` 文件：\n\n```python\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/types.py\n<<<<<<< SEARCH\nclass ByzerSQLConfig(BaseModel):\n    byzer_sql_url: str = \"\"\n    data_dir: str = \"\"\n=======\nclass ByzerSQLConfig(BaseModel):\n    byzer_sql_url: str = \"\"\n    data_dir: str = \"\"\n    owner: str = \"\"\n>>>>>>> REPLACE\n```\n\n2. 接下来修改 `rest.py` 文件：\n\n```python\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/rest.py\n<<<<<<< SEARCH\n    parser.add_argument(\"--byzer-sql-url\", type=str, required=True, help=\"Byzer SQL URL\")\n    parser.add_argument(\"--data-dir\", type=str, default=\"\", help=\"Data directory\")\n=======\n    parser.add_argument(\"--byzer-sql-url\", type=str, required=True, help=\"Byzer SQL URL\")\n    parser.add_argument(\"--data-dir\", type=str, default=\"\", help=\"Data directory\")\n    parser.add_argument(\"--owner\", type=str, default=\"\", help=\"Owner of the Byzer SQL session\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/rest.py\n<<<<<<< SEARCH\n    byzer_sql_config = ByzerSQLConfig(\n        byzer_sql_url=args.byzer_sql_url,\n        data_dir=args.data_dir\n    )\n=======\n    byzer_sql_config = ByzerSQLConfig(\n        byzer_sql_url=args.byzer_sql_url,\n        data_dir=args.data_dir,\n        owner=args.owner\n    )\n>>>>>>> REPLACE\n```\n\n3. 最后修改 `cli.py` 文件：\n\n```python\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/cli.py\n<<<<<<< SEARCH\n    parser.add_argument(\"--byzer-sql-url\", type=str, default=\"\", help=\"Byzer SQL URL\")\n    parser.add_argument(\"--data-dir\", type=str, default=\"\", help=\"Data directory\")\n=======\n    parser.add_argument(\"--byzer-sql-url\", type=str, default=\"\", help=\"Byzer SQL URL\")\n    parser.add_argument(\"--data-dir\", type=str, default=\"\", help=\"Data directory\")\n    parser.add_argument(\"--owner\", type=str, default=\"\", help=\"Owner of the Byzer SQL session\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/cli.py\n<<<<<<< SEARCH\n    byzer_sql_config = ByzerSQLConfig(\n        byzer_sql_url=args.byzer_sql_url, data_dir=args.data_dir\n    )\n=======\n    byzer_sql_config = ByzerSQLConfig(\n        byzer_sql_url=args.byzer_sql_url, data_dir=args.data_dir, owner=args.owner\n    )\n>>>>>>> REPLACE\n```\n\n这些修改将在 `ByzerSQLConfig` 中添加 `owner` 参数，并在 `rest.py` 和 `cli.py` 中添加相应的命令行参数。现在，用户可以通过命令行指定 `owner` 参数，该参数将被传递给 `ByzerSQLConfig`。"}], "model": "human", "yaml_file": "actions/137_chat_action.yml"}
{"instruction": "生成一个makefile 可以将整个项目打包成一个wheel 包", "conversations": [{"role": "user", "content": "\n如果你需要生成代码，对于每个需要更改的文件,你需要按 *SEARCH/REPLACE block* 的格式进行生成。\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The opening fence and code language, eg: ```python\n2. The file path alone on a line, starting with \"##File:\" and verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n*SEARCH/REPLACE* blocks will replace *all* matching occurrences.\nInclude enough lines to make the SEARCH blocks unique.\n\nInclude *ALL* the code being searched and replaced!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n下面我们来看一个例子：\n\n当前项目目录结构：\n1. 项目根目录： /tmp/projects/mathweb\n2. 项目子目录/文件列表(类似tree 命令输出)\nflask/\n    app.py\n    templates/\n        index.html\n    static/\n        style.css\n\n用户需求： Change get_factorial() to use math.factorial\n\n回答： To make this change we need to modify `/tmp/projects/mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n用户需求： Refactor hello() into its own file.\n\n回答：To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/hello.py\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/main.py\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n现在让我们开始一个新的任务:\n\n当前项目目录结构：\n1. 项目根目录： /Users/allwefantasy/projects/super_analysis\n2. 项目子目录/文件列表(类似tree 命令输出)：\nsetup.py\n.auto-coder/\n    libs/\n        llm_friendly_packages/\n            src/\n                example1/\n                    main.py\nsrc/\n    superanalysis/\n        serve/\n            rest.py\n            convert.py\n            openai_serve.py\n            __init__.py\n            llm_wrapper.py\n            api_server.py\n            cli.py\n        sql/\n            __init__.py\n            types.py\n            generator.py\n下面是一些文件路径以及每个文件对应的源码：\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/types.py\nfrom pydantic import BaseModel\n\nclass RAGConfig(BaseModel):\n    schema_rag_base_url: str = \"\"\n    schema_rag_api_key: str = \"xxxx\"\n\n    context_rag_base_url: str = \"\"\n    context_rag_api_key: str = \"xxxx\"\n\nclass ByzerSQLConfig(BaseModel):\n    byzer_sql_url: str = \"\"\n    data_dir: str = \"\"\n    owner: str = \"\"\n\nclass ModelConfig(BaseModel):\n    llm_model: str = \"\"\n    gen_sql_llm_model: str = \"\"\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/cli.py\nimport argparse\nimport byzerllm\nfrom byzerllm.utils.client import ByzerLLM\nfrom superanalysis.serve.api_server import serve, ServerArgs\nfrom superanalysis.sql.types import RAGConfig, ByzerSQLConfig, ModelConfig\nfrom superanalysis.sql.generator import SQLGenerator\nfrom superanalysis.serve.llm_wrapper import LLWrapper\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"ByzerLLM API Server\")\n    parser.add_argument(\"--ray-address\", type=str, default=\"auto\", help=\"ray address\")\n    parser.add_argument(\n        \"--host\", type=str, default=\"0.0.0.0\", help=\"Host to bind the server to\"\n    )\n    parser.add_argument(\n        \"--port\", type=int, default=8000, help=\"Port to bind the server to\"\n    )\n    parser.add_argument(\n        \"--uvicorn-log-level\", type=str, default=\"info\", help=\"Uvicorn log level\"\n    )\n    parser.add_argument(\n        \"--allow-credentials\", action=\"store_true\", help=\"Allow credentials\"\n    )\n    parser.add_argument(\n        \"--allowed-origins\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed origins\"\n    )\n    parser.add_argument(\n        \"--allowed-methods\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed methods\"\n    )\n    parser.add_argument(\n        \"--allowed-headers\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed headers\"\n    )\n    parser.add_argument(\n        \"--api-key\", type=str, default=\"\", help=\"API key for authentication\"\n    )\n    parser.add_argument(\n        \"--served-model-name\",\n        type=str,\n        required=True,\n        help=\"Name of the model to be served\",\n    )\n    parser.add_argument(\n        \"--prompt-template\", type=str, default=\"\", help=\"Prompt template\"\n    )\n    parser.add_argument(\n        \"--response-role\", type=str, default=\"assistant\", help=\"Response role\"\n    )\n    parser.add_argument(\"--ssl-keyfile\", type=str, default=\"\", help=\"SSL key file\")\n    parser.add_argument(\n        \"--ssl-certfile\", type=str, default=\"\", help=\"SSL certificate file\"\n    )\n\n    # Add new arguments for RAGConfig\n    parser.add_argument(\n        \"--schema-rag-base-url\", type=str, default=\"\", help=\"RAG base URL\"\n    )\n    parser.add_argument(\n        \"--schema-rag-api-key\", type=str, default=\"xxxx\", help=\"RAG API key\"\n    )\n\n    parser.add_argument(\n        \"--context-rag-base-url\", type=str, default=\"\", help=\"RAG base URL\"\n    )\n    parser.add_argument(\n        \"--context-rag-api-key\", type=str, default=\"xxxx\", help=\"RAG API key\"\n    )\n\n    # Add new argument for ByzerSQLConfig\n    parser.add_argument(\"--byzer-sql-url\", type=str, default=\"\", help=\"Byzer SQL URL\")\n    parser.add_argument(\"--data-dir\", type=str, default=\"\", help=\"Data directory\")\n    parser.add_argument(\"--owner\", type=str, default=\"hello\", help=\"Owner of the Byzer SQL session\")\n    parser.add_argument(\n        \"--llm-model\", type=str, default=\"\", help=\"LLM model for general use\"\n    )\n    parser.add_argument(\n        \"--gen-sql-llm-model\", type=str, default=\"\", help=\"LLM model for SQL generation\"\n    )\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile,\n    )\n\n    # Create RAGConfig instance\n    rag_config = RAGConfig(\n        schema_rag_base_url=args.schema_rag_base_url,\n        schema_rag_api_key=args.schema_rag_api_key,\n        context_rag_base_url=args.context_rag_base_url,\n        context_rag_api_key=args.context_rag_api_key,\n    )\n\n    # Create ByzerSQLConfig instance\n    byzer_sql_config = ByzerSQLConfig(\n        byzer_sql_url=args.byzer_sql_url, data_dir=args.data_dir, owner=args.owner\n    )\n\n    model_config = ModelConfig(\n        llm_model=args.llm_model or args.served_model_name,\n        gen_sql_llm_model=args.gen_sql_llm_model or args.served_model_name,\n    )\n\n    byzerllm.connect_cluster(address=args.ray_address)\n    llm = ByzerLLM()\n    llm.setup_default_model_name(args.served_model_name)\n    llm = byzerllm.ByzerLLM()\n    if not server_args.served_model_name:\n        raise ValueError(\"served_model_name is required\")\n    llm.setup_default_model_name(server_args.served_model_name)\n\n    print(\"RAG Config: \", rag_config)\n    print(\"Model Config: \", model_config)\n    rag = SQLGenerator(\n        llm=llm,\n        rag_config=rag_config,\n        byzer_sql_config=byzer_sql_config,\n        model_config=model_config,\n    )\n    llm_wrapper = LLWrapper(llm=llm, entry=rag)\n    serve(llm_wrapper, server_args)\n\n\nif __name__ == \"__main__\":\n    main()\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/__init__.py\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/__init__.py\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/llm_wrapper.py\nfrom typing import Any, Dict, List, Optional, Union,Callable\nfrom byzerllm.utils.client.types import (    \n    LLMFunctionCallResponse,\n    LLMClassResponse,LLMResponse    \n)\nimport pydantic\nfrom byzerllm import ByzerLLM\nfrom byzerllm.utils.client import LLMResponse\nfrom byzerllm.utils.types import SingleOutputMeta\nfrom byzerllm.utils.langutil import asyncfy_with_semaphore\nfrom superanalysis.sql.generator import SQLGenerator\nfrom loguru import logger\n\nclass LLWrapper:\n\n    def __init__(self,llm:ByzerLLM,entry:SQLGenerator):\n        self.entry = entry\n        self.llm = llm\n        \n\n    def chat_oai(self,\n                 conversations,\n                 tools:List[Union[Callable,str]]=[], \n                 tool_choice:Optional[Union[Callable,str]]=None,\n                 execute_tool:bool=False,  \n                 impl_func:Optional[Callable]=None,\n                 execute_impl_func:bool=False,\n                 impl_func_params:Optional[Dict[str,Any]]=None,\n                 func_params:Optional[Dict[str,Any]]=None,\n                 response_class:Optional[Union[pydantic.BaseModel,str]] = None, \n                 response_after_chat:Optional[Union[pydantic.BaseModel,str]] = False,\n                 enable_default_sys_message:bool=True,                 \n                 model:Optional[str] = None,\n                 role_mapping=None,llm_config:Dict[str,Any]={}\n                 )->Union[List[LLMResponse],List[LLMFunctionCallResponse],List[LLMClassResponse]]: \n        res,contexts = self.entry.stream_chat_oai(conversations)\n        s = \"\".join(res)        \n        return [LLMResponse(output=s,metadata={},input=\"\")]\n\n    def stream_chat_oai(self,conversations, \n                        model:Optional[str]=None, \n                        role_mapping=None,\n                        delta_mode=False,\n                        llm_config:Dict[str,Any]={}): \n        res,contexts = self.entry.stream_chat_oai(conversations)        \n        for t in res:                        \n            yield (t,SingleOutputMeta(0,0))\n\n    async def async_stream_chat_oai(self,conversations, \n                        model:Optional[str]=None, \n                        role_mapping=None,\n                        delta_mode=False,\n                        llm_config:Dict[str,Any]={}): \n        res,contexts = self.rag.stream_chat_oai(conversations)                \n        for t in res:                                    \n            yield (t,SingleOutputMeta(0,0))\n                         \n\n    def __getattr__(self, name):        \n        return getattr(self.llm, name)\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/openai_serve.py\nimport argparse\nfrom service_server.api_server import ServerArgs, serve\nfrom service_server.llm_wrapper import LLWrapper\nfrom service_server.summary_rag import SummaryRAG\nimport byzerllm\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"OpenAI Serve\")\n    parser.add_argument(\n        \"--model\",\n        default=\"\",\n    )\n    parser.add_argument(\"--emb_model\", default=\"\")\n    parser.add_argument(\n        \"--ray_address\",\n        default=\"auto\",\n    )\n    parser.add_argument(\"--source_dir\", default=\".\", help=\"\")\n    parser.add_argument(\"--host\", default=\"\", help=\"\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"\")\n    parser.add_argument(\"--uvicorn_log_level\", default=\"info\", help=\"\")\n    parser.add_argument(\"--allow_credentials\", action=\"store_true\", help=\"\")\n    parser.add_argument(\"--allowed_origins\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--allowed_methods\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--allowed_headers\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--api_key\", default=\"\", help=\"\")\n    parser.add_argument(\"--served_model_name\", default=\"\", help=\"\")\n    parser.add_argument(\"--prompt_template\", default=\"\", help=\"\")\n    parser.add_argument(\"--ssl_keyfile\", default=\"\", help=\"\")\n    parser.add_argument(\"--ssl_certfile\", default=\"\", help=\"\")\n    parser.add_argument(\"--response_role\", default=\"assistant\", help=\"\")\n    parser.add_argument(\n        \"--collections\", default=\"\", help=\"Collection name for indexing\"\n    )\n    parser.add_argument(\n        \"--base_dir\",\n        default=\"\",\n        help=\"Path where the processed text embeddings were stored\",\n    )\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name or args.model,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile,\n    )\n    byzerllm.connect_cluster(address=args.ray_address)\n    llm = byzerllm.ByzerLLM()\n    if not server_args.served_model_name:\n        raise ValueError(\"served_model_name is required\")\n    llm.setup_default_model_name(server_args.served_model_name)\n    rag = SummaryRAG(llm=llm)    \n    llm_wrapper = LLWrapper(llm=llm, rag=rag)\n    serve(llm=llm_wrapper, args=server_args)\n\nif __name__ == \"__main__\":\n    main()\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/api_server.py\nimport os\nimport time\nimport uvicorn\nfrom fastapi import Request\nfrom fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import JSONResponse, StreamingResponse, Response\n\nfrom byzerllm.log import init_logger\nfrom byzerllm.utils import random_uuid\nfrom byzerllm.version import __version__ as version\nfrom byzerllm.utils.client import ByzerLLM, LLMRequest\nfrom byzerllm.utils.client.entrypoints.openai.serving_chat import OpenAIServingChat\nfrom byzerllm.utils.client.entrypoints.openai.serving_completion import OpenAIServingCompletion\nfrom byzerllm.utils.client.entrypoints.openai.protocol import (\n    ModelList,\n    ModelCard,\n    ModelPermission,\n    ChatCompletionRequest,\n    ErrorResponse,\n    CompletionRequest,\n    Embeddings,\n    EmbeddingsOutput,\n    EmbeddingsData,\n    EmbeddingsUsage,\n)\nfrom pydantic import BaseModel\nfrom typing import List\n\nlogger = init_logger(__name__)\n\nllm_client: ByzerLLM = None\nopenai_serving_chat: OpenAIServingChat = None\nopenai_serving_completion: OpenAIServingCompletion = None\n\nTIMEOUT_KEEP_ALIVE = 5  # seconds\n# timeout in 10 minutes. Streaming can take longer than 3 min\nTIMEOUT = float(os.environ.get(\"BYZERLLM_APISERVER_HTTP_TIMEOUT\", 600))\n\nrouter_app = FastAPI()\n\n\n@router_app.get(\"/health\")\nasync def health() -> Response:\n    \"\"\"Health check.\"\"\"\n    return Response(status_code=200)\n\n\n@router_app.get(\"/v1/models\")\nasync def show_available_models():\n    models = await openai_serving_chat.show_available_models()\n    return JSONResponse(content=models.model_dump())\n\n\n@router_app.get(\"/version\")\nasync def show_version():\n    return JSONResponse(content={\"version\": version})\n\n\n@router_app.get(\"/v1/models\", response_model=ModelList)\nasync def models() -> ModelList:\n    \"\"\"Show available models. Right now we only have one model.\"\"\"\n    model_cards = [\n        ModelCard(\n            id=\"\",\n            root=\"\",\n            permission=[ModelPermission()]\n        )\n    ]\n    return ModelList(data=model_cards)\n\n\n@router_app.post(\"/v1/completions\")\nasync def create_completion(\n        body: CompletionRequest,\n        request: Request\n):\n    generator = await openai_serving_completion.create_completion(body, request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            status_code=generator.code\n        )\n    if body.stream:\n        return StreamingResponse(\n            content=generator,\n            media_type=\"text/event-stream\"\n        )\n    else:\n        return JSONResponse(content=generator.model_dump())\n\n\n@router_app.post(\"/v1/chat/completions\")\nasync def create_chat_completion(\n        body: ChatCompletionRequest,\n        request: Request,\n):\n    \"\"\"Completion API similar to OpenAI's API.\n\n    See  https://platform.openai.com/docs/api-reference/chat/create\n    for the API specification. This API mimics the OpenAI ChatCompletion API.\n\n    NOTE: Currently we do not support the following features:\n        - function_call (Users should implement this by themselves)\n        - logit_bias (to be supported by vLLM engine)\n    \"\"\"\n    # async with async_timeout.timeout(TIMEOUT):\n\n    generator = await openai_serving_chat.create_chat_completion(body, request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            status_code=generator.code\n        )\n    if body.stream:\n        return StreamingResponse(\n            content=generator,\n            media_type=\"text/event-stream\"\n        )\n    else:\n        return JSONResponse(content=generator.model_dump())\n\n\n@router_app.post(\"/v1/embeddings\")\nasync def embed(body: Embeddings):\n    \"\"\"Given a prompt, the model will return one embedding.\n\n    Returns:\n        A response object with an embedding.\n    \"\"\"\n    embedding_id = f\"embed-{random_uuid()}\"\n\n    results_list = llm_client.emb(body.model, request=LLMRequest(instruction=body.input))\n    tokens = 0\n\n    return EmbeddingsOutput(\n        data=[\n            EmbeddingsData(\n                embedding=results.output,\n                index=i,\n                object=\"embedding\",\n            )\n            for i, results in enumerate(results_list)\n        ],\n        id=embedding_id,\n        object=\"list\",\n        created=int(time.time()),\n        model=body.model,\n        usage=EmbeddingsUsage(\n            prompt_tokens=tokens,\n            total_tokens=tokens,\n        ),\n    )\n\nclass ServerArgs(BaseModel):\n    host: str = None\n    port: int = 8000\n    uvicorn_log_level: str = \"info\"\n    allow_credentials: bool = False\n    allowed_origins: List[str] = [\"*\"]  \n    allowed_methods: List[str] = [\"*\"]\n    allowed_headers: List[str] = [\"*\"]\n    api_key: str = None\n    served_model_name: str = None\n    prompt_template: str = None\n    response_role: str = \"assistant\"\n    ssl_keyfile: str = None\n    ssl_certfile: str = None    \n\ndef serve(llm:ByzerLLM, args: ServerArgs):\n    \n    logger.info(f\"ByzerLLM API server version {version}\")\n    logger.info(f\"args: {args}\")\n\n    router_app.add_middleware(\n        CORSMiddleware,\n        allow_origins=args.allowed_origins,\n        allow_credentials=args.allow_credentials,\n        allow_methods=args.allowed_methods,\n        allow_headers=args.allowed_headers,\n    )\n    \n    if token := os.environ.get(\"BYZERLLM_API_KEY\") or args.api_key:\n\n        @router_app.middleware(\"http\")\n        async def authentication(request: Request, call_next):\n            if not request.url.path.startswith(\"/v1\"):\n                return await call_next(request)\n            if request.headers.get(\"Authorization\") != \"Bearer \" + token:\n                return JSONResponse(\n                    content={\"error\": \"Unauthorized\"},\n                    status_code=401\n                )\n            return await call_next(request)\n\n    # Register labels for metrics\n    # add_global_metrics_labels(model_name=engine_args.model)\n    global llm_client\n    llm_client = llm\n    \n    global openai_serving_chat\n    openai_serving_chat = OpenAIServingChat(\n        llm_client=llm_client,\n        response_role=args.response_role,\n        server_model_name=args.served_model_name,\n        prompt_template=args.prompt_template\n    )\n    global openai_serving_completion\n    openai_serving_completion = OpenAIServingCompletion(\n        llm_client=llm_client,\n        server_model_name=args.served_model_name,\n        prompt_template=args.prompt_template\n    )\n\n    uvicorn.run(\n        router_app,\n        host=args.host,\n        port=args.port,\n        log_level=args.uvicorn_log_level,\n        timeout_keep_alive=TIMEOUT_KEEP_ALIVE,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile\n    )\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/generator.py\nfrom typing import Optional, Dict, Any, List\nfrom openai import OpenAI\nimport json\nfrom .types import RAGConfig, ByzerSQLConfig, ModelConfig\nfrom byzerllm.utils.client import code_utils\nimport byzerllm\nimport requests\nfrom loguru import logger\nimport traceback\nfrom byzerllm.utils.nontext import TagExtractor\n\n\nclass SQLGenerator:\n\n    @byzerllm.prompt()\n    def gen_context(self, conversations: List[Dict[str, str]]) -> str:\n        \"\"\"\n        对话内容：\n        {% for msg in conversations %}\n        <{{ msg.role }}>: {{ msg.content }}\n        {% endfor %}\n\n        根据提供的对话内容，理解用户最后的问题，然后提取可能会用到的 Byzer-SQL 相关的知识。\n        \"\"\"\n\n    @byzerllm.prompt()\n    def gen_sql(\n        self,\n        conversations: List[Dict[str, str]],\n        context: str,\n        schema: str,\n        model: str = \"deepseek_chat\",\n    ) -> str:\n        \"\"\"\n        相关知识：\n        {{ context }}\n\n        Schema信息：\n        {{ schema }}\n\n        对话内容：\n        {% for msg in conversations %}\n        <{{ msg.role }}>: {{ msg.content }}\n        {% endfor %}\n\n        模型UDF函数：\n        {{ model }}\n\n        根据提供的相关知识以及对话内容，理解用户最后的问题，然后根据看到的schema 信息，生成 Byzer-SQL 语句。\n        Byzer-SQL 首先需要用 load 语法加载 schema 中的路径映射成表格，然后根据表格的信息生成对应的 Byzer-SQL 语句。\n\n        特别需要关注的是:\n        1. 用户的很多需求是要对表格的文本内容做自然语言处理，你需要使用 Byzer-SQL 中的大模型辅助函数来完成这个需求。\n        2. 使用 `llm_result` 函数处理 LLM 模型函数的响应,`llm_result` 函数得到的是一段文本，虽然我们要求模型函数只输出特定的文本，但考虑到大模型会有一些额外的字符输出，不会只是输出我们要求的文字，所以通过 `like` 来进行匹配。\n        3. 注意 Byzer-SQL 的语法，比如select 语句最后一定需要有 as <table_name>。\n        3. 输出的 Byzer-SQL 语句务必使用 <_byzer_sql_></_byzer_sql_> 标签包裹起来。\n        \"\"\"\n\n    @byzerllm.prompt()\n    def fetch_schema(self, context: str) -> str:\n        \"\"\"\n        下面是一些基础知识，包括名词术语等等：\n        {{ context }}\n\n        当理解对话内容的时候，需要参考上面的基础知识，然后更好的理解对应的表格schema信息是不是有用。\n        \"\"\"\n\n    def __init__(\n        self,\n        llm: byzerllm.ByzerLLM,\n        rag_config: RAGConfig,\n        byzer_sql_config: ByzerSQLConfig,\n        model_config: ModelConfig,\n    ) -> None:\n        self.llm = llm\n        self.schema_rag_client = OpenAI(\n            api_key=rag_config.schema_rag_api_key,\n            base_url=rag_config.schema_rag_base_url,\n        )\n        self.context_rag_client = OpenAI(\n            api_key=rag_config.context_rag_api_key,\n            base_url=rag_config.context_rag_base_url,\n        )\n        self.byzer_sql_url = byzer_sql_config.byzer_sql_url\n\n        self.llm_model = byzerllm.ByzerLLM()\n        self.llm_model.setup_default_model_name(model_config.llm_model)\n        self.gen_sql_llm_model = byzerllm.ByzerLLM()\n        self.gen_sql_llm_model.setup_default_model_name(model_config.gen_sql_llm_model)\n\n    def stream_chat_oai(\n        self,\n        conversations,\n        model: Optional[str] = None,\n        role_mapping=None,\n        delta_mode=False,\n        llm_config: Dict[str, Any] = {},\n    ):\n        try:\n            v, c = self._stream_chat_oai(\n                conversations=conversations,\n                model=model,\n                role_mapping=role_mapping,\n                delta_mode=delta_mode,\n                llm_config=llm_config,\n            )\n        except Exception as e:\n            error_msg = f\"Error in stream_chat_oai: {e}\\n{traceback.format_exc()}\"\n            logger.error(error_msg)\n            v, c = [str(e)], []\n\n        return v, c\n\n    def _stream_chat_oai(\n        self,\n        conversations,\n        model: Optional[str] = None,\n        role_mapping=None,\n        delta_mode=False,\n        llm_config: Dict[str, Any] = {},\n    ):\n        logger.info(f\"Starting stream_chat_oai with model: {model}\")\n        logger.debug(f\"Conversations: {conversations}\")\n        logger.debug(f\"LLM Config: {llm_config}\")\n\n        if not model:\n            model = self.llm_model.default_model_name\n        logger.info(f\"Using model: {model}\")\n\n        query = conversations[-1][\"content\"]\n\n        if (\n            \"使用四到五个字直接返回这句话的简要主题，不要解释、不要标点、不要语气词、不要多余文本，不要加粗，如果没有主题\"\n            in query\n            or \"简要总结一下对话内容，用作后续的上下文提示 prompt，控制在 200 字以内\"\n            in query\n        ):\n            chunks = self.llm.stream_chat_oai(\n                conversations=conversations,\n                model=model,\n                role_mapping=role_mapping,\n                llm_config=llm_config,\n                delta_mode=True,\n            )\n            return (chunk[0] for chunk in chunks), []\n\n        logger.info(\"Fetching context from context RAG client\")\n        context_query = self.gen_context.prompt(conversations=conversations)\n        context_response = self.context_rag_client.chat.completions.create(\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": json.dumps(\n                        {\"query\": context_query, \"only_contexts\": True},\n                        ensure_ascii=False,\n                    ),\n                }\n            ],\n            model=model,\n        )\n\n        context = context_response.choices[0].message.content\n        new_context = \"\"\n        for msg in context.split(\"\\n\"):\n            if msg.strip():\n                v = json.loads(msg)\n                new_context += v[\"source_code\"]\n        logger.info(f\"context {new_context[0:100]}...\")\n        logger.info(\"Context retrieved successfully\")\n\n        logger.info(\"Fetching schemas from schema RAG client\")\n        schema_query = self.fetch_schema.prompt(context=new_context)\n\n        response = self.schema_rag_client.chat.completions.create(\n            messages=conversations[0:-1]\n            + [\n                {\n                    \"role\": \"user\",\n                    \"content\": json.dumps(\n                        {\"query\": schema_query, \"only_contexts\": True},\n                        ensure_ascii=False,\n                    ),\n                }\n            ],\n            model=model,\n        )        \n\n        schema_str = response.choices[0].message.content\n        logger.debug(f\"Schema RAG response: {schema_str}\")\n        logger.info(\"Schema retrieved successfully\")\n\n        logger.info(\"Generating SQL string\")\n        sql_str = self.gen_sql.with_llm(self.gen_sql_llm_model).run(\n            conversations=conversations,\n            context=new_context,\n            schema=schema_str,\n            model=model,\n        )\n\n        logger.info(\"SQL string generated\")\n        logger.debug(f\"Generated SQL string: {sql_str}\")\n\n        # Extract SQL code from tags\n        tag_extractor = TagExtractor(sql_str)\n        extracted_tags = tag_extractor.extract()\n        sql = extracted_tags.content[0].content\n        logger.info(f\"Extracted SQL: {sql}\")\n\n        logger.info(f\"Sending SQL to Byzer SQL URL: {self.byzer_sql_url}\")\n        try:\n            response = requests.post(\n                self.byzer_sql_url,\n                data={\n                    \"sql\": sql,\n                    \"owner\": \"hello\",\n                    \"sessionPerUser\": True,\n                    \"sessionPerRequest\": True,\n                    \"includeSchema\": True,\n                },\n            )\n            response.raise_for_status()\n            result = response.content.decode(\"utf-8\")\n            logger.info(\"SQL execution result decoded\")\n            logger.debug(f\"SQL execution result: {result}\")\n            return (item for item in [result]), []\n        except requests.exceptions.RequestException as e:\n            logger.error(f\"Failed to send SQL to Byzer SQL URL: {e}\")\n            if hasattr(e.response, \"content\"):\n                error_content = e.response.content.decode(\"utf-8\")\n                logger.error(f\"Server response: {error_content}\")\n            else:\n                logger.error(\"No server response available\")\n            raise Exception(f\"Byzer SQL request failed: {str(e)}\") from e\n\n\n##File: /Users/allwefantasy/projects/super_analysis/setup.py\nfrom setuptools import setup, find_packages\n\n# Read requirements from requirements.txt\nwith open('requirements.txt') as f:\n    requirements = f.read().splitlines()\n\nsetup(\n    name=\"super-analysis\",\n    version=\"0.1.0\",\n    packages=find_packages(where=\"src\"),\n    package_dir={\"\": \"src\"},\n    install_requires=requirements,\n    entry_points={\n        \"console_scripts\": [\n            \"super-analysis.serve=superanalysis.serve.cli:main\",\n            \"super-analysis.convert=superanalysis.serve.convert:main\",\n            \"superanalysis.api=superanalysis.serve.rest:main\",\n        ],\n    },\n    author=\"allwefantasy\",\n    author_email=\"allwefantasy@gmail.com\",\n    description=\"A package for super analysis with ByzerLLM\",\n    long_description=open(\"README.md\").read(),\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/yourusername/super-analysis\",\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n    ],\n    python_requires='>=3.6',\n)\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/convert.py\nimport os\nimport argparse\nimport pandas as pd\nimport hashlib\nfrom typing import List, Dict\n\ndef get_csv_files(directory: str) -> List[str]:\n    \"\"\"Recursively get all CSV files in the given directory.\"\"\"\n    csv_files = []\n    for root, _, files in os.walk(directory):\n        for file in files:\n            if file.endswith('.csv'):\n                csv_files.append(os.path.join(root, file))\n    return csv_files\n\ndef extract_schema(df: pd.DataFrame) -> Dict[str, str]:\n    \"\"\"Extract schema from pandas DataFrame.\"\"\"\n    schema = {}\n    for column in df.columns:\n        dtype = str(df[column].dtype)\n        if dtype == 'object':\n            schema[column] = 'STRING'\n        elif dtype.startswith('int'):\n            schema[column] = 'INTEGER'\n        elif dtype.startswith('float'):\n            schema[column] = 'FLOAT'\n        elif dtype == 'bool':\n            schema[column] = 'BOOLEAN'\n        elif dtype.startswith('datetime'):\n            schema[column] = 'TIMESTAMP'\n        else:\n            schema[column] = 'STRING'  # Default to STRING for unknown types\n    return schema\n\ndef generate_schema_text(schema: Dict[str, str], csv_file: str, df: pd.DataFrame, include_rows_num: int) -> str:\n    \"\"\"Generate schema text from the schema dictionary and include the CSV file name and sample data.\"\"\"\n    schema_lines = [f\"CSV File: {csv_file}\\n\"]\n    schema_lines.extend([f\"{field}: {dtype}\" for field, dtype in schema.items()])\n    \n    if include_rows_num > 0:\n        schema_lines.append(f\"\\nSample Data (first {include_rows_num} rows):\")\n        sample_data = df.head(include_rows_num).to_string(index=False)\n        schema_lines.append(sample_data)\n    \n    return \"\\n\".join(schema_lines)\n\ndef save_schema(schema_text: str, csv_file: str, output_dir: str) -> None:\n    \"\"\"Save schema text to a file in the output directory.\"\"\"\n    # Generate MD5 hash of the schema text\n    file_name = hashlib.md5(schema_text.encode()).hexdigest() + '.md'\n    output_path = os.path.join(output_dir, file_name)\n    \n    with open(output_path, 'w') as f:\n        f.write(schema_text)\n    print(f\"Schema for {csv_file} saved to {output_path}\")\n\ndef convert(data_dir: str, doc_dir: str, include_rows_num: int) -> None:\n    \"\"\"Main conversion function.\"\"\"\n    csv_files = get_csv_files(data_dir)\n    \n    for csv_file in csv_files:\n        print(f\"Processing {csv_file}\")\n        df = pd.read_csv(csv_file)\n        schema = extract_schema(df)\n        schema_text = generate_schema_text(schema, csv_file, df, include_rows_num)\n        save_schema(schema_text, csv_file, doc_dir)\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Convert CSV files to schema documents\")\n    parser.add_argument(\"--data_dir\", required=True, help=\"Directory containing CSV files\")\n    parser.add_argument(\"--doc_dir\", required=True, help=\"Output directory for schema documents\")\n    parser.add_argument(\"--include-rows-num\", type=int, default=0, help=\"Number of sample rows to include in the schema\")\n    \n    args = parser.parse_args()\n    \n    if not os.path.exists(args.doc_dir):\n        os.makedirs(args.doc_dir)\n    \n    convert(args.data_dir, args.doc_dir, args.include_rows_num)\n\nif __name__ == \"__main__\":\n    main()\n\n##File: /Users/allwefantasy/projects/super_analysis/README.md\n# Super Analysis\n\nSuper Analysis 支持 NLP2SQL 以及 使用 SQL 对表中的文本字段做自然语言分析。\n\n## 安装\n\n你可以使用 pip 安装 Super Analysis：\n\n```bash\ngit clone https://github.com/allwefantasy/super_analysis\ncd super-analysis\npip install -e .\n```\n\n\n\n## 示例数据\n\n下载电影数据集： https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset/download?datasetVersionNumber=7\n\n\n## 启动分析服务\n\n整个 super-analysis 服务分为四个部分：\n\n1. byzer-sql 分布式执行引擎 \n2. schema 文档知识库\n4. context 文档知识库\n5. super-analysis 自身\n\n在启动 super-analysis 之前，你需要启动 schema 文档知识库和 context 文档知识库以及 byzer-sql 引擎。\n\n\n在你的电脑上参考 [这个文档](./docs/4.3.1%20安装与配置%20Byzer-SQL.pdf) 完成部署.\n\n> 启动的时候需要在安装有 super-analysis 的conda环境中启动。\n\n当 byzer-sql 部署完成后，注册账号为 `hello`, 然后进入到 byzer-sql 的控制台中执行以下脚本：\n\n```sql\n!byzerllm setup single;\n\nrun command as LLM.`` where \naction=\"infer\"\nand reconnect=\"true\"\nand pretrainedModelType=\"saas/openai\"\nand udfName=\"deepseek_chat\";\n```\n\n现在可以按如下方式做一些数据预处理或者启动相关服务：\n\n```bash\n## 将电影数据集的csv文件转换为schema文档\nsuper-analysis.convert --data_dir /Users/allwefantasy/data/movice --doc_dir /Users/allwefantasy/data/movice/schemas/\n\n# 启动schema文档知识库\nauto-coder doc serve \\\n--model deepseek_chat --index_filter_workers 100 \\\n--tokenizer_path /Users/allwefantasy/Downloads/tokenizer.json \\\n--doc_dir /Users/allwefantasy/data/movice/schemas/ \\\n--port 8001\n\n\n# 下载 Byzer-SQL 专门为大模型优化的文档\ngit clone https://github.com/allwefantasy/llm_friendly_packages\n\n# 启动文档知识库\nauto-coder doc serve \\\n--model deepseek_chat --index_filter_workers 100 \\\n--tokenizer_path /Users/allwefantasy/Downloads/tokenizer.json \\\n--doc_dir  /Users/allwefantasy/projects/llm_friendly_packages/github.com/allwefantasy \\\n--port 8002\n\n# 启动兼容 OpenAI Server 的分析服务\nsuper-analysis.serve --served-model-name deepseek_chat --port 8000 \\\n--schema-rag-base-url http://127.0.0.1:8001/v1 \\\n--context-rag-base-url http://127.0.0.1:8002/v1 \\\n--byzer-sql-url http://127.0.0.1:9003/run/script\n```\n\n现在你可以用 OpenAI SDK 去使用了。 具体测试和接口使用参考 [openai_local_api.ipynb](./notebooks/openai_local_api.ipynb)\n\n## 非 OpenAI 兼容接口\n\n除了兼容 OpenAI 的接口外，Super Analysis 还提供了一个简单的 REST API 接口，可以直接与模型进行交互。以下是如何使用这个接口：\n\n### 启动服务\n\n使用以下命令启动 REST API 服务：\n\n```bash\nsuperanalysis.api --schema-rag-base-url <schema_rag_url> \\\n                  --schema-rag-api-key <schema_rag_api_key> \\\n                  --context-rag-base-url <context_rag_url> \\\n                  --context-rag-api-key <context_rag_api_key> \\\n                  --byzer-sql-url <byzer_sql_url> \\\n                  --llm-model <llm_model_name> \\\n                  --gen-sql-llm-model <gen_sql_llm_model_name> \\\n                  --host <host> \\\n                  --port <port>\n```\n\n请替换尖括号中的值为实际的配置信息。\n\n### API 端点\n\n服务启动后，将提供以下 API 端点：\n\n#### POST /chat\n\n此端点用于与模型进行对话。\n\n请求体格式：\n\n```json\n{\n  \"conversations\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"你的问题或指令\"\n    }\n  ],\n  \"model\": \"可选的模型名称\"\n}\n```\n\n响应格式：\n\n```json\n{\n  \"response\": \"模型的回答\"\n}\n```\n\n### 示例使用\n\n以下是使用 Python 请求库与 API 交互的示例：\n\n```python\nimport requests\nimport json\n\nurl = \"http://localhost:8000/chat\"\npayload = {\n    \"conversations\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"你好，请介绍一下你自己。\"\n        }\n    ],\n    \"model\": \"your_model_name\"\n}\n\nresponse = requests.post(url, json=payload)\nresult = response.json()\n\nprint(result[\"response\"])\n```\n\n这个 API 相比兼容 OpenAI 接口，可以在请求中提供提供更多可选参数。\n\n## 使用方法\n\n安装完成后，你可以使用 `super-analysis.serve` 命令来启动 API 服务器：\n\n```bash\nsuper-analysis.serve --served-model-name your_model_name\n```\n\n### 命令行参数\n\n以下是可用的命令行参数：\n\n- `--host`: 服务器绑定的主机地址（默认：0.0.0.0）\n- `--port`: 服务器绑定的端口（默认：8000）\n- `--uvicorn-log-level`: Uvicorn 日志级别（默认：info）\n- `--allow-credentials`: 允许凭证（默认：False）\n- `--allowed-origins`: 允许的源（默认：[\"*\"]）\n- `--allowed-methods`: 允许的方法（默认：[\"*\"]）\n- `--allowed-headers`: 允许的头部（默认：[\"*\"]）\n- `--api-key`: 用于认证的 API 密钥\n- `--served-model-name`: 要服务的模型名称（必需）\n- `--prompt-template`: 提示模板\n- `--response-role`: 响应角色（默认：assistant）\n- `--ssl-keyfile`: SSL 密钥文件路径\n- `--ssl-certfile`: SSL 证书文件路径\n\n例如：\n\n```bash\nsuper-analysis.serve --served-model-name deepseek_chat --port 8000\n```\n\n\n## 抽取csv文件schema\n\nThe `super-analysis.convert` command allows you to convert CSV files to schema documents. This is useful for generating metadata about your CSV files that can be used for further analysis or documentation.\n\n#### Usage\n\n```\nsuper-analysis.convert --data_dir <path_to_csv_files> --doc_dir <output_directory> [--include-rows-num <number_of_sample_rows>]\n```\n\nArguments:\n- `--data_dir`: Directory containing CSV files (required)\n- `--doc_dir`: Output directory for schema documents (required)\n- `--include-rows-num`: Number of sample rows to include in the schema (optional, default is 0)\n\nThe utility will:\n1. Recursively find all CSV files in the specified directory\n2. Extract the schema from each CSV file\n3. Generate a schema text that includes:\n   - The CSV file name\n   - Field names and their data types\n   - Sample data (if `--include-rows-num` is specified)\n4. Save the schema text as a markdown file in the output directory\n\n## 联系方式\n\n如有任何问题或建议，请联系 allwefantasy@gmail.com。\n\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/rest.py\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nfrom typing import List, Optional\nimport uvicorn\nimport argparse\nfrom superanalysis.sql.generator import SQLGenerator\nfrom superanalysis.sql.types import RAGConfig, ByzerSQLConfig, ModelConfig\nimport byzerllm\n\napp = FastAPI()\n\nclass ChatMessage(BaseModel):\n    role: str\n    content: str\n\nclass ChatRequest(BaseModel):\n    conversations: List[ChatMessage]\n    model: Optional[str] = None\n\nclass ChatResponse(BaseModel):\n    response: str\n\n@app.post(\"/chat\", response_model=ChatResponse)\nasync def chat(request: ChatRequest):\n    try:\n        result, _ = sql_generator.stream_chat_oai(\n            conversations=[{\"role\": msg.role, \"content\": msg.content} for msg in request.conversations],\n            model=request.model\n        )\n        return ChatResponse(response=\"\".join(result))\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"SuperAnalysis API Server\")\n    parser.add_argument(\"--host\", type=str, default=\"0.0.0.0\", help=\"Host to bind the server to\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"Port to bind the server to\")\n    parser.add_argument(\"--schema-rag-base-url\", type=str, required=True, help=\"Schema RAG base URL\")\n    parser.add_argument(\"--schema-rag-api-key\", type=str, required=True, help=\"Schema RAG API key\")\n    parser.add_argument(\"--context-rag-base-url\", type=str, required=True, help=\"Context RAG base URL\")\n    parser.add_argument(\"--context-rag-api-key\", type=str, required=True, help=\"Context RAG API key\")\n    parser.add_argument(\"--byzer-sql-url\", type=str, required=True, help=\"Byzer SQL URL\")\n    parser.add_argument(\"--data-dir\", type=str, default=\"\", help=\"Data directory\")\n    parser.add_argument(\"--owner\", type=str, default=\"hello\", help=\"Owner of the Byzer SQL session\")\n    parser.add_argument(\"--llm-model\", type=str, required=True, help=\"LLM model for general use\")\n    parser.add_argument(\"--gen-sql-llm-model\", type=str, required=True, help=\"LLM model for SQL generation\")\n    \n    args = parser.parse_args()\n\n    rag_config = RAGConfig(\n        schema_rag_base_url=args.schema_rag_base_url,\n        schema_rag_api_key=args.schema_rag_api_key,\n        context_rag_base_url=args.context_rag_base_url,\n        context_rag_api_key=args.context_rag_api_key\n    )\n\n    byzer_sql_config = ByzerSQLConfig(\n        byzer_sql_url=args.byzer_sql_url,\n        data_dir=args.data_dir,\n        owner=args.owner\n    )\n\n    model_config = ModelConfig(\n        llm_model=args.llm_model,\n        gen_sql_llm_model=args.gen_sql_llm_model\n    )\n\n    llm = byzerllm.ByzerLLM()\n    llm.setup_default_model_name(args.llm_model)\n\n    global sql_generator\n    sql_generator = SQLGenerator(\n        llm=llm,\n        rag_config=rag_config,\n        byzer_sql_config=byzer_sql_config,\n        model_config=model_config\n    )\n\n    uvicorn.run(app, host=args.host, port=args.port)\n\nif __name__ == \"__main__\":\n    main()\n\n##File: .auto-coder/libs/llm_friendly_packages/github.com/allwefantasy/byzer-llm/README.md\n# byzerllm 大模型编程快速指南\n\n本文示例 [Notebook](../../notebooks/003_byzerllm_大模型编程快速指南.ipynb)\n\n## 安装\n\n```bash\npip install -U byzerllm\nray start --head\n```\n## 启动一个模型代理\n\nbyzerllm 支持私有化模型或者SaaS模型的部署。\n\n这里以 deepseek 官方API 为例：\n\n```bash\neasy-byzerllm deploy deepseek-chat --token xxxxx --alias deepseek_chat\n```\n\n或者跬基流动API:\n\n```bash\neasy-byzerllm deploy alibaba/Qwen1.5-110B-Chat --token xxxxx --alias qwen110b_chat\n```\n\n将上面的 API KEY 替换成你们自己的。\n\n如果你想部署私有化模型或者对接 Ollama 等更多需求，参考 [002_使用byzerllm进行模型部署.md](./002_使用byzerllm进行模型部署.md) 或者 [README.md](../../README.md)。\n\n之后，你就可以在代码里使用  deepseek_chat 或者 qwen110b_chat  访问模型了。\n\n## hello world\n\n来和我们的大模型打个招呼:\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\n@byzerllm.prompt(llm=llm)\ndef hello(q:str) ->str:\n    '''\n    你好, {{ q }}\n    '''\n\ns = hello(\"你是谁\")    \nprint(s)\n\n## 输出:\n## '你好！我是一个人工智能助手，专门设计来回答问题、提供信息和帮助解决问题。如果你有任何疑问或需要帮助，请随时告诉我。'\n```\n\n恭喜，你和大模型成功打了招呼！\n\n可以看到，我们通过 `@byzerllm.prompt` 装饰器，将一个方法转换成了一个大模型的调用，然后这个方法的主题是一段文本，文本中\n使用了 jinja2 模板语法，来获得方法的参数。当正常调用该方法时，实际上就发起了和大模型的交互，并且返回了大模型的结果。\n\n在 byzerllm 中，我们把这种方法称之为 prompt 函数。\n\n## 查看发送给大模型的prompt\n\n很多情况你可能需要调试，查看自己的 prompt 渲染后到底是什么样子的，这个时候你可以通过如下方式\n获取渲染后的prompt:\n\n```python\nhello.prompt(\"你是谁\")\n## '你好, 你是谁'\n```            \n\n## 动态换一个模型\n\n前面的 hello 方法在初始化的时候，我们使用了默认的模型 deepseek_chat，如果我们想换一个模型，可以这样做：\n\n```python\nhello.with_llm(llm).run(\"你是谁\")\n## '你好！我是一个人工智能助手，专门设计来回答问题、提供信息和帮助解决问题。如果你有任何疑问或需要帮助，请随时告诉我。'\n```\n\n通过 with_llm 你可以设置一个新的 llm 对象，然后调用 run 方法，就可以使用新的模型了。\n\n## 超长文本生成\n\n我们知道，大模型一次生成的长度其实是有限的，如果你想生成超长文本，你可能需手动的不断获得\n生成结果，然后把他转化为输入，然后再次生成，这样的方式是比较麻烦的。\n\nbyzerllm 提供了更加易用的 API :\n\n```python\nimport byzerllm\nfrom byzerllm import ByzerLLM\n\nllm = ByzerLLM.from_default_model(\"deepseek_chat\")\n\n@byzerllm.prompt()\ndef tell_story() -> str:\n    \"\"\"\n    讲一个100字的故事。    \n    \"\"\"\n\n\ns = (\n    tell_story.with_llm(llm)\n    .with_response_markers()\n    .options({\"llm_config\": {\"max_length\": 10}})\n    .run()\n)\nprint(s)\n\n## 从前，森林里住着一只聪明的小狐狸。一天，它发现了一块闪闪发光的宝石。小狐狸决定用这块宝石帮助森林里的动物们。它用宝石的光芒指引迷路的小鹿找到了回家的路，用宝石的温暖治愈了受伤的小鸟。从此，小狐狸成了森林里的英雄，动物们都感激它的善良和智慧。\n```\n\n实际核心部分就是这一行：\n\n```python\ntell_story.with_llm(llm)\n    .with_response_markers()    \n    .run()\n```\n\n我们只需要调用 `with_response_markers` 方法，系统就会自动的帮我们生成超长文本。\n在上面的案例中，我们通过\n\n```python\n.options({\"llm_config\": {\"max_length\": 10}})\n```\n\n认为的限制大模型一次交互最多只能输出10个字符，但是系统依然自动完成了远超过10个字符的文本生成。\n\n## 对象输出\n\n前面我们的例子都是返回字符串，但是我们也可以返回对象，这样我们就可以更加灵活的处理返回结果。\n\n```python\nimport pydantic \n\nclass Story(pydantic.BaseModel):\n    '''\n    故事\n    '''\n\n    title: str = pydantic.Field(description=\"故事的标题\")\n    body: str = pydantic.Field(description=\"故事主体\")\n\n@byzerllm.prompt()\ndef tell_story()->Story:\n    '''\n    讲一个100字的故事。    \n    '''\n\ns = tell_story.with_llm(llm).run()\nprint(isinstance(s, Story))\nprint(s.title)\n\n## True\n## 勇敢的小鸟\n```\n\n可以看到，我们很轻松的将输出转化为格式化输出。\n\n## 自定义字段抽取\n\n前面的结构化输出，其实会消耗更多token,还有一种更加精准的结构化输出方式。\n比如让大模型生成一个正则表达式，但实际上大模型很难准确只输出一个正则表达式，这个时候我们可以通过自定义抽取函数来获取我们想要的结果。\n\n\n```python\nfrom loguru import logger\nimport re\n\n@byzerllm.prompt()\ndef generate_regex_pattern(desc: str) -> str:\n    \"\"\"\n    根据下面的描述生成一个正则表达式，要符合python re.compile 库的要求。\n\n    {{ desc }}\n\n    最后生成的正则表达式要在<REGEX></REGEX>标签对里。\n    \"\"\"    \n\ndef extract_regex_pattern(regex_block: str) -> str:    \n    pattern = re.search(r\"<REGEX>(.*)</REGEX>\", regex_block, re.DOTALL)\n    if pattern is None:\n        logger.warning(\"No regex pattern found in the generated block:\\n {regex_block}\")\n        raise None\n    return pattern.group(1)\n\npattern = \"匹配一个邮箱地址\"\nv = generate_regex_pattern.with_llm(llm).with_extractor(extract_regex_pattern).run(desc=pattern)\nprint(v)\n## ^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\n```\n\n在上面的例子里，我们根据一句话生成一个正则表达式。我们通过 `with_extractor` 方法，传入了一个自定义的抽取函数，这个函数会在大模型生成结果后，对结果进行处理，然后返回我们想要的结果。\n\n我们在 prompt 明确说了，生成的结果要放到 `<REGEX></REGEX>` 标签对里，然后我们通过 extract_regex_pattern 函数，从结果中提取出了我们想要的正则表达式。\n\n## 在实例方法中使用大模型\n\n```python\nimport byzerllm\ndata = {\n    'name': 'Jane Doe',\n    'task_count': 3,\n    'tasks': [\n        {'name': 'Submit report', 'due_date': '2024-03-10'},\n        {'name': 'Finish project', 'due_date': '2024-03-15'},\n        {'name': 'Reply to emails', 'due_date': '2024-03-08'}\n    ]\n}\n\n\nclass RAG():\n    def __init__(self):        \n        self.llm = byzerllm.ByzerLLM()\n        self.llm.setup_template(model=\"deepseek_chat\",template=\"auto\")\n        self.llm.setup_default_model_name(\"deepseek_chat\")        \n    \n    @byzerllm.prompt(lambda self:self.llm)\n    def generate_answer(self,name,task_count,tasks)->str:\n        '''\n        Hello {{ name }},\n\n        This is a reminder that you have {{ task_count }} pending tasks:\n        {% for task in tasks %}\n        - Task: {{ task.name }} | Due: {{ task.due_date }}\n        {% endfor %}\n\n        Best regards,\n        Your Reminder System\n        '''        \n\nt = RAG()\n\nresponse = t.generate_answer(**data)\nprint(response)\n\n## 输出:\n## Hello Jane Doe,\n##I hope this message finds you well. I wanted to remind you of your 3 pending tasks to ensure you stay on track:\n## 1. **Submit report** - This task is due on **2024-03-10**. Please ensure that you allocat\n```\n\n这里我们给了个比较复杂的例子，但我们可以看到，给一个实例prompt方法和普通prompt 方法差异不大。\n唯一的区别是如果你希望在定义的时候就指定大模型，使用一个lambda函数返回实例的 llm 对象即可。\n\n```python\n@byzerllm.prompt(lambda self:self.llm)\n```\n\n你也可以不返回，在调用的时候通过 `with_llm` 方法指定 llm 对象。\n\n此外，这个例子也展示了如何通过jinja2模板语法，来处理复杂的结构化数据。\n\n## 通过 Python 代码处理复杂入参\n\n上面的一个例子中，我们通过 jinja2 模板语法，来处理复杂的结构化数据，但是有时候我们可能需要更加复杂的处理，这个时候我们可以通过 Python 代码来处理。\n\n```python\nimport byzerllm\n\ndata = {\n    'name': 'Jane Doe',\n    'task_count': 3,\n    'tasks': [\n        {'name': 'Submit report', 'due_date': '2024-03-10'},\n        {'name': 'Finish project', 'due_date': '2024-03-15'},\n        {'name': 'Reply to emails', 'due_date': '2024-03-08'}\n    ]\n}\n\n\nclass RAG():\n    def __init__(self):        \n        self.llm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n    \n    @byzerllm.prompt()\n    def generate_answer(self,name,task_count,tasks)->str:\n        '''\n        Hello {{ name }},\n\n        This is a reminder that you have {{ task_count }} pending tasks:\n            \n        {{ tasks }}\n\n        Best regards,\n        Your Reminder System\n        '''\n        \n        tasks_str = \"\\n\".join([f\"- Task: {task['name']} | Due: { task['due_date'] }\" for task in tasks])\n        return {\"tasks\": tasks_str}\n\nt = RAG()\n\nresponse = t.generate_answer.with_llm(t.llm).run(**data)\nprint(response)\n\n## Just a gentle nudge to keep you on track with your pending tasks. Here's a quick recap:....\n```\n\n在这个例子里，我们直接把 tasks 在方法体里进行处理，然后作为一个字符串返回，最够构建一个字典，字典的key为 tasks,然后\n你就可以在 docstring 里使用 `{{ tasks }}` 来引用这个字符串。\n\n这样对于很复杂的入参，就不用谢繁琐的 jinja2 模板语法了。\n\n## 如何自动实现一个方法\n\n比如我定义一个签名，但是我不想自己实现里面的逻辑，让大模型来实现。这个在 byzerllm 中叫 function impl。我们来看看怎么\n实现:\n\n```python\nimport pydantic\nclass Time(pydantic.BaseModel):\n    time: str = pydantic.Field(...,description=\"时间，时间格式为 yyyy-MM-dd\")\n\n\n@llm.impl()\ndef calculate_current_time()->Time:\n    '''\n    计算当前时间\n    '''\n    pass \n\n\ncalculate_current_time()\n#output: Time(time='2024-06-14')\n```\n\n在这个例子里，我们定义了一个 calculate_current_time 方法，但是我们没有实现里面的逻辑，我们通过 `@llm.impl()` 装饰器，让大模型来实现这个方法。\n为了避免每次都要“生成”这个方法，导致无法适用，我们提供了缓存，用户可以按如下方式打印速度：\n\n```python\nstart = time.monotonic()\ncalculate_current_time()\nprint(f\"first time cost: {time.monotonic()-start}\")\n\nstart = time.monotonic()\ncalculate_current_time()\nprint(f\"second time cost: {time.monotonic()-start}\")\n\n# output:\n# first time cost: 6.067266260739416\n# second time cost: 4.347506910562515e-05\n```\n可以看到，第一次执行花费了6s,第二次几乎是瞬间完成的，这是因为第一次执行的时候，我们实际上是在生成这个方法，第二次执行的时候，我们是执行已经生成好的代码，所以速度会非常快。你可以显示的调用 `llm.clear_impl_cache()` 清理掉函数缓存。\n\n## Stream 模式\n\n前面的例子都是一次性生成结果，但是有时候我们可能需要一个流式的输出，这个时候我们可能需要用底层一点的API来完成了：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\nv = llm.stream_chat_oai(model=\"deepseek_chat\",conversations=[{\n    \"role\":\"user\",\n    \"content\":\"你好，你是谁\",\n}],delta_mode=True)\n\nfor t in v:\n    print(t,flush=True)  \n\n# 你好\n# ！\n# 我\n# 是一个\n# 人工智能\n# 助手\n# ，\n# 旨在\n# 提供\n# 信息\n# 、\n# 解答\n# 问题....\n```\n\n如果你不想要流式输出，但是想用底层一点的API，你可以使用 `llm.chat_oai` 方法：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\nv = llm.chat_oai(model=\"deepseek_chat\",conversations=[{\n    \"role\":\"user\",\n    \"content\":\"你好，你是谁\",\n}])\n\nprint(v[0].output)\n## 你好！我是一个人工智能助手，旨在提供信息、解答问题和帮助用户解决问题。如果你有任何问题或需要帮助，请随时告诉我。\n```\n\n## Function Calling \n\nbyzerllm 可以不依赖模型自身就能提供 function calling 支持，我们来看个例子：\n\n\n```python\nfrom typing import List,Dict,Any,Annotated\nimport pydantic \nimport datetime\nfrom dateutil.relativedelta import relativedelta\n\ndef compute_date_range(count:Annotated[int,\"时间跨度，数值类型\"],\n                       unit:Annotated[str,\"时间单位，字符串类型\",{\"enum\":[\"day\",\"week\",\"month\",\"year\"]}])->List[str]:\n    '''\n    计算日期范围\n\n    Args:\n        count: 时间跨度，数值类型\n        unit: 时间单位，字符串类型，可选值为 day,week,month,year\n    '''        \n    now = datetime.datetime.now()\n    now_str = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n    if unit == \"day\":\n        return [(now - relativedelta(days=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"week\":\n        return [(now - relativedelta(weeks=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"month\":\n        return [(now - relativedelta(months=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"year\":\n        return [(now - relativedelta(years=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    return [\"\",\"\"]\n\ndef compute_now()->str:\n    '''\n    计算当前时间\n    '''\n    return datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n```\n\n我们定义了两个方法，一个是计算日期范围，一个是计算当前时间。\n\n现在我么可以来测试下，系统如何根据自然语言决定调用哪个方法：\n\n```python\nt = llm.chat_oai([{\n    \"content\":'''计算当前时间''',\n    \"role\":\"user\"    \n}],tools=[compute_date_range,compute_now],execute_tool=True)\n\nt[0].values\n\n## output: ['2024-06-14 15:18:02']\n```\n\n我们可以看到，他正确的选择了 compute_now 方法。\n\n接着我们再试一个：\n\n```python\nt = llm.chat_oai([{\n    \"content\":'''最近三个月趋势''',\n    \"role\":\"user\"    \n}],tools=[compute_date_range,compute_now],execute_tool=True)\n\nt[0].values\n\n## output: [['2024-03-14 15:19:13', '2024-06-14 15:19:13']]\n```\n\n模型正确的选择了 compute_date_range 方法。\n\n## 多模态\n\nbyerllm 也能很好的支持多模态的交互，而且统一了多模态大模型的接口，比如你可以用一样的方式使用 openai 或者 claude 的图片转文字能力， 或者一致的方式使用火山，azuer, openai的语音合成接口。\n\n### 图生文\n\n```python\nimport byzerllm\nfrom byzerllm.types import ImagePath\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\n\n\n@byzerllm.prompt()\ndef what_in_image(image_path: ImagePath) -> str:\n    \"\"\"\n    {{ image_path }}\n    这个图片里有什么？\n    \"\"\"    \n\n\nv = what_in_image.with_llm(vl_llm).run(\n    ImagePath(value=\"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\")\n)\nv\n## OUTPUT: 这张图片展示了多只可爱的猫咪，采用了艺术风格的绘画。猫咪们有不同的颜色和花纹，背景是浅棕色，上面还点缀着一些红色的花朵。整体画面给人一种温馨和谐的感觉\n```\n\n可以看到，我们只需要把 prompt 函数的图片地址入参使用 byzerllm.types.ImagePath里进行包装，就可以直接在 prompt 函数体里\n带上图片。\n\n或者你可以这样：\n\n```python\nimport byzerllm\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\n\n\n@byzerllm.prompt()\ndef what_in_image(image_path: str) -> str:\n    \"\"\"\n    {{ image }}\n    这个图片里有什么？\n    \"\"\"\n    return {\"image\": byzerllm.Image.load_image_from_path(image_path)}\n\n\nv = what_in_image.with_llm(vl_llm).run(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv\n```\n\n通过 `image_path` 参数，然后通过 `byzerllm.Image.load_image_from_path` 方法，转化为一个图片对象 image，最后在 prompt 函数体里\n使用 `{{ image }}` 引用这个图片对象。\n\n另外我们也是可以支持配置多张图片的。\n\n另外，我们也可以使用基础的 `llm.chat_oai` 方法来实现：\n\n```python\nimport byzerllm\nimport json\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\nimage = byzerllm.Image.load_image_from_path(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv = vl_llm.chat_oai(\n    conversations=[\n        {\n            \"role\": \"user\",\n            \"content\": json.dumps(\n                [{\"image\": image, \"text\": \"这个图片里有什么？\"}], ensure_ascii=False\n            ),\n        }\n    ]\n)\nv[0].output\n```\n\n还可以这么写：\n\n```python\nimport byzerllm\nimport json\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\nimage = byzerllm.Image.load_image_from_path(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv = vl_llm.chat_oai(\n    conversations=[\n        {\n            \"role\": \"user\",\n            \"content\": json.dumps(\n                [\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\"url\": image, \"detail\": \"high\"},\n                    },\n                    {\"text\": \"这个图片里有什么？\", \"type\": \"text\"},\n                ],\n                ensure_ascii=False,\n            ),\n        }\n    ]\n)\nv[0].output\n```\n\n### 语音合成\n\n这里以 openai 的 tts 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=tts-1 \\\n--model openai_tts\n```\n\n此外，byzerllm 支持 azure,火山引擎等 tts 语音合成引擎。\n\n接着你可以这么用：\n\n```python\nimport byzerllm\nimport base64\nimport json\n\nllm = byzerllm.ByzerLLM.from_default_model(\"openai_tts\")\n\n\nt = llm.chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\": json.dumps({\n        \"input\":\"hello, open_tts\",\n        \"voice\": \"alloy\",\n        \"response_format\": \"mp3\"\n    },ensure_ascii=False)\n}])\n\nwith open(\"voice.mp3\",\"wb\") as f:\n    f.write(base64.b64decode(t[0].output))\n```\n\ntts 模型生成没有prompt函数可以用，你需要直接使用 chat_oai。\n\n\n### 语音识别\n\n这里以 openai 的 whisper-1 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--worker_concurrency 10 \\\n--infer_params saas.model=whisper-1 saas.api_key=${MODEL_OPENAI_TOKEN} \\\n--model speech_to_text\n```\n\n语音识别的使用方式和图生文类似，我们可以直接在 prompt 函数体里带上音频文件。\n\n```python\nimport byzerllm\nimport json\nimport base64\nfrom byzerllm.types import AudioPath\n\nllm = byzerllm.ByzerLLM.from_default_model(\"speech_to_text\")\n\naudio_file = \"/Users/allwefantasy/videos/output_audio.mp3\"\n\n@byzerllm.prompt(llm=llm)\ndef audio_to_text(audio_file: AudioPath):\n    \"\"\"\n    {{ audio_file }}\n    \"\"\"\n\nv = audio_to_text(AudioPath(value=audio_file))\njson.loads(v)\n```\n输出的数据格式略微复杂：\n\n```\n{'text': 'In the last chapter, you and I started to step through the internal workings of a transformer. This is one of the key pieces of technology inside large language models, and a lot of other tools in the modern wave of AI.',\n 'task': 'transcribe',\n 'language': 'english',\n 'duration': 10.0,\n 'segments': [{'id': 0,\n   'seek': 0,\n   'start': 0.0,\n   'end': 4.78000020980835,\n   'text': ' In the last chapter, you and I started to step through the internal workings of a transformer.',\n   'tokens': [50364,\n    682,\n    264,\n    1036,\n    7187,\n    11,\n    291,\n    293,\n    286,\n    1409,\n    281,\n    1823,\n    807,\n    264,\n    6920,\n    589,\n    1109,\n    295,\n    257,\n    31782,\n    13,\n    50586],\n   'temperature': 0.0,\n   'avg_logprob': -0.28872039914131165,\n   'compression_ratio': 1.4220778942108154,\n   'no_speech_prob': 0.016033057123422623},\n  ....\n  {'id': 2,\n   'seek': 0,\n   'start': 8.579999923706055,\n   'end': 9.979999542236328,\n   'text': ' and a lot of other tools in the modern wave of AI.',\n   'tokens': [50759,\n    293,\n    257,\n    688,\n    295,\n    661,\n    3873,\n    294,\n    264,\n    4363,\n    5772,\n    295,\n    7318,\n    13,\n    50867],\n   'temperature': 0.0,\n   'avg_logprob': -0.28872039914131165,\n   'compression_ratio': 1.4220778942108154,\n   'no_speech_prob': 0.016033057123422623}],\n 'words': [{'word': 'In', 'start': 0.0, 'end': 0.18000000715255737},\n  {'word': 'the', 'start': 0.18000000715255737, 'end': 0.23999999463558197},\n  {'word': 'last', 'start': 0.23999999463558197, 'end': 0.5400000214576721},\n  {'word': 'chapter', 'start': 0.5400000214576721, 'end': 0.800000011920929},\n  ....\n  {'word': 'AI', 'start': 9.920000076293945, 'end': 9.979999542236328}]}\n```\n\n会输出每一句话以及每一个字所在的起始时间和截止时间。你可以根据需要来使用。\n\n\n### 文生图\n\n文生图和语音合成类似，首先要启动合适的模型,以openai 的 dall-e-3 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=dall-e-3 \\\n--model openai_image_gen\n```\n\n启动模型后，只需要记住几个模板参数即可使用，这里直接使用 chat_oai 方法来使用：\n\n```python\n\nimport byzerllm\nimport json\nimport base64\n\nllm = byzerllm.ByzerLLM.from_default_model(\"openai_image_gen\")\nt = llm.chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\": json.dumps({\n        \"input\":\"a white siamese cat\",\n        \"size\": \"1024x1024\",\n        \"quality\": \"standard\"\n    },ensure_ascii=False)\n}])\n\nwith open(\"output1.jpg\",\"wb\") as f:\n    f.write(base64.b64decode(t[0].output))\n\n\nimport matplotlib.pyplot as plt\n\nimage_path = \"output1.jpg\"\nimage = plt.imread(image_path)\n\nplt.imshow(image)\nplt.axis('off')\nplt.show()\n```\n\n## Prompt 函数的流式输出\n\nbyzerllm 底层支持流式输出，非 prompt 函数的用法是这样的：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\n\nv = llm.stream_chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\":\"讲一个100字的故事\"\n}])\n\nfor s  in v:\n    print(s[0], end=\"\")\n```\n\n如果你像用 prompt 函数，可以这么用：\n\n\n```python\nimport byzerllm\nimport json\nimport base64\nfrom typing import Generator\n\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\n\n@byzerllm.prompt()\ndef tell_story() -> Generator[str, None, None]:\n    '''\n    给我讲一个一百多字的故事\n    '''\n\nv = tell_story.with_llm(llm).run()    \nfor i in v:\n    print(i, end=\"\")\n```\n\n可以看到，和普通的 prompt 函数的区别在于，返回值是一个生成器，然后你可以通过 for 循环来获取结果。\n\n## 向量化模型\n\nbyzerllm 支持向量化模型,你可以这样启动一个本地的模型：\n\n```bash\n!byzerllm deploy --pretrained_model_type custom/bge \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--worker_concurrency 10 \\\n--model_path /home/winubuntu/.auto-coder/storage/models/AI-ModelScope/bge-large-zh \\\n--infer_backend transformers \\\n--num_workers 1 \\\n--model emb\n```\n\n注意两个参数:\n\n1. --infer_backend transformers: 表示使用 transformers 作为推理后端。\n2. --model_path: 表示模型的路径。\n\n也可以启动一个 SaaS 的emb模型,比如 qwen 的 emb 模型：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/qianwen \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 2 \\\n--infer_params saas.api_key=${MODEL_QIANWEN_TOKEN}  saas.model=text-embedding-v2 \\\n--model qianwen_emb\n```\n\n或者 openai 的 emb 模型：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--worker_concurrency 10 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=text-embedding-3-small \\\n--model gpt_emb\n```\nSaaS 模型无需配置 `--infer_backend` 参数。\n\n无论是本地模型还是 SaaS 模型，我们都可以这样使用：\n\n```python\nimport byzerllm\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\nllm.setup_default_emb_model_name(\"emb\")\nllm.emb_query(\"你好\")\n```\n\n如果你配置 byzerllm 中的 Storage 使用，比如你这样启动了存储：\n\n```bash\nbyzerllm storage start --enable_emb\n```\n\n那么需要这么使用：\n\n```python\nfrom byzerllm.apps.byzer_storage.simple_api import ByzerStorage, DataType, FieldOption,SortOption\nstorage = ByzerStorage(\"byzerai_store\", \"memory\", \"memory\")\nstorage.emb(\"你好\")\n```\n\n## 一些辅助工具\n\n当调用 prompt 函数返回字符串的时候，如果想从里面抽取代码，可以使用如下方式：\n\n```python\nfrom byzerllm.utils.client import code_utils\ntext_with_markdown = '''\n```shell\nls -l\n```\n'''\ncode_blocks = code_utils.extract_code(text_with_markdown)\nfor code_block in code_blocks:\n    if code_block[0] == \"shell\":\n        print(code_block[1])\n##output: ls -l        \n```\n\nTagExtractor 工具用于对任意 `<_tag_></_tag_>` 标签对的抽取，下面是一个使用示例：\n\n```python\nfrom byzerllm.apps.utils import TagExtractor\n\nextractor = TagExtractor('''\n大家好\n<_image_>data:image/jpeg;base64,xxxxxx</_image_>\n                         大家好\n<_image_>data:image/jpeg;base64,xxxxxx2</_image_>\n''')\n\nv = extractor.extract()\nprint(v.content[0].content)\nprint(v.content[1].content)\n```\n输出为:\n\n```\ndata:image/jpeg;base64,xxxxxx\ndata:image/jpeg;base64,xxxxxx2\n```\n\n我们成功的将 <_image_></_image_> 标签对里的内容抽取出来了。\n\n\n## 注意事项\n\n1. prompt函数方法体返回只能是dict，实际的返回类型和方法签名可以不一样，但是方法体返回只能是dict。\n2. 大部分情况prompt函数体为空，如果一定要有方法体，可以返回一个空字典。\n3. 调用prompt方法的时候，如果在@byzerllm.prompt()里没有指定llm对象，那么需要在调用的时候通过with_llm方法指定llm对象。\n\n===========================\n\n# Byzer Storage: 最易用的AI存储引擎\n\nByzer Storage是一个为AI应用设计的高性能存储引擎,它提供了简单易用的API,支持向量搜索、全文检索以及结构化查询。本文将详细介绍Byzer Storage的使用方法和主要特性。\n\n## 0. 安装和启动\n\n```bash\npip install byzerllm\nbyzerllm storage start\n```\n\nThat's it! Byzer Storage已经安装并启动成功,现在我们可以开始使用它了。默认会启动一个 byzerai_store 的集群。\n\n注意，如果你是这样启动的：\n\n```bash\nbyzerllm storage start --enable_emb\n```\n\n那么会自动启动一个 emb 模型，名字就叫 emb， ByzerStorage 会自动使用该模型，无需做任何其他配置。\n\n## 1. 初始化\n\n创建一个 ByzerStorage 对象，链接 byzerai_store 集群，并且指定数据库和表名（可以不存在）。\n\n```python\nfrom byzerllm.apps.byzer_storage.simple_api import ByzerStorage, DataType, FieldOption, SortOption\n\nstorage = ByzerStorage(\"byzerai_store\", \"my_database1\", \"my_table4s\")\n```\n\n\n## 2. 创建库表（可选）\n\nByzer Storage使用Schema来定义数据结构。我们可以使用SchemaBuilder来构建Schema:\n\n```python\n_ = (\n    storage.schema_builder()\n    .add_field(\"_id\", DataType.STRING)\n    .add_field(\"name\", DataType.STRING)\n    .add_field(\"content\", DataType.STRING, [FieldOption.ANALYZE])\n    .add_field(\"raw_content\", DataType.STRING, [FieldOption.NO_INDEX])    \n    .add_array_field(\"summary\", DataType.FLOAT)    \n    .add_field(\"created_time\", DataType.LONG, [FieldOption.SORT])    \n    .execute()\n)\n```\n\n这个Schema定义了以下字段:\n- `_id`: 字符串类型的主键\n- `name`: 字符串类型,可用于过滤条件\n- `content`: 字符串类型,带有ANALYZE选项,用于全文搜索\n- `raw_content`: 字符串类型,带有NO_INDEX选项,不会被索引\n- `summary`: 浮点数组类型,用于存储向量\n- `created_time`: 长整型,带有SORT选项,可用于排序\n\n需要注意的是：\n\n1. 如果一个字段带有ANALYZE选项,则该字段会被分词,并且可以用于全文搜索，但是就无法返回原始的文本了。所以你需要添加一个新字段专门用来存储原文，比如在我们这个例子里，我们新增了 raw_content 字段，并且显示的指定了 NO_INDEX 选项，这样就不会被索引，也就不会被分词，可以后续被检索后用来获取原始文本。\n2. 对于需要作为向量检索和存储的字段，需要指定为数组类型，比如我们这个例子里的 summary 字段。\n3. 如果你需要拿到向量字段的原始文本，那么你也需要添加一个新字段专门用来存储原文，就像我们这个例子里的 raw_content 字段一样。\n\n\n## 3. 写入数据\n\n准备数据并使用WriteBuilder写入Storage:\n\n```python\ndata = [\n    {\"_id\": \"1\", \"name\": \"Hello\", \"content\": \"Hello, world!\", \"raw_content\": \"Hello, world!\", \"summary\": \"hello world\", \"created_time\": 1612137600},\n    {\"_id\": \"2\", \"name\": \"Byzer\", \"content\": \"Byzer, world!\", \"raw_content\": \"Byzer, world!\", \"summary\": \"byzer\", \"created_time\": 1612137601},\n    {\"_id\": \"3\", \"name\": \"AI\", \"content\": \"AI, world!\", \"raw_content\": \"AI, world!\", \"summary\": \"AI\", \"created_time\": 16121376002},\n    {\"_id\": \"4\", \"name\": \"ByzerAI\", \"content\": \"ByzerAI, world!\", \"raw_content\": \"ByzerAI, world!\", \"summary\": \"ByzerAi\", \"created_time\": 16121376003},\n]\n\nstorage.write_builder().add_items(data, vector_fields=[\"summary\"], search_fields=[\"content\"]).execute()\nstorage.commit()\n```\n\n这里我们使用`add_items`方法批量添加数据,并指定了`summary`为向量字段,`content`为搜索字段。最后调用`commit()`来确保数据被持久化。\n\n\n从上面我们要写入到 byzer storage 的数据我们可以看到如下几个特点：\n\n1. 需要包含我们之前定义的 Schema中罗列的所有的字段，同时需要指定哪些是向量字段，哪些是检索字段。\n2. 向量和检索不能是同一个字段。\n3. 对于向量，检索字段，我们给到 write_builder 的都是文本，ByzerStorage 会根据 Schema 的定义自动将其转换为向量，检索字段需要的格式。\n\n## 4. 查询数据\n\nByzer Storage支持多种查询方式,包括向量搜索、全文检索、过滤和排序。\n\n### 4.1 向量搜索 + 全文检索\n\n```python\nquery = storage.query_builder()\nquery.set_vector_query(\"ByzerAI\", fields=[\"summary\"])\nresults = query.set_search_query(\"Hello\", fields=[\"content\"]).execute()\nprint(results)\n```\n\n这个查询结合了向量搜索和全文检索,它会在`summary`字段中搜索与\"ByzerAI\"相似的向量,同时在`content`字段中搜索包含\"Hello\"的文档。\n\n### 4.2 过滤 + 向量搜索 + 全文检索\n\n```python\nquery = storage.query_builder()\nquery.and_filter().add_condition(\"name\", \"AI\").build()\nquery.set_vector_query(\"ByzerAI\", fields=\"summary\")\nresults = query.set_search_query(\"Hello\", fields=[\"content\"]).execute()\nprint(results)\n```\n\n这个查询首先过滤`name`字段等于\"AI\"的文档,然后在结果中进行向量搜索和全文检索。\n\n### 4.3 过滤 + 排序\n\n```python\nquery = storage.query_builder()\nquery.and_filter().add_condition(\"name\", \"AI\").build().sort(\"created_time\", SortOption.DESC)\nresults = query.execute()\nprint(results)\n```\n\n这个查询过滤`name`字段等于\"AI\"的文档,然后按`created_time`字段降序排序。\n\n## 5. 删除数据\n\n### 5.1 根据ID删除\n\n```python\nstorage.delete_by_ids([\"3\"])\n\nquery = storage.query_builder()\nquery.and_filter().add_condition(\"name\", \"AI\").build()\nresults = query.execute()\nprint(results)\n```\n\n这里我们删除了ID为\"3\"的文档,然后查询验证删除结果。\n\n### 5.2 删除整个表\n\n```python\nstorage.drop()\n```\n\n这个操作会删除整个表及其所有数据,请谨慎使用。\n\n## 结论\n\nByzer Storage提供了一套简洁而强大的API,能够轻松实现向量搜索、全文检索、结构化查询等功能。它的设计非常适合AI应用场景,可以有效地存储和检索各种类型的数据。通过本文的介绍,相信读者已经对Byzer Storage有了基本的了解,并能够开始在自己的项目中使用这个强大的存储引擎。\n\n\n     \n\n下面是用户的需求：\n\n生成一个makefile 可以将整个项目打包成一个wheel 包\n"}, {"role": "assistant", "content": "好的，我可以帮你创建一个 Makefile 来将整个项目打包成一个 wheel 包。这个 Makefile 将包含构建、清理和打包的命令。下面是一个适合你项目的 Makefile：\n\n```makefile\n##File: /Users/allwefantasy/projects/super_analysis/Makefile\n# Makefile for super-analysis project\n\n# Python interpreter\nPYTHON := python3\n\n# Project name\nPROJECT_NAME := super-analysis\n\n# Directories\nSRC_DIR := src\nDIST_DIR := dist\nBUILD_DIR := build\n\n# Commands\nSETUP := $(PYTHON) setup.py\n\n.PHONY: all clean build wheel\n\nall: clean build wheel\n\nclean:\n\t@echo \"Cleaning up...\"\n\trm -rf $(BUILD_DIR) $(DIST_DIR) *.egg-info\n\tfind . -type d -name \"__pycache__\" -exec rm -rf {} +\n\tfind . -type f -name \"*.pyc\" -delete\n\nbuild:\n\t@echo \"Building the project...\"\n\t$(SETUP) build\n\nwheel:\n\t@echo \"Creating wheel package...\"\n\t$(SETUP) bdist_wheel\n\ninstall:\n\t@echo \"Installing the package...\"\n\t$(PYTHON) -m pip install .\n\nuninstall:\n\t@echo \"Uninstalling the package...\"\n\t$(PYTHON) -m pip uninstall -y $(PROJECT_NAME)\n\ndevelop:\n\t@echo \"Installing the package in editable mode...\"\n\t$(PYTHON) -m pip install -e .\n\ntest:\n\t@echo \"Running tests...\"\n\t$(PYTHON) -m unittest discover tests\n\n```\n\n这个 Makefile 包含以下主要目标：\n\n1. `all`: 执行清理、构建和打包的完整流程。\n2. `clean`: 清理构建产生的文件和目录。\n3. `build`: 构建项目。\n4. `wheel`: 创建 wheel 包。\n5. `install`: 安装当前项目包。\n6. `uninstall`: 卸载当前项目包。\n7. `develop`: 以可编辑模式安装项目包（适用于开发）。\n8. `test`: 运行测试（假设你有测试目录）。\n\n使用方法：\n\n1. 将这个 Makefile 放在项目的根目录下。\n2. 打开终端，进入项目根目录。\n3. 运行以下命令来构建 wheel 包：\n\n```bash\nmake wheel\n```\n\n这将在 `dist` 目录下创建一个 `.whl` 文件。\n\n如果你想执行完整的清理、构建和打包流程，可以运行：\n\n```bash\nmake all\n```\n\n注意事项：\n\n1. 确保你的 `setup.py` 文件正确配置，包含所有必要的项目信息和依赖。\n2. 如果你的项目结构有所不同，可能需要调整 Makefile 中的一些路径或命令。\n3. 这个 Makefile 假设你使用的是 Python 3。如果使用其他版本，请相应地修改 `PYTHON` 变量。\n4. 如果你的项目没有测试，可以删除或注释掉 `test` 目标。\n\n这个 Makefile 应该能满足你将项目打包成 wheel 包的需求，同时也提供了一些额外的有用命令来管理你的项目。"}], "model": "human", "yaml_file": "actions/139_chat_action.yml"}
{"instruction": "生成一个makefile 可以将整个项目打包成一个wheel 包", "conversations": [{"role": "user", "content": "\n如果你需要生成代码，对于每个需要更改的文件,你需要按 *SEARCH/REPLACE block* 的格式进行生成。\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The opening fence and code language, eg: ```python\n2. The file path alone on a line, starting with \"##File:\" and verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n*SEARCH/REPLACE* blocks will replace *all* matching occurrences.\nInclude enough lines to make the SEARCH blocks unique.\n\nInclude *ALL* the code being searched and replaced!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n下面我们来看一个例子：\n\n当前项目目录结构：\n1. 项目根目录： /tmp/projects/mathweb\n2. 项目子目录/文件列表(类似tree 命令输出)\nflask/\n    app.py\n    templates/\n        index.html\n    static/\n        style.css\n\n用户需求： Change get_factorial() to use math.factorial\n\n回答： To make this change we need to modify `/tmp/projects/mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n用户需求： Refactor hello() into its own file.\n\n回答：To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/hello.py\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/main.py\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n现在让我们开始一个新的任务:\n\n当前项目目录结构：\n1. 项目根目录： /Users/allwefantasy/projects/super_analysis\n2. 项目子目录/文件列表(类似tree 命令输出)：\nsetup.py\n.auto-coder/\n    libs/\n        llm_friendly_packages/\n            src/\n                example1/\n                    main.py\nsrc/\n    superanalysis/\n        serve/\n            rest.py\n            convert.py\n            openai_serve.py\n            __init__.py\n            llm_wrapper.py\n            api_server.py\n            cli.py\n        sql/\n            __init__.py\n            types.py\n            generator.py\n下面是一些文件路径以及每个文件对应的源码：\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/types.py\nfrom pydantic import BaseModel\n\nclass RAGConfig(BaseModel):\n    schema_rag_base_url: str = \"\"\n    schema_rag_api_key: str = \"xxxx\"\n\n    context_rag_base_url: str = \"\"\n    context_rag_api_key: str = \"xxxx\"\n\nclass ByzerSQLConfig(BaseModel):\n    byzer_sql_url: str = \"\"\n    data_dir: str = \"\"\n    owner: str = \"\"\n\nclass ModelConfig(BaseModel):\n    llm_model: str = \"\"\n    gen_sql_llm_model: str = \"\"\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/cli.py\nimport argparse\nimport byzerllm\nfrom byzerllm.utils.client import ByzerLLM\nfrom superanalysis.serve.api_server import serve, ServerArgs\nfrom superanalysis.sql.types import RAGConfig, ByzerSQLConfig, ModelConfig\nfrom superanalysis.sql.generator import SQLGenerator\nfrom superanalysis.serve.llm_wrapper import LLWrapper\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"ByzerLLM API Server\")\n    parser.add_argument(\"--ray-address\", type=str, default=\"auto\", help=\"ray address\")\n    parser.add_argument(\n        \"--host\", type=str, default=\"0.0.0.0\", help=\"Host to bind the server to\"\n    )\n    parser.add_argument(\n        \"--port\", type=int, default=8000, help=\"Port to bind the server to\"\n    )\n    parser.add_argument(\n        \"--uvicorn-log-level\", type=str, default=\"info\", help=\"Uvicorn log level\"\n    )\n    parser.add_argument(\n        \"--allow-credentials\", action=\"store_true\", help=\"Allow credentials\"\n    )\n    parser.add_argument(\n        \"--allowed-origins\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed origins\"\n    )\n    parser.add_argument(\n        \"--allowed-methods\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed methods\"\n    )\n    parser.add_argument(\n        \"--allowed-headers\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed headers\"\n    )\n    parser.add_argument(\n        \"--api-key\", type=str, default=\"\", help=\"API key for authentication\"\n    )\n    parser.add_argument(\n        \"--served-model-name\",\n        type=str,\n        required=True,\n        help=\"Name of the model to be served\",\n    )\n    parser.add_argument(\n        \"--prompt-template\", type=str, default=\"\", help=\"Prompt template\"\n    )\n    parser.add_argument(\n        \"--response-role\", type=str, default=\"assistant\", help=\"Response role\"\n    )\n    parser.add_argument(\"--ssl-keyfile\", type=str, default=\"\", help=\"SSL key file\")\n    parser.add_argument(\n        \"--ssl-certfile\", type=str, default=\"\", help=\"SSL certificate file\"\n    )\n\n    # Add new arguments for RAGConfig\n    parser.add_argument(\n        \"--schema-rag-base-url\", type=str, default=\"\", help=\"RAG base URL\"\n    )\n    parser.add_argument(\n        \"--schema-rag-api-key\", type=str, default=\"xxxx\", help=\"RAG API key\"\n    )\n\n    parser.add_argument(\n        \"--context-rag-base-url\", type=str, default=\"\", help=\"RAG base URL\"\n    )\n    parser.add_argument(\n        \"--context-rag-api-key\", type=str, default=\"xxxx\", help=\"RAG API key\"\n    )\n\n    # Add new argument for ByzerSQLConfig\n    parser.add_argument(\"--byzer-sql-url\", type=str, default=\"\", help=\"Byzer SQL URL\")\n    parser.add_argument(\"--data-dir\", type=str, default=\"\", help=\"Data directory\")\n    parser.add_argument(\"--owner\", type=str, default=\"hello\", help=\"Owner of the Byzer SQL session\")\n    parser.add_argument(\n        \"--llm-model\", type=str, default=\"\", help=\"LLM model for general use\"\n    )\n    parser.add_argument(\n        \"--gen-sql-llm-model\", type=str, default=\"\", help=\"LLM model for SQL generation\"\n    )\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile,\n    )\n\n    # Create RAGConfig instance\n    rag_config = RAGConfig(\n        schema_rag_base_url=args.schema_rag_base_url,\n        schema_rag_api_key=args.schema_rag_api_key,\n        context_rag_base_url=args.context_rag_base_url,\n        context_rag_api_key=args.context_rag_api_key,\n    )\n\n    # Create ByzerSQLConfig instance\n    byzer_sql_config = ByzerSQLConfig(\n        byzer_sql_url=args.byzer_sql_url, data_dir=args.data_dir, owner=args.owner\n    )\n\n    model_config = ModelConfig(\n        llm_model=args.llm_model or args.served_model_name,\n        gen_sql_llm_model=args.gen_sql_llm_model or args.served_model_name,\n    )\n\n    byzerllm.connect_cluster(address=args.ray_address)\n    llm = ByzerLLM()\n    llm.setup_default_model_name(args.served_model_name)\n    llm = byzerllm.ByzerLLM()\n    if not server_args.served_model_name:\n        raise ValueError(\"served_model_name is required\")\n    llm.setup_default_model_name(server_args.served_model_name)\n\n    print(\"RAG Config: \", rag_config)\n    print(\"Model Config: \", model_config)\n    rag = SQLGenerator(\n        llm=llm,\n        rag_config=rag_config,\n        byzer_sql_config=byzer_sql_config,\n        model_config=model_config,\n    )\n    llm_wrapper = LLWrapper(llm=llm, entry=rag)\n    serve(llm_wrapper, server_args)\n\n\nif __name__ == \"__main__\":\n    main()\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/__init__.py\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/__init__.py\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/llm_wrapper.py\nfrom typing import Any, Dict, List, Optional, Union,Callable\nfrom byzerllm.utils.client.types import (    \n    LLMFunctionCallResponse,\n    LLMClassResponse,LLMResponse    \n)\nimport pydantic\nfrom byzerllm import ByzerLLM\nfrom byzerllm.utils.client import LLMResponse\nfrom byzerllm.utils.types import SingleOutputMeta\nfrom byzerllm.utils.langutil import asyncfy_with_semaphore\nfrom superanalysis.sql.generator import SQLGenerator\nfrom loguru import logger\n\nclass LLWrapper:\n\n    def __init__(self,llm:ByzerLLM,entry:SQLGenerator):\n        self.entry = entry\n        self.llm = llm\n        \n\n    def chat_oai(self,\n                 conversations,\n                 tools:List[Union[Callable,str]]=[], \n                 tool_choice:Optional[Union[Callable,str]]=None,\n                 execute_tool:bool=False,  \n                 impl_func:Optional[Callable]=None,\n                 execute_impl_func:bool=False,\n                 impl_func_params:Optional[Dict[str,Any]]=None,\n                 func_params:Optional[Dict[str,Any]]=None,\n                 response_class:Optional[Union[pydantic.BaseModel,str]] = None, \n                 response_after_chat:Optional[Union[pydantic.BaseModel,str]] = False,\n                 enable_default_sys_message:bool=True,                 \n                 model:Optional[str] = None,\n                 role_mapping=None,llm_config:Dict[str,Any]={}\n                 )->Union[List[LLMResponse],List[LLMFunctionCallResponse],List[LLMClassResponse]]: \n        res,contexts = self.entry.stream_chat_oai(conversations)\n        s = \"\".join(res)        \n        return [LLMResponse(output=s,metadata={},input=\"\")]\n\n    def stream_chat_oai(self,conversations, \n                        model:Optional[str]=None, \n                        role_mapping=None,\n                        delta_mode=False,\n                        llm_config:Dict[str,Any]={}): \n        res,contexts = self.entry.stream_chat_oai(conversations)        \n        for t in res:                        \n            yield (t,SingleOutputMeta(0,0))\n\n    async def async_stream_chat_oai(self,conversations, \n                        model:Optional[str]=None, \n                        role_mapping=None,\n                        delta_mode=False,\n                        llm_config:Dict[str,Any]={}): \n        res,contexts = self.rag.stream_chat_oai(conversations)                \n        for t in res:                                    \n            yield (t,SingleOutputMeta(0,0))\n                         \n\n    def __getattr__(self, name):        \n        return getattr(self.llm, name)\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/openai_serve.py\nimport argparse\nfrom service_server.api_server import ServerArgs, serve\nfrom service_server.llm_wrapper import LLWrapper\nfrom service_server.summary_rag import SummaryRAG\nimport byzerllm\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"OpenAI Serve\")\n    parser.add_argument(\n        \"--model\",\n        default=\"\",\n    )\n    parser.add_argument(\"--emb_model\", default=\"\")\n    parser.add_argument(\n        \"--ray_address\",\n        default=\"auto\",\n    )\n    parser.add_argument(\"--source_dir\", default=\".\", help=\"\")\n    parser.add_argument(\"--host\", default=\"\", help=\"\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"\")\n    parser.add_argument(\"--uvicorn_log_level\", default=\"info\", help=\"\")\n    parser.add_argument(\"--allow_credentials\", action=\"store_true\", help=\"\")\n    parser.add_argument(\"--allowed_origins\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--allowed_methods\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--allowed_headers\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--api_key\", default=\"\", help=\"\")\n    parser.add_argument(\"--served_model_name\", default=\"\", help=\"\")\n    parser.add_argument(\"--prompt_template\", default=\"\", help=\"\")\n    parser.add_argument(\"--ssl_keyfile\", default=\"\", help=\"\")\n    parser.add_argument(\"--ssl_certfile\", default=\"\", help=\"\")\n    parser.add_argument(\"--response_role\", default=\"assistant\", help=\"\")\n    parser.add_argument(\n        \"--collections\", default=\"\", help=\"Collection name for indexing\"\n    )\n    parser.add_argument(\n        \"--base_dir\",\n        default=\"\",\n        help=\"Path where the processed text embeddings were stored\",\n    )\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name or args.model,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile,\n    )\n    byzerllm.connect_cluster(address=args.ray_address)\n    llm = byzerllm.ByzerLLM()\n    if not server_args.served_model_name:\n        raise ValueError(\"served_model_name is required\")\n    llm.setup_default_model_name(server_args.served_model_name)\n    rag = SummaryRAG(llm=llm)    \n    llm_wrapper = LLWrapper(llm=llm, rag=rag)\n    serve(llm=llm_wrapper, args=server_args)\n\nif __name__ == \"__main__\":\n    main()\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/api_server.py\nimport os\nimport time\nimport uvicorn\nfrom fastapi import Request\nfrom fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import JSONResponse, StreamingResponse, Response\n\nfrom byzerllm.log import init_logger\nfrom byzerllm.utils import random_uuid\nfrom byzerllm.version import __version__ as version\nfrom byzerllm.utils.client import ByzerLLM, LLMRequest\nfrom byzerllm.utils.client.entrypoints.openai.serving_chat import OpenAIServingChat\nfrom byzerllm.utils.client.entrypoints.openai.serving_completion import OpenAIServingCompletion\nfrom byzerllm.utils.client.entrypoints.openai.protocol import (\n    ModelList,\n    ModelCard,\n    ModelPermission,\n    ChatCompletionRequest,\n    ErrorResponse,\n    CompletionRequest,\n    Embeddings,\n    EmbeddingsOutput,\n    EmbeddingsData,\n    EmbeddingsUsage,\n)\nfrom pydantic import BaseModel\nfrom typing import List\n\nlogger = init_logger(__name__)\n\nllm_client: ByzerLLM = None\nopenai_serving_chat: OpenAIServingChat = None\nopenai_serving_completion: OpenAIServingCompletion = None\n\nTIMEOUT_KEEP_ALIVE = 5  # seconds\n# timeout in 10 minutes. Streaming can take longer than 3 min\nTIMEOUT = float(os.environ.get(\"BYZERLLM_APISERVER_HTTP_TIMEOUT\", 600))\n\nrouter_app = FastAPI()\n\n\n@router_app.get(\"/health\")\nasync def health() -> Response:\n    \"\"\"Health check.\"\"\"\n    return Response(status_code=200)\n\n\n@router_app.get(\"/v1/models\")\nasync def show_available_models():\n    models = await openai_serving_chat.show_available_models()\n    return JSONResponse(content=models.model_dump())\n\n\n@router_app.get(\"/version\")\nasync def show_version():\n    return JSONResponse(content={\"version\": version})\n\n\n@router_app.get(\"/v1/models\", response_model=ModelList)\nasync def models() -> ModelList:\n    \"\"\"Show available models. Right now we only have one model.\"\"\"\n    model_cards = [\n        ModelCard(\n            id=\"\",\n            root=\"\",\n            permission=[ModelPermission()]\n        )\n    ]\n    return ModelList(data=model_cards)\n\n\n@router_app.post(\"/v1/completions\")\nasync def create_completion(\n        body: CompletionRequest,\n        request: Request\n):\n    generator = await openai_serving_completion.create_completion(body, request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            status_code=generator.code\n        )\n    if body.stream:\n        return StreamingResponse(\n            content=generator,\n            media_type=\"text/event-stream\"\n        )\n    else:\n        return JSONResponse(content=generator.model_dump())\n\n\n@router_app.post(\"/v1/chat/completions\")\nasync def create_chat_completion(\n        body: ChatCompletionRequest,\n        request: Request,\n):\n    \"\"\"Completion API similar to OpenAI's API.\n\n    See  https://platform.openai.com/docs/api-reference/chat/create\n    for the API specification. This API mimics the OpenAI ChatCompletion API.\n\n    NOTE: Currently we do not support the following features:\n        - function_call (Users should implement this by themselves)\n        - logit_bias (to be supported by vLLM engine)\n    \"\"\"\n    # async with async_timeout.timeout(TIMEOUT):\n\n    generator = await openai_serving_chat.create_chat_completion(body, request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            status_code=generator.code\n        )\n    if body.stream:\n        return StreamingResponse(\n            content=generator,\n            media_type=\"text/event-stream\"\n        )\n    else:\n        return JSONResponse(content=generator.model_dump())\n\n\n@router_app.post(\"/v1/embeddings\")\nasync def embed(body: Embeddings):\n    \"\"\"Given a prompt, the model will return one embedding.\n\n    Returns:\n        A response object with an embedding.\n    \"\"\"\n    embedding_id = f\"embed-{random_uuid()}\"\n\n    results_list = llm_client.emb(body.model, request=LLMRequest(instruction=body.input))\n    tokens = 0\n\n    return EmbeddingsOutput(\n        data=[\n            EmbeddingsData(\n                embedding=results.output,\n                index=i,\n                object=\"embedding\",\n            )\n            for i, results in enumerate(results_list)\n        ],\n        id=embedding_id,\n        object=\"list\",\n        created=int(time.time()),\n        model=body.model,\n        usage=EmbeddingsUsage(\n            prompt_tokens=tokens,\n            total_tokens=tokens,\n        ),\n    )\n\nclass ServerArgs(BaseModel):\n    host: str = None\n    port: int = 8000\n    uvicorn_log_level: str = \"info\"\n    allow_credentials: bool = False\n    allowed_origins: List[str] = [\"*\"]  \n    allowed_methods: List[str] = [\"*\"]\n    allowed_headers: List[str] = [\"*\"]\n    api_key: str = None\n    served_model_name: str = None\n    prompt_template: str = None\n    response_role: str = \"assistant\"\n    ssl_keyfile: str = None\n    ssl_certfile: str = None    \n\ndef serve(llm:ByzerLLM, args: ServerArgs):\n    \n    logger.info(f\"ByzerLLM API server version {version}\")\n    logger.info(f\"args: {args}\")\n\n    router_app.add_middleware(\n        CORSMiddleware,\n        allow_origins=args.allowed_origins,\n        allow_credentials=args.allow_credentials,\n        allow_methods=args.allowed_methods,\n        allow_headers=args.allowed_headers,\n    )\n    \n    if token := os.environ.get(\"BYZERLLM_API_KEY\") or args.api_key:\n\n        @router_app.middleware(\"http\")\n        async def authentication(request: Request, call_next):\n            if not request.url.path.startswith(\"/v1\"):\n                return await call_next(request)\n            if request.headers.get(\"Authorization\") != \"Bearer \" + token:\n                return JSONResponse(\n                    content={\"error\": \"Unauthorized\"},\n                    status_code=401\n                )\n            return await call_next(request)\n\n    # Register labels for metrics\n    # add_global_metrics_labels(model_name=engine_args.model)\n    global llm_client\n    llm_client = llm\n    \n    global openai_serving_chat\n    openai_serving_chat = OpenAIServingChat(\n        llm_client=llm_client,\n        response_role=args.response_role,\n        server_model_name=args.served_model_name,\n        prompt_template=args.prompt_template\n    )\n    global openai_serving_completion\n    openai_serving_completion = OpenAIServingCompletion(\n        llm_client=llm_client,\n        server_model_name=args.served_model_name,\n        prompt_template=args.prompt_template\n    )\n\n    uvicorn.run(\n        router_app,\n        host=args.host,\n        port=args.port,\n        log_level=args.uvicorn_log_level,\n        timeout_keep_alive=TIMEOUT_KEEP_ALIVE,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile\n    )\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/generator.py\nfrom typing import Optional, Dict, Any, List\nfrom openai import OpenAI\nimport json\nfrom .types import RAGConfig, ByzerSQLConfig, ModelConfig\nfrom byzerllm.utils.client import code_utils\nimport byzerllm\nimport requests\nfrom loguru import logger\nimport traceback\nfrom byzerllm.utils.nontext import TagExtractor\n\n\nclass SQLGenerator:\n\n    @byzerllm.prompt()\n    def gen_context(self, conversations: List[Dict[str, str]]) -> str:\n        \"\"\"\n        对话内容：\n        {% for msg in conversations %}\n        <{{ msg.role }}>: {{ msg.content }}\n        {% endfor %}\n\n        根据提供的对话内容，理解用户最后的问题，然后提取可能会用到的 Byzer-SQL 相关的知识。\n        \"\"\"\n\n    @byzerllm.prompt()\n    def gen_sql(\n        self,\n        conversations: List[Dict[str, str]],\n        context: str,\n        schema: str,\n        model: str = \"deepseek_chat\",\n    ) -> str:\n        \"\"\"\n        相关知识：\n        {{ context }}\n\n        Schema信息：\n        {{ schema }}\n\n        对话内容：\n        {% for msg in conversations %}\n        <{{ msg.role }}>: {{ msg.content }}\n        {% endfor %}\n\n        模型UDF函数：\n        {{ model }}\n\n        根据提供的相关知识以及对话内容，理解用户最后的问题，然后根据看到的schema 信息，生成 Byzer-SQL 语句。\n        Byzer-SQL 首先需要用 load 语法加载 schema 中的路径映射成表格，然后根据表格的信息生成对应的 Byzer-SQL 语句。\n\n        特别需要关注的是:\n        1. 用户的很多需求是要对表格的文本内容做自然语言处理，你需要使用 Byzer-SQL 中的大模型辅助函数来完成这个需求。\n        2. 使用 `llm_result` 函数处理 LLM 模型函数的响应,`llm_result` 函数得到的是一段文本，虽然我们要求模型函数只输出特定的文本，但考虑到大模型会有一些额外的字符输出，不会只是输出我们要求的文字，所以通过 `like` 来进行匹配。\n        3. 注意 Byzer-SQL 的语法，比如select 语句最后一定需要有 as <table_name>。\n        3. 输出的 Byzer-SQL 语句务必使用 <_byzer_sql_></_byzer_sql_> 标签包裹起来。\n        \"\"\"\n\n    @byzerllm.prompt()\n    def fetch_schema(self, context: str) -> str:\n        \"\"\"\n        下面是一些基础知识，包括名词术语等等：\n        {{ context }}\n\n        当理解对话内容的时候，需要参考上面的基础知识，然后更好的理解对应的表格schema信息是不是有用。\n        \"\"\"\n\n    def __init__(\n        self,\n        llm: byzerllm.ByzerLLM,\n        rag_config: RAGConfig,\n        byzer_sql_config: ByzerSQLConfig,\n        model_config: ModelConfig,\n    ) -> None:\n        self.llm = llm\n        self.schema_rag_client = OpenAI(\n            api_key=rag_config.schema_rag_api_key,\n            base_url=rag_config.schema_rag_base_url,\n        )\n        self.context_rag_client = OpenAI(\n            api_key=rag_config.context_rag_api_key,\n            base_url=rag_config.context_rag_base_url,\n        )\n        self.byzer_sql_url = byzer_sql_config.byzer_sql_url\n\n        self.llm_model = byzerllm.ByzerLLM()\n        self.llm_model.setup_default_model_name(model_config.llm_model)\n        self.gen_sql_llm_model = byzerllm.ByzerLLM()\n        self.gen_sql_llm_model.setup_default_model_name(model_config.gen_sql_llm_model)\n\n    def stream_chat_oai(\n        self,\n        conversations,\n        model: Optional[str] = None,\n        role_mapping=None,\n        delta_mode=False,\n        llm_config: Dict[str, Any] = {},\n    ):\n        try:\n            v, c = self._stream_chat_oai(\n                conversations=conversations,\n                model=model,\n                role_mapping=role_mapping,\n                delta_mode=delta_mode,\n                llm_config=llm_config,\n            )\n        except Exception as e:\n            error_msg = f\"Error in stream_chat_oai: {e}\\n{traceback.format_exc()}\"\n            logger.error(error_msg)\n            v, c = [str(e)], []\n\n        return v, c\n\n    def _stream_chat_oai(\n        self,\n        conversations,\n        model: Optional[str] = None,\n        role_mapping=None,\n        delta_mode=False,\n        llm_config: Dict[str, Any] = {},\n    ):\n        logger.info(f\"Starting stream_chat_oai with model: {model}\")\n        logger.debug(f\"Conversations: {conversations}\")\n        logger.debug(f\"LLM Config: {llm_config}\")\n\n        if not model:\n            model = self.llm_model.default_model_name\n        logger.info(f\"Using model: {model}\")\n\n        query = conversations[-1][\"content\"]\n\n        if (\n            \"使用四到五个字直接返回这句话的简要主题，不要解释、不要标点、不要语气词、不要多余文本，不要加粗，如果没有主题\"\n            in query\n            or \"简要总结一下对话内容，用作后续的上下文提示 prompt，控制在 200 字以内\"\n            in query\n        ):\n            chunks = self.llm.stream_chat_oai(\n                conversations=conversations,\n                model=model,\n                role_mapping=role_mapping,\n                llm_config=llm_config,\n                delta_mode=True,\n            )\n            return (chunk[0] for chunk in chunks), []\n\n        logger.info(\"Fetching context from context RAG client\")\n        context_query = self.gen_context.prompt(conversations=conversations)\n        context_response = self.context_rag_client.chat.completions.create(\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": json.dumps(\n                        {\"query\": context_query, \"only_contexts\": True},\n                        ensure_ascii=False,\n                    ),\n                }\n            ],\n            model=model,\n        )\n\n        context = context_response.choices[0].message.content\n        new_context = \"\"\n        for msg in context.split(\"\\n\"):\n            if msg.strip():\n                v = json.loads(msg)\n                new_context += v[\"source_code\"]\n        logger.info(f\"context {new_context[0:100]}...\")\n        logger.info(\"Context retrieved successfully\")\n\n        logger.info(\"Fetching schemas from schema RAG client\")\n        schema_query = self.fetch_schema.prompt(context=new_context)\n\n        response = self.schema_rag_client.chat.completions.create(\n            messages=conversations[0:-1]\n            + [\n                {\n                    \"role\": \"user\",\n                    \"content\": json.dumps(\n                        {\"query\": schema_query, \"only_contexts\": True},\n                        ensure_ascii=False,\n                    ),\n                }\n            ],\n            model=model,\n        )        \n\n        schema_str = response.choices[0].message.content\n        logger.debug(f\"Schema RAG response: {schema_str}\")\n        logger.info(\"Schema retrieved successfully\")\n\n        logger.info(\"Generating SQL string\")\n        sql_str = self.gen_sql.with_llm(self.gen_sql_llm_model).run(\n            conversations=conversations,\n            context=new_context,\n            schema=schema_str,\n            model=model,\n        )\n\n        logger.info(\"SQL string generated\")\n        logger.debug(f\"Generated SQL string: {sql_str}\")\n\n        # Extract SQL code from tags\n        tag_extractor = TagExtractor(sql_str)\n        extracted_tags = tag_extractor.extract()\n        sql = extracted_tags.content[0].content\n        logger.info(f\"Extracted SQL: {sql}\")\n\n        logger.info(f\"Sending SQL to Byzer SQL URL: {self.byzer_sql_url}\")\n        try:\n            response = requests.post(\n                self.byzer_sql_url,\n                data={\n                    \"sql\": sql,\n                    \"owner\": \"hello\",\n                    \"sessionPerUser\": True,\n                    \"sessionPerRequest\": True,\n                    \"includeSchema\": True,\n                },\n            )\n            response.raise_for_status()\n            result = response.content.decode(\"utf-8\")\n            logger.info(\"SQL execution result decoded\")\n            logger.debug(f\"SQL execution result: {result}\")\n            return (item for item in [result]), []\n        except requests.exceptions.RequestException as e:\n            logger.error(f\"Failed to send SQL to Byzer SQL URL: {e}\")\n            if hasattr(e.response, \"content\"):\n                error_content = e.response.content.decode(\"utf-8\")\n                logger.error(f\"Server response: {error_content}\")\n            else:\n                logger.error(\"No server response available\")\n            raise Exception(f\"Byzer SQL request failed: {str(e)}\") from e\n\n\n##File: /Users/allwefantasy/projects/super_analysis/setup.py\nfrom setuptools import setup, find_packages\n\n# Read requirements from requirements.txt\nwith open('requirements.txt') as f:\n    requirements = f.read().splitlines()\n\nsetup(\n    name=\"super-analysis\",\n    version=\"0.1.0\",\n    packages=find_packages(where=\"src\"),\n    package_dir={\"\": \"src\"},\n    install_requires=requirements,\n    entry_points={\n        \"console_scripts\": [\n            \"super-analysis.serve=superanalysis.serve.cli:main\",\n            \"super-analysis.convert=superanalysis.serve.convert:main\",\n            \"superanalysis.api=superanalysis.serve.rest:main\",\n        ],\n    },\n    author=\"allwefantasy\",\n    author_email=\"allwefantasy@gmail.com\",\n    description=\"A package for super analysis with ByzerLLM\",\n    long_description=open(\"README.md\").read(),\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/yourusername/super-analysis\",\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n    ],\n    python_requires='>=3.6',\n)\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/convert.py\nimport os\nimport argparse\nimport pandas as pd\nimport hashlib\nfrom typing import List, Dict\n\ndef get_csv_files(directory: str) -> List[str]:\n    \"\"\"Recursively get all CSV files in the given directory.\"\"\"\n    csv_files = []\n    for root, _, files in os.walk(directory):\n        for file in files:\n            if file.endswith('.csv'):\n                csv_files.append(os.path.join(root, file))\n    return csv_files\n\ndef extract_schema(df: pd.DataFrame) -> Dict[str, str]:\n    \"\"\"Extract schema from pandas DataFrame.\"\"\"\n    schema = {}\n    for column in df.columns:\n        dtype = str(df[column].dtype)\n        if dtype == 'object':\n            schema[column] = 'STRING'\n        elif dtype.startswith('int'):\n            schema[column] = 'INTEGER'\n        elif dtype.startswith('float'):\n            schema[column] = 'FLOAT'\n        elif dtype == 'bool':\n            schema[column] = 'BOOLEAN'\n        elif dtype.startswith('datetime'):\n            schema[column] = 'TIMESTAMP'\n        else:\n            schema[column] = 'STRING'  # Default to STRING for unknown types\n    return schema\n\ndef generate_schema_text(schema: Dict[str, str], csv_file: str, df: pd.DataFrame, include_rows_num: int) -> str:\n    \"\"\"Generate schema text from the schema dictionary and include the CSV file name and sample data.\"\"\"\n    schema_lines = [f\"CSV File: {csv_file}\\n\"]\n    schema_lines.extend([f\"{field}: {dtype}\" for field, dtype in schema.items()])\n    \n    if include_rows_num > 0:\n        schema_lines.append(f\"\\nSample Data (first {include_rows_num} rows):\")\n        sample_data = df.head(include_rows_num).to_string(index=False)\n        schema_lines.append(sample_data)\n    \n    return \"\\n\".join(schema_lines)\n\ndef save_schema(schema_text: str, csv_file: str, output_dir: str) -> None:\n    \"\"\"Save schema text to a file in the output directory.\"\"\"\n    # Generate MD5 hash of the schema text\n    file_name = hashlib.md5(schema_text.encode()).hexdigest() + '.md'\n    output_path = os.path.join(output_dir, file_name)\n    \n    with open(output_path, 'w') as f:\n        f.write(schema_text)\n    print(f\"Schema for {csv_file} saved to {output_path}\")\n\ndef convert(data_dir: str, doc_dir: str, include_rows_num: int) -> None:\n    \"\"\"Main conversion function.\"\"\"\n    csv_files = get_csv_files(data_dir)\n    \n    for csv_file in csv_files:\n        print(f\"Processing {csv_file}\")\n        df = pd.read_csv(csv_file)\n        schema = extract_schema(df)\n        schema_text = generate_schema_text(schema, csv_file, df, include_rows_num)\n        save_schema(schema_text, csv_file, doc_dir)\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Convert CSV files to schema documents\")\n    parser.add_argument(\"--data_dir\", required=True, help=\"Directory containing CSV files\")\n    parser.add_argument(\"--doc_dir\", required=True, help=\"Output directory for schema documents\")\n    parser.add_argument(\"--include-rows-num\", type=int, default=0, help=\"Number of sample rows to include in the schema\")\n    \n    args = parser.parse_args()\n    \n    if not os.path.exists(args.doc_dir):\n        os.makedirs(args.doc_dir)\n    \n    convert(args.data_dir, args.doc_dir, args.include_rows_num)\n\nif __name__ == \"__main__\":\n    main()\n\n##File: /Users/allwefantasy/projects/super_analysis/README.md\n# Super Analysis\n\nSuper Analysis 支持 NLP2SQL 以及 使用 SQL 对表中的文本字段做自然语言分析。\n\n## 安装\n\n你可以使用 pip 安装 Super Analysis：\n\n```bash\ngit clone https://github.com/allwefantasy/super_analysis\ncd super-analysis\npip install -e .\n```\n\n\n\n## 示例数据\n\n下载电影数据集： https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset/download?datasetVersionNumber=7\n\n\n## 启动分析服务\n\n整个 super-analysis 服务分为四个部分：\n\n1. byzer-sql 分布式执行引擎 \n2. schema 文档知识库\n4. context 文档知识库\n5. super-analysis 自身\n\n在启动 super-analysis 之前，你需要启动 schema 文档知识库和 context 文档知识库以及 byzer-sql 引擎。\n\n\n在你的电脑上参考 [这个文档](./docs/4.3.1%20安装与配置%20Byzer-SQL.pdf) 完成部署.\n\n> 启动的时候需要在安装有 super-analysis 的conda环境中启动。\n\n当 byzer-sql 部署完成后，注册账号为 `hello`, 然后进入到 byzer-sql 的控制台中执行以下脚本：\n\n```sql\n!byzerllm setup single;\n\nrun command as LLM.`` where \naction=\"infer\"\nand reconnect=\"true\"\nand pretrainedModelType=\"saas/openai\"\nand udfName=\"deepseek_chat\";\n```\n\n现在可以按如下方式做一些数据预处理或者启动相关服务：\n\n```bash\n## 将电影数据集的csv文件转换为schema文档\nsuper-analysis.convert --data_dir /Users/allwefantasy/data/movice --doc_dir /Users/allwefantasy/data/movice/schemas/\n\n# 启动schema文档知识库\nauto-coder doc serve \\\n--model deepseek_chat --index_filter_workers 100 \\\n--tokenizer_path /Users/allwefantasy/Downloads/tokenizer.json \\\n--doc_dir /Users/allwefantasy/data/movice/schemas/ \\\n--port 8001\n\n\n# 下载 Byzer-SQL 专门为大模型优化的文档\ngit clone https://github.com/allwefantasy/llm_friendly_packages\n\n# 启动文档知识库\nauto-coder doc serve \\\n--model deepseek_chat --index_filter_workers 100 \\\n--tokenizer_path /Users/allwefantasy/Downloads/tokenizer.json \\\n--doc_dir  /Users/allwefantasy/projects/llm_friendly_packages/github.com/allwefantasy \\\n--port 8002\n\n# 启动兼容 OpenAI Server 的分析服务\nsuper-analysis.serve --served-model-name deepseek_chat --port 8000 \\\n--schema-rag-base-url http://127.0.0.1:8001/v1 \\\n--context-rag-base-url http://127.0.0.1:8002/v1 \\\n--byzer-sql-url http://127.0.0.1:9003/run/script\n```\n\n现在你可以用 OpenAI SDK 去使用了。 具体测试和接口使用参考 [openai_local_api.ipynb](./notebooks/openai_local_api.ipynb)\n\n## 非 OpenAI 兼容接口\n\n除了兼容 OpenAI 的接口外，Super Analysis 还提供了一个简单的 REST API 接口，可以直接与模型进行交互。以下是如何使用这个接口：\n\n### 启动服务\n\n使用以下命令启动 REST API 服务：\n\n```bash\nsuperanalysis.api --schema-rag-base-url <schema_rag_url> \\\n                  --schema-rag-api-key <schema_rag_api_key> \\\n                  --context-rag-base-url <context_rag_url> \\\n                  --context-rag-api-key <context_rag_api_key> \\\n                  --byzer-sql-url <byzer_sql_url> \\\n                  --llm-model <llm_model_name> \\\n                  --gen-sql-llm-model <gen_sql_llm_model_name> \\\n                  --host <host> \\\n                  --port <port>\n```\n\n请替换尖括号中的值为实际的配置信息。\n\n### API 端点\n\n服务启动后，将提供以下 API 端点：\n\n#### POST /chat\n\n此端点用于与模型进行对话。\n\n请求体格式：\n\n```json\n{\n  \"conversations\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"你的问题或指令\"\n    }\n  ],\n  \"model\": \"可选的模型名称\"\n}\n```\n\n响应格式：\n\n```json\n{\n  \"response\": \"模型的回答\"\n}\n```\n\n### 示例使用\n\n以下是使用 Python 请求库与 API 交互的示例：\n\n```python\nimport requests\nimport json\n\nurl = \"http://localhost:8000/chat\"\npayload = {\n    \"conversations\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"你好，请介绍一下你自己。\"\n        }\n    ],\n    \"model\": \"your_model_name\"\n}\n\nresponse = requests.post(url, json=payload)\nresult = response.json()\n\nprint(result[\"response\"])\n```\n\n这个 API 相比兼容 OpenAI 接口，可以在请求中提供提供更多可选参数。\n\n## 使用方法\n\n安装完成后，你可以使用 `super-analysis.serve` 命令来启动 API 服务器：\n\n```bash\nsuper-analysis.serve --served-model-name your_model_name\n```\n\n### 命令行参数\n\n以下是可用的命令行参数：\n\n- `--host`: 服务器绑定的主机地址（默认：0.0.0.0）\n- `--port`: 服务器绑定的端口（默认：8000）\n- `--uvicorn-log-level`: Uvicorn 日志级别（默认：info）\n- `--allow-credentials`: 允许凭证（默认：False）\n- `--allowed-origins`: 允许的源（默认：[\"*\"]）\n- `--allowed-methods`: 允许的方法（默认：[\"*\"]）\n- `--allowed-headers`: 允许的头部（默认：[\"*\"]）\n- `--api-key`: 用于认证的 API 密钥\n- `--served-model-name`: 要服务的模型名称（必需）\n- `--prompt-template`: 提示模板\n- `--response-role`: 响应角色（默认：assistant）\n- `--ssl-keyfile`: SSL 密钥文件路径\n- `--ssl-certfile`: SSL 证书文件路径\n\n例如：\n\n```bash\nsuper-analysis.serve --served-model-name deepseek_chat --port 8000\n```\n\n\n## 抽取csv文件schema\n\nThe `super-analysis.convert` command allows you to convert CSV files to schema documents. This is useful for generating metadata about your CSV files that can be used for further analysis or documentation.\n\n#### Usage\n\n```\nsuper-analysis.convert --data_dir <path_to_csv_files> --doc_dir <output_directory> [--include-rows-num <number_of_sample_rows>]\n```\n\nArguments:\n- `--data_dir`: Directory containing CSV files (required)\n- `--doc_dir`: Output directory for schema documents (required)\n- `--include-rows-num`: Number of sample rows to include in the schema (optional, default is 0)\n\nThe utility will:\n1. Recursively find all CSV files in the specified directory\n2. Extract the schema from each CSV file\n3. Generate a schema text that includes:\n   - The CSV file name\n   - Field names and their data types\n   - Sample data (if `--include-rows-num` is specified)\n4. Save the schema text as a markdown file in the output directory\n\n## 联系方式\n\n如有任何问题或建议，请联系 allwefantasy@gmail.com。\n\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/rest.py\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nfrom typing import List, Optional\nimport uvicorn\nimport argparse\nfrom superanalysis.sql.generator import SQLGenerator\nfrom superanalysis.sql.types import RAGConfig, ByzerSQLConfig, ModelConfig\nimport byzerllm\n\napp = FastAPI()\n\nclass ChatMessage(BaseModel):\n    role: str\n    content: str\n\nclass ChatRequest(BaseModel):\n    conversations: List[ChatMessage]\n    model: Optional[str] = None\n\nclass ChatResponse(BaseModel):\n    response: str\n\n@app.post(\"/chat\", response_model=ChatResponse)\nasync def chat(request: ChatRequest):\n    try:\n        result, _ = sql_generator.stream_chat_oai(\n            conversations=[{\"role\": msg.role, \"content\": msg.content} for msg in request.conversations],\n            model=request.model\n        )\n        return ChatResponse(response=\"\".join(result))\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"SuperAnalysis API Server\")\n    parser.add_argument(\"--host\", type=str, default=\"0.0.0.0\", help=\"Host to bind the server to\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"Port to bind the server to\")\n    parser.add_argument(\"--schema-rag-base-url\", type=str, required=True, help=\"Schema RAG base URL\")\n    parser.add_argument(\"--schema-rag-api-key\", type=str, required=True, help=\"Schema RAG API key\")\n    parser.add_argument(\"--context-rag-base-url\", type=str, required=True, help=\"Context RAG base URL\")\n    parser.add_argument(\"--context-rag-api-key\", type=str, required=True, help=\"Context RAG API key\")\n    parser.add_argument(\"--byzer-sql-url\", type=str, required=True, help=\"Byzer SQL URL\")\n    parser.add_argument(\"--data-dir\", type=str, default=\"\", help=\"Data directory\")\n    parser.add_argument(\"--owner\", type=str, default=\"hello\", help=\"Owner of the Byzer SQL session\")\n    parser.add_argument(\"--llm-model\", type=str, required=True, help=\"LLM model for general use\")\n    parser.add_argument(\"--gen-sql-llm-model\", type=str, required=True, help=\"LLM model for SQL generation\")\n    \n    args = parser.parse_args()\n\n    rag_config = RAGConfig(\n        schema_rag_base_url=args.schema_rag_base_url,\n        schema_rag_api_key=args.schema_rag_api_key,\n        context_rag_base_url=args.context_rag_base_url,\n        context_rag_api_key=args.context_rag_api_key\n    )\n\n    byzer_sql_config = ByzerSQLConfig(\n        byzer_sql_url=args.byzer_sql_url,\n        data_dir=args.data_dir,\n        owner=args.owner\n    )\n\n    model_config = ModelConfig(\n        llm_model=args.llm_model,\n        gen_sql_llm_model=args.gen_sql_llm_model\n    )\n\n    llm = byzerllm.ByzerLLM()\n    llm.setup_default_model_name(args.llm_model)\n\n    global sql_generator\n    sql_generator = SQLGenerator(\n        llm=llm,\n        rag_config=rag_config,\n        byzer_sql_config=byzer_sql_config,\n        model_config=model_config\n    )\n\n    uvicorn.run(app, host=args.host, port=args.port)\n\nif __name__ == \"__main__\":\n    main()\n\n##File: .auto-coder/libs/llm_friendly_packages/github.com/allwefantasy/byzer-llm/README.md\n# byzerllm 大模型编程快速指南\n\n本文示例 [Notebook](../../notebooks/003_byzerllm_大模型编程快速指南.ipynb)\n\n## 安装\n\n```bash\npip install -U byzerllm\nray start --head\n```\n## 启动一个模型代理\n\nbyzerllm 支持私有化模型或者SaaS模型的部署。\n\n这里以 deepseek 官方API 为例：\n\n```bash\neasy-byzerllm deploy deepseek-chat --token xxxxx --alias deepseek_chat\n```\n\n或者跬基流动API:\n\n```bash\neasy-byzerllm deploy alibaba/Qwen1.5-110B-Chat --token xxxxx --alias qwen110b_chat\n```\n\n将上面的 API KEY 替换成你们自己的。\n\n如果你想部署私有化模型或者对接 Ollama 等更多需求，参考 [002_使用byzerllm进行模型部署.md](./002_使用byzerllm进行模型部署.md) 或者 [README.md](../../README.md)。\n\n之后，你就可以在代码里使用  deepseek_chat 或者 qwen110b_chat  访问模型了。\n\n## hello world\n\n来和我们的大模型打个招呼:\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\n@byzerllm.prompt(llm=llm)\ndef hello(q:str) ->str:\n    '''\n    你好, {{ q }}\n    '''\n\ns = hello(\"你是谁\")    \nprint(s)\n\n## 输出:\n## '你好！我是一个人工智能助手，专门设计来回答问题、提供信息和帮助解决问题。如果你有任何疑问或需要帮助，请随时告诉我。'\n```\n\n恭喜，你和大模型成功打了招呼！\n\n可以看到，我们通过 `@byzerllm.prompt` 装饰器，将一个方法转换成了一个大模型的调用，然后这个方法的主题是一段文本，文本中\n使用了 jinja2 模板语法，来获得方法的参数。当正常调用该方法时，实际上就发起了和大模型的交互，并且返回了大模型的结果。\n\n在 byzerllm 中，我们把这种方法称之为 prompt 函数。\n\n## 查看发送给大模型的prompt\n\n很多情况你可能需要调试，查看自己的 prompt 渲染后到底是什么样子的，这个时候你可以通过如下方式\n获取渲染后的prompt:\n\n```python\nhello.prompt(\"你是谁\")\n## '你好, 你是谁'\n```            \n\n## 动态换一个模型\n\n前面的 hello 方法在初始化的时候，我们使用了默认的模型 deepseek_chat，如果我们想换一个模型，可以这样做：\n\n```python\nhello.with_llm(llm).run(\"你是谁\")\n## '你好！我是一个人工智能助手，专门设计来回答问题、提供信息和帮助解决问题。如果你有任何疑问或需要帮助，请随时告诉我。'\n```\n\n通过 with_llm 你可以设置一个新的 llm 对象，然后调用 run 方法，就可以使用新的模型了。\n\n## 超长文本生成\n\n我们知道，大模型一次生成的长度其实是有限的，如果你想生成超长文本，你可能需手动的不断获得\n生成结果，然后把他转化为输入，然后再次生成，这样的方式是比较麻烦的。\n\nbyzerllm 提供了更加易用的 API :\n\n```python\nimport byzerllm\nfrom byzerllm import ByzerLLM\n\nllm = ByzerLLM.from_default_model(\"deepseek_chat\")\n\n@byzerllm.prompt()\ndef tell_story() -> str:\n    \"\"\"\n    讲一个100字的故事。    \n    \"\"\"\n\n\ns = (\n    tell_story.with_llm(llm)\n    .with_response_markers()\n    .options({\"llm_config\": {\"max_length\": 10}})\n    .run()\n)\nprint(s)\n\n## 从前，森林里住着一只聪明的小狐狸。一天，它发现了一块闪闪发光的宝石。小狐狸决定用这块宝石帮助森林里的动物们。它用宝石的光芒指引迷路的小鹿找到了回家的路，用宝石的温暖治愈了受伤的小鸟。从此，小狐狸成了森林里的英雄，动物们都感激它的善良和智慧。\n```\n\n实际核心部分就是这一行：\n\n```python\ntell_story.with_llm(llm)\n    .with_response_markers()    \n    .run()\n```\n\n我们只需要调用 `with_response_markers` 方法，系统就会自动的帮我们生成超长文本。\n在上面的案例中，我们通过\n\n```python\n.options({\"llm_config\": {\"max_length\": 10}})\n```\n\n认为的限制大模型一次交互最多只能输出10个字符，但是系统依然自动完成了远超过10个字符的文本生成。\n\n## 对象输出\n\n前面我们的例子都是返回字符串，但是我们也可以返回对象，这样我们就可以更加灵活的处理返回结果。\n\n```python\nimport pydantic \n\nclass Story(pydantic.BaseModel):\n    '''\n    故事\n    '''\n\n    title: str = pydantic.Field(description=\"故事的标题\")\n    body: str = pydantic.Field(description=\"故事主体\")\n\n@byzerllm.prompt()\ndef tell_story()->Story:\n    '''\n    讲一个100字的故事。    \n    '''\n\ns = tell_story.with_llm(llm).run()\nprint(isinstance(s, Story))\nprint(s.title)\n\n## True\n## 勇敢的小鸟\n```\n\n可以看到，我们很轻松的将输出转化为格式化输出。\n\n## 自定义字段抽取\n\n前面的结构化输出，其实会消耗更多token,还有一种更加精准的结构化输出方式。\n比如让大模型生成一个正则表达式，但实际上大模型很难准确只输出一个正则表达式，这个时候我们可以通过自定义抽取函数来获取我们想要的结果。\n\n\n```python\nfrom loguru import logger\nimport re\n\n@byzerllm.prompt()\ndef generate_regex_pattern(desc: str) -> str:\n    \"\"\"\n    根据下面的描述生成一个正则表达式，要符合python re.compile 库的要求。\n\n    {{ desc }}\n\n    最后生成的正则表达式要在<REGEX></REGEX>标签对里。\n    \"\"\"    \n\ndef extract_regex_pattern(regex_block: str) -> str:    \n    pattern = re.search(r\"<REGEX>(.*)</REGEX>\", regex_block, re.DOTALL)\n    if pattern is None:\n        logger.warning(\"No regex pattern found in the generated block:\\n {regex_block}\")\n        raise None\n    return pattern.group(1)\n\npattern = \"匹配一个邮箱地址\"\nv = generate_regex_pattern.with_llm(llm).with_extractor(extract_regex_pattern).run(desc=pattern)\nprint(v)\n## ^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\n```\n\n在上面的例子里，我们根据一句话生成一个正则表达式。我们通过 `with_extractor` 方法，传入了一个自定义的抽取函数，这个函数会在大模型生成结果后，对结果进行处理，然后返回我们想要的结果。\n\n我们在 prompt 明确说了，生成的结果要放到 `<REGEX></REGEX>` 标签对里，然后我们通过 extract_regex_pattern 函数，从结果中提取出了我们想要的正则表达式。\n\n## 在实例方法中使用大模型\n\n```python\nimport byzerllm\ndata = {\n    'name': 'Jane Doe',\n    'task_count': 3,\n    'tasks': [\n        {'name': 'Submit report', 'due_date': '2024-03-10'},\n        {'name': 'Finish project', 'due_date': '2024-03-15'},\n        {'name': 'Reply to emails', 'due_date': '2024-03-08'}\n    ]\n}\n\n\nclass RAG():\n    def __init__(self):        \n        self.llm = byzerllm.ByzerLLM()\n        self.llm.setup_template(model=\"deepseek_chat\",template=\"auto\")\n        self.llm.setup_default_model_name(\"deepseek_chat\")        \n    \n    @byzerllm.prompt(lambda self:self.llm)\n    def generate_answer(self,name,task_count,tasks)->str:\n        '''\n        Hello {{ name }},\n\n        This is a reminder that you have {{ task_count }} pending tasks:\n        {% for task in tasks %}\n        - Task: {{ task.name }} | Due: {{ task.due_date }}\n        {% endfor %}\n\n        Best regards,\n        Your Reminder System\n        '''        \n\nt = RAG()\n\nresponse = t.generate_answer(**data)\nprint(response)\n\n## 输出:\n## Hello Jane Doe,\n##I hope this message finds you well. I wanted to remind you of your 3 pending tasks to ensure you stay on track:\n## 1. **Submit report** - This task is due on **2024-03-10**. Please ensure that you allocat\n```\n\n这里我们给了个比较复杂的例子，但我们可以看到，给一个实例prompt方法和普通prompt 方法差异不大。\n唯一的区别是如果你希望在定义的时候就指定大模型，使用一个lambda函数返回实例的 llm 对象即可。\n\n```python\n@byzerllm.prompt(lambda self:self.llm)\n```\n\n你也可以不返回，在调用的时候通过 `with_llm` 方法指定 llm 对象。\n\n此外，这个例子也展示了如何通过jinja2模板语法，来处理复杂的结构化数据。\n\n## 通过 Python 代码处理复杂入参\n\n上面的一个例子中，我们通过 jinja2 模板语法，来处理复杂的结构化数据，但是有时候我们可能需要更加复杂的处理，这个时候我们可以通过 Python 代码来处理。\n\n```python\nimport byzerllm\n\ndata = {\n    'name': 'Jane Doe',\n    'task_count': 3,\n    'tasks': [\n        {'name': 'Submit report', 'due_date': '2024-03-10'},\n        {'name': 'Finish project', 'due_date': '2024-03-15'},\n        {'name': 'Reply to emails', 'due_date': '2024-03-08'}\n    ]\n}\n\n\nclass RAG():\n    def __init__(self):        \n        self.llm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n    \n    @byzerllm.prompt()\n    def generate_answer(self,name,task_count,tasks)->str:\n        '''\n        Hello {{ name }},\n\n        This is a reminder that you have {{ task_count }} pending tasks:\n            \n        {{ tasks }}\n\n        Best regards,\n        Your Reminder System\n        '''\n        \n        tasks_str = \"\\n\".join([f\"- Task: {task['name']} | Due: { task['due_date'] }\" for task in tasks])\n        return {\"tasks\": tasks_str}\n\nt = RAG()\n\nresponse = t.generate_answer.with_llm(t.llm).run(**data)\nprint(response)\n\n## Just a gentle nudge to keep you on track with your pending tasks. Here's a quick recap:....\n```\n\n在这个例子里，我们直接把 tasks 在方法体里进行处理，然后作为一个字符串返回，最够构建一个字典，字典的key为 tasks,然后\n你就可以在 docstring 里使用 `{{ tasks }}` 来引用这个字符串。\n\n这样对于很复杂的入参，就不用谢繁琐的 jinja2 模板语法了。\n\n## 如何自动实现一个方法\n\n比如我定义一个签名，但是我不想自己实现里面的逻辑，让大模型来实现。这个在 byzerllm 中叫 function impl。我们来看看怎么\n实现:\n\n```python\nimport pydantic\nclass Time(pydantic.BaseModel):\n    time: str = pydantic.Field(...,description=\"时间，时间格式为 yyyy-MM-dd\")\n\n\n@llm.impl()\ndef calculate_current_time()->Time:\n    '''\n    计算当前时间\n    '''\n    pass \n\n\ncalculate_current_time()\n#output: Time(time='2024-06-14')\n```\n\n在这个例子里，我们定义了一个 calculate_current_time 方法，但是我们没有实现里面的逻辑，我们通过 `@llm.impl()` 装饰器，让大模型来实现这个方法。\n为了避免每次都要“生成”这个方法，导致无法适用，我们提供了缓存，用户可以按如下方式打印速度：\n\n```python\nstart = time.monotonic()\ncalculate_current_time()\nprint(f\"first time cost: {time.monotonic()-start}\")\n\nstart = time.monotonic()\ncalculate_current_time()\nprint(f\"second time cost: {time.monotonic()-start}\")\n\n# output:\n# first time cost: 6.067266260739416\n# second time cost: 4.347506910562515e-05\n```\n可以看到，第一次执行花费了6s,第二次几乎是瞬间完成的，这是因为第一次执行的时候，我们实际上是在生成这个方法，第二次执行的时候，我们是执行已经生成好的代码，所以速度会非常快。你可以显示的调用 `llm.clear_impl_cache()` 清理掉函数缓存。\n\n## Stream 模式\n\n前面的例子都是一次性生成结果，但是有时候我们可能需要一个流式的输出，这个时候我们可能需要用底层一点的API来完成了：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\nv = llm.stream_chat_oai(model=\"deepseek_chat\",conversations=[{\n    \"role\":\"user\",\n    \"content\":\"你好，你是谁\",\n}],delta_mode=True)\n\nfor t in v:\n    print(t,flush=True)  \n\n# 你好\n# ！\n# 我\n# 是一个\n# 人工智能\n# 助手\n# ，\n# 旨在\n# 提供\n# 信息\n# 、\n# 解答\n# 问题....\n```\n\n如果你不想要流式输出，但是想用底层一点的API，你可以使用 `llm.chat_oai` 方法：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\nv = llm.chat_oai(model=\"deepseek_chat\",conversations=[{\n    \"role\":\"user\",\n    \"content\":\"你好，你是谁\",\n}])\n\nprint(v[0].output)\n## 你好！我是一个人工智能助手，旨在提供信息、解答问题和帮助用户解决问题。如果你有任何问题或需要帮助，请随时告诉我。\n```\n\n## Function Calling \n\nbyzerllm 可以不依赖模型自身就能提供 function calling 支持，我们来看个例子：\n\n\n```python\nfrom typing import List,Dict,Any,Annotated\nimport pydantic \nimport datetime\nfrom dateutil.relativedelta import relativedelta\n\ndef compute_date_range(count:Annotated[int,\"时间跨度，数值类型\"],\n                       unit:Annotated[str,\"时间单位，字符串类型\",{\"enum\":[\"day\",\"week\",\"month\",\"year\"]}])->List[str]:\n    '''\n    计算日期范围\n\n    Args:\n        count: 时间跨度，数值类型\n        unit: 时间单位，字符串类型，可选值为 day,week,month,year\n    '''        \n    now = datetime.datetime.now()\n    now_str = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n    if unit == \"day\":\n        return [(now - relativedelta(days=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"week\":\n        return [(now - relativedelta(weeks=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"month\":\n        return [(now - relativedelta(months=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"year\":\n        return [(now - relativedelta(years=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    return [\"\",\"\"]\n\ndef compute_now()->str:\n    '''\n    计算当前时间\n    '''\n    return datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n```\n\n我们定义了两个方法，一个是计算日期范围，一个是计算当前时间。\n\n现在我么可以来测试下，系统如何根据自然语言决定调用哪个方法：\n\n```python\nt = llm.chat_oai([{\n    \"content\":'''计算当前时间''',\n    \"role\":\"user\"    \n}],tools=[compute_date_range,compute_now],execute_tool=True)\n\nt[0].values\n\n## output: ['2024-06-14 15:18:02']\n```\n\n我们可以看到，他正确的选择了 compute_now 方法。\n\n接着我们再试一个：\n\n```python\nt = llm.chat_oai([{\n    \"content\":'''最近三个月趋势''',\n    \"role\":\"user\"    \n}],tools=[compute_date_range,compute_now],execute_tool=True)\n\nt[0].values\n\n## output: [['2024-03-14 15:19:13', '2024-06-14 15:19:13']]\n```\n\n模型正确的选择了 compute_date_range 方法。\n\n## 多模态\n\nbyerllm 也能很好的支持多模态的交互，而且统一了多模态大模型的接口，比如你可以用一样的方式使用 openai 或者 claude 的图片转文字能力， 或者一致的方式使用火山，azuer, openai的语音合成接口。\n\n### 图生文\n\n```python\nimport byzerllm\nfrom byzerllm.types import ImagePath\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\n\n\n@byzerllm.prompt()\ndef what_in_image(image_path: ImagePath) -> str:\n    \"\"\"\n    {{ image_path }}\n    这个图片里有什么？\n    \"\"\"    \n\n\nv = what_in_image.with_llm(vl_llm).run(\n    ImagePath(value=\"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\")\n)\nv\n## OUTPUT: 这张图片展示了多只可爱的猫咪，采用了艺术风格的绘画。猫咪们有不同的颜色和花纹，背景是浅棕色，上面还点缀着一些红色的花朵。整体画面给人一种温馨和谐的感觉\n```\n\n可以看到，我们只需要把 prompt 函数的图片地址入参使用 byzerllm.types.ImagePath里进行包装，就可以直接在 prompt 函数体里\n带上图片。\n\n或者你可以这样：\n\n```python\nimport byzerllm\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\n\n\n@byzerllm.prompt()\ndef what_in_image(image_path: str) -> str:\n    \"\"\"\n    {{ image }}\n    这个图片里有什么？\n    \"\"\"\n    return {\"image\": byzerllm.Image.load_image_from_path(image_path)}\n\n\nv = what_in_image.with_llm(vl_llm).run(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv\n```\n\n通过 `image_path` 参数，然后通过 `byzerllm.Image.load_image_from_path` 方法，转化为一个图片对象 image，最后在 prompt 函数体里\n使用 `{{ image }}` 引用这个图片对象。\n\n另外我们也是可以支持配置多张图片的。\n\n另外，我们也可以使用基础的 `llm.chat_oai` 方法来实现：\n\n```python\nimport byzerllm\nimport json\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\nimage = byzerllm.Image.load_image_from_path(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv = vl_llm.chat_oai(\n    conversations=[\n        {\n            \"role\": \"user\",\n            \"content\": json.dumps(\n                [{\"image\": image, \"text\": \"这个图片里有什么？\"}], ensure_ascii=False\n            ),\n        }\n    ]\n)\nv[0].output\n```\n\n还可以这么写：\n\n```python\nimport byzerllm\nimport json\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\nimage = byzerllm.Image.load_image_from_path(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv = vl_llm.chat_oai(\n    conversations=[\n        {\n            \"role\": \"user\",\n            \"content\": json.dumps(\n                [\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\"url\": image, \"detail\": \"high\"},\n                    },\n                    {\"text\": \"这个图片里有什么？\", \"type\": \"text\"},\n                ],\n                ensure_ascii=False,\n            ),\n        }\n    ]\n)\nv[0].output\n```\n\n### 语音合成\n\n这里以 openai 的 tts 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=tts-1 \\\n--model openai_tts\n```\n\n此外，byzerllm 支持 azure,火山引擎等 tts 语音合成引擎。\n\n接着你可以这么用：\n\n```python\nimport byzerllm\nimport base64\nimport json\n\nllm = byzerllm.ByzerLLM.from_default_model(\"openai_tts\")\n\n\nt = llm.chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\": json.dumps({\n        \"input\":\"hello, open_tts\",\n        \"voice\": \"alloy\",\n        \"response_format\": \"mp3\"\n    },ensure_ascii=False)\n}])\n\nwith open(\"voice.mp3\",\"wb\") as f:\n    f.write(base64.b64decode(t[0].output))\n```\n\ntts 模型生成没有prompt函数可以用，你需要直接使用 chat_oai。\n\n\n### 语音识别\n\n这里以 openai 的 whisper-1 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--worker_concurrency 10 \\\n--infer_params saas.model=whisper-1 saas.api_key=${MODEL_OPENAI_TOKEN} \\\n--model speech_to_text\n```\n\n语音识别的使用方式和图生文类似，我们可以直接在 prompt 函数体里带上音频文件。\n\n```python\nimport byzerllm\nimport json\nimport base64\nfrom byzerllm.types import AudioPath\n\nllm = byzerllm.ByzerLLM.from_default_model(\"speech_to_text\")\n\naudio_file = \"/Users/allwefantasy/videos/output_audio.mp3\"\n\n@byzerllm.prompt(llm=llm)\ndef audio_to_text(audio_file: AudioPath):\n    \"\"\"\n    {{ audio_file }}\n    \"\"\"\n\nv = audio_to_text(AudioPath(value=audio_file))\njson.loads(v)\n```\n输出的数据格式略微复杂：\n\n```\n{'text': 'In the last chapter, you and I started to step through the internal workings of a transformer. This is one of the key pieces of technology inside large language models, and a lot of other tools in the modern wave of AI.',\n 'task': 'transcribe',\n 'language': 'english',\n 'duration': 10.0,\n 'segments': [{'id': 0,\n   'seek': 0,\n   'start': 0.0,\n   'end': 4.78000020980835,\n   'text': ' In the last chapter, you and I started to step through the internal workings of a transformer.',\n   'tokens': [50364,\n    682,\n    264,\n    1036,\n    7187,\n    11,\n    291,\n    293,\n    286,\n    1409,\n    281,\n    1823,\n    807,\n    264,\n    6920,\n    589,\n    1109,\n    295,\n    257,\n    31782,\n    13,\n    50586],\n   'temperature': 0.0,\n   'avg_logprob': -0.28872039914131165,\n   'compression_ratio': 1.4220778942108154,\n   'no_speech_prob': 0.016033057123422623},\n  ....\n  {'id': 2,\n   'seek': 0,\n   'start': 8.579999923706055,\n   'end': 9.979999542236328,\n   'text': ' and a lot of other tools in the modern wave of AI.',\n   'tokens': [50759,\n    293,\n    257,\n    688,\n    295,\n    661,\n    3873,\n    294,\n    264,\n    4363,\n    5772,\n    295,\n    7318,\n    13,\n    50867],\n   'temperature': 0.0,\n   'avg_logprob': -0.28872039914131165,\n   'compression_ratio': 1.4220778942108154,\n   'no_speech_prob': 0.016033057123422623}],\n 'words': [{'word': 'In', 'start': 0.0, 'end': 0.18000000715255737},\n  {'word': 'the', 'start': 0.18000000715255737, 'end': 0.23999999463558197},\n  {'word': 'last', 'start': 0.23999999463558197, 'end': 0.5400000214576721},\n  {'word': 'chapter', 'start': 0.5400000214576721, 'end': 0.800000011920929},\n  ....\n  {'word': 'AI', 'start': 9.920000076293945, 'end': 9.979999542236328}]}\n```\n\n会输出每一句话以及每一个字所在的起始时间和截止时间。你可以根据需要来使用。\n\n\n### 文生图\n\n文生图和语音合成类似，首先要启动合适的模型,以openai 的 dall-e-3 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=dall-e-3 \\\n--model openai_image_gen\n```\n\n启动模型后，只需要记住几个模板参数即可使用，这里直接使用 chat_oai 方法来使用：\n\n```python\n\nimport byzerllm\nimport json\nimport base64\n\nllm = byzerllm.ByzerLLM.from_default_model(\"openai_image_gen\")\nt = llm.chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\": json.dumps({\n        \"input\":\"a white siamese cat\",\n        \"size\": \"1024x1024\",\n        \"quality\": \"standard\"\n    },ensure_ascii=False)\n}])\n\nwith open(\"output1.jpg\",\"wb\") as f:\n    f.write(base64.b64decode(t[0].output))\n\n\nimport matplotlib.pyplot as plt\n\nimage_path = \"output1.jpg\"\nimage = plt.imread(image_path)\n\nplt.imshow(image)\nplt.axis('off')\nplt.show()\n```\n\n## Prompt 函数的流式输出\n\nbyzerllm 底层支持流式输出，非 prompt 函数的用法是这样的：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\n\nv = llm.stream_chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\":\"讲一个100字的故事\"\n}])\n\nfor s  in v:\n    print(s[0], end=\"\")\n```\n\n如果你像用 prompt 函数，可以这么用：\n\n\n```python\nimport byzerllm\nimport json\nimport base64\nfrom typing import Generator\n\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\n\n@byzerllm.prompt()\ndef tell_story() -> Generator[str, None, None]:\n    '''\n    给我讲一个一百多字的故事\n    '''\n\nv = tell_story.with_llm(llm).run()    \nfor i in v:\n    print(i, end=\"\")\n```\n\n可以看到，和普通的 prompt 函数的区别在于，返回值是一个生成器，然后你可以通过 for 循环来获取结果。\n\n## 向量化模型\n\nbyzerllm 支持向量化模型,你可以这样启动一个本地的模型：\n\n```bash\n!byzerllm deploy --pretrained_model_type custom/bge \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--worker_concurrency 10 \\\n--model_path /home/winubuntu/.auto-coder/storage/models/AI-ModelScope/bge-large-zh \\\n--infer_backend transformers \\\n--num_workers 1 \\\n--model emb\n```\n\n注意两个参数:\n\n1. --infer_backend transformers: 表示使用 transformers 作为推理后端。\n2. --model_path: 表示模型的路径。\n\n也可以启动一个 SaaS 的emb模型,比如 qwen 的 emb 模型：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/qianwen \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 2 \\\n--infer_params saas.api_key=${MODEL_QIANWEN_TOKEN}  saas.model=text-embedding-v2 \\\n--model qianwen_emb\n```\n\n或者 openai 的 emb 模型：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--worker_concurrency 10 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=text-embedding-3-small \\\n--model gpt_emb\n```\nSaaS 模型无需配置 `--infer_backend` 参数。\n\n无论是本地模型还是 SaaS 模型，我们都可以这样使用：\n\n```python\nimport byzerllm\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\nllm.setup_default_emb_model_name(\"emb\")\nllm.emb_query(\"你好\")\n```\n\n如果你配置 byzerllm 中的 Storage 使用，比如你这样启动了存储：\n\n```bash\nbyzerllm storage start --enable_emb\n```\n\n那么需要这么使用：\n\n```python\nfrom byzerllm.apps.byzer_storage.simple_api import ByzerStorage, DataType, FieldOption,SortOption\nstorage = ByzerStorage(\"byzerai_store\", \"memory\", \"memory\")\nstorage.emb(\"你好\")\n```\n\n## 一些辅助工具\n\n当调用 prompt 函数返回字符串的时候，如果想从里面抽取代码，可以使用如下方式：\n\n```python\nfrom byzerllm.utils.client import code_utils\ntext_with_markdown = '''\n```shell\nls -l\n```\n'''\ncode_blocks = code_utils.extract_code(text_with_markdown)\nfor code_block in code_blocks:\n    if code_block[0] == \"shell\":\n        print(code_block[1])\n##output: ls -l        \n```\n\nTagExtractor 工具用于对任意 `<_tag_></_tag_>` 标签对的抽取，下面是一个使用示例：\n\n```python\nfrom byzerllm.apps.utils import TagExtractor\n\nextractor = TagExtractor('''\n大家好\n<_image_>data:image/jpeg;base64,xxxxxx</_image_>\n                         大家好\n<_image_>data:image/jpeg;base64,xxxxxx2</_image_>\n''')\n\nv = extractor.extract()\nprint(v.content[0].content)\nprint(v.content[1].content)\n```\n输出为:\n\n```\ndata:image/jpeg;base64,xxxxxx\ndata:image/jpeg;base64,xxxxxx2\n```\n\n我们成功的将 <_image_></_image_> 标签对里的内容抽取出来了。\n\n\n## 注意事项\n\n1. prompt函数方法体返回只能是dict，实际的返回类型和方法签名可以不一样，但是方法体返回只能是dict。\n2. 大部分情况prompt函数体为空，如果一定要有方法体，可以返回一个空字典。\n3. 调用prompt方法的时候，如果在@byzerllm.prompt()里没有指定llm对象，那么需要在调用的时候通过with_llm方法指定llm对象。\n\n===========================\n\n# Byzer Storage: 最易用的AI存储引擎\n\nByzer Storage是一个为AI应用设计的高性能存储引擎,它提供了简单易用的API,支持向量搜索、全文检索以及结构化查询。本文将详细介绍Byzer Storage的使用方法和主要特性。\n\n## 0. 安装和启动\n\n```bash\npip install byzerllm\nbyzerllm storage start\n```\n\nThat's it! Byzer Storage已经安装并启动成功,现在我们可以开始使用它了。默认会启动一个 byzerai_store 的集群。\n\n注意，如果你是这样启动的：\n\n```bash\nbyzerllm storage start --enable_emb\n```\n\n那么会自动启动一个 emb 模型，名字就叫 emb， ByzerStorage 会自动使用该模型，无需做任何其他配置。\n\n## 1. 初始化\n\n创建一个 ByzerStorage 对象，链接 byzerai_store 集群，并且指定数据库和表名（可以不存在）。\n\n```python\nfrom byzerllm.apps.byzer_storage.simple_api import ByzerStorage, DataType, FieldOption, SortOption\n\nstorage = ByzerStorage(\"byzerai_store\", \"my_database1\", \"my_table4s\")\n```\n\n\n## 2. 创建库表（可选）\n\nByzer Storage使用Schema来定义数据结构。我们可以使用SchemaBuilder来构建Schema:\n\n```python\n_ = (\n    storage.schema_builder()\n    .add_field(\"_id\", DataType.STRING)\n    .add_field(\"name\", DataType.STRING)\n    .add_field(\"content\", DataType.STRING, [FieldOption.ANALYZE])\n    .add_field(\"raw_content\", DataType.STRING, [FieldOption.NO_INDEX])    \n    .add_array_field(\"summary\", DataType.FLOAT)    \n    .add_field(\"created_time\", DataType.LONG, [FieldOption.SORT])    \n    .execute()\n)\n```\n\n这个Schema定义了以下字段:\n- `_id`: 字符串类型的主键\n- `name`: 字符串类型,可用于过滤条件\n- `content`: 字符串类型,带有ANALYZE选项,用于全文搜索\n- `raw_content`: 字符串类型,带有NO_INDEX选项,不会被索引\n- `summary`: 浮点数组类型,用于存储向量\n- `created_time`: 长整型,带有SORT选项,可用于排序\n\n需要注意的是：\n\n1. 如果一个字段带有ANALYZE选项,则该字段会被分词,并且可以用于全文搜索，但是就无法返回原始的文本了。所以你需要添加一个新字段专门用来存储原文，比如在我们这个例子里，我们新增了 raw_content 字段，并且显示的指定了 NO_INDEX 选项，这样就不会被索引，也就不会被分词，可以后续被检索后用来获取原始文本。\n2. 对于需要作为向量检索和存储的字段，需要指定为数组类型，比如我们这个例子里的 summary 字段。\n3. 如果你需要拿到向量字段的原始文本，那么你也需要添加一个新字段专门用来存储原文，就像我们这个例子里的 raw_content 字段一样。\n\n\n## 3. 写入数据\n\n准备数据并使用WriteBuilder写入Storage:\n\n```python\ndata = [\n    {\"_id\": \"1\", \"name\": \"Hello\", \"content\": \"Hello, world!\", \"raw_content\": \"Hello, world!\", \"summary\": \"hello world\", \"created_time\": 1612137600},\n    {\"_id\": \"2\", \"name\": \"Byzer\", \"content\": \"Byzer, world!\", \"raw_content\": \"Byzer, world!\", \"summary\": \"byzer\", \"created_time\": 1612137601},\n    {\"_id\": \"3\", \"name\": \"AI\", \"content\": \"AI, world!\", \"raw_content\": \"AI, world!\", \"summary\": \"AI\", \"created_time\": 16121376002},\n    {\"_id\": \"4\", \"name\": \"ByzerAI\", \"content\": \"ByzerAI, world!\", \"raw_content\": \"ByzerAI, world!\", \"summary\": \"ByzerAi\", \"created_time\": 16121376003},\n]\n\nstorage.write_builder().add_items(data, vector_fields=[\"summary\"], search_fields=[\"content\"]).execute()\nstorage.commit()\n```\n\n这里我们使用`add_items`方法批量添加数据,并指定了`summary`为向量字段,`content`为搜索字段。最后调用`commit()`来确保数据被持久化。\n\n\n从上面我们要写入到 byzer storage 的数据我们可以看到如下几个特点：\n\n1. 需要包含我们之前定义的 Schema中罗列的所有的字段，同时需要指定哪些是向量字段，哪些是检索字段。\n2. 向量和检索不能是同一个字段。\n3. 对于向量，检索字段，我们给到 write_builder 的都是文本，ByzerStorage 会根据 Schema 的定义自动将其转换为向量，检索字段需要的格式。\n\n## 4. 查询数据\n\nByzer Storage支持多种查询方式,包括向量搜索、全文检索、过滤和排序。\n\n### 4.1 向量搜索 + 全文检索\n\n```python\nquery = storage.query_builder()\nquery.set_vector_query(\"ByzerAI\", fields=[\"summary\"])\nresults = query.set_search_query(\"Hello\", fields=[\"content\"]).execute()\nprint(results)\n```\n\n这个查询结合了向量搜索和全文检索,它会在`summary`字段中搜索与\"ByzerAI\"相似的向量,同时在`content`字段中搜索包含\"Hello\"的文档。\n\n### 4.2 过滤 + 向量搜索 + 全文检索\n\n```python\nquery = storage.query_builder()\nquery.and_filter().add_condition(\"name\", \"AI\").build()\nquery.set_vector_query(\"ByzerAI\", fields=\"summary\")\nresults = query.set_search_query(\"Hello\", fields=[\"content\"]).execute()\nprint(results)\n```\n\n这个查询首先过滤`name`字段等于\"AI\"的文档,然后在结果中进行向量搜索和全文检索。\n\n### 4.3 过滤 + 排序\n\n```python\nquery = storage.query_builder()\nquery.and_filter().add_condition(\"name\", \"AI\").build().sort(\"created_time\", SortOption.DESC)\nresults = query.execute()\nprint(results)\n```\n\n这个查询过滤`name`字段等于\"AI\"的文档,然后按`created_time`字段降序排序。\n\n## 5. 删除数据\n\n### 5.1 根据ID删除\n\n```python\nstorage.delete_by_ids([\"3\"])\n\nquery = storage.query_builder()\nquery.and_filter().add_condition(\"name\", \"AI\").build()\nresults = query.execute()\nprint(results)\n```\n\n这里我们删除了ID为\"3\"的文档,然后查询验证删除结果。\n\n### 5.2 删除整个表\n\n```python\nstorage.drop()\n```\n\n这个操作会删除整个表及其所有数据,请谨慎使用。\n\n## 结论\n\nByzer Storage提供了一套简洁而强大的API,能够轻松实现向量搜索、全文检索、结构化查询等功能。它的设计非常适合AI应用场景,可以有效地存储和检索各种类型的数据。通过本文的介绍,相信读者已经对Byzer Storage有了基本的了解,并能够开始在自己的项目中使用这个强大的存储引擎。\n\n\n     \n\n下面是用户的需求：\n\n生成一个makefile 可以将整个项目打包成一个wheel 包\n"}, {"role": "assistant", "content": "当然，我会按照 SEARCH/REPLACE 格式重新输出 Makefile 的内容。这里是符合要求的输出：\n\n```python\n##File: /Users/allwefantasy/projects/super_analysis/Makefile\n<<<<<<< SEARCH\n=======\n# Makefile for super-analysis project\n\n# Python interpreter\nPYTHON := python3\n\n# Project name\nPROJECT_NAME := super-analysis\n\n# Directories\nSRC_DIR := src\nDIST_DIR := dist\nBUILD_DIR := build\n\n# Commands\nSETUP := $(PYTHON) setup.py\n\n.PHONY: all clean build wheel\n\nall: clean build wheel\n\nclean:\n\t@echo \"Cleaning up...\"\n\trm -rf $(BUILD_DIR) $(DIST_DIR) *.egg-info\n\tfind . -type d -name \"__pycache__\" -exec rm -rf {} +\n\tfind . -type f -name \"*.pyc\" -delete\n\nbuild:\n\t@echo \"Building the project...\"\n\t$(SETUP) build\n\nwheel:\n\t@echo \"Creating wheel package...\"\n\t$(SETUP) bdist_wheel\n\ninstall:\n\t@echo \"Installing the package...\"\n\t$(PYTHON) -m pip install .\n\nuninstall:\n\t@echo \"Uninstalling the package...\"\n\t$(PYTHON) -m pip uninstall -y $(PROJECT_NAME)\n\ndevelop:\n\t@echo \"Installing the package in editable mode...\"\n\t$(PYTHON) -m pip install -e .\n\ntest:\n\t@echo \"Running tests...\"\n\t$(PYTHON) -m unittest discover tests\n>>>>>>> REPLACE\n```\n\n这个 SEARCH/REPLACE 块将创建一个新的 Makefile 文件（如果不存在）或者替换现有 Makefile 的全部内容（如果已存在）。这个 Makefile 包含了构建、清理、打包以及其他有用的命令，可以帮助你管理项目和创建 wheel 包。"}], "model": "human", "yaml_file": "actions/140_chat_action.yml"}
{"instruction": "根据 @Makefile 更新 @README.md 文档", "conversations": [{"role": "user", "content": "\n如果你需要生成代码，对于每个需要更改的文件,你需要按 *SEARCH/REPLACE block* 的格式进行生成。\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The opening fence and code language, eg: ```python\n2. The file path alone on a line, starting with \"##File:\" and verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n*SEARCH/REPLACE* blocks will replace *all* matching occurrences.\nInclude enough lines to make the SEARCH blocks unique.\n\nInclude *ALL* the code being searched and replaced!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n下面我们来看一个例子：\n\n当前项目目录结构：\n1. 项目根目录： /tmp/projects/mathweb\n2. 项目子目录/文件列表(类似tree 命令输出)\nflask/\n    app.py\n    templates/\n        index.html\n    static/\n        style.css\n\n用户需求： Change get_factorial() to use math.factorial\n\n回答： To make this change we need to modify `/tmp/projects/mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n用户需求： Refactor hello() into its own file.\n\n回答：To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/hello.py\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/main.py\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n现在让我们开始一个新的任务:\n\n当前项目目录结构：\n1. 项目根目录： /Users/allwefantasy/projects/super_analysis\n2. 项目子目录/文件列表(类似tree 命令输出)：\nsetup.py\n.auto-coder/\n    libs/\n        llm_friendly_packages/\n            src/\n                example1/\n                    main.py\nsrc/\n    superanalysis/\n        serve/\n            rest.py\n            convert.py\n            openai_serve.py\n            __init__.py\n            llm_wrapper.py\n            api_server.py\n            cli.py\n        sql/\n            __init__.py\n            types.py\n            generator.py\n下面是一些文件路径以及每个文件对应的源码：\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/types.py\nfrom pydantic import BaseModel\n\nclass RAGConfig(BaseModel):\n    schema_rag_base_url: str = \"\"\n    schema_rag_api_key: str = \"xxxx\"\n\n    context_rag_base_url: str = \"\"\n    context_rag_api_key: str = \"xxxx\"\n\nclass ByzerSQLConfig(BaseModel):\n    byzer_sql_url: str = \"\"\n    data_dir: str = \"\"\n    owner: str = \"\"\n\nclass ModelConfig(BaseModel):\n    llm_model: str = \"\"\n    gen_sql_llm_model: str = \"\"\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/cli.py\nimport argparse\nimport byzerllm\nfrom byzerllm.utils.client import ByzerLLM\nfrom superanalysis.serve.api_server import serve, ServerArgs\nfrom superanalysis.sql.types import RAGConfig, ByzerSQLConfig, ModelConfig\nfrom superanalysis.sql.generator import SQLGenerator\nfrom superanalysis.serve.llm_wrapper import LLWrapper\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"ByzerLLM API Server\")\n    parser.add_argument(\"--ray-address\", type=str, default=\"auto\", help=\"ray address\")\n    parser.add_argument(\n        \"--host\", type=str, default=\"0.0.0.0\", help=\"Host to bind the server to\"\n    )\n    parser.add_argument(\n        \"--port\", type=int, default=8000, help=\"Port to bind the server to\"\n    )\n    parser.add_argument(\n        \"--uvicorn-log-level\", type=str, default=\"info\", help=\"Uvicorn log level\"\n    )\n    parser.add_argument(\n        \"--allow-credentials\", action=\"store_true\", help=\"Allow credentials\"\n    )\n    parser.add_argument(\n        \"--allowed-origins\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed origins\"\n    )\n    parser.add_argument(\n        \"--allowed-methods\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed methods\"\n    )\n    parser.add_argument(\n        \"--allowed-headers\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed headers\"\n    )\n    parser.add_argument(\n        \"--api-key\", type=str, default=\"\", help=\"API key for authentication\"\n    )\n    parser.add_argument(\n        \"--served-model-name\",\n        type=str,\n        required=True,\n        help=\"Name of the model to be served\",\n    )\n    parser.add_argument(\n        \"--prompt-template\", type=str, default=\"\", help=\"Prompt template\"\n    )\n    parser.add_argument(\n        \"--response-role\", type=str, default=\"assistant\", help=\"Response role\"\n    )\n    parser.add_argument(\"--ssl-keyfile\", type=str, default=\"\", help=\"SSL key file\")\n    parser.add_argument(\n        \"--ssl-certfile\", type=str, default=\"\", help=\"SSL certificate file\"\n    )\n\n    # Add new arguments for RAGConfig\n    parser.add_argument(\n        \"--schema-rag-base-url\", type=str, default=\"\", help=\"RAG base URL\"\n    )\n    parser.add_argument(\n        \"--schema-rag-api-key\", type=str, default=\"xxxx\", help=\"RAG API key\"\n    )\n\n    parser.add_argument(\n        \"--context-rag-base-url\", type=str, default=\"\", help=\"RAG base URL\"\n    )\n    parser.add_argument(\n        \"--context-rag-api-key\", type=str, default=\"xxxx\", help=\"RAG API key\"\n    )\n\n    # Add new argument for ByzerSQLConfig\n    parser.add_argument(\"--byzer-sql-url\", type=str, default=\"\", help=\"Byzer SQL URL\")\n    parser.add_argument(\"--data-dir\", type=str, default=\"\", help=\"Data directory\")\n    parser.add_argument(\"--owner\", type=str, default=\"hello\", help=\"Owner of the Byzer SQL session\")\n    parser.add_argument(\n        \"--llm-model\", type=str, default=\"\", help=\"LLM model for general use\"\n    )\n    parser.add_argument(\n        \"--gen-sql-llm-model\", type=str, default=\"\", help=\"LLM model for SQL generation\"\n    )\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile,\n    )\n\n    # Create RAGConfig instance\n    rag_config = RAGConfig(\n        schema_rag_base_url=args.schema_rag_base_url,\n        schema_rag_api_key=args.schema_rag_api_key,\n        context_rag_base_url=args.context_rag_base_url,\n        context_rag_api_key=args.context_rag_api_key,\n    )\n\n    # Create ByzerSQLConfig instance\n    byzer_sql_config = ByzerSQLConfig(\n        byzer_sql_url=args.byzer_sql_url, data_dir=args.data_dir, owner=args.owner\n    )\n\n    model_config = ModelConfig(\n        llm_model=args.llm_model or args.served_model_name,\n        gen_sql_llm_model=args.gen_sql_llm_model or args.served_model_name,\n    )\n\n    byzerllm.connect_cluster(address=args.ray_address)\n    llm = ByzerLLM()\n    llm.setup_default_model_name(args.served_model_name)\n    llm = byzerllm.ByzerLLM()\n    if not server_args.served_model_name:\n        raise ValueError(\"served_model_name is required\")\n    llm.setup_default_model_name(server_args.served_model_name)\n\n    print(\"RAG Config: \", rag_config)\n    print(\"Model Config: \", model_config)\n    rag = SQLGenerator(\n        llm=llm,\n        rag_config=rag_config,\n        byzer_sql_config=byzer_sql_config,\n        model_config=model_config,\n    )\n    llm_wrapper = LLWrapper(llm=llm, entry=rag)\n    serve(llm_wrapper, server_args)\n\n\nif __name__ == \"__main__\":\n    main()\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/__init__.py\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/__init__.py\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/llm_wrapper.py\nfrom typing import Any, Dict, List, Optional, Union,Callable\nfrom byzerllm.utils.client.types import (    \n    LLMFunctionCallResponse,\n    LLMClassResponse,LLMResponse    \n)\nimport pydantic\nfrom byzerllm import ByzerLLM\nfrom byzerllm.utils.client import LLMResponse\nfrom byzerllm.utils.types import SingleOutputMeta\nfrom byzerllm.utils.langutil import asyncfy_with_semaphore\nfrom superanalysis.sql.generator import SQLGenerator\nfrom loguru import logger\n\nclass LLWrapper:\n\n    def __init__(self,llm:ByzerLLM,entry:SQLGenerator):\n        self.entry = entry\n        self.llm = llm\n        \n\n    def chat_oai(self,\n                 conversations,\n                 tools:List[Union[Callable,str]]=[], \n                 tool_choice:Optional[Union[Callable,str]]=None,\n                 execute_tool:bool=False,  \n                 impl_func:Optional[Callable]=None,\n                 execute_impl_func:bool=False,\n                 impl_func_params:Optional[Dict[str,Any]]=None,\n                 func_params:Optional[Dict[str,Any]]=None,\n                 response_class:Optional[Union[pydantic.BaseModel,str]] = None, \n                 response_after_chat:Optional[Union[pydantic.BaseModel,str]] = False,\n                 enable_default_sys_message:bool=True,                 \n                 model:Optional[str] = None,\n                 role_mapping=None,llm_config:Dict[str,Any]={}\n                 )->Union[List[LLMResponse],List[LLMFunctionCallResponse],List[LLMClassResponse]]: \n        res,contexts = self.entry.stream_chat_oai(conversations)\n        s = \"\".join(res)        \n        return [LLMResponse(output=s,metadata={},input=\"\")]\n\n    def stream_chat_oai(self,conversations, \n                        model:Optional[str]=None, \n                        role_mapping=None,\n                        delta_mode=False,\n                        llm_config:Dict[str,Any]={}): \n        res,contexts = self.entry.stream_chat_oai(conversations)        \n        for t in res:                        \n            yield (t,SingleOutputMeta(0,0))\n\n    async def async_stream_chat_oai(self,conversations, \n                        model:Optional[str]=None, \n                        role_mapping=None,\n                        delta_mode=False,\n                        llm_config:Dict[str,Any]={}): \n        res,contexts = self.rag.stream_chat_oai(conversations)                \n        for t in res:                                    \n            yield (t,SingleOutputMeta(0,0))\n                         \n\n    def __getattr__(self, name):        \n        return getattr(self.llm, name)\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/openai_serve.py\nimport argparse\nfrom service_server.api_server import ServerArgs, serve\nfrom service_server.llm_wrapper import LLWrapper\nfrom service_server.summary_rag import SummaryRAG\nimport byzerllm\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"OpenAI Serve\")\n    parser.add_argument(\n        \"--model\",\n        default=\"\",\n    )\n    parser.add_argument(\"--emb_model\", default=\"\")\n    parser.add_argument(\n        \"--ray_address\",\n        default=\"auto\",\n    )\n    parser.add_argument(\"--source_dir\", default=\".\", help=\"\")\n    parser.add_argument(\"--host\", default=\"\", help=\"\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"\")\n    parser.add_argument(\"--uvicorn_log_level\", default=\"info\", help=\"\")\n    parser.add_argument(\"--allow_credentials\", action=\"store_true\", help=\"\")\n    parser.add_argument(\"--allowed_origins\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--allowed_methods\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--allowed_headers\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--api_key\", default=\"\", help=\"\")\n    parser.add_argument(\"--served_model_name\", default=\"\", help=\"\")\n    parser.add_argument(\"--prompt_template\", default=\"\", help=\"\")\n    parser.add_argument(\"--ssl_keyfile\", default=\"\", help=\"\")\n    parser.add_argument(\"--ssl_certfile\", default=\"\", help=\"\")\n    parser.add_argument(\"--response_role\", default=\"assistant\", help=\"\")\n    parser.add_argument(\n        \"--collections\", default=\"\", help=\"Collection name for indexing\"\n    )\n    parser.add_argument(\n        \"--base_dir\",\n        default=\"\",\n        help=\"Path where the processed text embeddings were stored\",\n    )\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name or args.model,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile,\n    )\n    byzerllm.connect_cluster(address=args.ray_address)\n    llm = byzerllm.ByzerLLM()\n    if not server_args.served_model_name:\n        raise ValueError(\"served_model_name is required\")\n    llm.setup_default_model_name(server_args.served_model_name)\n    rag = SummaryRAG(llm=llm)    \n    llm_wrapper = LLWrapper(llm=llm, rag=rag)\n    serve(llm=llm_wrapper, args=server_args)\n\nif __name__ == \"__main__\":\n    main()\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/api_server.py\nimport os\nimport time\nimport uvicorn\nfrom fastapi import Request\nfrom fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import JSONResponse, StreamingResponse, Response\n\nfrom byzerllm.log import init_logger\nfrom byzerllm.utils import random_uuid\nfrom byzerllm.version import __version__ as version\nfrom byzerllm.utils.client import ByzerLLM, LLMRequest\nfrom byzerllm.utils.client.entrypoints.openai.serving_chat import OpenAIServingChat\nfrom byzerllm.utils.client.entrypoints.openai.serving_completion import OpenAIServingCompletion\nfrom byzerllm.utils.client.entrypoints.openai.protocol import (\n    ModelList,\n    ModelCard,\n    ModelPermission,\n    ChatCompletionRequest,\n    ErrorResponse,\n    CompletionRequest,\n    Embeddings,\n    EmbeddingsOutput,\n    EmbeddingsData,\n    EmbeddingsUsage,\n)\nfrom pydantic import BaseModel\nfrom typing import List\n\nlogger = init_logger(__name__)\n\nllm_client: ByzerLLM = None\nopenai_serving_chat: OpenAIServingChat = None\nopenai_serving_completion: OpenAIServingCompletion = None\n\nTIMEOUT_KEEP_ALIVE = 5  # seconds\n# timeout in 10 minutes. Streaming can take longer than 3 min\nTIMEOUT = float(os.environ.get(\"BYZERLLM_APISERVER_HTTP_TIMEOUT\", 600))\n\nrouter_app = FastAPI()\n\n\n@router_app.get(\"/health\")\nasync def health() -> Response:\n    \"\"\"Health check.\"\"\"\n    return Response(status_code=200)\n\n\n@router_app.get(\"/v1/models\")\nasync def show_available_models():\n    models = await openai_serving_chat.show_available_models()\n    return JSONResponse(content=models.model_dump())\n\n\n@router_app.get(\"/version\")\nasync def show_version():\n    return JSONResponse(content={\"version\": version})\n\n\n@router_app.get(\"/v1/models\", response_model=ModelList)\nasync def models() -> ModelList:\n    \"\"\"Show available models. Right now we only have one model.\"\"\"\n    model_cards = [\n        ModelCard(\n            id=\"\",\n            root=\"\",\n            permission=[ModelPermission()]\n        )\n    ]\n    return ModelList(data=model_cards)\n\n\n@router_app.post(\"/v1/completions\")\nasync def create_completion(\n        body: CompletionRequest,\n        request: Request\n):\n    generator = await openai_serving_completion.create_completion(body, request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            status_code=generator.code\n        )\n    if body.stream:\n        return StreamingResponse(\n            content=generator,\n            media_type=\"text/event-stream\"\n        )\n    else:\n        return JSONResponse(content=generator.model_dump())\n\n\n@router_app.post(\"/v1/chat/completions\")\nasync def create_chat_completion(\n        body: ChatCompletionRequest,\n        request: Request,\n):\n    \"\"\"Completion API similar to OpenAI's API.\n\n    See  https://platform.openai.com/docs/api-reference/chat/create\n    for the API specification. This API mimics the OpenAI ChatCompletion API.\n\n    NOTE: Currently we do not support the following features:\n        - function_call (Users should implement this by themselves)\n        - logit_bias (to be supported by vLLM engine)\n    \"\"\"\n    # async with async_timeout.timeout(TIMEOUT):\n\n    generator = await openai_serving_chat.create_chat_completion(body, request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            status_code=generator.code\n        )\n    if body.stream:\n        return StreamingResponse(\n            content=generator,\n            media_type=\"text/event-stream\"\n        )\n    else:\n        return JSONResponse(content=generator.model_dump())\n\n\n@router_app.post(\"/v1/embeddings\")\nasync def embed(body: Embeddings):\n    \"\"\"Given a prompt, the model will return one embedding.\n\n    Returns:\n        A response object with an embedding.\n    \"\"\"\n    embedding_id = f\"embed-{random_uuid()}\"\n\n    results_list = llm_client.emb(body.model, request=LLMRequest(instruction=body.input))\n    tokens = 0\n\n    return EmbeddingsOutput(\n        data=[\n            EmbeddingsData(\n                embedding=results.output,\n                index=i,\n                object=\"embedding\",\n            )\n            for i, results in enumerate(results_list)\n        ],\n        id=embedding_id,\n        object=\"list\",\n        created=int(time.time()),\n        model=body.model,\n        usage=EmbeddingsUsage(\n            prompt_tokens=tokens,\n            total_tokens=tokens,\n        ),\n    )\n\nclass ServerArgs(BaseModel):\n    host: str = None\n    port: int = 8000\n    uvicorn_log_level: str = \"info\"\n    allow_credentials: bool = False\n    allowed_origins: List[str] = [\"*\"]  \n    allowed_methods: List[str] = [\"*\"]\n    allowed_headers: List[str] = [\"*\"]\n    api_key: str = None\n    served_model_name: str = None\n    prompt_template: str = None\n    response_role: str = \"assistant\"\n    ssl_keyfile: str = None\n    ssl_certfile: str = None    \n\ndef serve(llm:ByzerLLM, args: ServerArgs):\n    \n    logger.info(f\"ByzerLLM API server version {version}\")\n    logger.info(f\"args: {args}\")\n\n    router_app.add_middleware(\n        CORSMiddleware,\n        allow_origins=args.allowed_origins,\n        allow_credentials=args.allow_credentials,\n        allow_methods=args.allowed_methods,\n        allow_headers=args.allowed_headers,\n    )\n    \n    if token := os.environ.get(\"BYZERLLM_API_KEY\") or args.api_key:\n\n        @router_app.middleware(\"http\")\n        async def authentication(request: Request, call_next):\n            if not request.url.path.startswith(\"/v1\"):\n                return await call_next(request)\n            if request.headers.get(\"Authorization\") != \"Bearer \" + token:\n                return JSONResponse(\n                    content={\"error\": \"Unauthorized\"},\n                    status_code=401\n                )\n            return await call_next(request)\n\n    # Register labels for metrics\n    # add_global_metrics_labels(model_name=engine_args.model)\n    global llm_client\n    llm_client = llm\n    \n    global openai_serving_chat\n    openai_serving_chat = OpenAIServingChat(\n        llm_client=llm_client,\n        response_role=args.response_role,\n        server_model_name=args.served_model_name,\n        prompt_template=args.prompt_template\n    )\n    global openai_serving_completion\n    openai_serving_completion = OpenAIServingCompletion(\n        llm_client=llm_client,\n        server_model_name=args.served_model_name,\n        prompt_template=args.prompt_template\n    )\n\n    uvicorn.run(\n        router_app,\n        host=args.host,\n        port=args.port,\n        log_level=args.uvicorn_log_level,\n        timeout_keep_alive=TIMEOUT_KEEP_ALIVE,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile\n    )\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/generator.py\nfrom typing import Optional, Dict, Any, List\nfrom openai import OpenAI\nimport json\nfrom .types import RAGConfig, ByzerSQLConfig, ModelConfig\nfrom byzerllm.utils.client import code_utils\nimport byzerllm\nimport requests\nfrom loguru import logger\nimport traceback\nfrom byzerllm.utils.nontext import TagExtractor\n\n\nclass SQLGenerator:\n\n    @byzerllm.prompt()\n    def gen_context(self, conversations: List[Dict[str, str]]) -> str:\n        \"\"\"\n        对话内容：\n        {% for msg in conversations %}\n        <{{ msg.role }}>: {{ msg.content }}\n        {% endfor %}\n\n        根据提供的对话内容，理解用户最后的问题，然后提取可能会用到的 Byzer-SQL 相关的知识。\n        \"\"\"\n\n    @byzerllm.prompt()\n    def gen_sql(\n        self,\n        conversations: List[Dict[str, str]],\n        context: str,\n        schema: str,\n        model: str = \"deepseek_chat\",\n    ) -> str:\n        \"\"\"\n        相关知识：\n        {{ context }}\n\n        Schema信息：\n        {{ schema }}\n\n        对话内容：\n        {% for msg in conversations %}\n        <{{ msg.role }}>: {{ msg.content }}\n        {% endfor %}\n\n        模型UDF函数：\n        {{ model }}\n\n        根据提供的相关知识以及对话内容，理解用户最后的问题，然后根据看到的schema 信息，生成 Byzer-SQL 语句。\n        Byzer-SQL 首先需要用 load 语法加载 schema 中的路径映射成表格，然后根据表格的信息生成对应的 Byzer-SQL 语句。\n\n        特别需要关注的是:\n        1. 用户的很多需求是要对表格的文本内容做自然语言处理，你需要使用 Byzer-SQL 中的大模型辅助函数来完成这个需求。\n        2. 使用 `llm_result` 函数处理 LLM 模型函数的响应,`llm_result` 函数得到的是一段文本，虽然我们要求模型函数只输出特定的文本，但考虑到大模型会有一些额外的字符输出，不会只是输出我们要求的文字，所以通过 `like` 来进行匹配。\n        3. 注意 Byzer-SQL 的语法，比如select 语句最后一定需要有 as <table_name>。\n        3. 输出的 Byzer-SQL 语句务必使用 <_byzer_sql_></_byzer_sql_> 标签包裹起来。\n        \"\"\"\n\n    @byzerllm.prompt()\n    def fetch_schema(self, context: str) -> str:\n        \"\"\"\n        下面是一些基础知识，包括名词术语等等：\n        {{ context }}\n\n        当理解对话内容的时候，需要参考上面的基础知识，然后更好的理解对应的表格schema信息是不是有用。\n        \"\"\"\n\n    def __init__(\n        self,\n        llm: byzerllm.ByzerLLM,\n        rag_config: RAGConfig,\n        byzer_sql_config: ByzerSQLConfig,\n        model_config: ModelConfig,\n    ) -> None:\n        self.llm = llm\n        self.schema_rag_client = OpenAI(\n            api_key=rag_config.schema_rag_api_key,\n            base_url=rag_config.schema_rag_base_url,\n        )\n        self.context_rag_client = OpenAI(\n            api_key=rag_config.context_rag_api_key,\n            base_url=rag_config.context_rag_base_url,\n        )\n        self.byzer_sql_url = byzer_sql_config.byzer_sql_url\n\n        self.llm_model = byzerllm.ByzerLLM()\n        self.llm_model.setup_default_model_name(model_config.llm_model)\n        self.gen_sql_llm_model = byzerllm.ByzerLLM()\n        self.gen_sql_llm_model.setup_default_model_name(model_config.gen_sql_llm_model)\n\n    def stream_chat_oai(\n        self,\n        conversations,\n        model: Optional[str] = None,\n        role_mapping=None,\n        delta_mode=False,\n        llm_config: Dict[str, Any] = {},\n    ):\n        try:\n            v, c = self._stream_chat_oai(\n                conversations=conversations,\n                model=model,\n                role_mapping=role_mapping,\n                delta_mode=delta_mode,\n                llm_config=llm_config,\n            )\n        except Exception as e:\n            error_msg = f\"Error in stream_chat_oai: {e}\\n{traceback.format_exc()}\"\n            logger.error(error_msg)\n            v, c = [str(e)], []\n\n        return v, c\n\n    def _stream_chat_oai(\n        self,\n        conversations,\n        model: Optional[str] = None,\n        role_mapping=None,\n        delta_mode=False,\n        llm_config: Dict[str, Any] = {},\n    ):\n        logger.info(f\"Starting stream_chat_oai with model: {model}\")\n        logger.debug(f\"Conversations: {conversations}\")\n        logger.debug(f\"LLM Config: {llm_config}\")\n\n        if not model:\n            model = self.llm_model.default_model_name\n        logger.info(f\"Using model: {model}\")\n\n        query = conversations[-1][\"content\"]\n\n        if (\n            \"使用四到五个字直接返回这句话的简要主题，不要解释、不要标点、不要语气词、不要多余文本，不要加粗，如果没有主题\"\n            in query\n            or \"简要总结一下对话内容，用作后续的上下文提示 prompt，控制在 200 字以内\"\n            in query\n        ):\n            chunks = self.llm.stream_chat_oai(\n                conversations=conversations,\n                model=model,\n                role_mapping=role_mapping,\n                llm_config=llm_config,\n                delta_mode=True,\n            )\n            return (chunk[0] for chunk in chunks), []\n\n        logger.info(\"Fetching context from context RAG client\")\n        context_query = self.gen_context.prompt(conversations=conversations)\n        context_response = self.context_rag_client.chat.completions.create(\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": json.dumps(\n                        {\"query\": context_query, \"only_contexts\": True},\n                        ensure_ascii=False,\n                    ),\n                }\n            ],\n            model=model,\n        )\n\n        context = context_response.choices[0].message.content\n        new_context = \"\"\n        for msg in context.split(\"\\n\"):\n            if msg.strip():\n                v = json.loads(msg)\n                new_context += v[\"source_code\"]\n        logger.info(f\"context {new_context[0:100]}...\")\n        logger.info(\"Context retrieved successfully\")\n\n        logger.info(\"Fetching schemas from schema RAG client\")\n        schema_query = self.fetch_schema.prompt(context=new_context)\n\n        response = self.schema_rag_client.chat.completions.create(\n            messages=conversations[0:-1]\n            + [\n                {\n                    \"role\": \"user\",\n                    \"content\": json.dumps(\n                        {\"query\": schema_query, \"only_contexts\": True},\n                        ensure_ascii=False,\n                    ),\n                }\n            ],\n            model=model,\n        )        \n\n        schema_str = response.choices[0].message.content\n        logger.debug(f\"Schema RAG response: {schema_str}\")\n        logger.info(\"Schema retrieved successfully\")\n\n        logger.info(\"Generating SQL string\")\n        sql_str = self.gen_sql.with_llm(self.gen_sql_llm_model).run(\n            conversations=conversations,\n            context=new_context,\n            schema=schema_str,\n            model=model,\n        )\n\n        logger.info(\"SQL string generated\")\n        logger.debug(f\"Generated SQL string: {sql_str}\")\n\n        # Extract SQL code from tags\n        tag_extractor = TagExtractor(sql_str)\n        extracted_tags = tag_extractor.extract()\n        sql = extracted_tags.content[0].content\n        logger.info(f\"Extracted SQL: {sql}\")\n\n        logger.info(f\"Sending SQL to Byzer SQL URL: {self.byzer_sql_url}\")\n        try:\n            response = requests.post(\n                self.byzer_sql_url,\n                data={\n                    \"sql\": sql,\n                    \"owner\": \"hello\",\n                    \"sessionPerUser\": True,\n                    \"sessionPerRequest\": True,\n                    \"includeSchema\": True,\n                },\n            )\n            response.raise_for_status()\n            result = response.content.decode(\"utf-8\")\n            logger.info(\"SQL execution result decoded\")\n            logger.debug(f\"SQL execution result: {result}\")\n            return (item for item in [result]), []\n        except requests.exceptions.RequestException as e:\n            logger.error(f\"Failed to send SQL to Byzer SQL URL: {e}\")\n            if hasattr(e.response, \"content\"):\n                error_content = e.response.content.decode(\"utf-8\")\n                logger.error(f\"Server response: {error_content}\")\n            else:\n                logger.error(\"No server response available\")\n            raise Exception(f\"Byzer SQL request failed: {str(e)}\") from e\n\n\n##File: /Users/allwefantasy/projects/super_analysis/setup.py\nfrom setuptools import setup, find_packages\n\n# Read requirements from requirements.txt\nwith open('requirements.txt') as f:\n    requirements = f.read().splitlines()\n\nsetup(\n    name=\"super-analysis\",\n    version=\"0.1.0\",\n    packages=find_packages(where=\"src\"),\n    package_dir={\"\": \"src\"},\n    install_requires=requirements,\n    entry_points={\n        \"console_scripts\": [\n            \"super-analysis.serve=superanalysis.serve.cli:main\",\n            \"super-analysis.convert=superanalysis.serve.convert:main\",\n            \"superanalysis.api=superanalysis.serve.rest:main\",\n        ],\n    },\n    author=\"allwefantasy\",\n    author_email=\"allwefantasy@gmail.com\",\n    description=\"A package for super analysis with ByzerLLM\",\n    long_description=open(\"README.md\").read(),\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/yourusername/super-analysis\",\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n    ],\n    python_requires='>=3.6',\n)\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/convert.py\nimport os\nimport argparse\nimport pandas as pd\nimport hashlib\nfrom typing import List, Dict\n\ndef get_csv_files(directory: str) -> List[str]:\n    \"\"\"Recursively get all CSV files in the given directory.\"\"\"\n    csv_files = []\n    for root, _, files in os.walk(directory):\n        for file in files:\n            if file.endswith('.csv'):\n                csv_files.append(os.path.join(root, file))\n    return csv_files\n\ndef extract_schema(df: pd.DataFrame) -> Dict[str, str]:\n    \"\"\"Extract schema from pandas DataFrame.\"\"\"\n    schema = {}\n    for column in df.columns:\n        dtype = str(df[column].dtype)\n        if dtype == 'object':\n            schema[column] = 'STRING'\n        elif dtype.startswith('int'):\n            schema[column] = 'INTEGER'\n        elif dtype.startswith('float'):\n            schema[column] = 'FLOAT'\n        elif dtype == 'bool':\n            schema[column] = 'BOOLEAN'\n        elif dtype.startswith('datetime'):\n            schema[column] = 'TIMESTAMP'\n        else:\n            schema[column] = 'STRING'  # Default to STRING for unknown types\n    return schema\n\ndef generate_schema_text(schema: Dict[str, str], csv_file: str, df: pd.DataFrame, include_rows_num: int) -> str:\n    \"\"\"Generate schema text from the schema dictionary and include the CSV file name and sample data.\"\"\"\n    schema_lines = [f\"CSV File: {csv_file}\\n\"]\n    schema_lines.extend([f\"{field}: {dtype}\" for field, dtype in schema.items()])\n    \n    if include_rows_num > 0:\n        schema_lines.append(f\"\\nSample Data (first {include_rows_num} rows):\")\n        sample_data = df.head(include_rows_num).to_string(index=False)\n        schema_lines.append(sample_data)\n    \n    return \"\\n\".join(schema_lines)\n\ndef save_schema(schema_text: str, csv_file: str, output_dir: str) -> None:\n    \"\"\"Save schema text to a file in the output directory.\"\"\"\n    # Generate MD5 hash of the schema text\n    file_name = hashlib.md5(schema_text.encode()).hexdigest() + '.md'\n    output_path = os.path.join(output_dir, file_name)\n    \n    with open(output_path, 'w') as f:\n        f.write(schema_text)\n    print(f\"Schema for {csv_file} saved to {output_path}\")\n\ndef convert(data_dir: str, doc_dir: str, include_rows_num: int) -> None:\n    \"\"\"Main conversion function.\"\"\"\n    csv_files = get_csv_files(data_dir)\n    \n    for csv_file in csv_files:\n        print(f\"Processing {csv_file}\")\n        df = pd.read_csv(csv_file)\n        schema = extract_schema(df)\n        schema_text = generate_schema_text(schema, csv_file, df, include_rows_num)\n        save_schema(schema_text, csv_file, doc_dir)\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Convert CSV files to schema documents\")\n    parser.add_argument(\"--data_dir\", required=True, help=\"Directory containing CSV files\")\n    parser.add_argument(\"--doc_dir\", required=True, help=\"Output directory for schema documents\")\n    parser.add_argument(\"--include-rows-num\", type=int, default=0, help=\"Number of sample rows to include in the schema\")\n    \n    args = parser.parse_args()\n    \n    if not os.path.exists(args.doc_dir):\n        os.makedirs(args.doc_dir)\n    \n    convert(args.data_dir, args.doc_dir, args.include_rows_num)\n\nif __name__ == \"__main__\":\n    main()\n\n##File: /Users/allwefantasy/projects/super_analysis/README.md\n# Super Analysis\n\nSuper Analysis 支持 NLP2SQL 以及 使用 SQL 对表中的文本字段做自然语言分析。\n\n## 安装\n\n你可以使用 pip 安装 Super Analysis：\n\n```bash\ngit clone https://github.com/allwefantasy/super_analysis\ncd super-analysis\npip install -e .\n```\n\n\n\n## 示例数据\n\n下载电影数据集： https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset/download?datasetVersionNumber=7\n\n\n## 启动分析服务\n\n整个 super-analysis 服务分为四个部分：\n\n1. byzer-sql 分布式执行引擎 \n2. schema 文档知识库\n4. context 文档知识库\n5. super-analysis 自身\n\n在启动 super-analysis 之前，你需要启动 schema 文档知识库和 context 文档知识库以及 byzer-sql 引擎。\n\n\n在你的电脑上参考 [这个文档](./docs/4.3.1%20安装与配置%20Byzer-SQL.pdf) 完成部署.\n\n> 启动的时候需要在安装有 super-analysis 的conda环境中启动。\n\n当 byzer-sql 部署完成后，注册账号为 `hello`, 然后进入到 byzer-sql 的控制台中执行以下脚本：\n\n```sql\n!byzerllm setup single;\n\nrun command as LLM.`` where \naction=\"infer\"\nand reconnect=\"true\"\nand pretrainedModelType=\"saas/openai\"\nand udfName=\"deepseek_chat\";\n```\n\n现在可以按如下方式做一些数据预处理或者启动相关服务：\n\n```bash\n## 将电影数据集的csv文件转换为schema文档\nsuper-analysis.convert --data_dir /Users/allwefantasy/data/movice --doc_dir /Users/allwefantasy/data/movice/schemas/\n\n# 启动schema文档知识库\nauto-coder doc serve \\\n--model deepseek_chat --index_filter_workers 100 \\\n--tokenizer_path /Users/allwefantasy/Downloads/tokenizer.json \\\n--doc_dir /Users/allwefantasy/data/movice/schemas/ \\\n--port 8001\n\n\n# 下载 Byzer-SQL 专门为大模型优化的文档\ngit clone https://github.com/allwefantasy/llm_friendly_packages\n\n# 启动文档知识库\nauto-coder doc serve \\\n--model deepseek_chat --index_filter_workers 100 \\\n--tokenizer_path /Users/allwefantasy/Downloads/tokenizer.json \\\n--doc_dir  /Users/allwefantasy/projects/llm_friendly_packages/github.com/allwefantasy \\\n--port 8002\n\n# 启动兼容 OpenAI Server 的分析服务\nsuper-analysis.serve --served-model-name deepseek_chat --port 8000 \\\n--schema-rag-base-url http://127.0.0.1:8001/v1 \\\n--context-rag-base-url http://127.0.0.1:8002/v1 \\\n--byzer-sql-url http://127.0.0.1:9003/run/script\n```\n\n现在你可以用 OpenAI SDK 去使用了。 具体测试和接口使用参考 [openai_local_api.ipynb](./notebooks/openai_local_api.ipynb)\n\n## 非 OpenAI 兼容接口\n\n除了兼容 OpenAI 的接口外，Super Analysis 还提供了一个简单的 REST API 接口，可以直接与模型进行交互。以下是如何使用这个接口：\n\n### 启动服务\n\n使用以下命令启动 REST API 服务：\n\n```bash\nsuperanalysis.api --schema-rag-base-url <schema_rag_url> \\\n                  --schema-rag-api-key <schema_rag_api_key> \\\n                  --context-rag-base-url <context_rag_url> \\\n                  --context-rag-api-key <context_rag_api_key> \\\n                  --byzer-sql-url <byzer_sql_url> \\\n                  --llm-model <llm_model_name> \\\n                  --gen-sql-llm-model <gen_sql_llm_model_name> \\\n                  --host <host> \\\n                  --port <port>\n```\n\n请替换尖括号中的值为实际的配置信息。\n\n### API 端点\n\n服务启动后，将提供以下 API 端点：\n\n#### POST /chat\n\n此端点用于与模型进行对话。\n\n请求体格式：\n\n```json\n{\n  \"conversations\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"你的问题或指令\"\n    }\n  ],\n  \"model\": \"可选的模型名称\"\n}\n```\n\n响应格式：\n\n```json\n{\n  \"response\": \"模型的回答\"\n}\n```\n\n### 示例使用\n\n以下是使用 Python 请求库与 API 交互的示例：\n\n```python\nimport requests\nimport json\n\nurl = \"http://localhost:8000/chat\"\npayload = {\n    \"conversations\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"你好，请介绍一下你自己。\"\n        }\n    ],\n    \"model\": \"your_model_name\"\n}\n\nresponse = requests.post(url, json=payload)\nresult = response.json()\n\nprint(result[\"response\"])\n```\n\n这个 API 相比兼容 OpenAI 接口，可以在请求中提供提供更多可选参数。\n\n## 使用方法\n\n安装完成后，你可以使用 `super-analysis.serve` 命令来启动 API 服务器：\n\n```bash\nsuper-analysis.serve --served-model-name your_model_name\n```\n\n### 命令行参数\n\n以下是可用的命令行参数：\n\n- `--host`: 服务器绑定的主机地址（默认：0.0.0.0）\n- `--port`: 服务器绑定的端口（默认：8000）\n- `--uvicorn-log-level`: Uvicorn 日志级别（默认：info）\n- `--allow-credentials`: 允许凭证（默认：False）\n- `--allowed-origins`: 允许的源（默认：[\"*\"]）\n- `--allowed-methods`: 允许的方法（默认：[\"*\"]）\n- `--allowed-headers`: 允许的头部（默认：[\"*\"]）\n- `--api-key`: 用于认证的 API 密钥\n- `--served-model-name`: 要服务的模型名称（必需）\n- `--prompt-template`: 提示模板\n- `--response-role`: 响应角色（默认：assistant）\n- `--ssl-keyfile`: SSL 密钥文件路径\n- `--ssl-certfile`: SSL 证书文件路径\n\n例如：\n\n```bash\nsuper-analysis.serve --served-model-name deepseek_chat --port 8000\n```\n\n\n## 抽取csv文件schema\n\nThe `super-analysis.convert` command allows you to convert CSV files to schema documents. This is useful for generating metadata about your CSV files that can be used for further analysis or documentation.\n\n#### Usage\n\n```\nsuper-analysis.convert --data_dir <path_to_csv_files> --doc_dir <output_directory> [--include-rows-num <number_of_sample_rows>]\n```\n\nArguments:\n- `--data_dir`: Directory containing CSV files (required)\n- `--doc_dir`: Output directory for schema documents (required)\n- `--include-rows-num`: Number of sample rows to include in the schema (optional, default is 0)\n\nThe utility will:\n1. Recursively find all CSV files in the specified directory\n2. Extract the schema from each CSV file\n3. Generate a schema text that includes:\n   - The CSV file name\n   - Field names and their data types\n   - Sample data (if `--include-rows-num` is specified)\n4. Save the schema text as a markdown file in the output directory\n\n## 联系方式\n\n如有任何问题或建议，请联系 allwefantasy@gmail.com。\n\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/rest.py\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nfrom typing import List, Optional\nimport uvicorn\nimport argparse\nfrom superanalysis.sql.generator import SQLGenerator\nfrom superanalysis.sql.types import RAGConfig, ByzerSQLConfig, ModelConfig\nimport byzerllm\n\napp = FastAPI()\n\nclass ChatMessage(BaseModel):\n    role: str\n    content: str\n\nclass ChatRequest(BaseModel):\n    conversations: List[ChatMessage]\n    model: Optional[str] = None\n\nclass ChatResponse(BaseModel):\n    response: str\n\n@app.post(\"/chat\", response_model=ChatResponse)\nasync def chat(request: ChatRequest):\n    try:\n        result, _ = sql_generator.stream_chat_oai(\n            conversations=[{\"role\": msg.role, \"content\": msg.content} for msg in request.conversations],\n            model=request.model\n        )\n        return ChatResponse(response=\"\".join(result))\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"SuperAnalysis API Server\")\n    parser.add_argument(\"--host\", type=str, default=\"0.0.0.0\", help=\"Host to bind the server to\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"Port to bind the server to\")\n    parser.add_argument(\"--schema-rag-base-url\", type=str, required=True, help=\"Schema RAG base URL\")\n    parser.add_argument(\"--schema-rag-api-key\", type=str, required=True, help=\"Schema RAG API key\")\n    parser.add_argument(\"--context-rag-base-url\", type=str, required=True, help=\"Context RAG base URL\")\n    parser.add_argument(\"--context-rag-api-key\", type=str, required=True, help=\"Context RAG API key\")\n    parser.add_argument(\"--byzer-sql-url\", type=str, required=True, help=\"Byzer SQL URL\")\n    parser.add_argument(\"--data-dir\", type=str, default=\"\", help=\"Data directory\")\n    parser.add_argument(\"--owner\", type=str, default=\"hello\", help=\"Owner of the Byzer SQL session\")\n    parser.add_argument(\"--llm-model\", type=str, required=True, help=\"LLM model for general use\")\n    parser.add_argument(\"--gen-sql-llm-model\", type=str, required=True, help=\"LLM model for SQL generation\")\n    \n    args = parser.parse_args()\n\n    rag_config = RAGConfig(\n        schema_rag_base_url=args.schema_rag_base_url,\n        schema_rag_api_key=args.schema_rag_api_key,\n        context_rag_base_url=args.context_rag_base_url,\n        context_rag_api_key=args.context_rag_api_key\n    )\n\n    byzer_sql_config = ByzerSQLConfig(\n        byzer_sql_url=args.byzer_sql_url,\n        data_dir=args.data_dir,\n        owner=args.owner\n    )\n\n    model_config = ModelConfig(\n        llm_model=args.llm_model,\n        gen_sql_llm_model=args.gen_sql_llm_model\n    )\n\n    llm = byzerllm.ByzerLLM()\n    llm.setup_default_model_name(args.llm_model)\n\n    global sql_generator\n    sql_generator = SQLGenerator(\n        llm=llm,\n        rag_config=rag_config,\n        byzer_sql_config=byzer_sql_config,\n        model_config=model_config\n    )\n\n    uvicorn.run(app, host=args.host, port=args.port)\n\nif __name__ == \"__main__\":\n    main()\n\n##File: /Users/allwefantasy/projects/super_analysis/Makefile\n# Makefile for super-analysis project\n\n# Python interpreter\nPYTHON := python3\n\n# Project name\nPROJECT_NAME := super-analysis\n\n# Directories\nSRC_DIR := src\nDIST_DIR := dist\nBUILD_DIR := build\n\n# Commands\nSETUP := $(PYTHON) setup.py\n\n.PHONY: all clean build wheel\n\nall: clean build wheel\n\nclean:\n\t@echo \"Cleaning up...\"\n\trm -rf $(BUILD_DIR) $(DIST_DIR) *.egg-info\n\tfind . -type d -name \"__pycache__\" -exec rm -rf {} +\n\tfind . -type f -name \"*.pyc\" -delete\n\nbuild:\n\t@echo \"Building the project...\"\n\t$(SETUP) build\n\nwheel:\n\t@echo \"Creating wheel package...\"\n\t$(SETUP) bdist_wheel\n\ninstall:\n\t@echo \"Installing the package...\"\n\t$(PYTHON) -m pip install .\n\nuninstall:\n\t@echo \"Uninstalling the package...\"\n\t$(PYTHON) -m pip uninstall -y $(PROJECT_NAME)\n\ndevelop:\n\t@echo \"Installing the package in editable mode...\"\n\t$(PYTHON) -m pip install -e .\n\ntest:\n\t@echo \"Running tests...\"\n\t$(PYTHON) -m unittest discover tests\n\n##File: .auto-coder/libs/llm_friendly_packages/github.com/allwefantasy/byzer-llm/README.md\n# byzerllm 大模型编程快速指南\n\n本文示例 [Notebook](../../notebooks/003_byzerllm_大模型编程快速指南.ipynb)\n\n## 安装\n\n```bash\npip install -U byzerllm\nray start --head\n```\n## 启动一个模型代理\n\nbyzerllm 支持私有化模型或者SaaS模型的部署。\n\n这里以 deepseek 官方API 为例：\n\n```bash\neasy-byzerllm deploy deepseek-chat --token xxxxx --alias deepseek_chat\n```\n\n或者跬基流动API:\n\n```bash\neasy-byzerllm deploy alibaba/Qwen1.5-110B-Chat --token xxxxx --alias qwen110b_chat\n```\n\n将上面的 API KEY 替换成你们自己的。\n\n如果你想部署私有化模型或者对接 Ollama 等更多需求，参考 [002_使用byzerllm进行模型部署.md](./002_使用byzerllm进行模型部署.md) 或者 [README.md](../../README.md)。\n\n之后，你就可以在代码里使用  deepseek_chat 或者 qwen110b_chat  访问模型了。\n\n## hello world\n\n来和我们的大模型打个招呼:\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\n@byzerllm.prompt(llm=llm)\ndef hello(q:str) ->str:\n    '''\n    你好, {{ q }}\n    '''\n\ns = hello(\"你是谁\")    \nprint(s)\n\n## 输出:\n## '你好！我是一个人工智能助手，专门设计来回答问题、提供信息和帮助解决问题。如果你有任何疑问或需要帮助，请随时告诉我。'\n```\n\n恭喜，你和大模型成功打了招呼！\n\n可以看到，我们通过 `@byzerllm.prompt` 装饰器，将一个方法转换成了一个大模型的调用，然后这个方法的主题是一段文本，文本中\n使用了 jinja2 模板语法，来获得方法的参数。当正常调用该方法时，实际上就发起了和大模型的交互，并且返回了大模型的结果。\n\n在 byzerllm 中，我们把这种方法称之为 prompt 函数。\n\n## 查看发送给大模型的prompt\n\n很多情况你可能需要调试，查看自己的 prompt 渲染后到底是什么样子的，这个时候你可以通过如下方式\n获取渲染后的prompt:\n\n```python\nhello.prompt(\"你是谁\")\n## '你好, 你是谁'\n```            \n\n## 动态换一个模型\n\n前面的 hello 方法在初始化的时候，我们使用了默认的模型 deepseek_chat，如果我们想换一个模型，可以这样做：\n\n```python\nhello.with_llm(llm).run(\"你是谁\")\n## '你好！我是一个人工智能助手，专门设计来回答问题、提供信息和帮助解决问题。如果你有任何疑问或需要帮助，请随时告诉我。'\n```\n\n通过 with_llm 你可以设置一个新的 llm 对象，然后调用 run 方法，就可以使用新的模型了。\n\n## 超长文本生成\n\n我们知道，大模型一次生成的长度其实是有限的，如果你想生成超长文本，你可能需手动的不断获得\n生成结果，然后把他转化为输入，然后再次生成，这样的方式是比较麻烦的。\n\nbyzerllm 提供了更加易用的 API :\n\n```python\nimport byzerllm\nfrom byzerllm import ByzerLLM\n\nllm = ByzerLLM.from_default_model(\"deepseek_chat\")\n\n@byzerllm.prompt()\ndef tell_story() -> str:\n    \"\"\"\n    讲一个100字的故事。    \n    \"\"\"\n\n\ns = (\n    tell_story.with_llm(llm)\n    .with_response_markers()\n    .options({\"llm_config\": {\"max_length\": 10}})\n    .run()\n)\nprint(s)\n\n## 从前，森林里住着一只聪明的小狐狸。一天，它发现了一块闪闪发光的宝石。小狐狸决定用这块宝石帮助森林里的动物们。它用宝石的光芒指引迷路的小鹿找到了回家的路，用宝石的温暖治愈了受伤的小鸟。从此，小狐狸成了森林里的英雄，动物们都感激它的善良和智慧。\n```\n\n实际核心部分就是这一行：\n\n```python\ntell_story.with_llm(llm)\n    .with_response_markers()    \n    .run()\n```\n\n我们只需要调用 `with_response_markers` 方法，系统就会自动的帮我们生成超长文本。\n在上面的案例中，我们通过\n\n```python\n.options({\"llm_config\": {\"max_length\": 10}})\n```\n\n认为的限制大模型一次交互最多只能输出10个字符，但是系统依然自动完成了远超过10个字符的文本生成。\n\n## 对象输出\n\n前面我们的例子都是返回字符串，但是我们也可以返回对象，这样我们就可以更加灵活的处理返回结果。\n\n```python\nimport pydantic \n\nclass Story(pydantic.BaseModel):\n    '''\n    故事\n    '''\n\n    title: str = pydantic.Field(description=\"故事的标题\")\n    body: str = pydantic.Field(description=\"故事主体\")\n\n@byzerllm.prompt()\ndef tell_story()->Story:\n    '''\n    讲一个100字的故事。    \n    '''\n\ns = tell_story.with_llm(llm).run()\nprint(isinstance(s, Story))\nprint(s.title)\n\n## True\n## 勇敢的小鸟\n```\n\n可以看到，我们很轻松的将输出转化为格式化输出。\n\n## 自定义字段抽取\n\n前面的结构化输出，其实会消耗更多token,还有一种更加精准的结构化输出方式。\n比如让大模型生成一个正则表达式，但实际上大模型很难准确只输出一个正则表达式，这个时候我们可以通过自定义抽取函数来获取我们想要的结果。\n\n\n```python\nfrom loguru import logger\nimport re\n\n@byzerllm.prompt()\ndef generate_regex_pattern(desc: str) -> str:\n    \"\"\"\n    根据下面的描述生成一个正则表达式，要符合python re.compile 库的要求。\n\n    {{ desc }}\n\n    最后生成的正则表达式要在<REGEX></REGEX>标签对里。\n    \"\"\"    \n\ndef extract_regex_pattern(regex_block: str) -> str:    \n    pattern = re.search(r\"<REGEX>(.*)</REGEX>\", regex_block, re.DOTALL)\n    if pattern is None:\n        logger.warning(\"No regex pattern found in the generated block:\\n {regex_block}\")\n        raise None\n    return pattern.group(1)\n\npattern = \"匹配一个邮箱地址\"\nv = generate_regex_pattern.with_llm(llm).with_extractor(extract_regex_pattern).run(desc=pattern)\nprint(v)\n## ^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\n```\n\n在上面的例子里，我们根据一句话生成一个正则表达式。我们通过 `with_extractor` 方法，传入了一个自定义的抽取函数，这个函数会在大模型生成结果后，对结果进行处理，然后返回我们想要的结果。\n\n我们在 prompt 明确说了，生成的结果要放到 `<REGEX></REGEX>` 标签对里，然后我们通过 extract_regex_pattern 函数，从结果中提取出了我们想要的正则表达式。\n\n## 在实例方法中使用大模型\n\n```python\nimport byzerllm\ndata = {\n    'name': 'Jane Doe',\n    'task_count': 3,\n    'tasks': [\n        {'name': 'Submit report', 'due_date': '2024-03-10'},\n        {'name': 'Finish project', 'due_date': '2024-03-15'},\n        {'name': 'Reply to emails', 'due_date': '2024-03-08'}\n    ]\n}\n\n\nclass RAG():\n    def __init__(self):        \n        self.llm = byzerllm.ByzerLLM()\n        self.llm.setup_template(model=\"deepseek_chat\",template=\"auto\")\n        self.llm.setup_default_model_name(\"deepseek_chat\")        \n    \n    @byzerllm.prompt(lambda self:self.llm)\n    def generate_answer(self,name,task_count,tasks)->str:\n        '''\n        Hello {{ name }},\n\n        This is a reminder that you have {{ task_count }} pending tasks:\n        {% for task in tasks %}\n        - Task: {{ task.name }} | Due: {{ task.due_date }}\n        {% endfor %}\n\n        Best regards,\n        Your Reminder System\n        '''        \n\nt = RAG()\n\nresponse = t.generate_answer(**data)\nprint(response)\n\n## 输出:\n## Hello Jane Doe,\n##I hope this message finds you well. I wanted to remind you of your 3 pending tasks to ensure you stay on track:\n## 1. **Submit report** - This task is due on **2024-03-10**. Please ensure that you allocat\n```\n\n这里我们给了个比较复杂的例子，但我们可以看到，给一个实例prompt方法和普通prompt 方法差异不大。\n唯一的区别是如果你希望在定义的时候就指定大模型，使用一个lambda函数返回实例的 llm 对象即可。\n\n```python\n@byzerllm.prompt(lambda self:self.llm)\n```\n\n你也可以不返回，在调用的时候通过 `with_llm` 方法指定 llm 对象。\n\n此外，这个例子也展示了如何通过jinja2模板语法，来处理复杂的结构化数据。\n\n## 通过 Python 代码处理复杂入参\n\n上面的一个例子中，我们通过 jinja2 模板语法，来处理复杂的结构化数据，但是有时候我们可能需要更加复杂的处理，这个时候我们可以通过 Python 代码来处理。\n\n```python\nimport byzerllm\n\ndata = {\n    'name': 'Jane Doe',\n    'task_count': 3,\n    'tasks': [\n        {'name': 'Submit report', 'due_date': '2024-03-10'},\n        {'name': 'Finish project', 'due_date': '2024-03-15'},\n        {'name': 'Reply to emails', 'due_date': '2024-03-08'}\n    ]\n}\n\n\nclass RAG():\n    def __init__(self):        \n        self.llm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n    \n    @byzerllm.prompt()\n    def generate_answer(self,name,task_count,tasks)->str:\n        '''\n        Hello {{ name }},\n\n        This is a reminder that you have {{ task_count }} pending tasks:\n            \n        {{ tasks }}\n\n        Best regards,\n        Your Reminder System\n        '''\n        \n        tasks_str = \"\\n\".join([f\"- Task: {task['name']} | Due: { task['due_date'] }\" for task in tasks])\n        return {\"tasks\": tasks_str}\n\nt = RAG()\n\nresponse = t.generate_answer.with_llm(t.llm).run(**data)\nprint(response)\n\n## Just a gentle nudge to keep you on track with your pending tasks. Here's a quick recap:....\n```\n\n在这个例子里，我们直接把 tasks 在方法体里进行处理，然后作为一个字符串返回，最够构建一个字典，字典的key为 tasks,然后\n你就可以在 docstring 里使用 `{{ tasks }}` 来引用这个字符串。\n\n这样对于很复杂的入参，就不用谢繁琐的 jinja2 模板语法了。\n\n## 如何自动实现一个方法\n\n比如我定义一个签名，但是我不想自己实现里面的逻辑，让大模型来实现。这个在 byzerllm 中叫 function impl。我们来看看怎么\n实现:\n\n```python\nimport pydantic\nclass Time(pydantic.BaseModel):\n    time: str = pydantic.Field(...,description=\"时间，时间格式为 yyyy-MM-dd\")\n\n\n@llm.impl()\ndef calculate_current_time()->Time:\n    '''\n    计算当前时间\n    '''\n    pass \n\n\ncalculate_current_time()\n#output: Time(time='2024-06-14')\n```\n\n在这个例子里，我们定义了一个 calculate_current_time 方法，但是我们没有实现里面的逻辑，我们通过 `@llm.impl()` 装饰器，让大模型来实现这个方法。\n为了避免每次都要“生成”这个方法，导致无法适用，我们提供了缓存，用户可以按如下方式打印速度：\n\n```python\nstart = time.monotonic()\ncalculate_current_time()\nprint(f\"first time cost: {time.monotonic()-start}\")\n\nstart = time.monotonic()\ncalculate_current_time()\nprint(f\"second time cost: {time.monotonic()-start}\")\n\n# output:\n# first time cost: 6.067266260739416\n# second time cost: 4.347506910562515e-05\n```\n可以看到，第一次执行花费了6s,第二次几乎是瞬间完成的，这是因为第一次执行的时候，我们实际上是在生成这个方法，第二次执行的时候，我们是执行已经生成好的代码，所以速度会非常快。你可以显示的调用 `llm.clear_impl_cache()` 清理掉函数缓存。\n\n## Stream 模式\n\n前面的例子都是一次性生成结果，但是有时候我们可能需要一个流式的输出，这个时候我们可能需要用底层一点的API来完成了：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\nv = llm.stream_chat_oai(model=\"deepseek_chat\",conversations=[{\n    \"role\":\"user\",\n    \"content\":\"你好，你是谁\",\n}],delta_mode=True)\n\nfor t in v:\n    print(t,flush=True)  \n\n# 你好\n# ！\n# 我\n# 是一个\n# 人工智能\n# 助手\n# ，\n# 旨在\n# 提供\n# 信息\n# 、\n# 解答\n# 问题....\n```\n\n如果你不想要流式输出，但是想用底层一点的API，你可以使用 `llm.chat_oai` 方法：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\nv = llm.chat_oai(model=\"deepseek_chat\",conversations=[{\n    \"role\":\"user\",\n    \"content\":\"你好，你是谁\",\n}])\n\nprint(v[0].output)\n## 你好！我是一个人工智能助手，旨在提供信息、解答问题和帮助用户解决问题。如果你有任何问题或需要帮助，请随时告诉我。\n```\n\n## Function Calling \n\nbyzerllm 可以不依赖模型自身就能提供 function calling 支持，我们来看个例子：\n\n\n```python\nfrom typing import List,Dict,Any,Annotated\nimport pydantic \nimport datetime\nfrom dateutil.relativedelta import relativedelta\n\ndef compute_date_range(count:Annotated[int,\"时间跨度，数值类型\"],\n                       unit:Annotated[str,\"时间单位，字符串类型\",{\"enum\":[\"day\",\"week\",\"month\",\"year\"]}])->List[str]:\n    '''\n    计算日期范围\n\n    Args:\n        count: 时间跨度，数值类型\n        unit: 时间单位，字符串类型，可选值为 day,week,month,year\n    '''        \n    now = datetime.datetime.now()\n    now_str = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n    if unit == \"day\":\n        return [(now - relativedelta(days=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"week\":\n        return [(now - relativedelta(weeks=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"month\":\n        return [(now - relativedelta(months=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"year\":\n        return [(now - relativedelta(years=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    return [\"\",\"\"]\n\ndef compute_now()->str:\n    '''\n    计算当前时间\n    '''\n    return datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n```\n\n我们定义了两个方法，一个是计算日期范围，一个是计算当前时间。\n\n现在我么可以来测试下，系统如何根据自然语言决定调用哪个方法：\n\n```python\nt = llm.chat_oai([{\n    \"content\":'''计算当前时间''',\n    \"role\":\"user\"    \n}],tools=[compute_date_range,compute_now],execute_tool=True)\n\nt[0].values\n\n## output: ['2024-06-14 15:18:02']\n```\n\n我们可以看到，他正确的选择了 compute_now 方法。\n\n接着我们再试一个：\n\n```python\nt = llm.chat_oai([{\n    \"content\":'''最近三个月趋势''',\n    \"role\":\"user\"    \n}],tools=[compute_date_range,compute_now],execute_tool=True)\n\nt[0].values\n\n## output: [['2024-03-14 15:19:13', '2024-06-14 15:19:13']]\n```\n\n模型正确的选择了 compute_date_range 方法。\n\n## 多模态\n\nbyerllm 也能很好的支持多模态的交互，而且统一了多模态大模型的接口，比如你可以用一样的方式使用 openai 或者 claude 的图片转文字能力， 或者一致的方式使用火山，azuer, openai的语音合成接口。\n\n### 图生文\n\n```python\nimport byzerllm\nfrom byzerllm.types import ImagePath\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\n\n\n@byzerllm.prompt()\ndef what_in_image(image_path: ImagePath) -> str:\n    \"\"\"\n    {{ image_path }}\n    这个图片里有什么？\n    \"\"\"    \n\n\nv = what_in_image.with_llm(vl_llm).run(\n    ImagePath(value=\"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\")\n)\nv\n## OUTPUT: 这张图片展示了多只可爱的猫咪，采用了艺术风格的绘画。猫咪们有不同的颜色和花纹，背景是浅棕色，上面还点缀着一些红色的花朵。整体画面给人一种温馨和谐的感觉\n```\n\n可以看到，我们只需要把 prompt 函数的图片地址入参使用 byzerllm.types.ImagePath里进行包装，就可以直接在 prompt 函数体里\n带上图片。\n\n或者你可以这样：\n\n```python\nimport byzerllm\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\n\n\n@byzerllm.prompt()\ndef what_in_image(image_path: str) -> str:\n    \"\"\"\n    {{ image }}\n    这个图片里有什么？\n    \"\"\"\n    return {\"image\": byzerllm.Image.load_image_from_path(image_path)}\n\n\nv = what_in_image.with_llm(vl_llm).run(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv\n```\n\n通过 `image_path` 参数，然后通过 `byzerllm.Image.load_image_from_path` 方法，转化为一个图片对象 image，最后在 prompt 函数体里\n使用 `{{ image }}` 引用这个图片对象。\n\n另外我们也是可以支持配置多张图片的。\n\n另外，我们也可以使用基础的 `llm.chat_oai` 方法来实现：\n\n```python\nimport byzerllm\nimport json\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\nimage = byzerllm.Image.load_image_from_path(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv = vl_llm.chat_oai(\n    conversations=[\n        {\n            \"role\": \"user\",\n            \"content\": json.dumps(\n                [{\"image\": image, \"text\": \"这个图片里有什么？\"}], ensure_ascii=False\n            ),\n        }\n    ]\n)\nv[0].output\n```\n\n还可以这么写：\n\n```python\nimport byzerllm\nimport json\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\nimage = byzerllm.Image.load_image_from_path(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv = vl_llm.chat_oai(\n    conversations=[\n        {\n            \"role\": \"user\",\n            \"content\": json.dumps(\n                [\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\"url\": image, \"detail\": \"high\"},\n                    },\n                    {\"text\": \"这个图片里有什么？\", \"type\": \"text\"},\n                ],\n                ensure_ascii=False,\n            ),\n        }\n    ]\n)\nv[0].output\n```\n\n### 语音合成\n\n这里以 openai 的 tts 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=tts-1 \\\n--model openai_tts\n```\n\n此外，byzerllm 支持 azure,火山引擎等 tts 语音合成引擎。\n\n接着你可以这么用：\n\n```python\nimport byzerllm\nimport base64\nimport json\n\nllm = byzerllm.ByzerLLM.from_default_model(\"openai_tts\")\n\n\nt = llm.chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\": json.dumps({\n        \"input\":\"hello, open_tts\",\n        \"voice\": \"alloy\",\n        \"response_format\": \"mp3\"\n    },ensure_ascii=False)\n}])\n\nwith open(\"voice.mp3\",\"wb\") as f:\n    f.write(base64.b64decode(t[0].output))\n```\n\ntts 模型生成没有prompt函数可以用，你需要直接使用 chat_oai。\n\n\n### 语音识别\n\n这里以 openai 的 whisper-1 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--worker_concurrency 10 \\\n--infer_params saas.model=whisper-1 saas.api_key=${MODEL_OPENAI_TOKEN} \\\n--model speech_to_text\n```\n\n语音识别的使用方式和图生文类似，我们可以直接在 prompt 函数体里带上音频文件。\n\n```python\nimport byzerllm\nimport json\nimport base64\nfrom byzerllm.types import AudioPath\n\nllm = byzerllm.ByzerLLM.from_default_model(\"speech_to_text\")\n\naudio_file = \"/Users/allwefantasy/videos/output_audio.mp3\"\n\n@byzerllm.prompt(llm=llm)\ndef audio_to_text(audio_file: AudioPath):\n    \"\"\"\n    {{ audio_file }}\n    \"\"\"\n\nv = audio_to_text(AudioPath(value=audio_file))\njson.loads(v)\n```\n输出的数据格式略微复杂：\n\n```\n{'text': 'In the last chapter, you and I started to step through the internal workings of a transformer. This is one of the key pieces of technology inside large language models, and a lot of other tools in the modern wave of AI.',\n 'task': 'transcribe',\n 'language': 'english',\n 'duration': 10.0,\n 'segments': [{'id': 0,\n   'seek': 0,\n   'start': 0.0,\n   'end': 4.78000020980835,\n   'text': ' In the last chapter, you and I started to step through the internal workings of a transformer.',\n   'tokens': [50364,\n    682,\n    264,\n    1036,\n    7187,\n    11,\n    291,\n    293,\n    286,\n    1409,\n    281,\n    1823,\n    807,\n    264,\n    6920,\n    589,\n    1109,\n    295,\n    257,\n    31782,\n    13,\n    50586],\n   'temperature': 0.0,\n   'avg_logprob': -0.28872039914131165,\n   'compression_ratio': 1.4220778942108154,\n   'no_speech_prob': 0.016033057123422623},\n  ....\n  {'id': 2,\n   'seek': 0,\n   'start': 8.579999923706055,\n   'end': 9.979999542236328,\n   'text': ' and a lot of other tools in the modern wave of AI.',\n   'tokens': [50759,\n    293,\n    257,\n    688,\n    295,\n    661,\n    3873,\n    294,\n    264,\n    4363,\n    5772,\n    295,\n    7318,\n    13,\n    50867],\n   'temperature': 0.0,\n   'avg_logprob': -0.28872039914131165,\n   'compression_ratio': 1.4220778942108154,\n   'no_speech_prob': 0.016033057123422623}],\n 'words': [{'word': 'In', 'start': 0.0, 'end': 0.18000000715255737},\n  {'word': 'the', 'start': 0.18000000715255737, 'end': 0.23999999463558197},\n  {'word': 'last', 'start': 0.23999999463558197, 'end': 0.5400000214576721},\n  {'word': 'chapter', 'start': 0.5400000214576721, 'end': 0.800000011920929},\n  ....\n  {'word': 'AI', 'start': 9.920000076293945, 'end': 9.979999542236328}]}\n```\n\n会输出每一句话以及每一个字所在的起始时间和截止时间。你可以根据需要来使用。\n\n\n### 文生图\n\n文生图和语音合成类似，首先要启动合适的模型,以openai 的 dall-e-3 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=dall-e-3 \\\n--model openai_image_gen\n```\n\n启动模型后，只需要记住几个模板参数即可使用，这里直接使用 chat_oai 方法来使用：\n\n```python\n\nimport byzerllm\nimport json\nimport base64\n\nllm = byzerllm.ByzerLLM.from_default_model(\"openai_image_gen\")\nt = llm.chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\": json.dumps({\n        \"input\":\"a white siamese cat\",\n        \"size\": \"1024x1024\",\n        \"quality\": \"standard\"\n    },ensure_ascii=False)\n}])\n\nwith open(\"output1.jpg\",\"wb\") as f:\n    f.write(base64.b64decode(t[0].output))\n\n\nimport matplotlib.pyplot as plt\n\nimage_path = \"output1.jpg\"\nimage = plt.imread(image_path)\n\nplt.imshow(image)\nplt.axis('off')\nplt.show()\n```\n\n## Prompt 函数的流式输出\n\nbyzerllm 底层支持流式输出，非 prompt 函数的用法是这样的：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\n\nv = llm.stream_chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\":\"讲一个100字的故事\"\n}])\n\nfor s  in v:\n    print(s[0], end=\"\")\n```\n\n如果你像用 prompt 函数，可以这么用：\n\n\n```python\nimport byzerllm\nimport json\nimport base64\nfrom typing import Generator\n\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\n\n@byzerllm.prompt()\ndef tell_story() -> Generator[str, None, None]:\n    '''\n    给我讲一个一百多字的故事\n    '''\n\nv = tell_story.with_llm(llm).run()    \nfor i in v:\n    print(i, end=\"\")\n```\n\n可以看到，和普通的 prompt 函数的区别在于，返回值是一个生成器，然后你可以通过 for 循环来获取结果。\n\n## 向量化模型\n\nbyzerllm 支持向量化模型,你可以这样启动一个本地的模型：\n\n```bash\n!byzerllm deploy --pretrained_model_type custom/bge \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--worker_concurrency 10 \\\n--model_path /home/winubuntu/.auto-coder/storage/models/AI-ModelScope/bge-large-zh \\\n--infer_backend transformers \\\n--num_workers 1 \\\n--model emb\n```\n\n注意两个参数:\n\n1. --infer_backend transformers: 表示使用 transformers 作为推理后端。\n2. --model_path: 表示模型的路径。\n\n也可以启动一个 SaaS 的emb模型,比如 qwen 的 emb 模型：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/qianwen \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 2 \\\n--infer_params saas.api_key=${MODEL_QIANWEN_TOKEN}  saas.model=text-embedding-v2 \\\n--model qianwen_emb\n```\n\n或者 openai 的 emb 模型：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--worker_concurrency 10 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=text-embedding-3-small \\\n--model gpt_emb\n```\nSaaS 模型无需配置 `--infer_backend` 参数。\n\n无论是本地模型还是 SaaS 模型，我们都可以这样使用：\n\n```python\nimport byzerllm\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\nllm.setup_default_emb_model_name(\"emb\")\nllm.emb_query(\"你好\")\n```\n\n如果你配置 byzerllm 中的 Storage 使用，比如你这样启动了存储：\n\n```bash\nbyzerllm storage start --enable_emb\n```\n\n那么需要这么使用：\n\n```python\nfrom byzerllm.apps.byzer_storage.simple_api import ByzerStorage, DataType, FieldOption,SortOption\nstorage = ByzerStorage(\"byzerai_store\", \"memory\", \"memory\")\nstorage.emb(\"你好\")\n```\n\n## 一些辅助工具\n\n当调用 prompt 函数返回字符串的时候，如果想从里面抽取代码，可以使用如下方式：\n\n```python\nfrom byzerllm.utils.client import code_utils\ntext_with_markdown = '''\n```shell\nls -l\n```\n'''\ncode_blocks = code_utils.extract_code(text_with_markdown)\nfor code_block in code_blocks:\n    if code_block[0] == \"shell\":\n        print(code_block[1])\n##output: ls -l        \n```\n\nTagExtractor 工具用于对任意 `<_tag_></_tag_>` 标签对的抽取，下面是一个使用示例：\n\n```python\nfrom byzerllm.apps.utils import TagExtractor\n\nextractor = TagExtractor('''\n大家好\n<_image_>data:image/jpeg;base64,xxxxxx</_image_>\n                         大家好\n<_image_>data:image/jpeg;base64,xxxxxx2</_image_>\n''')\n\nv = extractor.extract()\nprint(v.content[0].content)\nprint(v.content[1].content)\n```\n输出为:\n\n```\ndata:image/jpeg;base64,xxxxxx\ndata:image/jpeg;base64,xxxxxx2\n```\n\n我们成功的将 <_image_></_image_> 标签对里的内容抽取出来了。\n\n\n## 注意事项\n\n1. prompt函数方法体返回只能是dict，实际的返回类型和方法签名可以不一样，但是方法体返回只能是dict。\n2. 大部分情况prompt函数体为空，如果一定要有方法体，可以返回一个空字典。\n3. 调用prompt方法的时候，如果在@byzerllm.prompt()里没有指定llm对象，那么需要在调用的时候通过with_llm方法指定llm对象。\n\n===========================\n\n# Byzer Storage: 最易用的AI存储引擎\n\nByzer Storage是一个为AI应用设计的高性能存储引擎,它提供了简单易用的API,支持向量搜索、全文检索以及结构化查询。本文将详细介绍Byzer Storage的使用方法和主要特性。\n\n## 0. 安装和启动\n\n```bash\npip install byzerllm\nbyzerllm storage start\n```\n\nThat's it! Byzer Storage已经安装并启动成功,现在我们可以开始使用它了。默认会启动一个 byzerai_store 的集群。\n\n注意，如果你是这样启动的：\n\n```bash\nbyzerllm storage start --enable_emb\n```\n\n那么会自动启动一个 emb 模型，名字就叫 emb， ByzerStorage 会自动使用该模型，无需做任何其他配置。\n\n## 1. 初始化\n\n创建一个 ByzerStorage 对象，链接 byzerai_store 集群，并且指定数据库和表名（可以不存在）。\n\n```python\nfrom byzerllm.apps.byzer_storage.simple_api import ByzerStorage, DataType, FieldOption, SortOption\n\nstorage = ByzerStorage(\"byzerai_store\", \"my_database1\", \"my_table4s\")\n```\n\n\n## 2. 创建库表（可选）\n\nByzer Storage使用Schema来定义数据结构。我们可以使用SchemaBuilder来构建Schema:\n\n```python\n_ = (\n    storage.schema_builder()\n    .add_field(\"_id\", DataType.STRING)\n    .add_field(\"name\", DataType.STRING)\n    .add_field(\"content\", DataType.STRING, [FieldOption.ANALYZE])\n    .add_field(\"raw_content\", DataType.STRING, [FieldOption.NO_INDEX])    \n    .add_array_field(\"summary\", DataType.FLOAT)    \n    .add_field(\"created_time\", DataType.LONG, [FieldOption.SORT])    \n    .execute()\n)\n```\n\n这个Schema定义了以下字段:\n- `_id`: 字符串类型的主键\n- `name`: 字符串类型,可用于过滤条件\n- `content`: 字符串类型,带有ANALYZE选项,用于全文搜索\n- `raw_content`: 字符串类型,带有NO_INDEX选项,不会被索引\n- `summary`: 浮点数组类型,用于存储向量\n- `created_time`: 长整型,带有SORT选项,可用于排序\n\n需要注意的是：\n\n1. 如果一个字段带有ANALYZE选项,则该字段会被分词,并且可以用于全文搜索，但是就无法返回原始的文本了。所以你需要添加一个新字段专门用来存储原文，比如在我们这个例子里，我们新增了 raw_content 字段，并且显示的指定了 NO_INDEX 选项，这样就不会被索引，也就不会被分词，可以后续被检索后用来获取原始文本。\n2. 对于需要作为向量检索和存储的字段，需要指定为数组类型，比如我们这个例子里的 summary 字段。\n3. 如果你需要拿到向量字段的原始文本，那么你也需要添加一个新字段专门用来存储原文，就像我们这个例子里的 raw_content 字段一样。\n\n\n## 3. 写入数据\n\n准备数据并使用WriteBuilder写入Storage:\n\n```python\ndata = [\n    {\"_id\": \"1\", \"name\": \"Hello\", \"content\": \"Hello, world!\", \"raw_content\": \"Hello, world!\", \"summary\": \"hello world\", \"created_time\": 1612137600},\n    {\"_id\": \"2\", \"name\": \"Byzer\", \"content\": \"Byzer, world!\", \"raw_content\": \"Byzer, world!\", \"summary\": \"byzer\", \"created_time\": 1612137601},\n    {\"_id\": \"3\", \"name\": \"AI\", \"content\": \"AI, world!\", \"raw_content\": \"AI, world!\", \"summary\": \"AI\", \"created_time\": 16121376002},\n    {\"_id\": \"4\", \"name\": \"ByzerAI\", \"content\": \"ByzerAI, world!\", \"raw_content\": \"ByzerAI, world!\", \"summary\": \"ByzerAi\", \"created_time\": 16121376003},\n]\n\nstorage.write_builder().add_items(data, vector_fields=[\"summary\"], search_fields=[\"content\"]).execute()\nstorage.commit()\n```\n\n这里我们使用`add_items`方法批量添加数据,并指定了`summary`为向量字段,`content`为搜索字段。最后调用`commit()`来确保数据被持久化。\n\n\n从上面我们要写入到 byzer storage 的数据我们可以看到如下几个特点：\n\n1. 需要包含我们之前定义的 Schema中罗列的所有的字段，同时需要指定哪些是向量字段，哪些是检索字段。\n2. 向量和检索不能是同一个字段。\n3. 对于向量，检索字段，我们给到 write_builder 的都是文本，ByzerStorage 会根据 Schema 的定义自动将其转换为向量，检索字段需要的格式。\n\n## 4. 查询数据\n\nByzer Storage支持多种查询方式,包括向量搜索、全文检索、过滤和排序。\n\n### 4.1 向量搜索 + 全文检索\n\n```python\nquery = storage.query_builder()\nquery.set_vector_query(\"ByzerAI\", fields=[\"summary\"])\nresults = query.set_search_query(\"Hello\", fields=[\"content\"]).execute()\nprint(results)\n```\n\n这个查询结合了向量搜索和全文检索,它会在`summary`字段中搜索与\"ByzerAI\"相似的向量,同时在`content`字段中搜索包含\"Hello\"的文档。\n\n### 4.2 过滤 + 向量搜索 + 全文检索\n\n```python\nquery = storage.query_builder()\nquery.and_filter().add_condition(\"name\", \"AI\").build()\nquery.set_vector_query(\"ByzerAI\", fields=\"summary\")\nresults = query.set_search_query(\"Hello\", fields=[\"content\"]).execute()\nprint(results)\n```\n\n这个查询首先过滤`name`字段等于\"AI\"的文档,然后在结果中进行向量搜索和全文检索。\n\n### 4.3 过滤 + 排序\n\n```python\nquery = storage.query_builder()\nquery.and_filter().add_condition(\"name\", \"AI\").build().sort(\"created_time\", SortOption.DESC)\nresults = query.execute()\nprint(results)\n```\n\n这个查询过滤`name`字段等于\"AI\"的文档,然后按`created_time`字段降序排序。\n\n## 5. 删除数据\n\n### 5.1 根据ID删除\n\n```python\nstorage.delete_by_ids([\"3\"])\n\nquery = storage.query_builder()\nquery.and_filter().add_condition(\"name\", \"AI\").build()\nresults = query.execute()\nprint(results)\n```\n\n这里我们删除了ID为\"3\"的文档,然后查询验证删除结果。\n\n### 5.2 删除整个表\n\n```python\nstorage.drop()\n```\n\n这个操作会删除整个表及其所有数据,请谨慎使用。\n\n## 结论\n\nByzer Storage提供了一套简洁而强大的API,能够轻松实现向量搜索、全文检索、结构化查询等功能。它的设计非常适合AI应用场景,可以有效地存储和检索各种类型的数据。通过本文的介绍,相信读者已经对Byzer Storage有了基本的了解,并能够开始在自己的项目中使用这个强大的存储引擎。\n\n\n     \n\n下面是用户的需求：\n\n根据 @Makefile 更新 @README.md 文档\n"}, {"role": "assistant", "content": "Here are the changes to update the README.md file based on the Makefile:\n\n```markdown\n##File: /Users/allwefantasy/projects/super_analysis/README.md\n<<<<<<< SEARCH\n## 安装\n\n你可以使用 pip 安装 Super Analysis：\n\n```bash\ngit clone https://github.com/allwefantasy/super_analysis\ncd super-analysis\npip install -e .\n```\n=======\n## 安装\n\n你可以使用 pip 安装 Super Analysis：\n\n```bash\ngit clone https://github.com/allwefantasy/super_analysis\ncd super-analysis\npip install -e .\n```\n\n或者使用 Makefile 来管理项目：\n\n```bash\n# 清理项目\nmake clean\n\n# 构建项目\nmake build\n\n# 创建wheel包\nmake wheel\n\n# 安装项目\nmake install\n\n# 卸载项目\nmake uninstall\n\n# 以开发模式安装项目\nmake develop\n\n# 运行测试\nmake test\n```\n>>>>>>> REPLACE\n```\n\nThis change adds information about using the Makefile for various project management tasks, based on the contents of the Makefile."}], "model": "human", "yaml_file": "actions/141_chat_action.yml"}
{"instruction": "make 添加一个启动 deepseek的命令: byzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--worker_concurrency 1000 \\\n--num_workers 1 \\\n--infer_params saas.base_url=\"https://api.deepseek.com/v1\" saas.api_key=${MODEL_DEEPSEEK_TOKEN} saas.model=deepseek-chat \\\n--model deepseek_chat  然后同步更新 @README.md 文档", "conversations": [{"role": "user", "content": "\n如果你需要生成代码，对于每个需要更改的文件,你需要按 *SEARCH/REPLACE block* 的格式进行生成。\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The opening fence and code language, eg: ```python\n2. The file path alone on a line, starting with \"##File:\" and verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n*SEARCH/REPLACE* blocks will replace *all* matching occurrences.\nInclude enough lines to make the SEARCH blocks unique.\n\nInclude *ALL* the code being searched and replaced!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n下面我们来看一个例子：\n\n当前项目目录结构：\n1. 项目根目录： /tmp/projects/mathweb\n2. 项目子目录/文件列表(类似tree 命令输出)\nflask/\n    app.py\n    templates/\n        index.html\n    static/\n        style.css\n\n用户需求： Change get_factorial() to use math.factorial\n\n回答： To make this change we need to modify `/tmp/projects/mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n用户需求： Refactor hello() into its own file.\n\n回答：To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/hello.py\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/main.py\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n现在让我们开始一个新的任务:\n\n当前项目目录结构：\n1. 项目根目录： /Users/allwefantasy/projects/super_analysis\n2. 项目子目录/文件列表(类似tree 命令输出)：\nsetup.py\n.auto-coder/\n    libs/\n        llm_friendly_packages/\n            src/\n                example1/\n                    main.py\nsrc/\n    superanalysis/\n        serve/\n            rest.py\n            convert.py\n            openai_serve.py\n            __init__.py\n            llm_wrapper.py\n            api_server.py\n            cli.py\n        sql/\n            __init__.py\n            types.py\n            generator.py\n下面是一些文件路径以及每个文件对应的源码：\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/types.py\nfrom pydantic import BaseModel\n\nclass RAGConfig(BaseModel):\n    schema_rag_base_url: str = \"\"\n    schema_rag_api_key: str = \"xxxx\"\n\n    context_rag_base_url: str = \"\"\n    context_rag_api_key: str = \"xxxx\"\n\nclass ByzerSQLConfig(BaseModel):\n    byzer_sql_url: str = \"\"\n    data_dir: str = \"\"\n    owner: str = \"\"\n\nclass ModelConfig(BaseModel):\n    llm_model: str = \"\"\n    gen_sql_llm_model: str = \"\"\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/cli.py\nimport argparse\nimport byzerllm\nfrom byzerllm.utils.client import ByzerLLM\nfrom superanalysis.serve.api_server import serve, ServerArgs\nfrom superanalysis.sql.types import RAGConfig, ByzerSQLConfig, ModelConfig\nfrom superanalysis.sql.generator import SQLGenerator\nfrom superanalysis.serve.llm_wrapper import LLWrapper\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"ByzerLLM API Server\")\n    parser.add_argument(\"--ray-address\", type=str, default=\"auto\", help=\"ray address\")\n    parser.add_argument(\n        \"--host\", type=str, default=\"0.0.0.0\", help=\"Host to bind the server to\"\n    )\n    parser.add_argument(\n        \"--port\", type=int, default=8000, help=\"Port to bind the server to\"\n    )\n    parser.add_argument(\n        \"--uvicorn-log-level\", type=str, default=\"info\", help=\"Uvicorn log level\"\n    )\n    parser.add_argument(\n        \"--allow-credentials\", action=\"store_true\", help=\"Allow credentials\"\n    )\n    parser.add_argument(\n        \"--allowed-origins\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed origins\"\n    )\n    parser.add_argument(\n        \"--allowed-methods\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed methods\"\n    )\n    parser.add_argument(\n        \"--allowed-headers\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed headers\"\n    )\n    parser.add_argument(\n        \"--api-key\", type=str, default=\"\", help=\"API key for authentication\"\n    )\n    parser.add_argument(\n        \"--served-model-name\",\n        type=str,\n        required=True,\n        help=\"Name of the model to be served\",\n    )\n    parser.add_argument(\n        \"--prompt-template\", type=str, default=\"\", help=\"Prompt template\"\n    )\n    parser.add_argument(\n        \"--response-role\", type=str, default=\"assistant\", help=\"Response role\"\n    )\n    parser.add_argument(\"--ssl-keyfile\", type=str, default=\"\", help=\"SSL key file\")\n    parser.add_argument(\n        \"--ssl-certfile\", type=str, default=\"\", help=\"SSL certificate file\"\n    )\n\n    # Add new arguments for RAGConfig\n    parser.add_argument(\n        \"--schema-rag-base-url\", type=str, default=\"\", help=\"RAG base URL\"\n    )\n    parser.add_argument(\n        \"--schema-rag-api-key\", type=str, default=\"xxxx\", help=\"RAG API key\"\n    )\n\n    parser.add_argument(\n        \"--context-rag-base-url\", type=str, default=\"\", help=\"RAG base URL\"\n    )\n    parser.add_argument(\n        \"--context-rag-api-key\", type=str, default=\"xxxx\", help=\"RAG API key\"\n    )\n\n    # Add new argument for ByzerSQLConfig\n    parser.add_argument(\"--byzer-sql-url\", type=str, default=\"\", help=\"Byzer SQL URL\")\n    parser.add_argument(\"--data-dir\", type=str, default=\"\", help=\"Data directory\")\n    parser.add_argument(\"--owner\", type=str, default=\"hello\", help=\"Owner of the Byzer SQL session\")\n    parser.add_argument(\n        \"--llm-model\", type=str, default=\"\", help=\"LLM model for general use\"\n    )\n    parser.add_argument(\n        \"--gen-sql-llm-model\", type=str, default=\"\", help=\"LLM model for SQL generation\"\n    )\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile,\n    )\n\n    # Create RAGConfig instance\n    rag_config = RAGConfig(\n        schema_rag_base_url=args.schema_rag_base_url,\n        schema_rag_api_key=args.schema_rag_api_key,\n        context_rag_base_url=args.context_rag_base_url,\n        context_rag_api_key=args.context_rag_api_key,\n    )\n\n    # Create ByzerSQLConfig instance\n    byzer_sql_config = ByzerSQLConfig(\n        byzer_sql_url=args.byzer_sql_url, data_dir=args.data_dir, owner=args.owner\n    )\n\n    model_config = ModelConfig(\n        llm_model=args.llm_model or args.served_model_name,\n        gen_sql_llm_model=args.gen_sql_llm_model or args.served_model_name,\n    )\n\n    byzerllm.connect_cluster(address=args.ray_address)\n    llm = ByzerLLM()\n    llm.setup_default_model_name(args.served_model_name)\n    llm = byzerllm.ByzerLLM()\n    if not server_args.served_model_name:\n        raise ValueError(\"served_model_name is required\")\n    llm.setup_default_model_name(server_args.served_model_name)\n\n    print(\"RAG Config: \", rag_config)\n    print(\"Model Config: \", model_config)\n    rag = SQLGenerator(\n        llm=llm,\n        rag_config=rag_config,\n        byzer_sql_config=byzer_sql_config,\n        model_config=model_config,\n    )\n    llm_wrapper = LLWrapper(llm=llm, entry=rag)\n    serve(llm_wrapper, server_args)\n\n\nif __name__ == \"__main__\":\n    main()\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/__init__.py\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/__init__.py\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/llm_wrapper.py\nfrom typing import Any, Dict, List, Optional, Union,Callable\nfrom byzerllm.utils.client.types import (    \n    LLMFunctionCallResponse,\n    LLMClassResponse,LLMResponse    \n)\nimport pydantic\nfrom byzerllm import ByzerLLM\nfrom byzerllm.utils.client import LLMResponse\nfrom byzerllm.utils.types import SingleOutputMeta\nfrom byzerllm.utils.langutil import asyncfy_with_semaphore\nfrom superanalysis.sql.generator import SQLGenerator\nfrom loguru import logger\n\nclass LLWrapper:\n\n    def __init__(self,llm:ByzerLLM,entry:SQLGenerator):\n        self.entry = entry\n        self.llm = llm\n        \n\n    def chat_oai(self,\n                 conversations,\n                 tools:List[Union[Callable,str]]=[], \n                 tool_choice:Optional[Union[Callable,str]]=None,\n                 execute_tool:bool=False,  \n                 impl_func:Optional[Callable]=None,\n                 execute_impl_func:bool=False,\n                 impl_func_params:Optional[Dict[str,Any]]=None,\n                 func_params:Optional[Dict[str,Any]]=None,\n                 response_class:Optional[Union[pydantic.BaseModel,str]] = None, \n                 response_after_chat:Optional[Union[pydantic.BaseModel,str]] = False,\n                 enable_default_sys_message:bool=True,                 \n                 model:Optional[str] = None,\n                 role_mapping=None,llm_config:Dict[str,Any]={}\n                 )->Union[List[LLMResponse],List[LLMFunctionCallResponse],List[LLMClassResponse]]: \n        res,contexts = self.entry.stream_chat_oai(conversations)\n        s = \"\".join(res)        \n        return [LLMResponse(output=s,metadata={},input=\"\")]\n\n    def stream_chat_oai(self,conversations, \n                        model:Optional[str]=None, \n                        role_mapping=None,\n                        delta_mode=False,\n                        llm_config:Dict[str,Any]={}): \n        res,contexts = self.entry.stream_chat_oai(conversations)        \n        for t in res:                        \n            yield (t,SingleOutputMeta(0,0))\n\n    async def async_stream_chat_oai(self,conversations, \n                        model:Optional[str]=None, \n                        role_mapping=None,\n                        delta_mode=False,\n                        llm_config:Dict[str,Any]={}): \n        res,contexts = self.rag.stream_chat_oai(conversations)                \n        for t in res:                                    \n            yield (t,SingleOutputMeta(0,0))\n                         \n\n    def __getattr__(self, name):        \n        return getattr(self.llm, name)\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/openai_serve.py\nimport argparse\nfrom service_server.api_server import ServerArgs, serve\nfrom service_server.llm_wrapper import LLWrapper\nfrom service_server.summary_rag import SummaryRAG\nimport byzerllm\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"OpenAI Serve\")\n    parser.add_argument(\n        \"--model\",\n        default=\"\",\n    )\n    parser.add_argument(\"--emb_model\", default=\"\")\n    parser.add_argument(\n        \"--ray_address\",\n        default=\"auto\",\n    )\n    parser.add_argument(\"--source_dir\", default=\".\", help=\"\")\n    parser.add_argument(\"--host\", default=\"\", help=\"\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"\")\n    parser.add_argument(\"--uvicorn_log_level\", default=\"info\", help=\"\")\n    parser.add_argument(\"--allow_credentials\", action=\"store_true\", help=\"\")\n    parser.add_argument(\"--allowed_origins\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--allowed_methods\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--allowed_headers\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--api_key\", default=\"\", help=\"\")\n    parser.add_argument(\"--served_model_name\", default=\"\", help=\"\")\n    parser.add_argument(\"--prompt_template\", default=\"\", help=\"\")\n    parser.add_argument(\"--ssl_keyfile\", default=\"\", help=\"\")\n    parser.add_argument(\"--ssl_certfile\", default=\"\", help=\"\")\n    parser.add_argument(\"--response_role\", default=\"assistant\", help=\"\")\n    parser.add_argument(\n        \"--collections\", default=\"\", help=\"Collection name for indexing\"\n    )\n    parser.add_argument(\n        \"--base_dir\",\n        default=\"\",\n        help=\"Path where the processed text embeddings were stored\",\n    )\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name or args.model,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile,\n    )\n    byzerllm.connect_cluster(address=args.ray_address)\n    llm = byzerllm.ByzerLLM()\n    if not server_args.served_model_name:\n        raise ValueError(\"served_model_name is required\")\n    llm.setup_default_model_name(server_args.served_model_name)\n    rag = SummaryRAG(llm=llm)    \n    llm_wrapper = LLWrapper(llm=llm, rag=rag)\n    serve(llm=llm_wrapper, args=server_args)\n\nif __name__ == \"__main__\":\n    main()\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/api_server.py\nimport os\nimport time\nimport uvicorn\nfrom fastapi import Request\nfrom fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import JSONResponse, StreamingResponse, Response\n\nfrom byzerllm.log import init_logger\nfrom byzerllm.utils import random_uuid\nfrom byzerllm.version import __version__ as version\nfrom byzerllm.utils.client import ByzerLLM, LLMRequest\nfrom byzerllm.utils.client.entrypoints.openai.serving_chat import OpenAIServingChat\nfrom byzerllm.utils.client.entrypoints.openai.serving_completion import OpenAIServingCompletion\nfrom byzerllm.utils.client.entrypoints.openai.protocol import (\n    ModelList,\n    ModelCard,\n    ModelPermission,\n    ChatCompletionRequest,\n    ErrorResponse,\n    CompletionRequest,\n    Embeddings,\n    EmbeddingsOutput,\n    EmbeddingsData,\n    EmbeddingsUsage,\n)\nfrom pydantic import BaseModel\nfrom typing import List\n\nlogger = init_logger(__name__)\n\nllm_client: ByzerLLM = None\nopenai_serving_chat: OpenAIServingChat = None\nopenai_serving_completion: OpenAIServingCompletion = None\n\nTIMEOUT_KEEP_ALIVE = 5  # seconds\n# timeout in 10 minutes. Streaming can take longer than 3 min\nTIMEOUT = float(os.environ.get(\"BYZERLLM_APISERVER_HTTP_TIMEOUT\", 600))\n\nrouter_app = FastAPI()\n\n\n@router_app.get(\"/health\")\nasync def health() -> Response:\n    \"\"\"Health check.\"\"\"\n    return Response(status_code=200)\n\n\n@router_app.get(\"/v1/models\")\nasync def show_available_models():\n    models = await openai_serving_chat.show_available_models()\n    return JSONResponse(content=models.model_dump())\n\n\n@router_app.get(\"/version\")\nasync def show_version():\n    return JSONResponse(content={\"version\": version})\n\n\n@router_app.get(\"/v1/models\", response_model=ModelList)\nasync def models() -> ModelList:\n    \"\"\"Show available models. Right now we only have one model.\"\"\"\n    model_cards = [\n        ModelCard(\n            id=\"\",\n            root=\"\",\n            permission=[ModelPermission()]\n        )\n    ]\n    return ModelList(data=model_cards)\n\n\n@router_app.post(\"/v1/completions\")\nasync def create_completion(\n        body: CompletionRequest,\n        request: Request\n):\n    generator = await openai_serving_completion.create_completion(body, request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            status_code=generator.code\n        )\n    if body.stream:\n        return StreamingResponse(\n            content=generator,\n            media_type=\"text/event-stream\"\n        )\n    else:\n        return JSONResponse(content=generator.model_dump())\n\n\n@router_app.post(\"/v1/chat/completions\")\nasync def create_chat_completion(\n        body: ChatCompletionRequest,\n        request: Request,\n):\n    \"\"\"Completion API similar to OpenAI's API.\n\n    See  https://platform.openai.com/docs/api-reference/chat/create\n    for the API specification. This API mimics the OpenAI ChatCompletion API.\n\n    NOTE: Currently we do not support the following features:\n        - function_call (Users should implement this by themselves)\n        - logit_bias (to be supported by vLLM engine)\n    \"\"\"\n    # async with async_timeout.timeout(TIMEOUT):\n\n    generator = await openai_serving_chat.create_chat_completion(body, request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            status_code=generator.code\n        )\n    if body.stream:\n        return StreamingResponse(\n            content=generator,\n            media_type=\"text/event-stream\"\n        )\n    else:\n        return JSONResponse(content=generator.model_dump())\n\n\n@router_app.post(\"/v1/embeddings\")\nasync def embed(body: Embeddings):\n    \"\"\"Given a prompt, the model will return one embedding.\n\n    Returns:\n        A response object with an embedding.\n    \"\"\"\n    embedding_id = f\"embed-{random_uuid()}\"\n\n    results_list = llm_client.emb(body.model, request=LLMRequest(instruction=body.input))\n    tokens = 0\n\n    return EmbeddingsOutput(\n        data=[\n            EmbeddingsData(\n                embedding=results.output,\n                index=i,\n                object=\"embedding\",\n            )\n            for i, results in enumerate(results_list)\n        ],\n        id=embedding_id,\n        object=\"list\",\n        created=int(time.time()),\n        model=body.model,\n        usage=EmbeddingsUsage(\n            prompt_tokens=tokens,\n            total_tokens=tokens,\n        ),\n    )\n\nclass ServerArgs(BaseModel):\n    host: str = None\n    port: int = 8000\n    uvicorn_log_level: str = \"info\"\n    allow_credentials: bool = False\n    allowed_origins: List[str] = [\"*\"]  \n    allowed_methods: List[str] = [\"*\"]\n    allowed_headers: List[str] = [\"*\"]\n    api_key: str = None\n    served_model_name: str = None\n    prompt_template: str = None\n    response_role: str = \"assistant\"\n    ssl_keyfile: str = None\n    ssl_certfile: str = None    \n\ndef serve(llm:ByzerLLM, args: ServerArgs):\n    \n    logger.info(f\"ByzerLLM API server version {version}\")\n    logger.info(f\"args: {args}\")\n\n    router_app.add_middleware(\n        CORSMiddleware,\n        allow_origins=args.allowed_origins,\n        allow_credentials=args.allow_credentials,\n        allow_methods=args.allowed_methods,\n        allow_headers=args.allowed_headers,\n    )\n    \n    if token := os.environ.get(\"BYZERLLM_API_KEY\") or args.api_key:\n\n        @router_app.middleware(\"http\")\n        async def authentication(request: Request, call_next):\n            if not request.url.path.startswith(\"/v1\"):\n                return await call_next(request)\n            if request.headers.get(\"Authorization\") != \"Bearer \" + token:\n                return JSONResponse(\n                    content={\"error\": \"Unauthorized\"},\n                    status_code=401\n                )\n            return await call_next(request)\n\n    # Register labels for metrics\n    # add_global_metrics_labels(model_name=engine_args.model)\n    global llm_client\n    llm_client = llm\n    \n    global openai_serving_chat\n    openai_serving_chat = OpenAIServingChat(\n        llm_client=llm_client,\n        response_role=args.response_role,\n        server_model_name=args.served_model_name,\n        prompt_template=args.prompt_template\n    )\n    global openai_serving_completion\n    openai_serving_completion = OpenAIServingCompletion(\n        llm_client=llm_client,\n        server_model_name=args.served_model_name,\n        prompt_template=args.prompt_template\n    )\n\n    uvicorn.run(\n        router_app,\n        host=args.host,\n        port=args.port,\n        log_level=args.uvicorn_log_level,\n        timeout_keep_alive=TIMEOUT_KEEP_ALIVE,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile\n    )\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/generator.py\nfrom typing import Optional, Dict, Any, List\nfrom openai import OpenAI\nimport json\nfrom .types import RAGConfig, ByzerSQLConfig, ModelConfig\nfrom byzerllm.utils.client import code_utils\nimport byzerllm\nimport requests\nfrom loguru import logger\nimport traceback\nfrom byzerllm.utils.nontext import TagExtractor\n\n\nclass SQLGenerator:\n\n    @byzerllm.prompt()\n    def gen_context(self, conversations: List[Dict[str, str]]) -> str:\n        \"\"\"\n        对话内容：\n        {% for msg in conversations %}\n        <{{ msg.role }}>: {{ msg.content }}\n        {% endfor %}\n\n        根据提供的对话内容，理解用户最后的问题，然后提取可能会用到的 Byzer-SQL 相关的知识。\n        \"\"\"\n\n    @byzerllm.prompt()\n    def gen_sql(\n        self,\n        conversations: List[Dict[str, str]],\n        context: str,\n        schema: str,\n        model: str = \"deepseek_chat\",\n    ) -> str:\n        \"\"\"\n        相关知识：\n        {{ context }}\n\n        Schema信息：\n        {{ schema }}\n\n        对话内容：\n        {% for msg in conversations %}\n        <{{ msg.role }}>: {{ msg.content }}\n        {% endfor %}\n\n        模型UDF函数：\n        {{ model }}\n\n        根据提供的相关知识以及对话内容，理解用户最后的问题，然后根据看到的schema 信息，生成 Byzer-SQL 语句。\n        Byzer-SQL 首先需要用 load 语法加载 schema 中的路径映射成表格，然后根据表格的信息生成对应的 Byzer-SQL 语句。\n\n        特别需要关注的是:\n        1. 用户的很多需求是要对表格的文本内容做自然语言处理，你需要使用 Byzer-SQL 中的大模型辅助函数来完成这个需求。\n        2. 使用 `llm_result` 函数处理 LLM 模型函数的响应,`llm_result` 函数得到的是一段文本，虽然我们要求模型函数只输出特定的文本，但考虑到大模型会有一些额外的字符输出，不会只是输出我们要求的文字，所以通过 `like` 来进行匹配。\n        3. 注意 Byzer-SQL 的语法，比如select 语句最后一定需要有 as <table_name>。\n        3. 输出的 Byzer-SQL 语句务必使用 <_byzer_sql_></_byzer_sql_> 标签包裹起来。\n        \"\"\"\n\n    @byzerllm.prompt()\n    def fetch_schema(self, context: str) -> str:\n        \"\"\"\n        下面是一些基础知识，包括名词术语等等：\n        {{ context }}\n\n        当理解对话内容的时候，需要参考上面的基础知识，然后更好的理解对应的表格schema信息是不是有用。\n        \"\"\"\n\n    def __init__(\n        self,\n        llm: byzerllm.ByzerLLM,\n        rag_config: RAGConfig,\n        byzer_sql_config: ByzerSQLConfig,\n        model_config: ModelConfig,\n    ) -> None:\n        self.llm = llm\n        self.schema_rag_client = OpenAI(\n            api_key=rag_config.schema_rag_api_key,\n            base_url=rag_config.schema_rag_base_url,\n        )\n        self.context_rag_client = OpenAI(\n            api_key=rag_config.context_rag_api_key,\n            base_url=rag_config.context_rag_base_url,\n        )\n        self.byzer_sql_url = byzer_sql_config.byzer_sql_url\n\n        self.llm_model = byzerllm.ByzerLLM()\n        self.llm_model.setup_default_model_name(model_config.llm_model)\n        self.gen_sql_llm_model = byzerllm.ByzerLLM()\n        self.gen_sql_llm_model.setup_default_model_name(model_config.gen_sql_llm_model)\n\n    def stream_chat_oai(\n        self,\n        conversations,\n        model: Optional[str] = None,\n        role_mapping=None,\n        delta_mode=False,\n        llm_config: Dict[str, Any] = {},\n    ):\n        try:\n            v, c = self._stream_chat_oai(\n                conversations=conversations,\n                model=model,\n                role_mapping=role_mapping,\n                delta_mode=delta_mode,\n                llm_config=llm_config,\n            )\n        except Exception as e:\n            error_msg = f\"Error in stream_chat_oai: {e}\\n{traceback.format_exc()}\"\n            logger.error(error_msg)\n            v, c = [str(e)], []\n\n        return v, c\n\n    def _stream_chat_oai(\n        self,\n        conversations,\n        model: Optional[str] = None,\n        role_mapping=None,\n        delta_mode=False,\n        llm_config: Dict[str, Any] = {},\n    ):\n        logger.info(f\"Starting stream_chat_oai with model: {model}\")\n        logger.debug(f\"Conversations: {conversations}\")\n        logger.debug(f\"LLM Config: {llm_config}\")\n\n        if not model:\n            model = self.llm_model.default_model_name\n        logger.info(f\"Using model: {model}\")\n\n        query = conversations[-1][\"content\"]\n\n        if (\n            \"使用四到五个字直接返回这句话的简要主题，不要解释、不要标点、不要语气词、不要多余文本，不要加粗，如果没有主题\"\n            in query\n            or \"简要总结一下对话内容，用作后续的上下文提示 prompt，控制在 200 字以内\"\n            in query\n        ):\n            chunks = self.llm.stream_chat_oai(\n                conversations=conversations,\n                model=model,\n                role_mapping=role_mapping,\n                llm_config=llm_config,\n                delta_mode=True,\n            )\n            return (chunk[0] for chunk in chunks), []\n\n        logger.info(\"Fetching context from context RAG client\")\n        context_query = self.gen_context.prompt(conversations=conversations)\n        context_response = self.context_rag_client.chat.completions.create(\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": json.dumps(\n                        {\"query\": context_query, \"only_contexts\": True},\n                        ensure_ascii=False,\n                    ),\n                }\n            ],\n            model=model,\n        )\n\n        context = context_response.choices[0].message.content\n        new_context = \"\"\n        for msg in context.split(\"\\n\"):\n            if msg.strip():\n                v = json.loads(msg)\n                new_context += v[\"source_code\"]\n        logger.info(f\"context {new_context[0:100]}...\")\n        logger.info(\"Context retrieved successfully\")\n\n        logger.info(\"Fetching schemas from schema RAG client\")\n        schema_query = self.fetch_schema.prompt(context=new_context)\n\n        response = self.schema_rag_client.chat.completions.create(\n            messages=conversations[0:-1]\n            + [\n                {\n                    \"role\": \"user\",\n                    \"content\": json.dumps(\n                        {\"query\": schema_query, \"only_contexts\": True},\n                        ensure_ascii=False,\n                    ),\n                }\n            ],\n            model=model,\n        )        \n\n        schema_str = response.choices[0].message.content\n        logger.debug(f\"Schema RAG response: {schema_str}\")\n        logger.info(\"Schema retrieved successfully\")\n\n        logger.info(\"Generating SQL string\")\n        sql_str = self.gen_sql.with_llm(self.gen_sql_llm_model).run(\n            conversations=conversations,\n            context=new_context,\n            schema=schema_str,\n            model=model,\n        )\n\n        logger.info(\"SQL string generated\")\n        logger.debug(f\"Generated SQL string: {sql_str}\")\n\n        # Extract SQL code from tags\n        tag_extractor = TagExtractor(sql_str)\n        extracted_tags = tag_extractor.extract()\n        sql = extracted_tags.content[0].content\n        logger.info(f\"Extracted SQL: {sql}\")\n\n        logger.info(f\"Sending SQL to Byzer SQL URL: {self.byzer_sql_url}\")\n        try:\n            response = requests.post(\n                self.byzer_sql_url,\n                data={\n                    \"sql\": sql,\n                    \"owner\": \"hello\",\n                    \"sessionPerUser\": True,\n                    \"sessionPerRequest\": True,\n                    \"includeSchema\": True,\n                },\n            )\n            response.raise_for_status()\n            result = response.content.decode(\"utf-8\")\n            logger.info(\"SQL execution result decoded\")\n            logger.debug(f\"SQL execution result: {result}\")\n            return (item for item in [result]), []\n        except requests.exceptions.RequestException as e:\n            logger.error(f\"Failed to send SQL to Byzer SQL URL: {e}\")\n            if hasattr(e.response, \"content\"):\n                error_content = e.response.content.decode(\"utf-8\")\n                logger.error(f\"Server response: {error_content}\")\n            else:\n                logger.error(\"No server response available\")\n            raise Exception(f\"Byzer SQL request failed: {str(e)}\") from e\n\n\n##File: /Users/allwefantasy/projects/super_analysis/setup.py\nfrom setuptools import setup, find_packages\n\n# Read requirements from requirements.txt\nwith open('requirements.txt') as f:\n    requirements = f.read().splitlines()\n\nsetup(\n    name=\"super-analysis\",\n    version=\"0.1.0\",\n    packages=find_packages(where=\"src\"),\n    package_dir={\"\": \"src\"},\n    install_requires=requirements,\n    entry_points={\n        \"console_scripts\": [\n            \"super-analysis.serve=superanalysis.serve.cli:main\",\n            \"super-analysis.convert=superanalysis.serve.convert:main\",\n            \"superanalysis.api=superanalysis.serve.rest:main\",\n        ],\n    },\n    author=\"allwefantasy\",\n    author_email=\"allwefantasy@gmail.com\",\n    description=\"A package for super analysis with ByzerLLM\",\n    long_description=open(\"README.md\").read(),\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/yourusername/super-analysis\",\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n    ],\n    python_requires='>=3.6',\n)\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/convert.py\nimport os\nimport argparse\nimport pandas as pd\nimport hashlib\nfrom typing import List, Dict\n\ndef get_csv_files(directory: str) -> List[str]:\n    \"\"\"Recursively get all CSV files in the given directory.\"\"\"\n    csv_files = []\n    for root, _, files in os.walk(directory):\n        for file in files:\n            if file.endswith('.csv'):\n                csv_files.append(os.path.join(root, file))\n    return csv_files\n\ndef extract_schema(df: pd.DataFrame) -> Dict[str, str]:\n    \"\"\"Extract schema from pandas DataFrame.\"\"\"\n    schema = {}\n    for column in df.columns:\n        dtype = str(df[column].dtype)\n        if dtype == 'object':\n            schema[column] = 'STRING'\n        elif dtype.startswith('int'):\n            schema[column] = 'INTEGER'\n        elif dtype.startswith('float'):\n            schema[column] = 'FLOAT'\n        elif dtype == 'bool':\n            schema[column] = 'BOOLEAN'\n        elif dtype.startswith('datetime'):\n            schema[column] = 'TIMESTAMP'\n        else:\n            schema[column] = 'STRING'  # Default to STRING for unknown types\n    return schema\n\ndef generate_schema_text(schema: Dict[str, str], csv_file: str, df: pd.DataFrame, include_rows_num: int) -> str:\n    \"\"\"Generate schema text from the schema dictionary and include the CSV file name and sample data.\"\"\"\n    schema_lines = [f\"CSV File: {csv_file}\\n\"]\n    schema_lines.extend([f\"{field}: {dtype}\" for field, dtype in schema.items()])\n    \n    if include_rows_num > 0:\n        schema_lines.append(f\"\\nSample Data (first {include_rows_num} rows):\")\n        sample_data = df.head(include_rows_num).to_string(index=False)\n        schema_lines.append(sample_data)\n    \n    return \"\\n\".join(schema_lines)\n\ndef save_schema(schema_text: str, csv_file: str, output_dir: str) -> None:\n    \"\"\"Save schema text to a file in the output directory.\"\"\"\n    # Generate MD5 hash of the schema text\n    file_name = hashlib.md5(schema_text.encode()).hexdigest() + '.md'\n    output_path = os.path.join(output_dir, file_name)\n    \n    with open(output_path, 'w') as f:\n        f.write(schema_text)\n    print(f\"Schema for {csv_file} saved to {output_path}\")\n\ndef convert(data_dir: str, doc_dir: str, include_rows_num: int) -> None:\n    \"\"\"Main conversion function.\"\"\"\n    csv_files = get_csv_files(data_dir)\n    \n    for csv_file in csv_files:\n        print(f\"Processing {csv_file}\")\n        df = pd.read_csv(csv_file)\n        schema = extract_schema(df)\n        schema_text = generate_schema_text(schema, csv_file, df, include_rows_num)\n        save_schema(schema_text, csv_file, doc_dir)\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Convert CSV files to schema documents\")\n    parser.add_argument(\"--data_dir\", required=True, help=\"Directory containing CSV files\")\n    parser.add_argument(\"--doc_dir\", required=True, help=\"Output directory for schema documents\")\n    parser.add_argument(\"--include-rows-num\", type=int, default=0, help=\"Number of sample rows to include in the schema\")\n    \n    args = parser.parse_args()\n    \n    if not os.path.exists(args.doc_dir):\n        os.makedirs(args.doc_dir)\n    \n    convert(args.data_dir, args.doc_dir, args.include_rows_num)\n\nif __name__ == \"__main__\":\n    main()\n\n##File: /Users/allwefantasy/projects/super_analysis/README.md\n# Super Analysis\n\nSuper Analysis 支持 NLP2SQL 以及 使用 SQL 对表中的文本字段做自然语言分析。\n\n## 安装\n\n你可以使用 pip 安装 Super Analysis：\n\n```bash\ngit clone https://github.com/allwefantasy/super_analysis\ncd super-analysis\npip install -e .\n```\n\n或者使用 Makefile 来管理项目：\n\n```bash\n# 清理项目\nmake clean\n\n# 构建项目\nmake build\n\n# 创建wheel包\nmake wheel\n\n# 安装项目\nmake install\n\n# 卸载项目\nmake uninstall\n\n# 以开发模式安装项目\nmake develop\n\n# 运行测试\nmake test\n```\n\n\n\n## 示例数据\n\n下载电影数据集： https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset/download?datasetVersionNumber=7\n\n\n## 启动分析服务\n\n整个 super-analysis 服务分为四个部分：\n\n1. byzer-sql 分布式执行引擎 \n2. schema 文档知识库\n4. context 文档知识库\n5. super-analysis 自身\n\n在启动 super-analysis 之前，你需要启动 schema 文档知识库和 context 文档知识库以及 byzer-sql 引擎。\n\n\n在你的电脑上参考 [这个文档](./docs/4.3.1%20安装与配置%20Byzer-SQL.pdf) 完成部署.\n\n> 启动的时候需要在安装有 super-analysis 的conda环境中启动。\n\n当 byzer-sql 部署完成后，注册账号为 `hello`, 然后进入到 byzer-sql 的控制台中执行以下脚本：\n\n```sql\n!byzerllm setup single;\n\nrun command as LLM.`` where \naction=\"infer\"\nand reconnect=\"true\"\nand pretrainedModelType=\"saas/openai\"\nand udfName=\"deepseek_chat\";\n```\n\n现在可以按如下方式做一些数据预处理或者启动相关服务：\n\n```bash\n## 将电影数据集的csv文件转换为schema文档\nsuper-analysis.convert --data_dir /Users/allwefantasy/data/movice --doc_dir /Users/allwefantasy/data/movice/schemas/\n\n# 启动schema文档知识库\nauto-coder doc serve \\\n--model deepseek_chat --index_filter_workers 100 \\\n--tokenizer_path /Users/allwefantasy/Downloads/tokenizer.json \\\n--doc_dir /Users/allwefantasy/data/movice/schemas/ \\\n--port 8001\n\n\n# 下载 Byzer-SQL 专门为大模型优化的文档\ngit clone https://github.com/allwefantasy/llm_friendly_packages\n\n# 启动文档知识库\nauto-coder doc serve \\\n--model deepseek_chat --index_filter_workers 100 \\\n--tokenizer_path /Users/allwefantasy/Downloads/tokenizer.json \\\n--doc_dir  /Users/allwefantasy/projects/llm_friendly_packages/github.com/allwefantasy \\\n--port 8002\n\n# 启动兼容 OpenAI Server 的分析服务\nsuper-analysis.serve --served-model-name deepseek_chat --port 8000 \\\n--schema-rag-base-url http://127.0.0.1:8001/v1 \\\n--context-rag-base-url http://127.0.0.1:8002/v1 \\\n--byzer-sql-url http://127.0.0.1:9003/run/script\n```\n\n现在你可以用 OpenAI SDK 去使用了。 具体测试和接口使用参考 [openai_local_api.ipynb](./notebooks/openai_local_api.ipynb)\n\n## 非 OpenAI 兼容接口\n\n除了兼容 OpenAI 的接口外，Super Analysis 还提供了一个简单的 REST API 接口，可以直接与模型进行交互。以下是如何使用这个接口：\n\n### 启动服务\n\n使用以下命令启动 REST API 服务：\n\n```bash\nsuperanalysis.api --schema-rag-base-url <schema_rag_url> \\\n                  --schema-rag-api-key <schema_rag_api_key> \\\n                  --context-rag-base-url <context_rag_url> \\\n                  --context-rag-api-key <context_rag_api_key> \\\n                  --byzer-sql-url <byzer_sql_url> \\\n                  --llm-model <llm_model_name> \\\n                  --gen-sql-llm-model <gen_sql_llm_model_name> \\\n                  --host <host> \\\n                  --port <port>\n```\n\n请替换尖括号中的值为实际的配置信息。\n\n### API 端点\n\n服务启动后，将提供以下 API 端点：\n\n#### POST /chat\n\n此端点用于与模型进行对话。\n\n请求体格式：\n\n```json\n{\n  \"conversations\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"你的问题或指令\"\n    }\n  ],\n  \"model\": \"可选的模型名称\"\n}\n```\n\n响应格式：\n\n```json\n{\n  \"response\": \"模型的回答\"\n}\n```\n\n### 示例使用\n\n以下是使用 Python 请求库与 API 交互的示例：\n\n```python\nimport requests\nimport json\n\nurl = \"http://localhost:8000/chat\"\npayload = {\n    \"conversations\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"你好，请介绍一下你自己。\"\n        }\n    ],\n    \"model\": \"your_model_name\"\n}\n\nresponse = requests.post(url, json=payload)\nresult = response.json()\n\nprint(result[\"response\"])\n```\n\n这个 API 相比兼容 OpenAI 接口，可以在请求中提供提供更多可选参数。\n\n## 使用方法\n\n安装完成后，你可以使用 `super-analysis.serve` 命令来启动 API 服务器：\n\n```bash\nsuper-analysis.serve --served-model-name your_model_name\n```\n\n### 命令行参数\n\n以下是可用的命令行参数：\n\n- `--host`: 服务器绑定的主机地址（默认：0.0.0.0）\n- `--port`: 服务器绑定的端口（默认：8000）\n- `--uvicorn-log-level`: Uvicorn 日志级别（默认：info）\n- `--allow-credentials`: 允许凭证（默认：False）\n- `--allowed-origins`: 允许的源（默认：[\"*\"]）\n- `--allowed-methods`: 允许的方法（默认：[\"*\"]）\n- `--allowed-headers`: 允许的头部（默认：[\"*\"]）\n- `--api-key`: 用于认证的 API 密钥\n- `--served-model-name`: 要服务的模型名称（必需）\n- `--prompt-template`: 提示模板\n- `--response-role`: 响应角色（默认：assistant）\n- `--ssl-keyfile`: SSL 密钥文件路径\n- `--ssl-certfile`: SSL 证书文件路径\n\n例如：\n\n```bash\nsuper-analysis.serve --served-model-name deepseek_chat --port 8000\n```\n\n\n## 抽取csv文件schema\n\nThe `super-analysis.convert` command allows you to convert CSV files to schema documents. This is useful for generating metadata about your CSV files that can be used for further analysis or documentation.\n\n#### Usage\n\n```\nsuper-analysis.convert --data_dir <path_to_csv_files> --doc_dir <output_directory> [--include-rows-num <number_of_sample_rows>]\n```\n\nArguments:\n- `--data_dir`: Directory containing CSV files (required)\n- `--doc_dir`: Output directory for schema documents (required)\n- `--include-rows-num`: Number of sample rows to include in the schema (optional, default is 0)\n\nThe utility will:\n1. Recursively find all CSV files in the specified directory\n2. Extract the schema from each CSV file\n3. Generate a schema text that includes:\n   - The CSV file name\n   - Field names and their data types\n   - Sample data (if `--include-rows-num` is specified)\n4. Save the schema text as a markdown file in the output directory\n\n## 联系方式\n\n如有任何问题或建议，请联系 allwefantasy@gmail.com。\n\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/rest.py\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nfrom typing import List, Optional\nimport uvicorn\nimport argparse\nfrom superanalysis.sql.generator import SQLGenerator\nfrom superanalysis.sql.types import RAGConfig, ByzerSQLConfig, ModelConfig\nimport byzerllm\n\napp = FastAPI()\n\nclass ChatMessage(BaseModel):\n    role: str\n    content: str\n\nclass ChatRequest(BaseModel):\n    conversations: List[ChatMessage]\n    model: Optional[str] = None\n\nclass ChatResponse(BaseModel):\n    response: str\n\n@app.post(\"/chat\", response_model=ChatResponse)\nasync def chat(request: ChatRequest):\n    try:\n        result, _ = sql_generator.stream_chat_oai(\n            conversations=[{\"role\": msg.role, \"content\": msg.content} for msg in request.conversations],\n            model=request.model\n        )\n        return ChatResponse(response=\"\".join(result))\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"SuperAnalysis API Server\")\n    parser.add_argument(\"--host\", type=str, default=\"0.0.0.0\", help=\"Host to bind the server to\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"Port to bind the server to\")\n    parser.add_argument(\"--schema-rag-base-url\", type=str, required=True, help=\"Schema RAG base URL\")\n    parser.add_argument(\"--schema-rag-api-key\", type=str, required=True, help=\"Schema RAG API key\")\n    parser.add_argument(\"--context-rag-base-url\", type=str, required=True, help=\"Context RAG base URL\")\n    parser.add_argument(\"--context-rag-api-key\", type=str, required=True, help=\"Context RAG API key\")\n    parser.add_argument(\"--byzer-sql-url\", type=str, required=True, help=\"Byzer SQL URL\")\n    parser.add_argument(\"--data-dir\", type=str, default=\"\", help=\"Data directory\")\n    parser.add_argument(\"--owner\", type=str, default=\"hello\", help=\"Owner of the Byzer SQL session\")\n    parser.add_argument(\"--llm-model\", type=str, required=True, help=\"LLM model for general use\")\n    parser.add_argument(\"--gen-sql-llm-model\", type=str, required=True, help=\"LLM model for SQL generation\")\n    \n    args = parser.parse_args()\n\n    rag_config = RAGConfig(\n        schema_rag_base_url=args.schema_rag_base_url,\n        schema_rag_api_key=args.schema_rag_api_key,\n        context_rag_base_url=args.context_rag_base_url,\n        context_rag_api_key=args.context_rag_api_key\n    )\n\n    byzer_sql_config = ByzerSQLConfig(\n        byzer_sql_url=args.byzer_sql_url,\n        data_dir=args.data_dir,\n        owner=args.owner\n    )\n\n    model_config = ModelConfig(\n        llm_model=args.llm_model,\n        gen_sql_llm_model=args.gen_sql_llm_model\n    )\n\n    llm = byzerllm.ByzerLLM()\n    llm.setup_default_model_name(args.llm_model)\n\n    global sql_generator\n    sql_generator = SQLGenerator(\n        llm=llm,\n        rag_config=rag_config,\n        byzer_sql_config=byzer_sql_config,\n        model_config=model_config\n    )\n\n    uvicorn.run(app, host=args.host, port=args.port)\n\nif __name__ == \"__main__\":\n    main()\n\n##File: /Users/allwefantasy/projects/super_analysis/Makefile\n# Makefile for super-analysis project\n\n# Python interpreter\nPYTHON := python3\n\n# Project name\nPROJECT_NAME := super-analysis\n\n# Directories\nSRC_DIR := src\nDIST_DIR := dist\nBUILD_DIR := build\n\n# Commands\nSETUP := $(PYTHON) setup.py\n\n.PHONY: all clean build wheel\n\nall: clean build wheel\n\nclean:\n\t@echo \"Cleaning up...\"\n\trm -rf $(BUILD_DIR) $(DIST_DIR) *.egg-info\n\tfind . -type d -name \"__pycache__\" -exec rm -rf {} +\n\tfind . -type f -name \"*.pyc\" -delete\n\nbuild:\n\t@echo \"Building the project...\"\n\t$(SETUP) build\n\nwheel:\n\t@echo \"Creating wheel package...\"\n\t$(SETUP) bdist_wheel\n\ninstall:\n\t@echo \"Installing the package...\"\n\t$(PYTHON) -m pip install .\n\nuninstall:\n\t@echo \"Uninstalling the package...\"\n\t$(PYTHON) -m pip uninstall -y $(PROJECT_NAME)\n\ndevelop:\n\t@echo \"Installing the package in editable mode...\"\n\t$(PYTHON) -m pip install -e .\n\ntest:\n\t@echo \"Running tests...\"\n\t$(PYTHON) -m unittest discover tests\n\n##File: .auto-coder/libs/llm_friendly_packages/github.com/allwefantasy/byzer-llm/README.md\n# byzerllm 大模型编程快速指南\n\n本文示例 [Notebook](../../notebooks/003_byzerllm_大模型编程快速指南.ipynb)\n\n## 安装\n\n```bash\npip install -U byzerllm\nray start --head\n```\n## 启动一个模型代理\n\nbyzerllm 支持私有化模型或者SaaS模型的部署。\n\n这里以 deepseek 官方API 为例：\n\n```bash\neasy-byzerllm deploy deepseek-chat --token xxxxx --alias deepseek_chat\n```\n\n或者跬基流动API:\n\n```bash\neasy-byzerllm deploy alibaba/Qwen1.5-110B-Chat --token xxxxx --alias qwen110b_chat\n```\n\n将上面的 API KEY 替换成你们自己的。\n\n如果你想部署私有化模型或者对接 Ollama 等更多需求，参考 [002_使用byzerllm进行模型部署.md](./002_使用byzerllm进行模型部署.md) 或者 [README.md](../../README.md)。\n\n之后，你就可以在代码里使用  deepseek_chat 或者 qwen110b_chat  访问模型了。\n\n## hello world\n\n来和我们的大模型打个招呼:\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\n@byzerllm.prompt(llm=llm)\ndef hello(q:str) ->str:\n    '''\n    你好, {{ q }}\n    '''\n\ns = hello(\"你是谁\")    \nprint(s)\n\n## 输出:\n## '你好！我是一个人工智能助手，专门设计来回答问题、提供信息和帮助解决问题。如果你有任何疑问或需要帮助，请随时告诉我。'\n```\n\n恭喜，你和大模型成功打了招呼！\n\n可以看到，我们通过 `@byzerllm.prompt` 装饰器，将一个方法转换成了一个大模型的调用，然后这个方法的主题是一段文本，文本中\n使用了 jinja2 模板语法，来获得方法的参数。当正常调用该方法时，实际上就发起了和大模型的交互，并且返回了大模型的结果。\n\n在 byzerllm 中，我们把这种方法称之为 prompt 函数。\n\n## 查看发送给大模型的prompt\n\n很多情况你可能需要调试，查看自己的 prompt 渲染后到底是什么样子的，这个时候你可以通过如下方式\n获取渲染后的prompt:\n\n```python\nhello.prompt(\"你是谁\")\n## '你好, 你是谁'\n```            \n\n## 动态换一个模型\n\n前面的 hello 方法在初始化的时候，我们使用了默认的模型 deepseek_chat，如果我们想换一个模型，可以这样做：\n\n```python\nhello.with_llm(llm).run(\"你是谁\")\n## '你好！我是一个人工智能助手，专门设计来回答问题、提供信息和帮助解决问题。如果你有任何疑问或需要帮助，请随时告诉我。'\n```\n\n通过 with_llm 你可以设置一个新的 llm 对象，然后调用 run 方法，就可以使用新的模型了。\n\n## 超长文本生成\n\n我们知道，大模型一次生成的长度其实是有限的，如果你想生成超长文本，你可能需手动的不断获得\n生成结果，然后把他转化为输入，然后再次生成，这样的方式是比较麻烦的。\n\nbyzerllm 提供了更加易用的 API :\n\n```python\nimport byzerllm\nfrom byzerllm import ByzerLLM\n\nllm = ByzerLLM.from_default_model(\"deepseek_chat\")\n\n@byzerllm.prompt()\ndef tell_story() -> str:\n    \"\"\"\n    讲一个100字的故事。    \n    \"\"\"\n\n\ns = (\n    tell_story.with_llm(llm)\n    .with_response_markers()\n    .options({\"llm_config\": {\"max_length\": 10}})\n    .run()\n)\nprint(s)\n\n## 从前，森林里住着一只聪明的小狐狸。一天，它发现了一块闪闪发光的宝石。小狐狸决定用这块宝石帮助森林里的动物们。它用宝石的光芒指引迷路的小鹿找到了回家的路，用宝石的温暖治愈了受伤的小鸟。从此，小狐狸成了森林里的英雄，动物们都感激它的善良和智慧。\n```\n\n实际核心部分就是这一行：\n\n```python\ntell_story.with_llm(llm)\n    .with_response_markers()    \n    .run()\n```\n\n我们只需要调用 `with_response_markers` 方法，系统就会自动的帮我们生成超长文本。\n在上面的案例中，我们通过\n\n```python\n.options({\"llm_config\": {\"max_length\": 10}})\n```\n\n认为的限制大模型一次交互最多只能输出10个字符，但是系统依然自动完成了远超过10个字符的文本生成。\n\n## 对象输出\n\n前面我们的例子都是返回字符串，但是我们也可以返回对象，这样我们就可以更加灵活的处理返回结果。\n\n```python\nimport pydantic \n\nclass Story(pydantic.BaseModel):\n    '''\n    故事\n    '''\n\n    title: str = pydantic.Field(description=\"故事的标题\")\n    body: str = pydantic.Field(description=\"故事主体\")\n\n@byzerllm.prompt()\ndef tell_story()->Story:\n    '''\n    讲一个100字的故事。    \n    '''\n\ns = tell_story.with_llm(llm).run()\nprint(isinstance(s, Story))\nprint(s.title)\n\n## True\n## 勇敢的小鸟\n```\n\n可以看到，我们很轻松的将输出转化为格式化输出。\n\n## 自定义字段抽取\n\n前面的结构化输出，其实会消耗更多token,还有一种更加精准的结构化输出方式。\n比如让大模型生成一个正则表达式，但实际上大模型很难准确只输出一个正则表达式，这个时候我们可以通过自定义抽取函数来获取我们想要的结果。\n\n\n```python\nfrom loguru import logger\nimport re\n\n@byzerllm.prompt()\ndef generate_regex_pattern(desc: str) -> str:\n    \"\"\"\n    根据下面的描述生成一个正则表达式，要符合python re.compile 库的要求。\n\n    {{ desc }}\n\n    最后生成的正则表达式要在<REGEX></REGEX>标签对里。\n    \"\"\"    \n\ndef extract_regex_pattern(regex_block: str) -> str:    \n    pattern = re.search(r\"<REGEX>(.*)</REGEX>\", regex_block, re.DOTALL)\n    if pattern is None:\n        logger.warning(\"No regex pattern found in the generated block:\\n {regex_block}\")\n        raise None\n    return pattern.group(1)\n\npattern = \"匹配一个邮箱地址\"\nv = generate_regex_pattern.with_llm(llm).with_extractor(extract_regex_pattern).run(desc=pattern)\nprint(v)\n## ^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\n```\n\n在上面的例子里，我们根据一句话生成一个正则表达式。我们通过 `with_extractor` 方法，传入了一个自定义的抽取函数，这个函数会在大模型生成结果后，对结果进行处理，然后返回我们想要的结果。\n\n我们在 prompt 明确说了，生成的结果要放到 `<REGEX></REGEX>` 标签对里，然后我们通过 extract_regex_pattern 函数，从结果中提取出了我们想要的正则表达式。\n\n## 在实例方法中使用大模型\n\n```python\nimport byzerllm\ndata = {\n    'name': 'Jane Doe',\n    'task_count': 3,\n    'tasks': [\n        {'name': 'Submit report', 'due_date': '2024-03-10'},\n        {'name': 'Finish project', 'due_date': '2024-03-15'},\n        {'name': 'Reply to emails', 'due_date': '2024-03-08'}\n    ]\n}\n\n\nclass RAG():\n    def __init__(self):        \n        self.llm = byzerllm.ByzerLLM()\n        self.llm.setup_template(model=\"deepseek_chat\",template=\"auto\")\n        self.llm.setup_default_model_name(\"deepseek_chat\")        \n    \n    @byzerllm.prompt(lambda self:self.llm)\n    def generate_answer(self,name,task_count,tasks)->str:\n        '''\n        Hello {{ name }},\n\n        This is a reminder that you have {{ task_count }} pending tasks:\n        {% for task in tasks %}\n        - Task: {{ task.name }} | Due: {{ task.due_date }}\n        {% endfor %}\n\n        Best regards,\n        Your Reminder System\n        '''        \n\nt = RAG()\n\nresponse = t.generate_answer(**data)\nprint(response)\n\n## 输出:\n## Hello Jane Doe,\n##I hope this message finds you well. I wanted to remind you of your 3 pending tasks to ensure you stay on track:\n## 1. **Submit report** - This task is due on **2024-03-10**. Please ensure that you allocat\n```\n\n这里我们给了个比较复杂的例子，但我们可以看到，给一个实例prompt方法和普通prompt 方法差异不大。\n唯一的区别是如果你希望在定义的时候就指定大模型，使用一个lambda函数返回实例的 llm 对象即可。\n\n```python\n@byzerllm.prompt(lambda self:self.llm)\n```\n\n你也可以不返回，在调用的时候通过 `with_llm` 方法指定 llm 对象。\n\n此外，这个例子也展示了如何通过jinja2模板语法，来处理复杂的结构化数据。\n\n## 通过 Python 代码处理复杂入参\n\n上面的一个例子中，我们通过 jinja2 模板语法，来处理复杂的结构化数据，但是有时候我们可能需要更加复杂的处理，这个时候我们可以通过 Python 代码来处理。\n\n```python\nimport byzerllm\n\ndata = {\n    'name': 'Jane Doe',\n    'task_count': 3,\n    'tasks': [\n        {'name': 'Submit report', 'due_date': '2024-03-10'},\n        {'name': 'Finish project', 'due_date': '2024-03-15'},\n        {'name': 'Reply to emails', 'due_date': '2024-03-08'}\n    ]\n}\n\n\nclass RAG():\n    def __init__(self):        \n        self.llm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n    \n    @byzerllm.prompt()\n    def generate_answer(self,name,task_count,tasks)->str:\n        '''\n        Hello {{ name }},\n\n        This is a reminder that you have {{ task_count }} pending tasks:\n            \n        {{ tasks }}\n\n        Best regards,\n        Your Reminder System\n        '''\n        \n        tasks_str = \"\\n\".join([f\"- Task: {task['name']} | Due: { task['due_date'] }\" for task in tasks])\n        return {\"tasks\": tasks_str}\n\nt = RAG()\n\nresponse = t.generate_answer.with_llm(t.llm).run(**data)\nprint(response)\n\n## Just a gentle nudge to keep you on track with your pending tasks. Here's a quick recap:....\n```\n\n在这个例子里，我们直接把 tasks 在方法体里进行处理，然后作为一个字符串返回，最够构建一个字典，字典的key为 tasks,然后\n你就可以在 docstring 里使用 `{{ tasks }}` 来引用这个字符串。\n\n这样对于很复杂的入参，就不用谢繁琐的 jinja2 模板语法了。\n\n## 如何自动实现一个方法\n\n比如我定义一个签名，但是我不想自己实现里面的逻辑，让大模型来实现。这个在 byzerllm 中叫 function impl。我们来看看怎么\n实现:\n\n```python\nimport pydantic\nclass Time(pydantic.BaseModel):\n    time: str = pydantic.Field(...,description=\"时间，时间格式为 yyyy-MM-dd\")\n\n\n@llm.impl()\ndef calculate_current_time()->Time:\n    '''\n    计算当前时间\n    '''\n    pass \n\n\ncalculate_current_time()\n#output: Time(time='2024-06-14')\n```\n\n在这个例子里，我们定义了一个 calculate_current_time 方法，但是我们没有实现里面的逻辑，我们通过 `@llm.impl()` 装饰器，让大模型来实现这个方法。\n为了避免每次都要“生成”这个方法，导致无法适用，我们提供了缓存，用户可以按如下方式打印速度：\n\n```python\nstart = time.monotonic()\ncalculate_current_time()\nprint(f\"first time cost: {time.monotonic()-start}\")\n\nstart = time.monotonic()\ncalculate_current_time()\nprint(f\"second time cost: {time.monotonic()-start}\")\n\n# output:\n# first time cost: 6.067266260739416\n# second time cost: 4.347506910562515e-05\n```\n可以看到，第一次执行花费了6s,第二次几乎是瞬间完成的，这是因为第一次执行的时候，我们实际上是在生成这个方法，第二次执行的时候，我们是执行已经生成好的代码，所以速度会非常快。你可以显示的调用 `llm.clear_impl_cache()` 清理掉函数缓存。\n\n## Stream 模式\n\n前面的例子都是一次性生成结果，但是有时候我们可能需要一个流式的输出，这个时候我们可能需要用底层一点的API来完成了：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\nv = llm.stream_chat_oai(model=\"deepseek_chat\",conversations=[{\n    \"role\":\"user\",\n    \"content\":\"你好，你是谁\",\n}],delta_mode=True)\n\nfor t in v:\n    print(t,flush=True)  \n\n# 你好\n# ！\n# 我\n# 是一个\n# 人工智能\n# 助手\n# ，\n# 旨在\n# 提供\n# 信息\n# 、\n# 解答\n# 问题....\n```\n\n如果你不想要流式输出，但是想用底层一点的API，你可以使用 `llm.chat_oai` 方法：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\nv = llm.chat_oai(model=\"deepseek_chat\",conversations=[{\n    \"role\":\"user\",\n    \"content\":\"你好，你是谁\",\n}])\n\nprint(v[0].output)\n## 你好！我是一个人工智能助手，旨在提供信息、解答问题和帮助用户解决问题。如果你有任何问题或需要帮助，请随时告诉我。\n```\n\n## Function Calling \n\nbyzerllm 可以不依赖模型自身就能提供 function calling 支持，我们来看个例子：\n\n\n```python\nfrom typing import List,Dict,Any,Annotated\nimport pydantic \nimport datetime\nfrom dateutil.relativedelta import relativedelta\n\ndef compute_date_range(count:Annotated[int,\"时间跨度，数值类型\"],\n                       unit:Annotated[str,\"时间单位，字符串类型\",{\"enum\":[\"day\",\"week\",\"month\",\"year\"]}])->List[str]:\n    '''\n    计算日期范围\n\n    Args:\n        count: 时间跨度，数值类型\n        unit: 时间单位，字符串类型，可选值为 day,week,month,year\n    '''        \n    now = datetime.datetime.now()\n    now_str = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n    if unit == \"day\":\n        return [(now - relativedelta(days=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"week\":\n        return [(now - relativedelta(weeks=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"month\":\n        return [(now - relativedelta(months=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"year\":\n        return [(now - relativedelta(years=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    return [\"\",\"\"]\n\ndef compute_now()->str:\n    '''\n    计算当前时间\n    '''\n    return datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n```\n\n我们定义了两个方法，一个是计算日期范围，一个是计算当前时间。\n\n现在我么可以来测试下，系统如何根据自然语言决定调用哪个方法：\n\n```python\nt = llm.chat_oai([{\n    \"content\":'''计算当前时间''',\n    \"role\":\"user\"    \n}],tools=[compute_date_range,compute_now],execute_tool=True)\n\nt[0].values\n\n## output: ['2024-06-14 15:18:02']\n```\n\n我们可以看到，他正确的选择了 compute_now 方法。\n\n接着我们再试一个：\n\n```python\nt = llm.chat_oai([{\n    \"content\":'''最近三个月趋势''',\n    \"role\":\"user\"    \n}],tools=[compute_date_range,compute_now],execute_tool=True)\n\nt[0].values\n\n## output: [['2024-03-14 15:19:13', '2024-06-14 15:19:13']]\n```\n\n模型正确的选择了 compute_date_range 方法。\n\n## 多模态\n\nbyerllm 也能很好的支持多模态的交互，而且统一了多模态大模型的接口，比如你可以用一样的方式使用 openai 或者 claude 的图片转文字能力， 或者一致的方式使用火山，azuer, openai的语音合成接口。\n\n### 图生文\n\n```python\nimport byzerllm\nfrom byzerllm.types import ImagePath\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\n\n\n@byzerllm.prompt()\ndef what_in_image(image_path: ImagePath) -> str:\n    \"\"\"\n    {{ image_path }}\n    这个图片里有什么？\n    \"\"\"    \n\n\nv = what_in_image.with_llm(vl_llm).run(\n    ImagePath(value=\"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\")\n)\nv\n## OUTPUT: 这张图片展示了多只可爱的猫咪，采用了艺术风格的绘画。猫咪们有不同的颜色和花纹，背景是浅棕色，上面还点缀着一些红色的花朵。整体画面给人一种温馨和谐的感觉\n```\n\n可以看到，我们只需要把 prompt 函数的图片地址入参使用 byzerllm.types.ImagePath里进行包装，就可以直接在 prompt 函数体里\n带上图片。\n\n或者你可以这样：\n\n```python\nimport byzerllm\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\n\n\n@byzerllm.prompt()\ndef what_in_image(image_path: str) -> str:\n    \"\"\"\n    {{ image }}\n    这个图片里有什么？\n    \"\"\"\n    return {\"image\": byzerllm.Image.load_image_from_path(image_path)}\n\n\nv = what_in_image.with_llm(vl_llm).run(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv\n```\n\n通过 `image_path` 参数，然后通过 `byzerllm.Image.load_image_from_path` 方法，转化为一个图片对象 image，最后在 prompt 函数体里\n使用 `{{ image }}` 引用这个图片对象。\n\n另外我们也是可以支持配置多张图片的。\n\n另外，我们也可以使用基础的 `llm.chat_oai` 方法来实现：\n\n```python\nimport byzerllm\nimport json\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\nimage = byzerllm.Image.load_image_from_path(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv = vl_llm.chat_oai(\n    conversations=[\n        {\n            \"role\": \"user\",\n            \"content\": json.dumps(\n                [{\"image\": image, \"text\": \"这个图片里有什么？\"}], ensure_ascii=False\n            ),\n        }\n    ]\n)\nv[0].output\n```\n\n还可以这么写：\n\n```python\nimport byzerllm\nimport json\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\nimage = byzerllm.Image.load_image_from_path(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv = vl_llm.chat_oai(\n    conversations=[\n        {\n            \"role\": \"user\",\n            \"content\": json.dumps(\n                [\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\"url\": image, \"detail\": \"high\"},\n                    },\n                    {\"text\": \"这个图片里有什么？\", \"type\": \"text\"},\n                ],\n                ensure_ascii=False,\n            ),\n        }\n    ]\n)\nv[0].output\n```\n\n### 语音合成\n\n这里以 openai 的 tts 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=tts-1 \\\n--model openai_tts\n```\n\n此外，byzerllm 支持 azure,火山引擎等 tts 语音合成引擎。\n\n接着你可以这么用：\n\n```python\nimport byzerllm\nimport base64\nimport json\n\nllm = byzerllm.ByzerLLM.from_default_model(\"openai_tts\")\n\n\nt = llm.chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\": json.dumps({\n        \"input\":\"hello, open_tts\",\n        \"voice\": \"alloy\",\n        \"response_format\": \"mp3\"\n    },ensure_ascii=False)\n}])\n\nwith open(\"voice.mp3\",\"wb\") as f:\n    f.write(base64.b64decode(t[0].output))\n```\n\ntts 模型生成没有prompt函数可以用，你需要直接使用 chat_oai。\n\n\n### 语音识别\n\n这里以 openai 的 whisper-1 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--worker_concurrency 10 \\\n--infer_params saas.model=whisper-1 saas.api_key=${MODEL_OPENAI_TOKEN} \\\n--model speech_to_text\n```\n\n语音识别的使用方式和图生文类似，我们可以直接在 prompt 函数体里带上音频文件。\n\n```python\nimport byzerllm\nimport json\nimport base64\nfrom byzerllm.types import AudioPath\n\nllm = byzerllm.ByzerLLM.from_default_model(\"speech_to_text\")\n\naudio_file = \"/Users/allwefantasy/videos/output_audio.mp3\"\n\n@byzerllm.prompt(llm=llm)\ndef audio_to_text(audio_file: AudioPath):\n    \"\"\"\n    {{ audio_file }}\n    \"\"\"\n\nv = audio_to_text(AudioPath(value=audio_file))\njson.loads(v)\n```\n输出的数据格式略微复杂：\n\n```\n{'text': 'In the last chapter, you and I started to step through the internal workings of a transformer. This is one of the key pieces of technology inside large language models, and a lot of other tools in the modern wave of AI.',\n 'task': 'transcribe',\n 'language': 'english',\n 'duration': 10.0,\n 'segments': [{'id': 0,\n   'seek': 0,\n   'start': 0.0,\n   'end': 4.78000020980835,\n   'text': ' In the last chapter, you and I started to step through the internal workings of a transformer.',\n   'tokens': [50364,\n    682,\n    264,\n    1036,\n    7187,\n    11,\n    291,\n    293,\n    286,\n    1409,\n    281,\n    1823,\n    807,\n    264,\n    6920,\n    589,\n    1109,\n    295,\n    257,\n    31782,\n    13,\n    50586],\n   'temperature': 0.0,\n   'avg_logprob': -0.28872039914131165,\n   'compression_ratio': 1.4220778942108154,\n   'no_speech_prob': 0.016033057123422623},\n  ....\n  {'id': 2,\n   'seek': 0,\n   'start': 8.579999923706055,\n   'end': 9.979999542236328,\n   'text': ' and a lot of other tools in the modern wave of AI.',\n   'tokens': [50759,\n    293,\n    257,\n    688,\n    295,\n    661,\n    3873,\n    294,\n    264,\n    4363,\n    5772,\n    295,\n    7318,\n    13,\n    50867],\n   'temperature': 0.0,\n   'avg_logprob': -0.28872039914131165,\n   'compression_ratio': 1.4220778942108154,\n   'no_speech_prob': 0.016033057123422623}],\n 'words': [{'word': 'In', 'start': 0.0, 'end': 0.18000000715255737},\n  {'word': 'the', 'start': 0.18000000715255737, 'end': 0.23999999463558197},\n  {'word': 'last', 'start': 0.23999999463558197, 'end': 0.5400000214576721},\n  {'word': 'chapter', 'start': 0.5400000214576721, 'end': 0.800000011920929},\n  ....\n  {'word': 'AI', 'start': 9.920000076293945, 'end': 9.979999542236328}]}\n```\n\n会输出每一句话以及每一个字所在的起始时间和截止时间。你可以根据需要来使用。\n\n\n### 文生图\n\n文生图和语音合成类似，首先要启动合适的模型,以openai 的 dall-e-3 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=dall-e-3 \\\n--model openai_image_gen\n```\n\n启动模型后，只需要记住几个模板参数即可使用，这里直接使用 chat_oai 方法来使用：\n\n```python\n\nimport byzerllm\nimport json\nimport base64\n\nllm = byzerllm.ByzerLLM.from_default_model(\"openai_image_gen\")\nt = llm.chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\": json.dumps({\n        \"input\":\"a white siamese cat\",\n        \"size\": \"1024x1024\",\n        \"quality\": \"standard\"\n    },ensure_ascii=False)\n}])\n\nwith open(\"output1.jpg\",\"wb\") as f:\n    f.write(base64.b64decode(t[0].output))\n\n\nimport matplotlib.pyplot as plt\n\nimage_path = \"output1.jpg\"\nimage = plt.imread(image_path)\n\nplt.imshow(image)\nplt.axis('off')\nplt.show()\n```\n\n## Prompt 函数的流式输出\n\nbyzerllm 底层支持流式输出，非 prompt 函数的用法是这样的：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\n\nv = llm.stream_chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\":\"讲一个100字的故事\"\n}])\n\nfor s  in v:\n    print(s[0], end=\"\")\n```\n\n如果你像用 prompt 函数，可以这么用：\n\n\n```python\nimport byzerllm\nimport json\nimport base64\nfrom typing import Generator\n\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\n\n@byzerllm.prompt()\ndef tell_story() -> Generator[str, None, None]:\n    '''\n    给我讲一个一百多字的故事\n    '''\n\nv = tell_story.with_llm(llm).run()    \nfor i in v:\n    print(i, end=\"\")\n```\n\n可以看到，和普通的 prompt 函数的区别在于，返回值是一个生成器，然后你可以通过 for 循环来获取结果。\n\n## 向量化模型\n\nbyzerllm 支持向量化模型,你可以这样启动一个本地的模型：\n\n```bash\n!byzerllm deploy --pretrained_model_type custom/bge \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--worker_concurrency 10 \\\n--model_path /home/winubuntu/.auto-coder/storage/models/AI-ModelScope/bge-large-zh \\\n--infer_backend transformers \\\n--num_workers 1 \\\n--model emb\n```\n\n注意两个参数:\n\n1. --infer_backend transformers: 表示使用 transformers 作为推理后端。\n2. --model_path: 表示模型的路径。\n\n也可以启动一个 SaaS 的emb模型,比如 qwen 的 emb 模型：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/qianwen \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 2 \\\n--infer_params saas.api_key=${MODEL_QIANWEN_TOKEN}  saas.model=text-embedding-v2 \\\n--model qianwen_emb\n```\n\n或者 openai 的 emb 模型：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--worker_concurrency 10 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=text-embedding-3-small \\\n--model gpt_emb\n```\nSaaS 模型无需配置 `--infer_backend` 参数。\n\n无论是本地模型还是 SaaS 模型，我们都可以这样使用：\n\n```python\nimport byzerllm\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\nllm.setup_default_emb_model_name(\"emb\")\nllm.emb_query(\"你好\")\n```\n\n如果你配置 byzerllm 中的 Storage 使用，比如你这样启动了存储：\n\n```bash\nbyzerllm storage start --enable_emb\n```\n\n那么需要这么使用：\n\n```python\nfrom byzerllm.apps.byzer_storage.simple_api import ByzerStorage, DataType, FieldOption,SortOption\nstorage = ByzerStorage(\"byzerai_store\", \"memory\", \"memory\")\nstorage.emb(\"你好\")\n```\n\n## 一些辅助工具\n\n当调用 prompt 函数返回字符串的时候，如果想从里面抽取代码，可以使用如下方式：\n\n```python\nfrom byzerllm.utils.client import code_utils\ntext_with_markdown = '''\n```shell\nls -l\n```\n'''\ncode_blocks = code_utils.extract_code(text_with_markdown)\nfor code_block in code_blocks:\n    if code_block[0] == \"shell\":\n        print(code_block[1])\n##output: ls -l        \n```\n\nTagExtractor 工具用于对任意 `<_tag_></_tag_>` 标签对的抽取，下面是一个使用示例：\n\n```python\nfrom byzerllm.apps.utils import TagExtractor\n\nextractor = TagExtractor('''\n大家好\n<_image_>data:image/jpeg;base64,xxxxxx</_image_>\n                         大家好\n<_image_>data:image/jpeg;base64,xxxxxx2</_image_>\n''')\n\nv = extractor.extract()\nprint(v.content[0].content)\nprint(v.content[1].content)\n```\n输出为:\n\n```\ndata:image/jpeg;base64,xxxxxx\ndata:image/jpeg;base64,xxxxxx2\n```\n\n我们成功的将 <_image_></_image_> 标签对里的内容抽取出来了。\n\n\n## 注意事项\n\n1. prompt函数方法体返回只能是dict，实际的返回类型和方法签名可以不一样，但是方法体返回只能是dict。\n2. 大部分情况prompt函数体为空，如果一定要有方法体，可以返回一个空字典。\n3. 调用prompt方法的时候，如果在@byzerllm.prompt()里没有指定llm对象，那么需要在调用的时候通过with_llm方法指定llm对象。\n\n===========================\n\n# Byzer Storage: 最易用的AI存储引擎\n\nByzer Storage是一个为AI应用设计的高性能存储引擎,它提供了简单易用的API,支持向量搜索、全文检索以及结构化查询。本文将详细介绍Byzer Storage的使用方法和主要特性。\n\n## 0. 安装和启动\n\n```bash\npip install byzerllm\nbyzerllm storage start\n```\n\nThat's it! Byzer Storage已经安装并启动成功,现在我们可以开始使用它了。默认会启动一个 byzerai_store 的集群。\n\n注意，如果你是这样启动的：\n\n```bash\nbyzerllm storage start --enable_emb\n```\n\n那么会自动启动一个 emb 模型，名字就叫 emb， ByzerStorage 会自动使用该模型，无需做任何其他配置。\n\n## 1. 初始化\n\n创建一个 ByzerStorage 对象，链接 byzerai_store 集群，并且指定数据库和表名（可以不存在）。\n\n```python\nfrom byzerllm.apps.byzer_storage.simple_api import ByzerStorage, DataType, FieldOption, SortOption\n\nstorage = ByzerStorage(\"byzerai_store\", \"my_database1\", \"my_table4s\")\n```\n\n\n## 2. 创建库表（可选）\n\nByzer Storage使用Schema来定义数据结构。我们可以使用SchemaBuilder来构建Schema:\n\n```python\n_ = (\n    storage.schema_builder()\n    .add_field(\"_id\", DataType.STRING)\n    .add_field(\"name\", DataType.STRING)\n    .add_field(\"content\", DataType.STRING, [FieldOption.ANALYZE])\n    .add_field(\"raw_content\", DataType.STRING, [FieldOption.NO_INDEX])    \n    .add_array_field(\"summary\", DataType.FLOAT)    \n    .add_field(\"created_time\", DataType.LONG, [FieldOption.SORT])    \n    .execute()\n)\n```\n\n这个Schema定义了以下字段:\n- `_id`: 字符串类型的主键\n- `name`: 字符串类型,可用于过滤条件\n- `content`: 字符串类型,带有ANALYZE选项,用于全文搜索\n- `raw_content`: 字符串类型,带有NO_INDEX选项,不会被索引\n- `summary`: 浮点数组类型,用于存储向量\n- `created_time`: 长整型,带有SORT选项,可用于排序\n\n需要注意的是：\n\n1. 如果一个字段带有ANALYZE选项,则该字段会被分词,并且可以用于全文搜索，但是就无法返回原始的文本了。所以你需要添加一个新字段专门用来存储原文，比如在我们这个例子里，我们新增了 raw_content 字段，并且显示的指定了 NO_INDEX 选项，这样就不会被索引，也就不会被分词，可以后续被检索后用来获取原始文本。\n2. 对于需要作为向量检索和存储的字段，需要指定为数组类型，比如我们这个例子里的 summary 字段。\n3. 如果你需要拿到向量字段的原始文本，那么你也需要添加一个新字段专门用来存储原文，就像我们这个例子里的 raw_content 字段一样。\n\n\n## 3. 写入数据\n\n准备数据并使用WriteBuilder写入Storage:\n\n```python\ndata = [\n    {\"_id\": \"1\", \"name\": \"Hello\", \"content\": \"Hello, world!\", \"raw_content\": \"Hello, world!\", \"summary\": \"hello world\", \"created_time\": 1612137600},\n    {\"_id\": \"2\", \"name\": \"Byzer\", \"content\": \"Byzer, world!\", \"raw_content\": \"Byzer, world!\", \"summary\": \"byzer\", \"created_time\": 1612137601},\n    {\"_id\": \"3\", \"name\": \"AI\", \"content\": \"AI, world!\", \"raw_content\": \"AI, world!\", \"summary\": \"AI\", \"created_time\": 16121376002},\n    {\"_id\": \"4\", \"name\": \"ByzerAI\", \"content\": \"ByzerAI, world!\", \"raw_content\": \"ByzerAI, world!\", \"summary\": \"ByzerAi\", \"created_time\": 16121376003},\n]\n\nstorage.write_builder().add_items(data, vector_fields=[\"summary\"], search_fields=[\"content\"]).execute()\nstorage.commit()\n```\n\n这里我们使用`add_items`方法批量添加数据,并指定了`summary`为向量字段,`content`为搜索字段。最后调用`commit()`来确保数据被持久化。\n\n\n从上面我们要写入到 byzer storage 的数据我们可以看到如下几个特点：\n\n1. 需要包含我们之前定义的 Schema中罗列的所有的字段，同时需要指定哪些是向量字段，哪些是检索字段。\n2. 向量和检索不能是同一个字段。\n3. 对于向量，检索字段，我们给到 write_builder 的都是文本，ByzerStorage 会根据 Schema 的定义自动将其转换为向量，检索字段需要的格式。\n\n## 4. 查询数据\n\nByzer Storage支持多种查询方式,包括向量搜索、全文检索、过滤和排序。\n\n### 4.1 向量搜索 + 全文检索\n\n```python\nquery = storage.query_builder()\nquery.set_vector_query(\"ByzerAI\", fields=[\"summary\"])\nresults = query.set_search_query(\"Hello\", fields=[\"content\"]).execute()\nprint(results)\n```\n\n这个查询结合了向量搜索和全文检索,它会在`summary`字段中搜索与\"ByzerAI\"相似的向量,同时在`content`字段中搜索包含\"Hello\"的文档。\n\n### 4.2 过滤 + 向量搜索 + 全文检索\n\n```python\nquery = storage.query_builder()\nquery.and_filter().add_condition(\"name\", \"AI\").build()\nquery.set_vector_query(\"ByzerAI\", fields=\"summary\")\nresults = query.set_search_query(\"Hello\", fields=[\"content\"]).execute()\nprint(results)\n```\n\n这个查询首先过滤`name`字段等于\"AI\"的文档,然后在结果中进行向量搜索和全文检索。\n\n### 4.3 过滤 + 排序\n\n```python\nquery = storage.query_builder()\nquery.and_filter().add_condition(\"name\", \"AI\").build().sort(\"created_time\", SortOption.DESC)\nresults = query.execute()\nprint(results)\n```\n\n这个查询过滤`name`字段等于\"AI\"的文档,然后按`created_time`字段降序排序。\n\n## 5. 删除数据\n\n### 5.1 根据ID删除\n\n```python\nstorage.delete_by_ids([\"3\"])\n\nquery = storage.query_builder()\nquery.and_filter().add_condition(\"name\", \"AI\").build()\nresults = query.execute()\nprint(results)\n```\n\n这里我们删除了ID为\"3\"的文档,然后查询验证删除结果。\n\n### 5.2 删除整个表\n\n```python\nstorage.drop()\n```\n\n这个操作会删除整个表及其所有数据,请谨慎使用。\n\n## 结论\n\nByzer Storage提供了一套简洁而强大的API,能够轻松实现向量搜索、全文检索、结构化查询等功能。它的设计非常适合AI应用场景,可以有效地存储和检索各种类型的数据。通过本文的介绍,相信读者已经对Byzer Storage有了基本的了解,并能够开始在自己的项目中使用这个强大的存储引擎。\n\n\n     \n\n下面是用户的需求：\n\nmake 添加一个启动 deepseek的命令: byzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--worker_concurrency 1000 \\\n--num_workers 1 \\\n--infer_params saas.base_url=\"https://api.deepseek.com/v1\" saas.api_key=${MODEL_DEEPSEEK_TOKEN} saas.model=deepseek-chat \\\n--model deepseek_chat  然后同步更新 @README.md 文档\n"}, {"role": "assistant", "content": "好的，我会按照您的要求添加一个启动 deepseek 的命令到 Makefile 中，并更新 README.md 文档。\n\n首先，让我们修改 Makefile：\n\n```makefile\n##File: /Users/allwefantasy/projects/super_analysis/Makefile\n<<<<<<< SEARCH\n.PHONY: all clean build wheel\n\nall: clean build wheel\n=======\n.PHONY: all clean build wheel deploy_deepseek\n\nall: clean build wheel\n\ndeploy_deepseek:\n\tbyzerllm deploy --pretrained_model_type saas/openai \\\n\t--cpus_per_worker 0.001 \\\n\t--gpus_per_worker 0 \\\n\t--worker_concurrency 1000 \\\n\t--num_workers 1 \\\n\t--infer_params saas.base_url=\"https://api.deepseek.com/v1\" saas.api_key=${MODEL_DEEPSEEK_TOKEN} saas.model=deepseek-chat \\\n\t--model deepseek_chat\n>>>>>>> REPLACE\n```\n\n现在，让我们更新 README.md 文档：\n\n```markdown\n##File: /Users/allwefantasy/projects/super_analysis/README.md\n<<<<<<< SEARCH\n# 启动分析服务\n\n整个 super-analysis 服务分为四个部分：\n\n1. byzer-sql 分布式执行引擎 \n2. schema 文档知识库\n4. context 文档知识库\n5. super-analysis 自身\n\n在启动 super-analysis 之前，你需要启动 schema 文档知识库和 context 文档知识库以及 byzer-sql 引擎。\n\n\n在你的电脑上参考 [这个文档](./docs/4.3.1%20安装与配置%20Byzer-SQL.pdf) 完成部署.\n\n> 启动的时候需要在安装有 super-analysis 的conda环境中启动。\n\n当 byzer-sql 部署完成后，注册账号为 `hello`, 然后进入到 byzer-sql 的控制台中执行以下脚本：\n\n```sql\n!byzerllm setup single;\n\nrun command as LLM.`` where \naction=\"infer\"\nand reconnect=\"true\"\nand pretrainedModelType=\"saas/openai\"\nand udfName=\"deepseek_chat\";\n```\n\n现在可以按如下方式做一些数据预处理或者启动相关服务：\n\n```bash\n## 将电影数据集的csv文件转换为schema文档\nsuper-analysis.convert --data_dir /Users/allwefantasy/data/movice --doc_dir /Users/allwefantasy/data/movice/schemas/\n\n# 启动schema文档知识库\nauto-coder doc serve \\\n--model deepseek_chat --index_filter_workers 100 \\\n--tokenizer_path /Users/allwefantasy/Downloads/tokenizer.json \\\n--doc_dir /Users/allwefantasy/data/movice/schemas/ \\\n--port 8001\n\n\n# 下载 Byzer-SQL 专门为大模型优化的文档\ngit clone https://github.com/allwefantasy/llm_friendly_packages\n\n# 启动文档知识库\nauto-coder doc serve \\\n--model deepseek_chat --index_filter_workers 100 \\\n--tokenizer_path /Users/allwefantasy/Downloads/tokenizer.json \\\n--doc_dir  /Users/allwefantasy/projects/llm_friendly_packages/github.com/allwefantasy \\\n--port 8002\n\n# 启动兼容 OpenAI Server 的分析服务\nsuper-analysis.serve --served-model-name deepseek_chat --port 8000 \\\n--schema-rag-base-url http://127.0.0.1:8001/v1 \\\n--context-rag-base-url http://127.0.0.1:8002/v1 \\\n--byzer-sql-url http://127.0.0.1:9003/run/script\n```\n\n现在你可以用 OpenAI SDK 去使用了。 具体测试和接口使用参考 [openai_local_api.ipynb](./notebooks/openai_local_api.ipynb)\n\n## 非 OpenAI 兼容接口\n=======\n# 启动分析服务\n\n整个 super-analysis 服务分为四个部分：\n\n1. byzer-sql 分布式执行引擎 \n2. schema 文档知识库\n3. context 文档知识库\n4. super-analysis 自身\n\n在启动 super-analysis 之前，你需要启动 schema 文档知识库和 context 文档知识库以及 byzer-sql 引擎。\n\n## 部署 Byzer-SQL\n\n在你的电脑上参考 [这个文档](./docs/4.3.1%20安装与配置%20Byzer-SQL.pdf) 完成部署.\n\n> 启动的时候需要在安装有 super-analysis 的conda环境中启动。\n\n当 byzer-sql 部署完成后，注册账号为 `hello`, 然后进入到 byzer-sql 的控制台中执行以下脚本：\n\n```sql\n!byzerllm setup single;\n\nrun command as LLM.`` where \naction=\"infer\"\nand reconnect=\"true\"\nand pretrainedModelType=\"saas/openai\"\nand udfName=\"deepseek_chat\";\n```\n\n## 部署 Deepseek 模型\n\n在启动其他服务之前，我们需要先部署 Deepseek 模型。你可以使用以下命令来部署：\n\n```bash\nmake deploy_deepseek\n```\n\n或者直接运行以下命令：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--worker_concurrency 1000 \\\n--num_workers 1 \\\n--infer_params saas.base_url=\"https://api.deepseek.com/v1\" saas.api_key=${MODEL_DEEPSEEK_TOKEN} saas.model=deepseek-chat \\\n--model deepseek_chat\n```\n\n注意：请确保在运行此命令之前，已经设置了环境变量 `MODEL_DEEPSEEK_TOKEN`。\n\n## 启动其他服务\n\n现在可以按如下方式做一些数据预处理或者启动相关服务：\n\n```bash\n## 将电影数据集的csv文件转换为schema文档\nsuper-analysis.convert --data_dir /Users/allwefantasy/data/movice --doc_dir /Users/allwefantasy/data/movice/schemas/\n\n# 启动schema文档知识库\nauto-coder doc serve \\\n--model deepseek_chat --index_filter_workers 100 \\\n--tokenizer_path /Users/allwefantasy/Downloads/tokenizer.json \\\n--doc_dir /Users/allwefantasy/data/movice/schemas/ \\\n--port 8001\n\n# 下载 Byzer-SQL 专门为大模型优化的文档\ngit clone https://github.com/allwefantasy/llm_friendly_packages\n\n# 启动文档知识库\nauto-coder doc serve \\\n--model deepseek_chat --index_filter_workers 100 \\\n--tokenizer_path /Users/allwefantasy/Downloads/tokenizer.json \\\n--doc_dir  /Users/allwefantasy/projects/llm_friendly_packages/github.com/allwefantasy \\\n--port 8002\n\n# 启动兼容 OpenAI Server 的分析服务\nsuper-analysis.serve --served-model-name deepseek_chat --port 8000 \\\n--schema-rag-base-url http://127.0.0.1:8001/v1 \\\n--context-rag-base-url http://127.0.0.1:8002/v1 \\\n--byzer-sql-url http://127.0.0.1:9003/run/script\n```\n\n现在你可以用 OpenAI SDK 去使用了。 具体测试和接口使用参考 [openai_local_api.ipynb](./notebooks/openai_local_api.ipynb)\n\n## 非 OpenAI 兼容接口\n>>>>>>> REPLACE\n```\n\n这些更改添加了一个新的 `deploy_deepseek` 命令到 Makefile 中，并更新了 README.md 文档，添加了部署 Deepseek 模型的说明。现在用户可以使用 `make deploy_deepseek` 命令来部署 Deepseek 模型，或者直接运行相应的 `byzerllm deploy` 命令。"}], "model": "human", "yaml_file": "actions/142_chat_action.yml"}
{"instruction": "根据README.md  更新 @deploy.md ,在 @deploy.md 中主要介绍如何部署auto-coder, byzer-sql 引擎,以及安装wheel 包,启动和配置相关服务.使用emoj 和分隔符", "conversations": [{"role": "user", "content": "\n如果你需要生成代码，对于每个需要更改的文件,你需要按 *SEARCH/REPLACE block* 的格式进行生成。\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The opening fence and code language, eg: ```python\n2. The file path alone on a line, starting with \"##File:\" and verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n*SEARCH/REPLACE* blocks will replace *all* matching occurrences.\nInclude enough lines to make the SEARCH blocks unique.\n\nInclude *ALL* the code being searched and replaced!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n下面我们来看一个例子：\n\n当前项目目录结构：\n1. 项目根目录： /tmp/projects/mathweb\n2. 项目子目录/文件列表(类似tree 命令输出)\nflask/\n    app.py\n    templates/\n        index.html\n    static/\n        style.css\n\n用户需求： Change get_factorial() to use math.factorial\n\n回答： To make this change we need to modify `/tmp/projects/mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n用户需求： Refactor hello() into its own file.\n\n回答：To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/hello.py\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/main.py\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n现在让我们开始一个新的任务:\n\n当前项目目录结构：\n1. 项目根目录： /Users/allwefantasy/projects/super_analysis\n2. 项目子目录/文件列表(类似tree 命令输出)：\nsetup.py\n.auto-coder/\n    libs/\n        llm_friendly_packages/\n            src/\n                example1/\n                    main.py\nsrc/\n    superanalysis/\n        serve/\n            rest.py\n            convert.py\n            openai_serve.py\n            __init__.py\n            llm_wrapper.py\n            api_server.py\n            cli.py\n        sql/\n            __init__.py\n            types.py\n            generator.py\n下面是一些文件路径以及每个文件对应的源码：\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/types.py\nfrom pydantic import BaseModel\n\nclass RAGConfig(BaseModel):\n    schema_rag_base_url: str = \"\"\n    schema_rag_api_key: str = \"xxxx\"\n\n    context_rag_base_url: str = \"\"\n    context_rag_api_key: str = \"xxxx\"\n\nclass ByzerSQLConfig(BaseModel):\n    byzer_sql_url: str = \"\"\n    data_dir: str = \"\"\n    owner: str = \"\"\n\nclass ModelConfig(BaseModel):\n    llm_model: str = \"\"\n    gen_sql_llm_model: str = \"\"\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/cli.py\nimport argparse\nimport byzerllm\nfrom byzerllm.utils.client import ByzerLLM\nfrom superanalysis.serve.api_server import serve, ServerArgs\nfrom superanalysis.sql.types import RAGConfig, ByzerSQLConfig, ModelConfig\nfrom superanalysis.sql.generator import SQLGenerator\nfrom superanalysis.serve.llm_wrapper import LLWrapper\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"ByzerLLM API Server\")\n    parser.add_argument(\"--ray-address\", type=str, default=\"auto\", help=\"ray address\")\n    parser.add_argument(\n        \"--host\", type=str, default=\"0.0.0.0\", help=\"Host to bind the server to\"\n    )\n    parser.add_argument(\n        \"--port\", type=int, default=8000, help=\"Port to bind the server to\"\n    )\n    parser.add_argument(\n        \"--uvicorn-log-level\", type=str, default=\"info\", help=\"Uvicorn log level\"\n    )\n    parser.add_argument(\n        \"--allow-credentials\", action=\"store_true\", help=\"Allow credentials\"\n    )\n    parser.add_argument(\n        \"--allowed-origins\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed origins\"\n    )\n    parser.add_argument(\n        \"--allowed-methods\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed methods\"\n    )\n    parser.add_argument(\n        \"--allowed-headers\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed headers\"\n    )\n    parser.add_argument(\n        \"--api-key\", type=str, default=\"\", help=\"API key for authentication\"\n    )\n    parser.add_argument(\n        \"--served-model-name\",\n        type=str,\n        required=True,\n        help=\"Name of the model to be served\",\n    )\n    parser.add_argument(\n        \"--prompt-template\", type=str, default=\"\", help=\"Prompt template\"\n    )\n    parser.add_argument(\n        \"--response-role\", type=str, default=\"assistant\", help=\"Response role\"\n    )\n    parser.add_argument(\"--ssl-keyfile\", type=str, default=\"\", help=\"SSL key file\")\n    parser.add_argument(\n        \"--ssl-certfile\", type=str, default=\"\", help=\"SSL certificate file\"\n    )\n\n    # Add new arguments for RAGConfig\n    parser.add_argument(\n        \"--schema-rag-base-url\", type=str, default=\"\", help=\"RAG base URL\"\n    )\n    parser.add_argument(\n        \"--schema-rag-api-key\", type=str, default=\"xxxx\", help=\"RAG API key\"\n    )\n\n    parser.add_argument(\n        \"--context-rag-base-url\", type=str, default=\"\", help=\"RAG base URL\"\n    )\n    parser.add_argument(\n        \"--context-rag-api-key\", type=str, default=\"xxxx\", help=\"RAG API key\"\n    )\n\n    # Add new argument for ByzerSQLConfig\n    parser.add_argument(\"--byzer-sql-url\", type=str, default=\"\", help=\"Byzer SQL URL\")\n    parser.add_argument(\"--data-dir\", type=str, default=\"\", help=\"Data directory\")\n    parser.add_argument(\"--owner\", type=str, default=\"hello\", help=\"Owner of the Byzer SQL session\")\n    parser.add_argument(\n        \"--llm-model\", type=str, default=\"\", help=\"LLM model for general use\"\n    )\n    parser.add_argument(\n        \"--gen-sql-llm-model\", type=str, default=\"\", help=\"LLM model for SQL generation\"\n    )\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile,\n    )\n\n    # Create RAGConfig instance\n    rag_config = RAGConfig(\n        schema_rag_base_url=args.schema_rag_base_url,\n        schema_rag_api_key=args.schema_rag_api_key,\n        context_rag_base_url=args.context_rag_base_url,\n        context_rag_api_key=args.context_rag_api_key,\n    )\n\n    # Create ByzerSQLConfig instance\n    byzer_sql_config = ByzerSQLConfig(\n        byzer_sql_url=args.byzer_sql_url, data_dir=args.data_dir, owner=args.owner\n    )\n\n    model_config = ModelConfig(\n        llm_model=args.llm_model or args.served_model_name,\n        gen_sql_llm_model=args.gen_sql_llm_model or args.served_model_name,\n    )\n\n    byzerllm.connect_cluster(address=args.ray_address)\n    llm = ByzerLLM()\n    llm.setup_default_model_name(args.served_model_name)\n    llm = byzerllm.ByzerLLM()\n    if not server_args.served_model_name:\n        raise ValueError(\"served_model_name is required\")\n    llm.setup_default_model_name(server_args.served_model_name)\n\n    print(\"RAG Config: \", rag_config)\n    print(\"Model Config: \", model_config)\n    rag = SQLGenerator(\n        llm=llm,\n        rag_config=rag_config,\n        byzer_sql_config=byzer_sql_config,\n        model_config=model_config,\n    )\n    llm_wrapper = LLWrapper(llm=llm, entry=rag)\n    serve(llm_wrapper, server_args)\n\n\nif __name__ == \"__main__\":\n    main()\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/__init__.py\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/__init__.py\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/llm_wrapper.py\nfrom typing import Any, Dict, List, Optional, Union,Callable\nfrom byzerllm.utils.client.types import (    \n    LLMFunctionCallResponse,\n    LLMClassResponse,LLMResponse    \n)\nimport pydantic\nfrom byzerllm import ByzerLLM\nfrom byzerllm.utils.client import LLMResponse\nfrom byzerllm.utils.types import SingleOutputMeta\nfrom byzerllm.utils.langutil import asyncfy_with_semaphore\nfrom superanalysis.sql.generator import SQLGenerator\nfrom loguru import logger\n\nclass LLWrapper:\n\n    def __init__(self,llm:ByzerLLM,entry:SQLGenerator):\n        self.entry = entry\n        self.llm = llm\n        \n\n    def chat_oai(self,\n                 conversations,\n                 tools:List[Union[Callable,str]]=[], \n                 tool_choice:Optional[Union[Callable,str]]=None,\n                 execute_tool:bool=False,  \n                 impl_func:Optional[Callable]=None,\n                 execute_impl_func:bool=False,\n                 impl_func_params:Optional[Dict[str,Any]]=None,\n                 func_params:Optional[Dict[str,Any]]=None,\n                 response_class:Optional[Union[pydantic.BaseModel,str]] = None, \n                 response_after_chat:Optional[Union[pydantic.BaseModel,str]] = False,\n                 enable_default_sys_message:bool=True,                 \n                 model:Optional[str] = None,\n                 role_mapping=None,llm_config:Dict[str,Any]={}\n                 )->Union[List[LLMResponse],List[LLMFunctionCallResponse],List[LLMClassResponse]]: \n        res,contexts = self.entry.stream_chat_oai(conversations)\n        s = \"\".join(res)        \n        return [LLMResponse(output=s,metadata={},input=\"\")]\n\n    def stream_chat_oai(self,conversations, \n                        model:Optional[str]=None, \n                        role_mapping=None,\n                        delta_mode=False,\n                        llm_config:Dict[str,Any]={}): \n        res,contexts = self.entry.stream_chat_oai(conversations)        \n        for t in res:                        \n            yield (t,SingleOutputMeta(0,0))\n\n    async def async_stream_chat_oai(self,conversations, \n                        model:Optional[str]=None, \n                        role_mapping=None,\n                        delta_mode=False,\n                        llm_config:Dict[str,Any]={}): \n        res,contexts = self.rag.stream_chat_oai(conversations)                \n        for t in res:                                    \n            yield (t,SingleOutputMeta(0,0))\n                         \n\n    def __getattr__(self, name):        \n        return getattr(self.llm, name)\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/openai_serve.py\nimport argparse\nfrom service_server.api_server import ServerArgs, serve\nfrom service_server.llm_wrapper import LLWrapper\nfrom service_server.summary_rag import SummaryRAG\nimport byzerllm\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"OpenAI Serve\")\n    parser.add_argument(\n        \"--model\",\n        default=\"\",\n    )\n    parser.add_argument(\"--emb_model\", default=\"\")\n    parser.add_argument(\n        \"--ray_address\",\n        default=\"auto\",\n    )\n    parser.add_argument(\"--source_dir\", default=\".\", help=\"\")\n    parser.add_argument(\"--host\", default=\"\", help=\"\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"\")\n    parser.add_argument(\"--uvicorn_log_level\", default=\"info\", help=\"\")\n    parser.add_argument(\"--allow_credentials\", action=\"store_true\", help=\"\")\n    parser.add_argument(\"--allowed_origins\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--allowed_methods\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--allowed_headers\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--api_key\", default=\"\", help=\"\")\n    parser.add_argument(\"--served_model_name\", default=\"\", help=\"\")\n    parser.add_argument(\"--prompt_template\", default=\"\", help=\"\")\n    parser.add_argument(\"--ssl_keyfile\", default=\"\", help=\"\")\n    parser.add_argument(\"--ssl_certfile\", default=\"\", help=\"\")\n    parser.add_argument(\"--response_role\", default=\"assistant\", help=\"\")\n    parser.add_argument(\n        \"--collections\", default=\"\", help=\"Collection name for indexing\"\n    )\n    parser.add_argument(\n        \"--base_dir\",\n        default=\"\",\n        help=\"Path where the processed text embeddings were stored\",\n    )\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name or args.model,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile,\n    )\n    byzerllm.connect_cluster(address=args.ray_address)\n    llm = byzerllm.ByzerLLM()\n    if not server_args.served_model_name:\n        raise ValueError(\"served_model_name is required\")\n    llm.setup_default_model_name(server_args.served_model_name)\n    rag = SummaryRAG(llm=llm)    \n    llm_wrapper = LLWrapper(llm=llm, rag=rag)\n    serve(llm=llm_wrapper, args=server_args)\n\nif __name__ == \"__main__\":\n    main()\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/api_server.py\nimport os\nimport time\nimport uvicorn\nfrom fastapi import Request\nfrom fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import JSONResponse, StreamingResponse, Response\n\nfrom byzerllm.log import init_logger\nfrom byzerllm.utils import random_uuid\nfrom byzerllm.version import __version__ as version\nfrom byzerllm.utils.client import ByzerLLM, LLMRequest\nfrom byzerllm.utils.client.entrypoints.openai.serving_chat import OpenAIServingChat\nfrom byzerllm.utils.client.entrypoints.openai.serving_completion import OpenAIServingCompletion\nfrom byzerllm.utils.client.entrypoints.openai.protocol import (\n    ModelList,\n    ModelCard,\n    ModelPermission,\n    ChatCompletionRequest,\n    ErrorResponse,\n    CompletionRequest,\n    Embeddings,\n    EmbeddingsOutput,\n    EmbeddingsData,\n    EmbeddingsUsage,\n)\nfrom pydantic import BaseModel\nfrom typing import List\n\nlogger = init_logger(__name__)\n\nllm_client: ByzerLLM = None\nopenai_serving_chat: OpenAIServingChat = None\nopenai_serving_completion: OpenAIServingCompletion = None\n\nTIMEOUT_KEEP_ALIVE = 5  # seconds\n# timeout in 10 minutes. Streaming can take longer than 3 min\nTIMEOUT = float(os.environ.get(\"BYZERLLM_APISERVER_HTTP_TIMEOUT\", 600))\n\nrouter_app = FastAPI()\n\n\n@router_app.get(\"/health\")\nasync def health() -> Response:\n    \"\"\"Health check.\"\"\"\n    return Response(status_code=200)\n\n\n@router_app.get(\"/v1/models\")\nasync def show_available_models():\n    models = await openai_serving_chat.show_available_models()\n    return JSONResponse(content=models.model_dump())\n\n\n@router_app.get(\"/version\")\nasync def show_version():\n    return JSONResponse(content={\"version\": version})\n\n\n@router_app.get(\"/v1/models\", response_model=ModelList)\nasync def models() -> ModelList:\n    \"\"\"Show available models. Right now we only have one model.\"\"\"\n    model_cards = [\n        ModelCard(\n            id=\"\",\n            root=\"\",\n            permission=[ModelPermission()]\n        )\n    ]\n    return ModelList(data=model_cards)\n\n\n@router_app.post(\"/v1/completions\")\nasync def create_completion(\n        body: CompletionRequest,\n        request: Request\n):\n    generator = await openai_serving_completion.create_completion(body, request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            status_code=generator.code\n        )\n    if body.stream:\n        return StreamingResponse(\n            content=generator,\n            media_type=\"text/event-stream\"\n        )\n    else:\n        return JSONResponse(content=generator.model_dump())\n\n\n@router_app.post(\"/v1/chat/completions\")\nasync def create_chat_completion(\n        body: ChatCompletionRequest,\n        request: Request,\n):\n    \"\"\"Completion API similar to OpenAI's API.\n\n    See  https://platform.openai.com/docs/api-reference/chat/create\n    for the API specification. This API mimics the OpenAI ChatCompletion API.\n\n    NOTE: Currently we do not support the following features:\n        - function_call (Users should implement this by themselves)\n        - logit_bias (to be supported by vLLM engine)\n    \"\"\"\n    # async with async_timeout.timeout(TIMEOUT):\n\n    generator = await openai_serving_chat.create_chat_completion(body, request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            status_code=generator.code\n        )\n    if body.stream:\n        return StreamingResponse(\n            content=generator,\n            media_type=\"text/event-stream\"\n        )\n    else:\n        return JSONResponse(content=generator.model_dump())\n\n\n@router_app.post(\"/v1/embeddings\")\nasync def embed(body: Embeddings):\n    \"\"\"Given a prompt, the model will return one embedding.\n\n    Returns:\n        A response object with an embedding.\n    \"\"\"\n    embedding_id = f\"embed-{random_uuid()}\"\n\n    results_list = llm_client.emb(body.model, request=LLMRequest(instruction=body.input))\n    tokens = 0\n\n    return EmbeddingsOutput(\n        data=[\n            EmbeddingsData(\n                embedding=results.output,\n                index=i,\n                object=\"embedding\",\n            )\n            for i, results in enumerate(results_list)\n        ],\n        id=embedding_id,\n        object=\"list\",\n        created=int(time.time()),\n        model=body.model,\n        usage=EmbeddingsUsage(\n            prompt_tokens=tokens,\n            total_tokens=tokens,\n        ),\n    )\n\nclass ServerArgs(BaseModel):\n    host: str = None\n    port: int = 8000\n    uvicorn_log_level: str = \"info\"\n    allow_credentials: bool = False\n    allowed_origins: List[str] = [\"*\"]  \n    allowed_methods: List[str] = [\"*\"]\n    allowed_headers: List[str] = [\"*\"]\n    api_key: str = None\n    served_model_name: str = None\n    prompt_template: str = None\n    response_role: str = \"assistant\"\n    ssl_keyfile: str = None\n    ssl_certfile: str = None    \n\ndef serve(llm:ByzerLLM, args: ServerArgs):\n    \n    logger.info(f\"ByzerLLM API server version {version}\")\n    logger.info(f\"args: {args}\")\n\n    router_app.add_middleware(\n        CORSMiddleware,\n        allow_origins=args.allowed_origins,\n        allow_credentials=args.allow_credentials,\n        allow_methods=args.allowed_methods,\n        allow_headers=args.allowed_headers,\n    )\n    \n    if token := os.environ.get(\"BYZERLLM_API_KEY\") or args.api_key:\n\n        @router_app.middleware(\"http\")\n        async def authentication(request: Request, call_next):\n            if not request.url.path.startswith(\"/v1\"):\n                return await call_next(request)\n            if request.headers.get(\"Authorization\") != \"Bearer \" + token:\n                return JSONResponse(\n                    content={\"error\": \"Unauthorized\"},\n                    status_code=401\n                )\n            return await call_next(request)\n\n    # Register labels for metrics\n    # add_global_metrics_labels(model_name=engine_args.model)\n    global llm_client\n    llm_client = llm\n    \n    global openai_serving_chat\n    openai_serving_chat = OpenAIServingChat(\n        llm_client=llm_client,\n        response_role=args.response_role,\n        server_model_name=args.served_model_name,\n        prompt_template=args.prompt_template\n    )\n    global openai_serving_completion\n    openai_serving_completion = OpenAIServingCompletion(\n        llm_client=llm_client,\n        server_model_name=args.served_model_name,\n        prompt_template=args.prompt_template\n    )\n\n    uvicorn.run(\n        router_app,\n        host=args.host,\n        port=args.port,\n        log_level=args.uvicorn_log_level,\n        timeout_keep_alive=TIMEOUT_KEEP_ALIVE,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile\n    )\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/generator.py\nfrom typing import Optional, Dict, Any, List\nfrom openai import OpenAI\nimport json\nfrom .types import RAGConfig, ByzerSQLConfig, ModelConfig\nfrom byzerllm.utils.client import code_utils\nimport byzerllm\nimport requests\nfrom loguru import logger\nimport traceback\nfrom byzerllm.utils.nontext import TagExtractor\n\n\nclass SQLGenerator:\n\n    @byzerllm.prompt()\n    def gen_context(self, conversations: List[Dict[str, str]]) -> str:\n        \"\"\"\n        对话内容：\n        {% for msg in conversations %}\n        <{{ msg.role }}>: {{ msg.content }}\n        {% endfor %}\n\n        根据提供的对话内容，理解用户最后的问题，然后提取可能会用到的 Byzer-SQL 相关的知识。\n        \"\"\"\n\n    @byzerllm.prompt()\n    def gen_sql(\n        self,\n        conversations: List[Dict[str, str]],\n        context: str,\n        schema: str,\n        model: str = \"deepseek_chat\",\n    ) -> str:\n        \"\"\"\n        相关知识：\n        {{ context }}\n\n        Schema信息：\n        {{ schema }}\n\n        对话内容：\n        {% for msg in conversations %}\n        <{{ msg.role }}>: {{ msg.content }}\n        {% endfor %}\n\n        模型UDF函数：\n        {{ model }}\n\n        根据提供的相关知识以及对话内容，理解用户最后的问题，然后根据看到的schema 信息，生成 Byzer-SQL 语句。\n        Byzer-SQL 首先需要用 load 语法加载 schema 中的路径映射成表格，然后根据表格的信息生成对应的 Byzer-SQL 语句。\n\n        特别需要关注的是:\n        1. 用户的很多需求是要对表格的文本内容做自然语言处理，你需要使用 Byzer-SQL 中的大模型辅助函数来完成这个需求。\n        2. 使用 `llm_result` 函数处理 LLM 模型函数的响应,`llm_result` 函数得到的是一段文本，虽然我们要求模型函数只输出特定的文本，但考虑到大模型会有一些额外的字符输出，不会只是输出我们要求的文字，所以通过 `like` 来进行匹配。\n        3. 注意 Byzer-SQL 的语法，比如select 语句最后一定需要有 as <table_name>。\n        3. 输出的 Byzer-SQL 语句务必使用 <_byzer_sql_></_byzer_sql_> 标签包裹起来。\n        \"\"\"\n\n    @byzerllm.prompt()\n    def fetch_schema(self, context: str) -> str:\n        \"\"\"\n        下面是一些基础知识，包括名词术语等等：\n        {{ context }}\n\n        当理解对话内容的时候，需要参考上面的基础知识，然后更好的理解对应的表格schema信息是不是有用。\n        \"\"\"\n\n    def __init__(\n        self,\n        llm: byzerllm.ByzerLLM,\n        rag_config: RAGConfig,\n        byzer_sql_config: ByzerSQLConfig,\n        model_config: ModelConfig,\n    ) -> None:\n        self.llm = llm\n        self.schema_rag_client = OpenAI(\n            api_key=rag_config.schema_rag_api_key,\n            base_url=rag_config.schema_rag_base_url,\n        )\n        self.context_rag_client = OpenAI(\n            api_key=rag_config.context_rag_api_key,\n            base_url=rag_config.context_rag_base_url,\n        )\n        self.byzer_sql_url = byzer_sql_config.byzer_sql_url\n\n        self.llm_model = byzerllm.ByzerLLM()\n        self.llm_model.setup_default_model_name(model_config.llm_model)\n        self.gen_sql_llm_model = byzerllm.ByzerLLM()\n        self.gen_sql_llm_model.setup_default_model_name(model_config.gen_sql_llm_model)\n\n    def stream_chat_oai(\n        self,\n        conversations,\n        model: Optional[str] = None,\n        role_mapping=None,\n        delta_mode=False,\n        llm_config: Dict[str, Any] = {},\n    ):\n        try:\n            v, c = self._stream_chat_oai(\n                conversations=conversations,\n                model=model,\n                role_mapping=role_mapping,\n                delta_mode=delta_mode,\n                llm_config=llm_config,\n            )\n        except Exception as e:\n            error_msg = f\"Error in stream_chat_oai: {e}\\n{traceback.format_exc()}\"\n            logger.error(error_msg)\n            v, c = [str(e)], []\n\n        return v, c\n\n    def _stream_chat_oai(\n        self,\n        conversations,\n        model: Optional[str] = None,\n        role_mapping=None,\n        delta_mode=False,\n        llm_config: Dict[str, Any] = {},\n    ):\n        logger.info(f\"Starting stream_chat_oai with model: {model}\")\n        logger.debug(f\"Conversations: {conversations}\")\n        logger.debug(f\"LLM Config: {llm_config}\")\n\n        if not model:\n            model = self.llm_model.default_model_name\n        logger.info(f\"Using model: {model}\")\n\n        query = conversations[-1][\"content\"]\n\n        if (\n            \"使用四到五个字直接返回这句话的简要主题，不要解释、不要标点、不要语气词、不要多余文本，不要加粗，如果没有主题\"\n            in query\n            or \"简要总结一下对话内容，用作后续的上下文提示 prompt，控制在 200 字以内\"\n            in query\n        ):\n            chunks = self.llm.stream_chat_oai(\n                conversations=conversations,\n                model=model,\n                role_mapping=role_mapping,\n                llm_config=llm_config,\n                delta_mode=True,\n            )\n            return (chunk[0] for chunk in chunks), []\n\n        logger.info(\"Fetching context from context RAG client\")\n        context_query = self.gen_context.prompt(conversations=conversations)\n        context_response = self.context_rag_client.chat.completions.create(\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": json.dumps(\n                        {\"query\": context_query, \"only_contexts\": True},\n                        ensure_ascii=False,\n                    ),\n                }\n            ],\n            model=model,\n        )\n\n        context = context_response.choices[0].message.content\n        new_context = \"\"\n        for msg in context.split(\"\\n\"):\n            if msg.strip():\n                v = json.loads(msg)\n                new_context += v[\"source_code\"]\n        logger.info(f\"context {new_context[0:100]}...\")\n        logger.info(\"Context retrieved successfully\")\n\n        logger.info(\"Fetching schemas from schema RAG client\")\n        schema_query = self.fetch_schema.prompt(context=new_context)\n\n        response = self.schema_rag_client.chat.completions.create(\n            messages=conversations[0:-1]\n            + [\n                {\n                    \"role\": \"user\",\n                    \"content\": json.dumps(\n                        {\"query\": schema_query, \"only_contexts\": True},\n                        ensure_ascii=False,\n                    ),\n                }\n            ],\n            model=model,\n        )        \n\n        schema_str = response.choices[0].message.content\n        logger.debug(f\"Schema RAG response: {schema_str}\")\n        logger.info(\"Schema retrieved successfully\")\n\n        logger.info(\"Generating SQL string\")\n        sql_str = self.gen_sql.with_llm(self.gen_sql_llm_model).run(\n            conversations=conversations,\n            context=new_context,\n            schema=schema_str,\n            model=model,\n        )\n\n        logger.info(\"SQL string generated\")\n        logger.debug(f\"Generated SQL string: {sql_str}\")\n\n        # Extract SQL code from tags\n        tag_extractor = TagExtractor(sql_str)\n        extracted_tags = tag_extractor.extract()\n        sql = extracted_tags.content[0].content\n        logger.info(f\"Extracted SQL: {sql}\")\n\n        logger.info(f\"Sending SQL to Byzer SQL URL: {self.byzer_sql_url}\")\n        try:\n            response = requests.post(\n                self.byzer_sql_url,\n                data={\n                    \"sql\": sql,\n                    \"owner\": \"hello\",\n                    \"sessionPerUser\": True,\n                    \"sessionPerRequest\": True,\n                    \"includeSchema\": True,\n                },\n            )\n            response.raise_for_status()\n            result = response.content.decode(\"utf-8\")\n            logger.info(\"SQL execution result decoded\")\n            logger.debug(f\"SQL execution result: {result}\")\n            return (item for item in [result]), []\n        except requests.exceptions.RequestException as e:\n            logger.error(f\"Failed to send SQL to Byzer SQL URL: {e}\")\n            if hasattr(e.response, \"content\"):\n                error_content = e.response.content.decode(\"utf-8\")\n                logger.error(f\"Server response: {error_content}\")\n            else:\n                logger.error(\"No server response available\")\n            raise Exception(f\"Byzer SQL request failed: {str(e)}\") from e\n\n\n##File: /Users/allwefantasy/projects/super_analysis/setup.py\nfrom setuptools import setup, find_packages\n\n# Read requirements from requirements.txt\nwith open('requirements.txt') as f:\n    requirements = f.read().splitlines()\n\nsetup(\n    name=\"super-analysis\",\n    version=\"0.1.0\",\n    packages=find_packages(where=\"src\"),\n    package_dir={\"\": \"src\"},\n    install_requires=requirements,\n    entry_points={\n        \"console_scripts\": [\n            \"super-analysis.serve=superanalysis.serve.cli:main\",\n            \"super-analysis.convert=superanalysis.serve.convert:main\",\n            \"superanalysis.api=superanalysis.serve.rest:main\",\n        ],\n    },\n    author=\"allwefantasy\",\n    author_email=\"allwefantasy@gmail.com\",\n    description=\"A package for super analysis with ByzerLLM\",\n    long_description=open(\"README.md\").read(),\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/yourusername/super-analysis\",\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n    ],\n    python_requires='>=3.6',\n)\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/convert.py\nimport os\nimport argparse\nimport pandas as pd\nimport hashlib\nfrom typing import List, Dict\n\ndef get_csv_files(directory: str) -> List[str]:\n    \"\"\"Recursively get all CSV files in the given directory.\"\"\"\n    csv_files = []\n    for root, _, files in os.walk(directory):\n        for file in files:\n            if file.endswith('.csv'):\n                csv_files.append(os.path.join(root, file))\n    return csv_files\n\ndef extract_schema(df: pd.DataFrame) -> Dict[str, str]:\n    \"\"\"Extract schema from pandas DataFrame.\"\"\"\n    schema = {}\n    for column in df.columns:\n        dtype = str(df[column].dtype)\n        if dtype == 'object':\n            schema[column] = 'STRING'\n        elif dtype.startswith('int'):\n            schema[column] = 'INTEGER'\n        elif dtype.startswith('float'):\n            schema[column] = 'FLOAT'\n        elif dtype == 'bool':\n            schema[column] = 'BOOLEAN'\n        elif dtype.startswith('datetime'):\n            schema[column] = 'TIMESTAMP'\n        else:\n            schema[column] = 'STRING'  # Default to STRING for unknown types\n    return schema\n\ndef generate_schema_text(schema: Dict[str, str], csv_file: str, df: pd.DataFrame, include_rows_num: int) -> str:\n    \"\"\"Generate schema text from the schema dictionary and include the CSV file name and sample data.\"\"\"\n    schema_lines = [f\"CSV File: {csv_file}\\n\"]\n    schema_lines.extend([f\"{field}: {dtype}\" for field, dtype in schema.items()])\n    \n    if include_rows_num > 0:\n        schema_lines.append(f\"\\nSample Data (first {include_rows_num} rows):\")\n        sample_data = df.head(include_rows_num).to_string(index=False)\n        schema_lines.append(sample_data)\n    \n    return \"\\n\".join(schema_lines)\n\ndef save_schema(schema_text: str, csv_file: str, output_dir: str) -> None:\n    \"\"\"Save schema text to a file in the output directory.\"\"\"\n    # Generate MD5 hash of the schema text\n    file_name = hashlib.md5(schema_text.encode()).hexdigest() + '.md'\n    output_path = os.path.join(output_dir, file_name)\n    \n    with open(output_path, 'w') as f:\n        f.write(schema_text)\n    print(f\"Schema for {csv_file} saved to {output_path}\")\n\ndef convert(data_dir: str, doc_dir: str, include_rows_num: int) -> None:\n    \"\"\"Main conversion function.\"\"\"\n    csv_files = get_csv_files(data_dir)\n    \n    for csv_file in csv_files:\n        print(f\"Processing {csv_file}\")\n        df = pd.read_csv(csv_file)\n        schema = extract_schema(df)\n        schema_text = generate_schema_text(schema, csv_file, df, include_rows_num)\n        save_schema(schema_text, csv_file, doc_dir)\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Convert CSV files to schema documents\")\n    parser.add_argument(\"--data_dir\", required=True, help=\"Directory containing CSV files\")\n    parser.add_argument(\"--doc_dir\", required=True, help=\"Output directory for schema documents\")\n    parser.add_argument(\"--include-rows-num\", type=int, default=0, help=\"Number of sample rows to include in the schema\")\n    \n    args = parser.parse_args()\n    \n    if not os.path.exists(args.doc_dir):\n        os.makedirs(args.doc_dir)\n    \n    convert(args.data_dir, args.doc_dir, args.include_rows_num)\n\nif __name__ == \"__main__\":\n    main()\n\n##File: /Users/allwefantasy/projects/super_analysis/README.md\n# Super Analysis\n\nSuper Analysis 支持 NLP2SQL 以及 使用 SQL 对表中的文本字段做自然语言分析。\n\n## 安装\n\n你可以使用 pip 安装 Super Analysis：\n\n```bash\ngit clone https://github.com/allwefantasy/super_analysis\ncd super-analysis\npip install -e .\n```\n\n或者使用 Makefile 来管理项目：\n\n```bash\n# 清理项目\nmake clean\n\n# 构建项目\nmake build\n\n# 创建wheel包\nmake wheel\n\n# 安装项目\nmake install\n\n# 卸载项目\nmake uninstall\n\n# 以开发模式安装项目\nmake develop\n\n# 运行测试\nmake test\n```\n\n\n\n## 示例数据\n\n下载电影数据集： https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset/download?datasetVersionNumber=7\n\n\n## 启动分析服务\n\n整个 super-analysis 服务分为四个部分：\n\n1. byzer-sql 分布式执行引擎 \n2. schema 文档知识库\n3. context 文档知识库\n4. super-analysis 自身\n\n在启动 super-analysis 之前，你需要启动 schema 文档知识库和 context 文档知识库以及 byzer-sql 引擎。\n\n## 部署 Deepseek 模型代理\n\n在启动其他服务之前，我们需要先部署 Deepseek 模型。你可以使用以下命令来部署：\n\n```bash\nmake deploy_deepseek\n```\n\n或者直接运行以下命令：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--worker_concurrency 1000 \\\n--num_workers 1 \\\n--infer_params saas.base_url=\"https://api.deepseek.com/v1\" saas.api_key=${MODEL_DEEPSEEK_TOKEN} saas.model=deepseek-chat \\\n--model deepseek_chat\n```\n\n注意：请确保在运行此命令之前，已经设置了环境变量 `MODEL_DEEPSEEK_TOKEN`。\n\n## 部署 Byzer-SQL\n\n在你的电脑上参考 [这个文档](./docs/4.3.1%20安装与配置%20Byzer-SQL.pdf) 完成部署.\n\n> 启动的时候需要在安装有 super-analysis 的conda环境中启动。\n\n\n\n当 byzer-sql 部署完成后，注册账号为 `hello`, 然后进入到 byzer-sql 的控制台中执行以下脚本：\n\n```sql\n!byzerllm setup single;\n\nrun command as LLM.`` where \naction=\"infer\"\nand reconnect=\"true\"\nand pretrainedModelType=\"saas/openai\"\nand udfName=\"deepseek_chat\";\n```\n\n## 启动其他服务\n\n现在可以按如下方式做一些数据预处理或者启动相关服务：\n\n```bash\n## 将电影数据集的csv文件转换为schema文档\nsuper-analysis.convert --data_dir /Users/allwefantasy/data/movice --doc_dir /Users/allwefantasy/data/movice/schemas/\n\n# 启动schema文档知识库\nauto-coder doc serve \\\n--model deepseek_chat --index_filter_workers 100 \\\n--tokenizer_path /Users/allwefantasy/Downloads/tokenizer.json \\\n--doc_dir /Users/allwefantasy/data/movice/schemas/ \\\n--port 8001\n\n# 下载 Byzer-SQL 专门为大模型优化的文档\ngit clone https://github.com/allwefantasy/llm_friendly_packages\n\n# 启动文档知识库\nauto-coder doc serve \\\n--model deepseek_chat --index_filter_workers 100 \\\n--tokenizer_path /Users/allwefantasy/Downloads/tokenizer.json \\\n--doc_dir  /Users/allwefantasy/projects/llm_friendly_packages/github.com/allwefantasy \\\n--port 8002\n\n# 启动兼容 OpenAI Server 的分析服务\nsuper-analysis.serve --served-model-name deepseek_chat --port 8000 \\\n--schema-rag-base-url http://127.0.0.1:8001/v1 \\\n--context-rag-base-url http://127.0.0.1:8002/v1 \\\n--byzer-sql-url http://127.0.0.1:9003/run/script\n```\n\n现在你可以用 OpenAI SDK 去使用了。 具体测试和接口使用参考 [openai_local_api.ipynb](./notebooks/openai_local_api.ipynb)\n\n\n## 非 OpenAI 兼容接口\n\n除了兼容 OpenAI 的接口外，Super Analysis 还提供了一个简单的 REST API 接口，可以直接与模型进行交互。以下是如何使用这个接口：\n\n### 启动服务\n\n使用以下命令启动 REST API 服务：\n\n```bash\nsuperanalysis.api --schema-rag-base-url <schema_rag_url> \\\n                  --schema-rag-api-key <schema_rag_api_key> \\\n                  --context-rag-base-url <context_rag_url> \\\n                  --context-rag-api-key <context_rag_api_key> \\\n                  --byzer-sql-url <byzer_sql_url> \\\n                  --llm-model <llm_model_name> \\\n                  --gen-sql-llm-model <gen_sql_llm_model_name> \\\n                  --host <host> \\\n                  --port <port>\n```\n\n请替换尖括号中的值为实际的配置信息。\n\n### API 端点\n\n服务启动后，将提供以下 API 端点：\n\n#### POST /chat\n\n此端点用于与模型进行对话。\n\n请求体格式：\n\n```json\n{\n  \"conversations\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"你的问题或指令\"\n    }\n  ],\n  \"model\": \"可选的模型名称\"\n}\n```\n\n响应格式：\n\n```json\n{\n  \"response\": \"模型的回答\"\n}\n```\n\n### 示例使用\n\n以下是使用 Python 请求库与 API 交互的示例：\n\n```python\nimport requests\nimport json\n\nurl = \"http://localhost:8000/chat\"\npayload = {\n    \"conversations\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"你好，请介绍一下你自己。\"\n        }\n    ],\n    \"model\": \"your_model_name\"\n}\n\nresponse = requests.post(url, json=payload)\nresult = response.json()\n\nprint(result[\"response\"])\n```\n\n这个 API 相比兼容 OpenAI 接口，可以在请求中提供提供更多可选参数。\n\n## 使用方法\n\n安装完成后，你可以使用 `super-analysis.serve` 命令来启动 API 服务器：\n\n```bash\nsuper-analysis.serve --served-model-name your_model_name\n```\n\n### 命令行参数\n\n以下是可用的命令行参数：\n\n- `--host`: 服务器绑定的主机地址（默认：0.0.0.0）\n- `--port`: 服务器绑定的端口（默认：8000）\n- `--uvicorn-log-level`: Uvicorn 日志级别（默认：info）\n- `--allow-credentials`: 允许凭证（默认：False）\n- `--allowed-origins`: 允许的源（默认：[\"*\"]）\n- `--allowed-methods`: 允许的方法（默认：[\"*\"]）\n- `--allowed-headers`: 允许的头部（默认：[\"*\"]）\n- `--api-key`: 用于认证的 API 密钥\n- `--served-model-name`: 要服务的模型名称（必需）\n- `--prompt-template`: 提示模板\n- `--response-role`: 响应角色（默认：assistant）\n- `--ssl-keyfile`: SSL 密钥文件路径\n- `--ssl-certfile`: SSL 证书文件路径\n\n例如：\n\n```bash\nsuper-analysis.serve --served-model-name deepseek_chat --port 8000\n```\n\n\n## 抽取csv文件schema\n\nThe `super-analysis.convert` command allows you to convert CSV files to schema documents. This is useful for generating metadata about your CSV files that can be used for further analysis or documentation.\n\n#### Usage\n\n```\nsuper-analysis.convert --data_dir <path_to_csv_files> --doc_dir <output_directory> [--include-rows-num <number_of_sample_rows>]\n```\n\nArguments:\n- `--data_dir`: Directory containing CSV files (required)\n- `--doc_dir`: Output directory for schema documents (required)\n- `--include-rows-num`: Number of sample rows to include in the schema (optional, default is 0)\n\nThe utility will:\n1. Recursively find all CSV files in the specified directory\n2. Extract the schema from each CSV file\n3. Generate a schema text that includes:\n   - The CSV file name\n   - Field names and their data types\n   - Sample data (if `--include-rows-num` is specified)\n4. Save the schema text as a markdown file in the output directory\n\n## 联系方式\n\n如有任何问题或建议，请联系 allwefantasy@gmail.com。\n\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/rest.py\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nfrom typing import List, Optional\nimport uvicorn\nimport argparse\nfrom superanalysis.sql.generator import SQLGenerator\nfrom superanalysis.sql.types import RAGConfig, ByzerSQLConfig, ModelConfig\nimport byzerllm\n\napp = FastAPI()\n\nclass ChatMessage(BaseModel):\n    role: str\n    content: str\n\nclass ChatRequest(BaseModel):\n    conversations: List[ChatMessage]\n    model: Optional[str] = None\n\nclass ChatResponse(BaseModel):\n    response: str\n\n@app.post(\"/chat\", response_model=ChatResponse)\nasync def chat(request: ChatRequest):\n    try:\n        result, _ = sql_generator.stream_chat_oai(\n            conversations=[{\"role\": msg.role, \"content\": msg.content} for msg in request.conversations],\n            model=request.model\n        )\n        return ChatResponse(response=\"\".join(result))\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"SuperAnalysis API Server\")\n    parser.add_argument(\"--host\", type=str, default=\"0.0.0.0\", help=\"Host to bind the server to\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"Port to bind the server to\")\n    parser.add_argument(\"--schema-rag-base-url\", type=str, required=True, help=\"Schema RAG base URL\")\n    parser.add_argument(\"--schema-rag-api-key\", type=str, required=True, help=\"Schema RAG API key\")\n    parser.add_argument(\"--context-rag-base-url\", type=str, required=True, help=\"Context RAG base URL\")\n    parser.add_argument(\"--context-rag-api-key\", type=str, required=True, help=\"Context RAG API key\")\n    parser.add_argument(\"--byzer-sql-url\", type=str, required=True, help=\"Byzer SQL URL\")\n    parser.add_argument(\"--data-dir\", type=str, default=\"\", help=\"Data directory\")\n    parser.add_argument(\"--owner\", type=str, default=\"hello\", help=\"Owner of the Byzer SQL session\")\n    parser.add_argument(\"--llm-model\", type=str, required=True, help=\"LLM model for general use\")\n    parser.add_argument(\"--gen-sql-llm-model\", type=str, required=True, help=\"LLM model for SQL generation\")\n    \n    args = parser.parse_args()\n\n    rag_config = RAGConfig(\n        schema_rag_base_url=args.schema_rag_base_url,\n        schema_rag_api_key=args.schema_rag_api_key,\n        context_rag_base_url=args.context_rag_base_url,\n        context_rag_api_key=args.context_rag_api_key\n    )\n\n    byzer_sql_config = ByzerSQLConfig(\n        byzer_sql_url=args.byzer_sql_url,\n        data_dir=args.data_dir,\n        owner=args.owner\n    )\n\n    model_config = ModelConfig(\n        llm_model=args.llm_model,\n        gen_sql_llm_model=args.gen_sql_llm_model\n    )\n\n    llm = byzerllm.ByzerLLM()\n    llm.setup_default_model_name(args.llm_model)\n\n    global sql_generator\n    sql_generator = SQLGenerator(\n        llm=llm,\n        rag_config=rag_config,\n        byzer_sql_config=byzer_sql_config,\n        model_config=model_config\n    )\n\n    uvicorn.run(app, host=args.host, port=args.port)\n\nif __name__ == \"__main__\":\n    main()\n\n##File: /Users/allwefantasy/projects/super_analysis/Makefile\n# Makefile for super-analysis project\n\n# Python interpreter\nPYTHON := python3\n\n# Project name\nPROJECT_NAME := super-analysis\n\n# Directories\nSRC_DIR := src\nDIST_DIR := dist\nBUILD_DIR := build\n\n# Commands\nSETUP := $(PYTHON) setup.py\n\n.PHONY: all clean build wheel deploy_deepseek\n\nall: clean build wheel\n\ndeploy_deepseek:\n\tbyzerllm deploy --pretrained_model_type saas/openai \\\n\t--cpus_per_worker 0.001 \\\n\t--gpus_per_worker 0 \\\n\t--worker_concurrency 1000 \\\n\t--num_workers 1 \\\n\t--infer_params saas.base_url=\"https://api.deepseek.com/v1\" saas.api_key=${MODEL_DEEPSEEK_TOKEN} saas.model=deepseek-chat \\\n\t--model deepseek_chat\n\nclean:\n\t@echo \"Cleaning up...\"\n\trm -rf $(BUILD_DIR) $(DIST_DIR) *.egg-info\n\tfind . -type d -name \"__pycache__\" -exec rm -rf {} +\n\tfind . -type f -name \"*.pyc\" -delete\n\nbuild:\n\t@echo \"Building the project...\"\n\t$(SETUP) build\n\nwheel:\n\t@echo \"Creating wheel package...\"\n\t$(SETUP) bdist_wheel\n\ninstall:\n\t@echo \"Installing the package...\"\n\t$(PYTHON) -m pip install .\n\nuninstall:\n\t@echo \"Uninstalling the package...\"\n\t$(PYTHON) -m pip uninstall -y $(PROJECT_NAME)\n\ndevelop:\n\t@echo \"Installing the package in editable mode...\"\n\t$(PYTHON) -m pip install -e .\n\ntest:\n\t@echo \"Running tests...\"\n\t$(PYTHON) -m unittest discover tests\n\n##File: /Users/allwefantasy/projects/super_analysis/docs/deploy.md\n\n\n##File: .auto-coder/libs/llm_friendly_packages/github.com/allwefantasy/byzer-llm/README.md\n# byzerllm 大模型编程快速指南\n\n本文示例 [Notebook](../../notebooks/003_byzerllm_大模型编程快速指南.ipynb)\n\n## 安装\n\n```bash\npip install -U byzerllm\nray start --head\n```\n## 启动一个模型代理\n\nbyzerllm 支持私有化模型或者SaaS模型的部署。\n\n这里以 deepseek 官方API 为例：\n\n```bash\neasy-byzerllm deploy deepseek-chat --token xxxxx --alias deepseek_chat\n```\n\n或者跬基流动API:\n\n```bash\neasy-byzerllm deploy alibaba/Qwen1.5-110B-Chat --token xxxxx --alias qwen110b_chat\n```\n\n将上面的 API KEY 替换成你们自己的。\n\n如果你想部署私有化模型或者对接 Ollama 等更多需求，参考 [002_使用byzerllm进行模型部署.md](./002_使用byzerllm进行模型部署.md) 或者 [README.md](../../README.md)。\n\n之后，你就可以在代码里使用  deepseek_chat 或者 qwen110b_chat  访问模型了。\n\n## hello world\n\n来和我们的大模型打个招呼:\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\n@byzerllm.prompt(llm=llm)\ndef hello(q:str) ->str:\n    '''\n    你好, {{ q }}\n    '''\n\ns = hello(\"你是谁\")    \nprint(s)\n\n## 输出:\n## '你好！我是一个人工智能助手，专门设计来回答问题、提供信息和帮助解决问题。如果你有任何疑问或需要帮助，请随时告诉我。'\n```\n\n恭喜，你和大模型成功打了招呼！\n\n可以看到，我们通过 `@byzerllm.prompt` 装饰器，将一个方法转换成了一个大模型的调用，然后这个方法的主题是一段文本，文本中\n使用了 jinja2 模板语法，来获得方法的参数。当正常调用该方法时，实际上就发起了和大模型的交互，并且返回了大模型的结果。\n\n在 byzerllm 中，我们把这种方法称之为 prompt 函数。\n\n## 查看发送给大模型的prompt\n\n很多情况你可能需要调试，查看自己的 prompt 渲染后到底是什么样子的，这个时候你可以通过如下方式\n获取渲染后的prompt:\n\n```python\nhello.prompt(\"你是谁\")\n## '你好, 你是谁'\n```            \n\n## 动态换一个模型\n\n前面的 hello 方法在初始化的时候，我们使用了默认的模型 deepseek_chat，如果我们想换一个模型，可以这样做：\n\n```python\nhello.with_llm(llm).run(\"你是谁\")\n## '你好！我是一个人工智能助手，专门设计来回答问题、提供信息和帮助解决问题。如果你有任何疑问或需要帮助，请随时告诉我。'\n```\n\n通过 with_llm 你可以设置一个新的 llm 对象，然后调用 run 方法，就可以使用新的模型了。\n\n## 超长文本生成\n\n我们知道，大模型一次生成的长度其实是有限的，如果你想生成超长文本，你可能需手动的不断获得\n生成结果，然后把他转化为输入，然后再次生成，这样的方式是比较麻烦的。\n\nbyzerllm 提供了更加易用的 API :\n\n```python\nimport byzerllm\nfrom byzerllm import ByzerLLM\n\nllm = ByzerLLM.from_default_model(\"deepseek_chat\")\n\n@byzerllm.prompt()\ndef tell_story() -> str:\n    \"\"\"\n    讲一个100字的故事。    \n    \"\"\"\n\n\ns = (\n    tell_story.with_llm(llm)\n    .with_response_markers()\n    .options({\"llm_config\": {\"max_length\": 10}})\n    .run()\n)\nprint(s)\n\n## 从前，森林里住着一只聪明的小狐狸。一天，它发现了一块闪闪发光的宝石。小狐狸决定用这块宝石帮助森林里的动物们。它用宝石的光芒指引迷路的小鹿找到了回家的路，用宝石的温暖治愈了受伤的小鸟。从此，小狐狸成了森林里的英雄，动物们都感激它的善良和智慧。\n```\n\n实际核心部分就是这一行：\n\n```python\ntell_story.with_llm(llm)\n    .with_response_markers()    \n    .run()\n```\n\n我们只需要调用 `with_response_markers` 方法，系统就会自动的帮我们生成超长文本。\n在上面的案例中，我们通过\n\n```python\n.options({\"llm_config\": {\"max_length\": 10}})\n```\n\n认为的限制大模型一次交互最多只能输出10个字符，但是系统依然自动完成了远超过10个字符的文本生成。\n\n## 对象输出\n\n前面我们的例子都是返回字符串，但是我们也可以返回对象，这样我们就可以更加灵活的处理返回结果。\n\n```python\nimport pydantic \n\nclass Story(pydantic.BaseModel):\n    '''\n    故事\n    '''\n\n    title: str = pydantic.Field(description=\"故事的标题\")\n    body: str = pydantic.Field(description=\"故事主体\")\n\n@byzerllm.prompt()\ndef tell_story()->Story:\n    '''\n    讲一个100字的故事。    \n    '''\n\ns = tell_story.with_llm(llm).run()\nprint(isinstance(s, Story))\nprint(s.title)\n\n## True\n## 勇敢的小鸟\n```\n\n可以看到，我们很轻松的将输出转化为格式化输出。\n\n## 自定义字段抽取\n\n前面的结构化输出，其实会消耗更多token,还有一种更加精准的结构化输出方式。\n比如让大模型生成一个正则表达式，但实际上大模型很难准确只输出一个正则表达式，这个时候我们可以通过自定义抽取函数来获取我们想要的结果。\n\n\n```python\nfrom loguru import logger\nimport re\n\n@byzerllm.prompt()\ndef generate_regex_pattern(desc: str) -> str:\n    \"\"\"\n    根据下面的描述生成一个正则表达式，要符合python re.compile 库的要求。\n\n    {{ desc }}\n\n    最后生成的正则表达式要在<REGEX></REGEX>标签对里。\n    \"\"\"    \n\ndef extract_regex_pattern(regex_block: str) -> str:    \n    pattern = re.search(r\"<REGEX>(.*)</REGEX>\", regex_block, re.DOTALL)\n    if pattern is None:\n        logger.warning(\"No regex pattern found in the generated block:\\n {regex_block}\")\n        raise None\n    return pattern.group(1)\n\npattern = \"匹配一个邮箱地址\"\nv = generate_regex_pattern.with_llm(llm).with_extractor(extract_regex_pattern).run(desc=pattern)\nprint(v)\n## ^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\n```\n\n在上面的例子里，我们根据一句话生成一个正则表达式。我们通过 `with_extractor` 方法，传入了一个自定义的抽取函数，这个函数会在大模型生成结果后，对结果进行处理，然后返回我们想要的结果。\n\n我们在 prompt 明确说了，生成的结果要放到 `<REGEX></REGEX>` 标签对里，然后我们通过 extract_regex_pattern 函数，从结果中提取出了我们想要的正则表达式。\n\n## 在实例方法中使用大模型\n\n```python\nimport byzerllm\ndata = {\n    'name': 'Jane Doe',\n    'task_count': 3,\n    'tasks': [\n        {'name': 'Submit report', 'due_date': '2024-03-10'},\n        {'name': 'Finish project', 'due_date': '2024-03-15'},\n        {'name': 'Reply to emails', 'due_date': '2024-03-08'}\n    ]\n}\n\n\nclass RAG():\n    def __init__(self):        \n        self.llm = byzerllm.ByzerLLM()\n        self.llm.setup_template(model=\"deepseek_chat\",template=\"auto\")\n        self.llm.setup_default_model_name(\"deepseek_chat\")        \n    \n    @byzerllm.prompt(lambda self:self.llm)\n    def generate_answer(self,name,task_count,tasks)->str:\n        '''\n        Hello {{ name }},\n\n        This is a reminder that you have {{ task_count }} pending tasks:\n        {% for task in tasks %}\n        - Task: {{ task.name }} | Due: {{ task.due_date }}\n        {% endfor %}\n\n        Best regards,\n        Your Reminder System\n        '''        \n\nt = RAG()\n\nresponse = t.generate_answer(**data)\nprint(response)\n\n## 输出:\n## Hello Jane Doe,\n##I hope this message finds you well. I wanted to remind you of your 3 pending tasks to ensure you stay on track:\n## 1. **Submit report** - This task is due on **2024-03-10**. Please ensure that you allocat\n```\n\n这里我们给了个比较复杂的例子，但我们可以看到，给一个实例prompt方法和普通prompt 方法差异不大。\n唯一的区别是如果你希望在定义的时候就指定大模型，使用一个lambda函数返回实例的 llm 对象即可。\n\n```python\n@byzerllm.prompt(lambda self:self.llm)\n```\n\n你也可以不返回，在调用的时候通过 `with_llm` 方法指定 llm 对象。\n\n此外，这个例子也展示了如何通过jinja2模板语法，来处理复杂的结构化数据。\n\n## 通过 Python 代码处理复杂入参\n\n上面的一个例子中，我们通过 jinja2 模板语法，来处理复杂的结构化数据，但是有时候我们可能需要更加复杂的处理，这个时候我们可以通过 Python 代码来处理。\n\n```python\nimport byzerllm\n\ndata = {\n    'name': 'Jane Doe',\n    'task_count': 3,\n    'tasks': [\n        {'name': 'Submit report', 'due_date': '2024-03-10'},\n        {'name': 'Finish project', 'due_date': '2024-03-15'},\n        {'name': 'Reply to emails', 'due_date': '2024-03-08'}\n    ]\n}\n\n\nclass RAG():\n    def __init__(self):        \n        self.llm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n    \n    @byzerllm.prompt()\n    def generate_answer(self,name,task_count,tasks)->str:\n        '''\n        Hello {{ name }},\n\n        This is a reminder that you have {{ task_count }} pending tasks:\n            \n        {{ tasks }}\n\n        Best regards,\n        Your Reminder System\n        '''\n        \n        tasks_str = \"\\n\".join([f\"- Task: {task['name']} | Due: { task['due_date'] }\" for task in tasks])\n        return {\"tasks\": tasks_str}\n\nt = RAG()\n\nresponse = t.generate_answer.with_llm(t.llm).run(**data)\nprint(response)\n\n## Just a gentle nudge to keep you on track with your pending tasks. Here's a quick recap:....\n```\n\n在这个例子里，我们直接把 tasks 在方法体里进行处理，然后作为一个字符串返回，最够构建一个字典，字典的key为 tasks,然后\n你就可以在 docstring 里使用 `{{ tasks }}` 来引用这个字符串。\n\n这样对于很复杂的入参，就不用谢繁琐的 jinja2 模板语法了。\n\n## 如何自动实现一个方法\n\n比如我定义一个签名，但是我不想自己实现里面的逻辑，让大模型来实现。这个在 byzerllm 中叫 function impl。我们来看看怎么\n实现:\n\n```python\nimport pydantic\nclass Time(pydantic.BaseModel):\n    time: str = pydantic.Field(...,description=\"时间，时间格式为 yyyy-MM-dd\")\n\n\n@llm.impl()\ndef calculate_current_time()->Time:\n    '''\n    计算当前时间\n    '''\n    pass \n\n\ncalculate_current_time()\n#output: Time(time='2024-06-14')\n```\n\n在这个例子里，我们定义了一个 calculate_current_time 方法，但是我们没有实现里面的逻辑，我们通过 `@llm.impl()` 装饰器，让大模型来实现这个方法。\n为了避免每次都要“生成”这个方法，导致无法适用，我们提供了缓存，用户可以按如下方式打印速度：\n\n```python\nstart = time.monotonic()\ncalculate_current_time()\nprint(f\"first time cost: {time.monotonic()-start}\")\n\nstart = time.monotonic()\ncalculate_current_time()\nprint(f\"second time cost: {time.monotonic()-start}\")\n\n# output:\n# first time cost: 6.067266260739416\n# second time cost: 4.347506910562515e-05\n```\n可以看到，第一次执行花费了6s,第二次几乎是瞬间完成的，这是因为第一次执行的时候，我们实际上是在生成这个方法，第二次执行的时候，我们是执行已经生成好的代码，所以速度会非常快。你可以显示的调用 `llm.clear_impl_cache()` 清理掉函数缓存。\n\n## Stream 模式\n\n前面的例子都是一次性生成结果，但是有时候我们可能需要一个流式的输出，这个时候我们可能需要用底层一点的API来完成了：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\nv = llm.stream_chat_oai(model=\"deepseek_chat\",conversations=[{\n    \"role\":\"user\",\n    \"content\":\"你好，你是谁\",\n}],delta_mode=True)\n\nfor t in v:\n    print(t,flush=True)  \n\n# 你好\n# ！\n# 我\n# 是一个\n# 人工智能\n# 助手\n# ，\n# 旨在\n# 提供\n# 信息\n# 、\n# 解答\n# 问题....\n```\n\n如果你不想要流式输出，但是想用底层一点的API，你可以使用 `llm.chat_oai` 方法：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\nv = llm.chat_oai(model=\"deepseek_chat\",conversations=[{\n    \"role\":\"user\",\n    \"content\":\"你好，你是谁\",\n}])\n\nprint(v[0].output)\n## 你好！我是一个人工智能助手，旨在提供信息、解答问题和帮助用户解决问题。如果你有任何问题或需要帮助，请随时告诉我。\n```\n\n## Function Calling \n\nbyzerllm 可以不依赖模型自身就能提供 function calling 支持，我们来看个例子：\n\n\n```python\nfrom typing import List,Dict,Any,Annotated\nimport pydantic \nimport datetime\nfrom dateutil.relativedelta import relativedelta\n\ndef compute_date_range(count:Annotated[int,\"时间跨度，数值类型\"],\n                       unit:Annotated[str,\"时间单位，字符串类型\",{\"enum\":[\"day\",\"week\",\"month\",\"year\"]}])->List[str]:\n    '''\n    计算日期范围\n\n    Args:\n        count: 时间跨度，数值类型\n        unit: 时间单位，字符串类型，可选值为 day,week,month,year\n    '''        \n    now = datetime.datetime.now()\n    now_str = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n    if unit == \"day\":\n        return [(now - relativedelta(days=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"week\":\n        return [(now - relativedelta(weeks=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"month\":\n        return [(now - relativedelta(months=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"year\":\n        return [(now - relativedelta(years=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    return [\"\",\"\"]\n\ndef compute_now()->str:\n    '''\n    计算当前时间\n    '''\n    return datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n```\n\n我们定义了两个方法，一个是计算日期范围，一个是计算当前时间。\n\n现在我么可以来测试下，系统如何根据自然语言决定调用哪个方法：\n\n```python\nt = llm.chat_oai([{\n    \"content\":'''计算当前时间''',\n    \"role\":\"user\"    \n}],tools=[compute_date_range,compute_now],execute_tool=True)\n\nt[0].values\n\n## output: ['2024-06-14 15:18:02']\n```\n\n我们可以看到，他正确的选择了 compute_now 方法。\n\n接着我们再试一个：\n\n```python\nt = llm.chat_oai([{\n    \"content\":'''最近三个月趋势''',\n    \"role\":\"user\"    \n}],tools=[compute_date_range,compute_now],execute_tool=True)\n\nt[0].values\n\n## output: [['2024-03-14 15:19:13', '2024-06-14 15:19:13']]\n```\n\n模型正确的选择了 compute_date_range 方法。\n\n## 多模态\n\nbyerllm 也能很好的支持多模态的交互，而且统一了多模态大模型的接口，比如你可以用一样的方式使用 openai 或者 claude 的图片转文字能力， 或者一致的方式使用火山，azuer, openai的语音合成接口。\n\n### 图生文\n\n```python\nimport byzerllm\nfrom byzerllm.types import ImagePath\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\n\n\n@byzerllm.prompt()\ndef what_in_image(image_path: ImagePath) -> str:\n    \"\"\"\n    {{ image_path }}\n    这个图片里有什么？\n    \"\"\"    \n\n\nv = what_in_image.with_llm(vl_llm).run(\n    ImagePath(value=\"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\")\n)\nv\n## OUTPUT: 这张图片展示了多只可爱的猫咪，采用了艺术风格的绘画。猫咪们有不同的颜色和花纹，背景是浅棕色，上面还点缀着一些红色的花朵。整体画面给人一种温馨和谐的感觉\n```\n\n可以看到，我们只需要把 prompt 函数的图片地址入参使用 byzerllm.types.ImagePath里进行包装，就可以直接在 prompt 函数体里\n带上图片。\n\n或者你可以这样：\n\n```python\nimport byzerllm\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\n\n\n@byzerllm.prompt()\ndef what_in_image(image_path: str) -> str:\n    \"\"\"\n    {{ image }}\n    这个图片里有什么？\n    \"\"\"\n    return {\"image\": byzerllm.Image.load_image_from_path(image_path)}\n\n\nv = what_in_image.with_llm(vl_llm).run(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv\n```\n\n通过 `image_path` 参数，然后通过 `byzerllm.Image.load_image_from_path` 方法，转化为一个图片对象 image，最后在 prompt 函数体里\n使用 `{{ image }}` 引用这个图片对象。\n\n另外我们也是可以支持配置多张图片的。\n\n另外，我们也可以使用基础的 `llm.chat_oai` 方法来实现：\n\n```python\nimport byzerllm\nimport json\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\nimage = byzerllm.Image.load_image_from_path(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv = vl_llm.chat_oai(\n    conversations=[\n        {\n            \"role\": \"user\",\n            \"content\": json.dumps(\n                [{\"image\": image, \"text\": \"这个图片里有什么？\"}], ensure_ascii=False\n            ),\n        }\n    ]\n)\nv[0].output\n```\n\n还可以这么写：\n\n```python\nimport byzerllm\nimport json\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\nimage = byzerllm.Image.load_image_from_path(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv = vl_llm.chat_oai(\n    conversations=[\n        {\n            \"role\": \"user\",\n            \"content\": json.dumps(\n                [\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\"url\": image, \"detail\": \"high\"},\n                    },\n                    {\"text\": \"这个图片里有什么？\", \"type\": \"text\"},\n                ],\n                ensure_ascii=False,\n            ),\n        }\n    ]\n)\nv[0].output\n```\n\n### 语音合成\n\n这里以 openai 的 tts 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=tts-1 \\\n--model openai_tts\n```\n\n此外，byzerllm 支持 azure,火山引擎等 tts 语音合成引擎。\n\n接着你可以这么用：\n\n```python\nimport byzerllm\nimport base64\nimport json\n\nllm = byzerllm.ByzerLLM.from_default_model(\"openai_tts\")\n\n\nt = llm.chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\": json.dumps({\n        \"input\":\"hello, open_tts\",\n        \"voice\": \"alloy\",\n        \"response_format\": \"mp3\"\n    },ensure_ascii=False)\n}])\n\nwith open(\"voice.mp3\",\"wb\") as f:\n    f.write(base64.b64decode(t[0].output))\n```\n\ntts 模型生成没有prompt函数可以用，你需要直接使用 chat_oai。\n\n\n### 语音识别\n\n这里以 openai 的 whisper-1 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--worker_concurrency 10 \\\n--infer_params saas.model=whisper-1 saas.api_key=${MODEL_OPENAI_TOKEN} \\\n--model speech_to_text\n```\n\n语音识别的使用方式和图生文类似，我们可以直接在 prompt 函数体里带上音频文件。\n\n```python\nimport byzerllm\nimport json\nimport base64\nfrom byzerllm.types import AudioPath\n\nllm = byzerllm.ByzerLLM.from_default_model(\"speech_to_text\")\n\naudio_file = \"/Users/allwefantasy/videos/output_audio.mp3\"\n\n@byzerllm.prompt(llm=llm)\ndef audio_to_text(audio_file: AudioPath):\n    \"\"\"\n    {{ audio_file }}\n    \"\"\"\n\nv = audio_to_text(AudioPath(value=audio_file))\njson.loads(v)\n```\n输出的数据格式略微复杂：\n\n```\n{'text': 'In the last chapter, you and I started to step through the internal workings of a transformer. This is one of the key pieces of technology inside large language models, and a lot of other tools in the modern wave of AI.',\n 'task': 'transcribe',\n 'language': 'english',\n 'duration': 10.0,\n 'segments': [{'id': 0,\n   'seek': 0,\n   'start': 0.0,\n   'end': 4.78000020980835,\n   'text': ' In the last chapter, you and I started to step through the internal workings of a transformer.',\n   'tokens': [50364,\n    682,\n    264,\n    1036,\n    7187,\n    11,\n    291,\n    293,\n    286,\n    1409,\n    281,\n    1823,\n    807,\n    264,\n    6920,\n    589,\n    1109,\n    295,\n    257,\n    31782,\n    13,\n    50586],\n   'temperature': 0.0,\n   'avg_logprob': -0.28872039914131165,\n   'compression_ratio': 1.4220778942108154,\n   'no_speech_prob': 0.016033057123422623},\n  ....\n  {'id': 2,\n   'seek': 0,\n   'start': 8.579999923706055,\n   'end': 9.979999542236328,\n   'text': ' and a lot of other tools in the modern wave of AI.',\n   'tokens': [50759,\n    293,\n    257,\n    688,\n    295,\n    661,\n    3873,\n    294,\n    264,\n    4363,\n    5772,\n    295,\n    7318,\n    13,\n    50867],\n   'temperature': 0.0,\n   'avg_logprob': -0.28872039914131165,\n   'compression_ratio': 1.4220778942108154,\n   'no_speech_prob': 0.016033057123422623}],\n 'words': [{'word': 'In', 'start': 0.0, 'end': 0.18000000715255737},\n  {'word': 'the', 'start': 0.18000000715255737, 'end': 0.23999999463558197},\n  {'word': 'last', 'start': 0.23999999463558197, 'end': 0.5400000214576721},\n  {'word': 'chapter', 'start': 0.5400000214576721, 'end': 0.800000011920929},\n  ....\n  {'word': 'AI', 'start': 9.920000076293945, 'end': 9.979999542236328}]}\n```\n\n会输出每一句话以及每一个字所在的起始时间和截止时间。你可以根据需要来使用。\n\n\n### 文生图\n\n文生图和语音合成类似，首先要启动合适的模型,以openai 的 dall-e-3 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=dall-e-3 \\\n--model openai_image_gen\n```\n\n启动模型后，只需要记住几个模板参数即可使用，这里直接使用 chat_oai 方法来使用：\n\n```python\n\nimport byzerllm\nimport json\nimport base64\n\nllm = byzerllm.ByzerLLM.from_default_model(\"openai_image_gen\")\nt = llm.chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\": json.dumps({\n        \"input\":\"a white siamese cat\",\n        \"size\": \"1024x1024\",\n        \"quality\": \"standard\"\n    },ensure_ascii=False)\n}])\n\nwith open(\"output1.jpg\",\"wb\") as f:\n    f.write(base64.b64decode(t[0].output))\n\n\nimport matplotlib.pyplot as plt\n\nimage_path = \"output1.jpg\"\nimage = plt.imread(image_path)\n\nplt.imshow(image)\nplt.axis('off')\nplt.show()\n```\n\n## Prompt 函数的流式输出\n\nbyzerllm 底层支持流式输出，非 prompt 函数的用法是这样的：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\n\nv = llm.stream_chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\":\"讲一个100字的故事\"\n}])\n\nfor s  in v:\n    print(s[0], end=\"\")\n```\n\n如果你像用 prompt 函数，可以这么用：\n\n\n```python\nimport byzerllm\nimport json\nimport base64\nfrom typing import Generator\n\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\n\n@byzerllm.prompt()\ndef tell_story() -> Generator[str, None, None]:\n    '''\n    给我讲一个一百多字的故事\n    '''\n\nv = tell_story.with_llm(llm).run()    \nfor i in v:\n    print(i, end=\"\")\n```\n\n可以看到，和普通的 prompt 函数的区别在于，返回值是一个生成器，然后你可以通过 for 循环来获取结果。\n\n## 向量化模型\n\nbyzerllm 支持向量化模型,你可以这样启动一个本地的模型：\n\n```bash\n!byzerllm deploy --pretrained_model_type custom/bge \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--worker_concurrency 10 \\\n--model_path /home/winubuntu/.auto-coder/storage/models/AI-ModelScope/bge-large-zh \\\n--infer_backend transformers \\\n--num_workers 1 \\\n--model emb\n```\n\n注意两个参数:\n\n1. --infer_backend transformers: 表示使用 transformers 作为推理后端。\n2. --model_path: 表示模型的路径。\n\n也可以启动一个 SaaS 的emb模型,比如 qwen 的 emb 模型：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/qianwen \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 2 \\\n--infer_params saas.api_key=${MODEL_QIANWEN_TOKEN}  saas.model=text-embedding-v2 \\\n--model qianwen_emb\n```\n\n或者 openai 的 emb 模型：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--worker_concurrency 10 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=text-embedding-3-small \\\n--model gpt_emb\n```\nSaaS 模型无需配置 `--infer_backend` 参数。\n\n无论是本地模型还是 SaaS 模型，我们都可以这样使用：\n\n```python\nimport byzerllm\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\nllm.setup_default_emb_model_name(\"emb\")\nllm.emb_query(\"你好\")\n```\n\n如果你配置 byzerllm 中的 Storage 使用，比如你这样启动了存储：\n\n```bash\nbyzerllm storage start --enable_emb\n```\n\n那么需要这么使用：\n\n```python\nfrom byzerllm.apps.byzer_storage.simple_api import ByzerStorage, DataType, FieldOption,SortOption\nstorage = ByzerStorage(\"byzerai_store\", \"memory\", \"memory\")\nstorage.emb(\"你好\")\n```\n\n## 一些辅助工具\n\n当调用 prompt 函数返回字符串的时候，如果想从里面抽取代码，可以使用如下方式：\n\n```python\nfrom byzerllm.utils.client import code_utils\ntext_with_markdown = '''\n```shell\nls -l\n```\n'''\ncode_blocks = code_utils.extract_code(text_with_markdown)\nfor code_block in code_blocks:\n    if code_block[0] == \"shell\":\n        print(code_block[1])\n##output: ls -l        \n```\n\nTagExtractor 工具用于对任意 `<_tag_></_tag_>` 标签对的抽取，下面是一个使用示例：\n\n```python\nfrom byzerllm.apps.utils import TagExtractor\n\nextractor = TagExtractor('''\n大家好\n<_image_>data:image/jpeg;base64,xxxxxx</_image_>\n                         大家好\n<_image_>data:image/jpeg;base64,xxxxxx2</_image_>\n''')\n\nv = extractor.extract()\nprint(v.content[0].content)\nprint(v.content[1].content)\n```\n输出为:\n\n```\ndata:image/jpeg;base64,xxxxxx\ndata:image/jpeg;base64,xxxxxx2\n```\n\n我们成功的将 <_image_></_image_> 标签对里的内容抽取出来了。\n\n\n## 注意事项\n\n1. prompt函数方法体返回只能是dict，实际的返回类型和方法签名可以不一样，但是方法体返回只能是dict。\n2. 大部分情况prompt函数体为空，如果一定要有方法体，可以返回一个空字典。\n3. 调用prompt方法的时候，如果在@byzerllm.prompt()里没有指定llm对象，那么需要在调用的时候通过with_llm方法指定llm对象。\n\n===========================\n\n# Byzer Storage: 最易用的AI存储引擎\n\nByzer Storage是一个为AI应用设计的高性能存储引擎,它提供了简单易用的API,支持向量搜索、全文检索以及结构化查询。本文将详细介绍Byzer Storage的使用方法和主要特性。\n\n## 0. 安装和启动\n\n```bash\npip install byzerllm\nbyzerllm storage start\n```\n\nThat's it! Byzer Storage已经安装并启动成功,现在我们可以开始使用它了。默认会启动一个 byzerai_store 的集群。\n\n注意，如果你是这样启动的：\n\n```bash\nbyzerllm storage start --enable_emb\n```\n\n那么会自动启动一个 emb 模型，名字就叫 emb， ByzerStorage 会自动使用该模型，无需做任何其他配置。\n\n## 1. 初始化\n\n创建一个 ByzerStorage 对象，链接 byzerai_store 集群，并且指定数据库和表名（可以不存在）。\n\n```python\nfrom byzerllm.apps.byzer_storage.simple_api import ByzerStorage, DataType, FieldOption, SortOption\n\nstorage = ByzerStorage(\"byzerai_store\", \"my_database1\", \"my_table4s\")\n```\n\n\n## 2. 创建库表（可选）\n\nByzer Storage使用Schema来定义数据结构。我们可以使用SchemaBuilder来构建Schema:\n\n```python\n_ = (\n    storage.schema_builder()\n    .add_field(\"_id\", DataType.STRING)\n    .add_field(\"name\", DataType.STRING)\n    .add_field(\"content\", DataType.STRING, [FieldOption.ANALYZE])\n    .add_field(\"raw_content\", DataType.STRING, [FieldOption.NO_INDEX])    \n    .add_array_field(\"summary\", DataType.FLOAT)    \n    .add_field(\"created_time\", DataType.LONG, [FieldOption.SORT])    \n    .execute()\n)\n```\n\n这个Schema定义了以下字段:\n- `_id`: 字符串类型的主键\n- `name`: 字符串类型,可用于过滤条件\n- `content`: 字符串类型,带有ANALYZE选项,用于全文搜索\n- `raw_content`: 字符串类型,带有NO_INDEX选项,不会被索引\n- `summary`: 浮点数组类型,用于存储向量\n- `created_time`: 长整型,带有SORT选项,可用于排序\n\n需要注意的是：\n\n1. 如果一个字段带有ANALYZE选项,则该字段会被分词,并且可以用于全文搜索，但是就无法返回原始的文本了。所以你需要添加一个新字段专门用来存储原文，比如在我们这个例子里，我们新增了 raw_content 字段，并且显示的指定了 NO_INDEX 选项，这样就不会被索引，也就不会被分词，可以后续被检索后用来获取原始文本。\n2. 对于需要作为向量检索和存储的字段，需要指定为数组类型，比如我们这个例子里的 summary 字段。\n3. 如果你需要拿到向量字段的原始文本，那么你也需要添加一个新字段专门用来存储原文，就像我们这个例子里的 raw_content 字段一样。\n\n\n## 3. 写入数据\n\n准备数据并使用WriteBuilder写入Storage:\n\n```python\ndata = [\n    {\"_id\": \"1\", \"name\": \"Hello\", \"content\": \"Hello, world!\", \"raw_content\": \"Hello, world!\", \"summary\": \"hello world\", \"created_time\": 1612137600},\n    {\"_id\": \"2\", \"name\": \"Byzer\", \"content\": \"Byzer, world!\", \"raw_content\": \"Byzer, world!\", \"summary\": \"byzer\", \"created_time\": 1612137601},\n    {\"_id\": \"3\", \"name\": \"AI\", \"content\": \"AI, world!\", \"raw_content\": \"AI, world!\", \"summary\": \"AI\", \"created_time\": 16121376002},\n    {\"_id\": \"4\", \"name\": \"ByzerAI\", \"content\": \"ByzerAI, world!\", \"raw_content\": \"ByzerAI, world!\", \"summary\": \"ByzerAi\", \"created_time\": 16121376003},\n]\n\nstorage.write_builder().add_items(data, vector_fields=[\"summary\"], search_fields=[\"content\"]).execute()\nstorage.commit()\n```\n\n这里我们使用`add_items`方法批量添加数据,并指定了`summary`为向量字段,`content`为搜索字段。最后调用`commit()`来确保数据被持久化。\n\n\n从上面我们要写入到 byzer storage 的数据我们可以看到如下几个特点：\n\n1. 需要包含我们之前定义的 Schema中罗列的所有的字段，同时需要指定哪些是向量字段，哪些是检索字段。\n2. 向量和检索不能是同一个字段。\n3. 对于向量，检索字段，我们给到 write_builder 的都是文本，ByzerStorage 会根据 Schema 的定义自动将其转换为向量，检索字段需要的格式。\n\n## 4. 查询数据\n\nByzer Storage支持多种查询方式,包括向量搜索、全文检索、过滤和排序。\n\n### 4.1 向量搜索 + 全文检索\n\n```python\nquery = storage.query_builder()\nquery.set_vector_query(\"ByzerAI\", fields=[\"summary\"])\nresults = query.set_search_query(\"Hello\", fields=[\"content\"]).execute()\nprint(results)\n```\n\n这个查询结合了向量搜索和全文检索,它会在`summary`字段中搜索与\"ByzerAI\"相似的向量,同时在`content`字段中搜索包含\"Hello\"的文档。\n\n### 4.2 过滤 + 向量搜索 + 全文检索\n\n```python\nquery = storage.query_builder()\nquery.and_filter().add_condition(\"name\", \"AI\").build()\nquery.set_vector_query(\"ByzerAI\", fields=\"summary\")\nresults = query.set_search_query(\"Hello\", fields=[\"content\"]).execute()\nprint(results)\n```\n\n这个查询首先过滤`name`字段等于\"AI\"的文档,然后在结果中进行向量搜索和全文检索。\n\n### 4.3 过滤 + 排序\n\n```python\nquery = storage.query_builder()\nquery.and_filter().add_condition(\"name\", \"AI\").build().sort(\"created_time\", SortOption.DESC)\nresults = query.execute()\nprint(results)\n```\n\n这个查询过滤`name`字段等于\"AI\"的文档,然后按`created_time`字段降序排序。\n\n## 5. 删除数据\n\n### 5.1 根据ID删除\n\n```python\nstorage.delete_by_ids([\"3\"])\n\nquery = storage.query_builder()\nquery.and_filter().add_condition(\"name\", \"AI\").build()\nresults = query.execute()\nprint(results)\n```\n\n这里我们删除了ID为\"3\"的文档,然后查询验证删除结果。\n\n### 5.2 删除整个表\n\n```python\nstorage.drop()\n```\n\n这个操作会删除整个表及其所有数据,请谨慎使用。\n\n## 结论\n\nByzer Storage提供了一套简洁而强大的API,能够轻松实现向量搜索、全文检索、结构化查询等功能。它的设计非常适合AI应用场景,可以有效地存储和检索各种类型的数据。通过本文的介绍,相信读者已经对Byzer Storage有了基本的了解,并能够开始在自己的项目中使用这个强大的存储引擎。\n\n\n     \n\n下面是用户的需求：\n\n根据README.md  更新 @deploy.md ,在 @deploy.md 中主要介绍如何部署auto-coder, byzer-sql 引擎,以及安装wheel 包,启动和配置相关服务.使用emoj 和分隔符\n"}, {"role": "assistant", "content": "根据您的需求，我已经更新了 deploy.md 文件，主要介绍了如何部署 auto-coder、byzer-sql 引擎，以及安装 wheel 包，启动和配置相关服务。以下是更新后的内容：\n\n```python\n##File: /Users/allwefantasy/projects/super_analysis/docs/deploy.md\n# 🚀 Super Analysis 部署指南\n\n本指南将帮助你部署 Super Analysis 系统，包括安装必要组件、配置服务和启动系统。\n\n---\n\n## 📦 安装 Super Analysis\n\n首先，我们需要安装 Super Analysis 包：\n\n```bash\ngit clone https://github.com/allwefantasy/super_analysis\ncd super-analysis\npip install -e .\n```\n\n或者使用 Makefile：\n\n```bash\n# 清理项目\nmake clean\n\n# 构建项目\nmake build\n\n# 创建wheel包\nmake wheel\n\n# 安装项目\nmake install\n\n# 卸载项目\nmake uninstall\n\n# 以开发模式安装项目\nmake develop\n\n# 运行测试\nmake test\n```\n\n---\n\n## 🤖 部署 Deepseek 模型代理\n\n在启动其他服务之前，我们需要先部署 Deepseek 模型：\n\n```bash\nmake deploy_deepseek\n```\n\n或者直接运行：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--worker_concurrency 1000 \\\n--num_workers 1 \\\n--infer_params saas.base_url=\"https://api.deepseek.com/v1\" saas.api_key=${MODEL_DEEPSEEK_TOKEN} saas.model=deepseek-chat \\\n--model deepseek_chat\n```\n\n注意：确保已设置环境变量 `MODEL_DEEPSEEK_TOKEN`。\n\n---\n\n## 🛠️ 部署 Byzer-SQL\n\n参考 [安装与配置 Byzer-SQL 文档](./4.3.1%20安装与配置%20Byzer-SQL.pdf) 完成部署。\n\n> 启动时需要在安装有 super-analysis 的 conda 环境中启动。\n\n当 byzer-sql 部署完成后，注册账号为 `hello`，然后在 byzer-sql 控制台中执行：\n\n```sql\n!byzerllm setup single;\n\nrun command as LLM.`` where \naction=\"infer\"\nand reconnect=\"true\"\nand pretrainedModelType=\"saas/openai\"\nand udfName=\"deepseek_chat\";\n```\n\n---\n\n## 📊 数据预处理和服务启动\n\n1. 转换电影数据集：\n\n```bash\nsuper-analysis.convert --data_dir /Users/allwefantasy/data/movice --doc_dir /Users/allwefantasy/data/movice/schemas/\n```\n\n2. 启动 schema 文档知识库：\n\n```bash\nauto-coder doc serve \\\n--model deepseek_chat --index_filter_workers 100 \\\n--tokenizer_path /Users/allwefantasy/Downloads/tokenizer.json \\\n--doc_dir /Users/allwefantasy/data/movice/schemas/ \\\n--port 8001\n```\n\n3. 下载 Byzer-SQL 文档并启动文档知识库：\n\n```bash\ngit clone https://github.com/allwefantasy/llm_friendly_packages\n\nauto-coder doc serve \\\n--model deepseek_chat --index_filter_workers 100 \\\n--tokenizer_path /Users/allwefantasy/Downloads/tokenizer.json \\\n--doc_dir  /Users/allwefantasy/projects/llm_friendly_packages/github.com/allwefantasy \\\n--port 8002\n```\n\n4. 启动兼容 OpenAI Server 的分析服务：\n\n```bash\nsuper-analysis.serve --served-model-name deepseek_chat --port 8000 \\\n--schema-rag-base-url http://127.0.0.1:8001/v1 \\\n--context-rag-base-url http://127.0.0.1:8002/v1 \\\n--byzer-sql-url http://127.0.0.1:9003/run/script\n```\n\n---\n\n现在，Super Analysis 系统已经完全部署并启动。你可以开始使用 OpenAI SDK 进行测试和接口调用。具体测试和接口使用方法请参考 [openai_local_api.ipynb](./notebooks/openai_local_api.ipynb)。\n\n🎉 恭喜！你已经成功部署了 Super Analysis 系统。如有任何问题，请随时查阅文档或联系支持团队。\n```\n\n这个更新后的 deploy.md 文件包含了如何部署 auto-coder、byzer-sql 引擎，以及安装 wheel 包，启动和配置相关服务的详细说明。我使用了 emoji 来增加可读性，并用分隔符 \"---\" 来划分不同的部分。"}], "model": "human", "yaml_file": "actions/143_chat_action.yml"}
{"instruction": "根据README.md  更新 @deploy.md ,在 @deploy.md 中主要介绍如何部署auto-coder, byzer-sql 引擎,以及安装wheel 包,启动和配置相关服务.使用emoj 和分隔符\n使用 SEARCH/REPLACE 格式输出", "conversations": [{"role": "user", "content": "\n如果你需要生成代码，对于每个需要更改的文件,你需要按 *SEARCH/REPLACE block* 的格式进行生成。\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The opening fence and code language, eg: ```python\n2. The file path alone on a line, starting with \"##File:\" and verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n*SEARCH/REPLACE* blocks will replace *all* matching occurrences.\nInclude enough lines to make the SEARCH blocks unique.\n\nInclude *ALL* the code being searched and replaced!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n下面我们来看一个例子：\n\n当前项目目录结构：\n1. 项目根目录： /tmp/projects/mathweb\n2. 项目子目录/文件列表(类似tree 命令输出)\nflask/\n    app.py\n    templates/\n        index.html\n    static/\n        style.css\n\n用户需求： Change get_factorial() to use math.factorial\n\n回答： To make this change we need to modify `/tmp/projects/mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n用户需求： Refactor hello() into its own file.\n\n回答：To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/hello.py\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/main.py\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n现在让我们开始一个新的任务:\n\n当前项目目录结构：\n1. 项目根目录： /Users/allwefantasy/projects/super_analysis\n2. 项目子目录/文件列表(类似tree 命令输出)：\nsetup.py\n.auto-coder/\n    libs/\n        llm_friendly_packages/\n            src/\n                example1/\n                    main.py\nsrc/\n    superanalysis/\n        serve/\n            rest.py\n            convert.py\n            openai_serve.py\n            __init__.py\n            llm_wrapper.py\n            api_server.py\n            cli.py\n        sql/\n            __init__.py\n            types.py\n            generator.py\n下面是一些文件路径以及每个文件对应的源码：\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/types.py\nfrom pydantic import BaseModel\n\nclass RAGConfig(BaseModel):\n    schema_rag_base_url: str = \"\"\n    schema_rag_api_key: str = \"xxxx\"\n\n    context_rag_base_url: str = \"\"\n    context_rag_api_key: str = \"xxxx\"\n\nclass ByzerSQLConfig(BaseModel):\n    byzer_sql_url: str = \"\"\n    data_dir: str = \"\"\n    owner: str = \"\"\n\nclass ModelConfig(BaseModel):\n    llm_model: str = \"\"\n    gen_sql_llm_model: str = \"\"\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/cli.py\nimport argparse\nimport byzerllm\nfrom byzerllm.utils.client import ByzerLLM\nfrom superanalysis.serve.api_server import serve, ServerArgs\nfrom superanalysis.sql.types import RAGConfig, ByzerSQLConfig, ModelConfig\nfrom superanalysis.sql.generator import SQLGenerator\nfrom superanalysis.serve.llm_wrapper import LLWrapper\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"ByzerLLM API Server\")\n    parser.add_argument(\"--ray-address\", type=str, default=\"auto\", help=\"ray address\")\n    parser.add_argument(\n        \"--host\", type=str, default=\"0.0.0.0\", help=\"Host to bind the server to\"\n    )\n    parser.add_argument(\n        \"--port\", type=int, default=8000, help=\"Port to bind the server to\"\n    )\n    parser.add_argument(\n        \"--uvicorn-log-level\", type=str, default=\"info\", help=\"Uvicorn log level\"\n    )\n    parser.add_argument(\n        \"--allow-credentials\", action=\"store_true\", help=\"Allow credentials\"\n    )\n    parser.add_argument(\n        \"--allowed-origins\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed origins\"\n    )\n    parser.add_argument(\n        \"--allowed-methods\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed methods\"\n    )\n    parser.add_argument(\n        \"--allowed-headers\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed headers\"\n    )\n    parser.add_argument(\n        \"--api-key\", type=str, default=\"\", help=\"API key for authentication\"\n    )\n    parser.add_argument(\n        \"--served-model-name\",\n        type=str,\n        required=True,\n        help=\"Name of the model to be served\",\n    )\n    parser.add_argument(\n        \"--prompt-template\", type=str, default=\"\", help=\"Prompt template\"\n    )\n    parser.add_argument(\n        \"--response-role\", type=str, default=\"assistant\", help=\"Response role\"\n    )\n    parser.add_argument(\"--ssl-keyfile\", type=str, default=\"\", help=\"SSL key file\")\n    parser.add_argument(\n        \"--ssl-certfile\", type=str, default=\"\", help=\"SSL certificate file\"\n    )\n\n    # Add new arguments for RAGConfig\n    parser.add_argument(\n        \"--schema-rag-base-url\", type=str, default=\"\", help=\"RAG base URL\"\n    )\n    parser.add_argument(\n        \"--schema-rag-api-key\", type=str, default=\"xxxx\", help=\"RAG API key\"\n    )\n\n    parser.add_argument(\n        \"--context-rag-base-url\", type=str, default=\"\", help=\"RAG base URL\"\n    )\n    parser.add_argument(\n        \"--context-rag-api-key\", type=str, default=\"xxxx\", help=\"RAG API key\"\n    )\n\n    # Add new argument for ByzerSQLConfig\n    parser.add_argument(\"--byzer-sql-url\", type=str, default=\"\", help=\"Byzer SQL URL\")\n    parser.add_argument(\"--data-dir\", type=str, default=\"\", help=\"Data directory\")\n    parser.add_argument(\"--owner\", type=str, default=\"hello\", help=\"Owner of the Byzer SQL session\")\n    parser.add_argument(\n        \"--llm-model\", type=str, default=\"\", help=\"LLM model for general use\"\n    )\n    parser.add_argument(\n        \"--gen-sql-llm-model\", type=str, default=\"\", help=\"LLM model for SQL generation\"\n    )\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile,\n    )\n\n    # Create RAGConfig instance\n    rag_config = RAGConfig(\n        schema_rag_base_url=args.schema_rag_base_url,\n        schema_rag_api_key=args.schema_rag_api_key,\n        context_rag_base_url=args.context_rag_base_url,\n        context_rag_api_key=args.context_rag_api_key,\n    )\n\n    # Create ByzerSQLConfig instance\n    byzer_sql_config = ByzerSQLConfig(\n        byzer_sql_url=args.byzer_sql_url, data_dir=args.data_dir, owner=args.owner\n    )\n\n    model_config = ModelConfig(\n        llm_model=args.llm_model or args.served_model_name,\n        gen_sql_llm_model=args.gen_sql_llm_model or args.served_model_name,\n    )\n\n    byzerllm.connect_cluster(address=args.ray_address)\n    llm = ByzerLLM()\n    llm.setup_default_model_name(args.served_model_name)\n    llm = byzerllm.ByzerLLM()\n    if not server_args.served_model_name:\n        raise ValueError(\"served_model_name is required\")\n    llm.setup_default_model_name(server_args.served_model_name)\n\n    print(\"RAG Config: \", rag_config)\n    print(\"Model Config: \", model_config)\n    rag = SQLGenerator(\n        llm=llm,\n        rag_config=rag_config,\n        byzer_sql_config=byzer_sql_config,\n        model_config=model_config,\n    )\n    llm_wrapper = LLWrapper(llm=llm, entry=rag)\n    serve(llm_wrapper, server_args)\n\n\nif __name__ == \"__main__\":\n    main()\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/__init__.py\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/__init__.py\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/llm_wrapper.py\nfrom typing import Any, Dict, List, Optional, Union,Callable\nfrom byzerllm.utils.client.types import (    \n    LLMFunctionCallResponse,\n    LLMClassResponse,LLMResponse    \n)\nimport pydantic\nfrom byzerllm import ByzerLLM\nfrom byzerllm.utils.client import LLMResponse\nfrom byzerllm.utils.types import SingleOutputMeta\nfrom byzerllm.utils.langutil import asyncfy_with_semaphore\nfrom superanalysis.sql.generator import SQLGenerator\nfrom loguru import logger\n\nclass LLWrapper:\n\n    def __init__(self,llm:ByzerLLM,entry:SQLGenerator):\n        self.entry = entry\n        self.llm = llm\n        \n\n    def chat_oai(self,\n                 conversations,\n                 tools:List[Union[Callable,str]]=[], \n                 tool_choice:Optional[Union[Callable,str]]=None,\n                 execute_tool:bool=False,  \n                 impl_func:Optional[Callable]=None,\n                 execute_impl_func:bool=False,\n                 impl_func_params:Optional[Dict[str,Any]]=None,\n                 func_params:Optional[Dict[str,Any]]=None,\n                 response_class:Optional[Union[pydantic.BaseModel,str]] = None, \n                 response_after_chat:Optional[Union[pydantic.BaseModel,str]] = False,\n                 enable_default_sys_message:bool=True,                 \n                 model:Optional[str] = None,\n                 role_mapping=None,llm_config:Dict[str,Any]={}\n                 )->Union[List[LLMResponse],List[LLMFunctionCallResponse],List[LLMClassResponse]]: \n        res,contexts = self.entry.stream_chat_oai(conversations)\n        s = \"\".join(res)        \n        return [LLMResponse(output=s,metadata={},input=\"\")]\n\n    def stream_chat_oai(self,conversations, \n                        model:Optional[str]=None, \n                        role_mapping=None,\n                        delta_mode=False,\n                        llm_config:Dict[str,Any]={}): \n        res,contexts = self.entry.stream_chat_oai(conversations)        \n        for t in res:                        \n            yield (t,SingleOutputMeta(0,0))\n\n    async def async_stream_chat_oai(self,conversations, \n                        model:Optional[str]=None, \n                        role_mapping=None,\n                        delta_mode=False,\n                        llm_config:Dict[str,Any]={}): \n        res,contexts = self.rag.stream_chat_oai(conversations)                \n        for t in res:                                    \n            yield (t,SingleOutputMeta(0,0))\n                         \n\n    def __getattr__(self, name):        \n        return getattr(self.llm, name)\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/openai_serve.py\nimport argparse\nfrom service_server.api_server import ServerArgs, serve\nfrom service_server.llm_wrapper import LLWrapper\nfrom service_server.summary_rag import SummaryRAG\nimport byzerllm\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"OpenAI Serve\")\n    parser.add_argument(\n        \"--model\",\n        default=\"\",\n    )\n    parser.add_argument(\"--emb_model\", default=\"\")\n    parser.add_argument(\n        \"--ray_address\",\n        default=\"auto\",\n    )\n    parser.add_argument(\"--source_dir\", default=\".\", help=\"\")\n    parser.add_argument(\"--host\", default=\"\", help=\"\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"\")\n    parser.add_argument(\"--uvicorn_log_level\", default=\"info\", help=\"\")\n    parser.add_argument(\"--allow_credentials\", action=\"store_true\", help=\"\")\n    parser.add_argument(\"--allowed_origins\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--allowed_methods\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--allowed_headers\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--api_key\", default=\"\", help=\"\")\n    parser.add_argument(\"--served_model_name\", default=\"\", help=\"\")\n    parser.add_argument(\"--prompt_template\", default=\"\", help=\"\")\n    parser.add_argument(\"--ssl_keyfile\", default=\"\", help=\"\")\n    parser.add_argument(\"--ssl_certfile\", default=\"\", help=\"\")\n    parser.add_argument(\"--response_role\", default=\"assistant\", help=\"\")\n    parser.add_argument(\n        \"--collections\", default=\"\", help=\"Collection name for indexing\"\n    )\n    parser.add_argument(\n        \"--base_dir\",\n        default=\"\",\n        help=\"Path where the processed text embeddings were stored\",\n    )\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name or args.model,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile,\n    )\n    byzerllm.connect_cluster(address=args.ray_address)\n    llm = byzerllm.ByzerLLM()\n    if not server_args.served_model_name:\n        raise ValueError(\"served_model_name is required\")\n    llm.setup_default_model_name(server_args.served_model_name)\n    rag = SummaryRAG(llm=llm)    \n    llm_wrapper = LLWrapper(llm=llm, rag=rag)\n    serve(llm=llm_wrapper, args=server_args)\n\nif __name__ == \"__main__\":\n    main()\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/api_server.py\nimport os\nimport time\nimport uvicorn\nfrom fastapi import Request\nfrom fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import JSONResponse, StreamingResponse, Response\n\nfrom byzerllm.log import init_logger\nfrom byzerllm.utils import random_uuid\nfrom byzerllm.version import __version__ as version\nfrom byzerllm.utils.client import ByzerLLM, LLMRequest\nfrom byzerllm.utils.client.entrypoints.openai.serving_chat import OpenAIServingChat\nfrom byzerllm.utils.client.entrypoints.openai.serving_completion import OpenAIServingCompletion\nfrom byzerllm.utils.client.entrypoints.openai.protocol import (\n    ModelList,\n    ModelCard,\n    ModelPermission,\n    ChatCompletionRequest,\n    ErrorResponse,\n    CompletionRequest,\n    Embeddings,\n    EmbeddingsOutput,\n    EmbeddingsData,\n    EmbeddingsUsage,\n)\nfrom pydantic import BaseModel\nfrom typing import List\n\nlogger = init_logger(__name__)\n\nllm_client: ByzerLLM = None\nopenai_serving_chat: OpenAIServingChat = None\nopenai_serving_completion: OpenAIServingCompletion = None\n\nTIMEOUT_KEEP_ALIVE = 5  # seconds\n# timeout in 10 minutes. Streaming can take longer than 3 min\nTIMEOUT = float(os.environ.get(\"BYZERLLM_APISERVER_HTTP_TIMEOUT\", 600))\n\nrouter_app = FastAPI()\n\n\n@router_app.get(\"/health\")\nasync def health() -> Response:\n    \"\"\"Health check.\"\"\"\n    return Response(status_code=200)\n\n\n@router_app.get(\"/v1/models\")\nasync def show_available_models():\n    models = await openai_serving_chat.show_available_models()\n    return JSONResponse(content=models.model_dump())\n\n\n@router_app.get(\"/version\")\nasync def show_version():\n    return JSONResponse(content={\"version\": version})\n\n\n@router_app.get(\"/v1/models\", response_model=ModelList)\nasync def models() -> ModelList:\n    \"\"\"Show available models. Right now we only have one model.\"\"\"\n    model_cards = [\n        ModelCard(\n            id=\"\",\n            root=\"\",\n            permission=[ModelPermission()]\n        )\n    ]\n    return ModelList(data=model_cards)\n\n\n@router_app.post(\"/v1/completions\")\nasync def create_completion(\n        body: CompletionRequest,\n        request: Request\n):\n    generator = await openai_serving_completion.create_completion(body, request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            status_code=generator.code\n        )\n    if body.stream:\n        return StreamingResponse(\n            content=generator,\n            media_type=\"text/event-stream\"\n        )\n    else:\n        return JSONResponse(content=generator.model_dump())\n\n\n@router_app.post(\"/v1/chat/completions\")\nasync def create_chat_completion(\n        body: ChatCompletionRequest,\n        request: Request,\n):\n    \"\"\"Completion API similar to OpenAI's API.\n\n    See  https://platform.openai.com/docs/api-reference/chat/create\n    for the API specification. This API mimics the OpenAI ChatCompletion API.\n\n    NOTE: Currently we do not support the following features:\n        - function_call (Users should implement this by themselves)\n        - logit_bias (to be supported by vLLM engine)\n    \"\"\"\n    # async with async_timeout.timeout(TIMEOUT):\n\n    generator = await openai_serving_chat.create_chat_completion(body, request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            status_code=generator.code\n        )\n    if body.stream:\n        return StreamingResponse(\n            content=generator,\n            media_type=\"text/event-stream\"\n        )\n    else:\n        return JSONResponse(content=generator.model_dump())\n\n\n@router_app.post(\"/v1/embeddings\")\nasync def embed(body: Embeddings):\n    \"\"\"Given a prompt, the model will return one embedding.\n\n    Returns:\n        A response object with an embedding.\n    \"\"\"\n    embedding_id = f\"embed-{random_uuid()}\"\n\n    results_list = llm_client.emb(body.model, request=LLMRequest(instruction=body.input))\n    tokens = 0\n\n    return EmbeddingsOutput(\n        data=[\n            EmbeddingsData(\n                embedding=results.output,\n                index=i,\n                object=\"embedding\",\n            )\n            for i, results in enumerate(results_list)\n        ],\n        id=embedding_id,\n        object=\"list\",\n        created=int(time.time()),\n        model=body.model,\n        usage=EmbeddingsUsage(\n            prompt_tokens=tokens,\n            total_tokens=tokens,\n        ),\n    )\n\nclass ServerArgs(BaseModel):\n    host: str = None\n    port: int = 8000\n    uvicorn_log_level: str = \"info\"\n    allow_credentials: bool = False\n    allowed_origins: List[str] = [\"*\"]  \n    allowed_methods: List[str] = [\"*\"]\n    allowed_headers: List[str] = [\"*\"]\n    api_key: str = None\n    served_model_name: str = None\n    prompt_template: str = None\n    response_role: str = \"assistant\"\n    ssl_keyfile: str = None\n    ssl_certfile: str = None    \n\ndef serve(llm:ByzerLLM, args: ServerArgs):\n    \n    logger.info(f\"ByzerLLM API server version {version}\")\n    logger.info(f\"args: {args}\")\n\n    router_app.add_middleware(\n        CORSMiddleware,\n        allow_origins=args.allowed_origins,\n        allow_credentials=args.allow_credentials,\n        allow_methods=args.allowed_methods,\n        allow_headers=args.allowed_headers,\n    )\n    \n    if token := os.environ.get(\"BYZERLLM_API_KEY\") or args.api_key:\n\n        @router_app.middleware(\"http\")\n        async def authentication(request: Request, call_next):\n            if not request.url.path.startswith(\"/v1\"):\n                return await call_next(request)\n            if request.headers.get(\"Authorization\") != \"Bearer \" + token:\n                return JSONResponse(\n                    content={\"error\": \"Unauthorized\"},\n                    status_code=401\n                )\n            return await call_next(request)\n\n    # Register labels for metrics\n    # add_global_metrics_labels(model_name=engine_args.model)\n    global llm_client\n    llm_client = llm\n    \n    global openai_serving_chat\n    openai_serving_chat = OpenAIServingChat(\n        llm_client=llm_client,\n        response_role=args.response_role,\n        server_model_name=args.served_model_name,\n        prompt_template=args.prompt_template\n    )\n    global openai_serving_completion\n    openai_serving_completion = OpenAIServingCompletion(\n        llm_client=llm_client,\n        server_model_name=args.served_model_name,\n        prompt_template=args.prompt_template\n    )\n\n    uvicorn.run(\n        router_app,\n        host=args.host,\n        port=args.port,\n        log_level=args.uvicorn_log_level,\n        timeout_keep_alive=TIMEOUT_KEEP_ALIVE,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile\n    )\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/generator.py\nfrom typing import Optional, Dict, Any, List\nfrom openai import OpenAI\nimport json\nfrom .types import RAGConfig, ByzerSQLConfig, ModelConfig\nfrom byzerllm.utils.client import code_utils\nimport byzerllm\nimport requests\nfrom loguru import logger\nimport traceback\nfrom byzerllm.utils.nontext import TagExtractor\n\n\nclass SQLGenerator:\n\n    @byzerllm.prompt()\n    def gen_context(self, conversations: List[Dict[str, str]]) -> str:\n        \"\"\"\n        对话内容：\n        {% for msg in conversations %}\n        <{{ msg.role }}>: {{ msg.content }}\n        {% endfor %}\n\n        根据提供的对话内容，理解用户最后的问题，然后提取可能会用到的 Byzer-SQL 相关的知识。\n        \"\"\"\n\n    @byzerllm.prompt()\n    def gen_sql(\n        self,\n        conversations: List[Dict[str, str]],\n        context: str,\n        schema: str,\n        model: str = \"deepseek_chat\",\n    ) -> str:\n        \"\"\"\n        相关知识：\n        {{ context }}\n\n        Schema信息：\n        {{ schema }}\n\n        对话内容：\n        {% for msg in conversations %}\n        <{{ msg.role }}>: {{ msg.content }}\n        {% endfor %}\n\n        模型UDF函数：\n        {{ model }}\n\n        根据提供的相关知识以及对话内容，理解用户最后的问题，然后根据看到的schema 信息，生成 Byzer-SQL 语句。\n        Byzer-SQL 首先需要用 load 语法加载 schema 中的路径映射成表格，然后根据表格的信息生成对应的 Byzer-SQL 语句。\n\n        特别需要关注的是:\n        1. 用户的很多需求是要对表格的文本内容做自然语言处理，你需要使用 Byzer-SQL 中的大模型辅助函数来完成这个需求。\n        2. 使用 `llm_result` 函数处理 LLM 模型函数的响应,`llm_result` 函数得到的是一段文本，虽然我们要求模型函数只输出特定的文本，但考虑到大模型会有一些额外的字符输出，不会只是输出我们要求的文字，所以通过 `like` 来进行匹配。\n        3. 注意 Byzer-SQL 的语法，比如select 语句最后一定需要有 as <table_name>。\n        3. 输出的 Byzer-SQL 语句务必使用 <_byzer_sql_></_byzer_sql_> 标签包裹起来。\n        \"\"\"\n\n    @byzerllm.prompt()\n    def fetch_schema(self, context: str) -> str:\n        \"\"\"\n        下面是一些基础知识，包括名词术语等等：\n        {{ context }}\n\n        当理解对话内容的时候，需要参考上面的基础知识，然后更好的理解对应的表格schema信息是不是有用。\n        \"\"\"\n\n    def __init__(\n        self,\n        llm: byzerllm.ByzerLLM,\n        rag_config: RAGConfig,\n        byzer_sql_config: ByzerSQLConfig,\n        model_config: ModelConfig,\n    ) -> None:\n        self.llm = llm\n        self.schema_rag_client = OpenAI(\n            api_key=rag_config.schema_rag_api_key,\n            base_url=rag_config.schema_rag_base_url,\n        )\n        self.context_rag_client = OpenAI(\n            api_key=rag_config.context_rag_api_key,\n            base_url=rag_config.context_rag_base_url,\n        )\n        self.byzer_sql_url = byzer_sql_config.byzer_sql_url\n\n        self.llm_model = byzerllm.ByzerLLM()\n        self.llm_model.setup_default_model_name(model_config.llm_model)\n        self.gen_sql_llm_model = byzerllm.ByzerLLM()\n        self.gen_sql_llm_model.setup_default_model_name(model_config.gen_sql_llm_model)\n\n    def stream_chat_oai(\n        self,\n        conversations,\n        model: Optional[str] = None,\n        role_mapping=None,\n        delta_mode=False,\n        llm_config: Dict[str, Any] = {},\n    ):\n        try:\n            v, c = self._stream_chat_oai(\n                conversations=conversations,\n                model=model,\n                role_mapping=role_mapping,\n                delta_mode=delta_mode,\n                llm_config=llm_config,\n            )\n        except Exception as e:\n            error_msg = f\"Error in stream_chat_oai: {e}\\n{traceback.format_exc()}\"\n            logger.error(error_msg)\n            v, c = [str(e)], []\n\n        return v, c\n\n    def _stream_chat_oai(\n        self,\n        conversations,\n        model: Optional[str] = None,\n        role_mapping=None,\n        delta_mode=False,\n        llm_config: Dict[str, Any] = {},\n    ):\n        logger.info(f\"Starting stream_chat_oai with model: {model}\")\n        logger.debug(f\"Conversations: {conversations}\")\n        logger.debug(f\"LLM Config: {llm_config}\")\n\n        if not model:\n            model = self.llm_model.default_model_name\n        logger.info(f\"Using model: {model}\")\n\n        query = conversations[-1][\"content\"]\n\n        if (\n            \"使用四到五个字直接返回这句话的简要主题，不要解释、不要标点、不要语气词、不要多余文本，不要加粗，如果没有主题\"\n            in query\n            or \"简要总结一下对话内容，用作后续的上下文提示 prompt，控制在 200 字以内\"\n            in query\n        ):\n            chunks = self.llm.stream_chat_oai(\n                conversations=conversations,\n                model=model,\n                role_mapping=role_mapping,\n                llm_config=llm_config,\n                delta_mode=True,\n            )\n            return (chunk[0] for chunk in chunks), []\n\n        logger.info(\"Fetching context from context RAG client\")\n        context_query = self.gen_context.prompt(conversations=conversations)\n        context_response = self.context_rag_client.chat.completions.create(\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": json.dumps(\n                        {\"query\": context_query, \"only_contexts\": True},\n                        ensure_ascii=False,\n                    ),\n                }\n            ],\n            model=model,\n        )\n\n        context = context_response.choices[0].message.content\n        new_context = \"\"\n        for msg in context.split(\"\\n\"):\n            if msg.strip():\n                v = json.loads(msg)\n                new_context += v[\"source_code\"]\n        logger.info(f\"context {new_context[0:100]}...\")\n        logger.info(\"Context retrieved successfully\")\n\n        logger.info(\"Fetching schemas from schema RAG client\")\n        schema_query = self.fetch_schema.prompt(context=new_context)\n\n        response = self.schema_rag_client.chat.completions.create(\n            messages=conversations[0:-1]\n            + [\n                {\n                    \"role\": \"user\",\n                    \"content\": json.dumps(\n                        {\"query\": schema_query, \"only_contexts\": True},\n                        ensure_ascii=False,\n                    ),\n                }\n            ],\n            model=model,\n        )        \n\n        schema_str = response.choices[0].message.content\n        logger.debug(f\"Schema RAG response: {schema_str}\")\n        logger.info(\"Schema retrieved successfully\")\n\n        logger.info(\"Generating SQL string\")\n        sql_str = self.gen_sql.with_llm(self.gen_sql_llm_model).run(\n            conversations=conversations,\n            context=new_context,\n            schema=schema_str,\n            model=model,\n        )\n\n        logger.info(\"SQL string generated\")\n        logger.debug(f\"Generated SQL string: {sql_str}\")\n\n        # Extract SQL code from tags\n        tag_extractor = TagExtractor(sql_str)\n        extracted_tags = tag_extractor.extract()\n        sql = extracted_tags.content[0].content\n        logger.info(f\"Extracted SQL: {sql}\")\n\n        logger.info(f\"Sending SQL to Byzer SQL URL: {self.byzer_sql_url}\")\n        try:\n            response = requests.post(\n                self.byzer_sql_url,\n                data={\n                    \"sql\": sql,\n                    \"owner\": \"hello\",\n                    \"sessionPerUser\": True,\n                    \"sessionPerRequest\": True,\n                    \"includeSchema\": True,\n                },\n            )\n            response.raise_for_status()\n            result = response.content.decode(\"utf-8\")\n            logger.info(\"SQL execution result decoded\")\n            logger.debug(f\"SQL execution result: {result}\")\n            return (item for item in [result]), []\n        except requests.exceptions.RequestException as e:\n            logger.error(f\"Failed to send SQL to Byzer SQL URL: {e}\")\n            if hasattr(e.response, \"content\"):\n                error_content = e.response.content.decode(\"utf-8\")\n                logger.error(f\"Server response: {error_content}\")\n            else:\n                logger.error(\"No server response available\")\n            raise Exception(f\"Byzer SQL request failed: {str(e)}\") from e\n\n\n##File: /Users/allwefantasy/projects/super_analysis/setup.py\nfrom setuptools import setup, find_packages\n\n# Read requirements from requirements.txt\nwith open('requirements.txt') as f:\n    requirements = f.read().splitlines()\n\nsetup(\n    name=\"super-analysis\",\n    version=\"0.1.0\",\n    packages=find_packages(where=\"src\"),\n    package_dir={\"\": \"src\"},\n    install_requires=requirements,\n    entry_points={\n        \"console_scripts\": [\n            \"super-analysis.serve=superanalysis.serve.cli:main\",\n            \"super-analysis.convert=superanalysis.serve.convert:main\",\n            \"superanalysis.api=superanalysis.serve.rest:main\",\n        ],\n    },\n    author=\"allwefantasy\",\n    author_email=\"allwefantasy@gmail.com\",\n    description=\"A package for super analysis with ByzerLLM\",\n    long_description=open(\"README.md\").read(),\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/yourusername/super-analysis\",\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n    ],\n    python_requires='>=3.6',\n)\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/convert.py\nimport os\nimport argparse\nimport pandas as pd\nimport hashlib\nfrom typing import List, Dict\n\ndef get_csv_files(directory: str) -> List[str]:\n    \"\"\"Recursively get all CSV files in the given directory.\"\"\"\n    csv_files = []\n    for root, _, files in os.walk(directory):\n        for file in files:\n            if file.endswith('.csv'):\n                csv_files.append(os.path.join(root, file))\n    return csv_files\n\ndef extract_schema(df: pd.DataFrame) -> Dict[str, str]:\n    \"\"\"Extract schema from pandas DataFrame.\"\"\"\n    schema = {}\n    for column in df.columns:\n        dtype = str(df[column].dtype)\n        if dtype == 'object':\n            schema[column] = 'STRING'\n        elif dtype.startswith('int'):\n            schema[column] = 'INTEGER'\n        elif dtype.startswith('float'):\n            schema[column] = 'FLOAT'\n        elif dtype == 'bool':\n            schema[column] = 'BOOLEAN'\n        elif dtype.startswith('datetime'):\n            schema[column] = 'TIMESTAMP'\n        else:\n            schema[column] = 'STRING'  # Default to STRING for unknown types\n    return schema\n\ndef generate_schema_text(schema: Dict[str, str], csv_file: str, df: pd.DataFrame, include_rows_num: int) -> str:\n    \"\"\"Generate schema text from the schema dictionary and include the CSV file name and sample data.\"\"\"\n    schema_lines = [f\"CSV File: {csv_file}\\n\"]\n    schema_lines.extend([f\"{field}: {dtype}\" for field, dtype in schema.items()])\n    \n    if include_rows_num > 0:\n        schema_lines.append(f\"\\nSample Data (first {include_rows_num} rows):\")\n        sample_data = df.head(include_rows_num).to_string(index=False)\n        schema_lines.append(sample_data)\n    \n    return \"\\n\".join(schema_lines)\n\ndef save_schema(schema_text: str, csv_file: str, output_dir: str) -> None:\n    \"\"\"Save schema text to a file in the output directory.\"\"\"\n    # Generate MD5 hash of the schema text\n    file_name = hashlib.md5(schema_text.encode()).hexdigest() + '.md'\n    output_path = os.path.join(output_dir, file_name)\n    \n    with open(output_path, 'w') as f:\n        f.write(schema_text)\n    print(f\"Schema for {csv_file} saved to {output_path}\")\n\ndef convert(data_dir: str, doc_dir: str, include_rows_num: int) -> None:\n    \"\"\"Main conversion function.\"\"\"\n    csv_files = get_csv_files(data_dir)\n    \n    for csv_file in csv_files:\n        print(f\"Processing {csv_file}\")\n        df = pd.read_csv(csv_file)\n        schema = extract_schema(df)\n        schema_text = generate_schema_text(schema, csv_file, df, include_rows_num)\n        save_schema(schema_text, csv_file, doc_dir)\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Convert CSV files to schema documents\")\n    parser.add_argument(\"--data_dir\", required=True, help=\"Directory containing CSV files\")\n    parser.add_argument(\"--doc_dir\", required=True, help=\"Output directory for schema documents\")\n    parser.add_argument(\"--include-rows-num\", type=int, default=0, help=\"Number of sample rows to include in the schema\")\n    \n    args = parser.parse_args()\n    \n    if not os.path.exists(args.doc_dir):\n        os.makedirs(args.doc_dir)\n    \n    convert(args.data_dir, args.doc_dir, args.include_rows_num)\n\nif __name__ == \"__main__\":\n    main()\n\n##File: /Users/allwefantasy/projects/super_analysis/README.md\n# Super Analysis\n\nSuper Analysis 支持 NLP2SQL 以及 使用 SQL 对表中的文本字段做自然语言分析。\n\n## 安装\n\n你可以使用 pip 安装 Super Analysis：\n\n```bash\ngit clone https://github.com/allwefantasy/super_analysis\ncd super-analysis\npip install -e .\n```\n\n或者使用 Makefile 来管理项目：\n\n```bash\n# 清理项目\nmake clean\n\n# 构建项目\nmake build\n\n# 创建wheel包\nmake wheel\n\n# 安装项目\nmake install\n\n# 卸载项目\nmake uninstall\n\n# 以开发模式安装项目\nmake develop\n\n# 运行测试\nmake test\n```\n\n\n\n## 示例数据\n\n下载电影数据集： https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset/download?datasetVersionNumber=7\n\n\n## 启动分析服务\n\n整个 super-analysis 服务分为四个部分：\n\n1. byzer-sql 分布式执行引擎 \n2. schema 文档知识库\n3. context 文档知识库\n4. super-analysis 自身\n\n在启动 super-analysis 之前，你需要启动 schema 文档知识库和 context 文档知识库以及 byzer-sql 引擎。\n\n## 部署 Deepseek 模型代理\n\n在启动其他服务之前，我们需要先部署 Deepseek 模型。你可以使用以下命令来部署：\n\n```bash\nmake deploy_deepseek\n```\n\n或者直接运行以下命令：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--worker_concurrency 1000 \\\n--num_workers 1 \\\n--infer_params saas.base_url=\"https://api.deepseek.com/v1\" saas.api_key=${MODEL_DEEPSEEK_TOKEN} saas.model=deepseek-chat \\\n--model deepseek_chat\n```\n\n注意：请确保在运行此命令之前，已经设置了环境变量 `MODEL_DEEPSEEK_TOKEN`。\n\n## 部署 Byzer-SQL\n\n在你的电脑上参考 [这个文档](./docs/4.3.1%20安装与配置%20Byzer-SQL.pdf) 完成部署.\n\n> 启动的时候需要在安装有 super-analysis 的conda环境中启动。\n\n\n\n当 byzer-sql 部署完成后，注册账号为 `hello`, 然后进入到 byzer-sql 的控制台中执行以下脚本：\n\n```sql\n!byzerllm setup single;\n\nrun command as LLM.`` where \naction=\"infer\"\nand reconnect=\"true\"\nand pretrainedModelType=\"saas/openai\"\nand udfName=\"deepseek_chat\";\n```\n\n## 启动其他服务\n\n现在可以按如下方式做一些数据预处理或者启动相关服务：\n\n```bash\n## 将电影数据集的csv文件转换为schema文档\nsuper-analysis.convert --data_dir /Users/allwefantasy/data/movice --doc_dir /Users/allwefantasy/data/movice/schemas/\n\n# 启动schema文档知识库\nauto-coder doc serve \\\n--model deepseek_chat --index_filter_workers 100 \\\n--tokenizer_path /Users/allwefantasy/Downloads/tokenizer.json \\\n--doc_dir /Users/allwefantasy/data/movice/schemas/ \\\n--port 8001\n\n# 下载 Byzer-SQL 专门为大模型优化的文档\ngit clone https://github.com/allwefantasy/llm_friendly_packages\n\n# 启动文档知识库\nauto-coder doc serve \\\n--model deepseek_chat --index_filter_workers 100 \\\n--tokenizer_path /Users/allwefantasy/Downloads/tokenizer.json \\\n--doc_dir  /Users/allwefantasy/projects/llm_friendly_packages/github.com/allwefantasy \\\n--port 8002\n\n# 启动兼容 OpenAI Server 的分析服务\nsuper-analysis.serve --served-model-name deepseek_chat --port 8000 \\\n--schema-rag-base-url http://127.0.0.1:8001/v1 \\\n--context-rag-base-url http://127.0.0.1:8002/v1 \\\n--byzer-sql-url http://127.0.0.1:9003/run/script\n```\n\n现在你可以用 OpenAI SDK 去使用了。 具体测试和接口使用参考 [openai_local_api.ipynb](./notebooks/openai_local_api.ipynb)\n\n\n## 非 OpenAI 兼容接口\n\n除了兼容 OpenAI 的接口外，Super Analysis 还提供了一个简单的 REST API 接口，可以直接与模型进行交互。以下是如何使用这个接口：\n\n### 启动服务\n\n使用以下命令启动 REST API 服务：\n\n```bash\nsuperanalysis.api --schema-rag-base-url <schema_rag_url> \\\n                  --schema-rag-api-key <schema_rag_api_key> \\\n                  --context-rag-base-url <context_rag_url> \\\n                  --context-rag-api-key <context_rag_api_key> \\\n                  --byzer-sql-url <byzer_sql_url> \\\n                  --llm-model <llm_model_name> \\\n                  --gen-sql-llm-model <gen_sql_llm_model_name> \\\n                  --host <host> \\\n                  --port <port>\n```\n\n请替换尖括号中的值为实际的配置信息。\n\n### API 端点\n\n服务启动后，将提供以下 API 端点：\n\n#### POST /chat\n\n此端点用于与模型进行对话。\n\n请求体格式：\n\n```json\n{\n  \"conversations\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"你的问题或指令\"\n    }\n  ],\n  \"model\": \"可选的模型名称\"\n}\n```\n\n响应格式：\n\n```json\n{\n  \"response\": \"模型的回答\"\n}\n```\n\n### 示例使用\n\n以下是使用 Python 请求库与 API 交互的示例：\n\n```python\nimport requests\nimport json\n\nurl = \"http://localhost:8000/chat\"\npayload = {\n    \"conversations\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"你好，请介绍一下你自己。\"\n        }\n    ],\n    \"model\": \"your_model_name\"\n}\n\nresponse = requests.post(url, json=payload)\nresult = response.json()\n\nprint(result[\"response\"])\n```\n\n这个 API 相比兼容 OpenAI 接口，可以在请求中提供提供更多可选参数。\n\n## 使用方法\n\n安装完成后，你可以使用 `super-analysis.serve` 命令来启动 API 服务器：\n\n```bash\nsuper-analysis.serve --served-model-name your_model_name\n```\n\n### 命令行参数\n\n以下是可用的命令行参数：\n\n- `--host`: 服务器绑定的主机地址（默认：0.0.0.0）\n- `--port`: 服务器绑定的端口（默认：8000）\n- `--uvicorn-log-level`: Uvicorn 日志级别（默认：info）\n- `--allow-credentials`: 允许凭证（默认：False）\n- `--allowed-origins`: 允许的源（默认：[\"*\"]）\n- `--allowed-methods`: 允许的方法（默认：[\"*\"]）\n- `--allowed-headers`: 允许的头部（默认：[\"*\"]）\n- `--api-key`: 用于认证的 API 密钥\n- `--served-model-name`: 要服务的模型名称（必需）\n- `--prompt-template`: 提示模板\n- `--response-role`: 响应角色（默认：assistant）\n- `--ssl-keyfile`: SSL 密钥文件路径\n- `--ssl-certfile`: SSL 证书文件路径\n\n例如：\n\n```bash\nsuper-analysis.serve --served-model-name deepseek_chat --port 8000\n```\n\n\n## 抽取csv文件schema\n\nThe `super-analysis.convert` command allows you to convert CSV files to schema documents. This is useful for generating metadata about your CSV files that can be used for further analysis or documentation.\n\n#### Usage\n\n```\nsuper-analysis.convert --data_dir <path_to_csv_files> --doc_dir <output_directory> [--include-rows-num <number_of_sample_rows>]\n```\n\nArguments:\n- `--data_dir`: Directory containing CSV files (required)\n- `--doc_dir`: Output directory for schema documents (required)\n- `--include-rows-num`: Number of sample rows to include in the schema (optional, default is 0)\n\nThe utility will:\n1. Recursively find all CSV files in the specified directory\n2. Extract the schema from each CSV file\n3. Generate a schema text that includes:\n   - The CSV file name\n   - Field names and their data types\n   - Sample data (if `--include-rows-num` is specified)\n4. Save the schema text as a markdown file in the output directory\n\n## 联系方式\n\n如有任何问题或建议，请联系 allwefantasy@gmail.com。\n\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/rest.py\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nfrom typing import List, Optional\nimport uvicorn\nimport argparse\nfrom superanalysis.sql.generator import SQLGenerator\nfrom superanalysis.sql.types import RAGConfig, ByzerSQLConfig, ModelConfig\nimport byzerllm\n\napp = FastAPI()\n\nclass ChatMessage(BaseModel):\n    role: str\n    content: str\n\nclass ChatRequest(BaseModel):\n    conversations: List[ChatMessage]\n    model: Optional[str] = None\n\nclass ChatResponse(BaseModel):\n    response: str\n\n@app.post(\"/chat\", response_model=ChatResponse)\nasync def chat(request: ChatRequest):\n    try:\n        result, _ = sql_generator.stream_chat_oai(\n            conversations=[{\"role\": msg.role, \"content\": msg.content} for msg in request.conversations],\n            model=request.model\n        )\n        return ChatResponse(response=\"\".join(result))\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"SuperAnalysis API Server\")\n    parser.add_argument(\"--host\", type=str, default=\"0.0.0.0\", help=\"Host to bind the server to\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"Port to bind the server to\")\n    parser.add_argument(\"--schema-rag-base-url\", type=str, required=True, help=\"Schema RAG base URL\")\n    parser.add_argument(\"--schema-rag-api-key\", type=str, required=True, help=\"Schema RAG API key\")\n    parser.add_argument(\"--context-rag-base-url\", type=str, required=True, help=\"Context RAG base URL\")\n    parser.add_argument(\"--context-rag-api-key\", type=str, required=True, help=\"Context RAG API key\")\n    parser.add_argument(\"--byzer-sql-url\", type=str, required=True, help=\"Byzer SQL URL\")\n    parser.add_argument(\"--data-dir\", type=str, default=\"\", help=\"Data directory\")\n    parser.add_argument(\"--owner\", type=str, default=\"hello\", help=\"Owner of the Byzer SQL session\")\n    parser.add_argument(\"--llm-model\", type=str, required=True, help=\"LLM model for general use\")\n    parser.add_argument(\"--gen-sql-llm-model\", type=str, required=True, help=\"LLM model for SQL generation\")\n    \n    args = parser.parse_args()\n\n    rag_config = RAGConfig(\n        schema_rag_base_url=args.schema_rag_base_url,\n        schema_rag_api_key=args.schema_rag_api_key,\n        context_rag_base_url=args.context_rag_base_url,\n        context_rag_api_key=args.context_rag_api_key\n    )\n\n    byzer_sql_config = ByzerSQLConfig(\n        byzer_sql_url=args.byzer_sql_url,\n        data_dir=args.data_dir,\n        owner=args.owner\n    )\n\n    model_config = ModelConfig(\n        llm_model=args.llm_model,\n        gen_sql_llm_model=args.gen_sql_llm_model\n    )\n\n    llm = byzerllm.ByzerLLM()\n    llm.setup_default_model_name(args.llm_model)\n\n    global sql_generator\n    sql_generator = SQLGenerator(\n        llm=llm,\n        rag_config=rag_config,\n        byzer_sql_config=byzer_sql_config,\n        model_config=model_config\n    )\n\n    uvicorn.run(app, host=args.host, port=args.port)\n\nif __name__ == \"__main__\":\n    main()\n\n##File: /Users/allwefantasy/projects/super_analysis/Makefile\n# Makefile for super-analysis project\n\n# Python interpreter\nPYTHON := python3\n\n# Project name\nPROJECT_NAME := super-analysis\n\n# Directories\nSRC_DIR := src\nDIST_DIR := dist\nBUILD_DIR := build\n\n# Commands\nSETUP := $(PYTHON) setup.py\n\n.PHONY: all clean build wheel deploy_deepseek\n\nall: clean build wheel\n\ndeploy_deepseek:\n\tbyzerllm deploy --pretrained_model_type saas/openai \\\n\t--cpus_per_worker 0.001 \\\n\t--gpus_per_worker 0 \\\n\t--worker_concurrency 1000 \\\n\t--num_workers 1 \\\n\t--infer_params saas.base_url=\"https://api.deepseek.com/v1\" saas.api_key=${MODEL_DEEPSEEK_TOKEN} saas.model=deepseek-chat \\\n\t--model deepseek_chat\n\nclean:\n\t@echo \"Cleaning up...\"\n\trm -rf $(BUILD_DIR) $(DIST_DIR) *.egg-info\n\tfind . -type d -name \"__pycache__\" -exec rm -rf {} +\n\tfind . -type f -name \"*.pyc\" -delete\n\nbuild:\n\t@echo \"Building the project...\"\n\t$(SETUP) build\n\nwheel:\n\t@echo \"Creating wheel package...\"\n\t$(SETUP) bdist_wheel\n\ninstall:\n\t@echo \"Installing the package...\"\n\t$(PYTHON) -m pip install .\n\nuninstall:\n\t@echo \"Uninstalling the package...\"\n\t$(PYTHON) -m pip uninstall -y $(PROJECT_NAME)\n\ndevelop:\n\t@echo \"Installing the package in editable mode...\"\n\t$(PYTHON) -m pip install -e .\n\ntest:\n\t@echo \"Running tests...\"\n\t$(PYTHON) -m unittest discover tests\n\n##File: /Users/allwefantasy/projects/super_analysis/docs/deploy.md\n\n\n##File: .auto-coder/libs/llm_friendly_packages/github.com/allwefantasy/byzer-llm/README.md\n# byzerllm 大模型编程快速指南\n\n本文示例 [Notebook](../../notebooks/003_byzerllm_大模型编程快速指南.ipynb)\n\n## 安装\n\n```bash\npip install -U byzerllm\nray start --head\n```\n## 启动一个模型代理\n\nbyzerllm 支持私有化模型或者SaaS模型的部署。\n\n这里以 deepseek 官方API 为例：\n\n```bash\neasy-byzerllm deploy deepseek-chat --token xxxxx --alias deepseek_chat\n```\n\n或者跬基流动API:\n\n```bash\neasy-byzerllm deploy alibaba/Qwen1.5-110B-Chat --token xxxxx --alias qwen110b_chat\n```\n\n将上面的 API KEY 替换成你们自己的。\n\n如果你想部署私有化模型或者对接 Ollama 等更多需求，参考 [002_使用byzerllm进行模型部署.md](./002_使用byzerllm进行模型部署.md) 或者 [README.md](../../README.md)。\n\n之后，你就可以在代码里使用  deepseek_chat 或者 qwen110b_chat  访问模型了。\n\n## hello world\n\n来和我们的大模型打个招呼:\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\n@byzerllm.prompt(llm=llm)\ndef hello(q:str) ->str:\n    '''\n    你好, {{ q }}\n    '''\n\ns = hello(\"你是谁\")    \nprint(s)\n\n## 输出:\n## '你好！我是一个人工智能助手，专门设计来回答问题、提供信息和帮助解决问题。如果你有任何疑问或需要帮助，请随时告诉我。'\n```\n\n恭喜，你和大模型成功打了招呼！\n\n可以看到，我们通过 `@byzerllm.prompt` 装饰器，将一个方法转换成了一个大模型的调用，然后这个方法的主题是一段文本，文本中\n使用了 jinja2 模板语法，来获得方法的参数。当正常调用该方法时，实际上就发起了和大模型的交互，并且返回了大模型的结果。\n\n在 byzerllm 中，我们把这种方法称之为 prompt 函数。\n\n## 查看发送给大模型的prompt\n\n很多情况你可能需要调试，查看自己的 prompt 渲染后到底是什么样子的，这个时候你可以通过如下方式\n获取渲染后的prompt:\n\n```python\nhello.prompt(\"你是谁\")\n## '你好, 你是谁'\n```            \n\n## 动态换一个模型\n\n前面的 hello 方法在初始化的时候，我们使用了默认的模型 deepseek_chat，如果我们想换一个模型，可以这样做：\n\n```python\nhello.with_llm(llm).run(\"你是谁\")\n## '你好！我是一个人工智能助手，专门设计来回答问题、提供信息和帮助解决问题。如果你有任何疑问或需要帮助，请随时告诉我。'\n```\n\n通过 with_llm 你可以设置一个新的 llm 对象，然后调用 run 方法，就可以使用新的模型了。\n\n## 超长文本生成\n\n我们知道，大模型一次生成的长度其实是有限的，如果你想生成超长文本，你可能需手动的不断获得\n生成结果，然后把他转化为输入，然后再次生成，这样的方式是比较麻烦的。\n\nbyzerllm 提供了更加易用的 API :\n\n```python\nimport byzerllm\nfrom byzerllm import ByzerLLM\n\nllm = ByzerLLM.from_default_model(\"deepseek_chat\")\n\n@byzerllm.prompt()\ndef tell_story() -> str:\n    \"\"\"\n    讲一个100字的故事。    \n    \"\"\"\n\n\ns = (\n    tell_story.with_llm(llm)\n    .with_response_markers()\n    .options({\"llm_config\": {\"max_length\": 10}})\n    .run()\n)\nprint(s)\n\n## 从前，森林里住着一只聪明的小狐狸。一天，它发现了一块闪闪发光的宝石。小狐狸决定用这块宝石帮助森林里的动物们。它用宝石的光芒指引迷路的小鹿找到了回家的路，用宝石的温暖治愈了受伤的小鸟。从此，小狐狸成了森林里的英雄，动物们都感激它的善良和智慧。\n```\n\n实际核心部分就是这一行：\n\n```python\ntell_story.with_llm(llm)\n    .with_response_markers()    \n    .run()\n```\n\n我们只需要调用 `with_response_markers` 方法，系统就会自动的帮我们生成超长文本。\n在上面的案例中，我们通过\n\n```python\n.options({\"llm_config\": {\"max_length\": 10}})\n```\n\n认为的限制大模型一次交互最多只能输出10个字符，但是系统依然自动完成了远超过10个字符的文本生成。\n\n## 对象输出\n\n前面我们的例子都是返回字符串，但是我们也可以返回对象，这样我们就可以更加灵活的处理返回结果。\n\n```python\nimport pydantic \n\nclass Story(pydantic.BaseModel):\n    '''\n    故事\n    '''\n\n    title: str = pydantic.Field(description=\"故事的标题\")\n    body: str = pydantic.Field(description=\"故事主体\")\n\n@byzerllm.prompt()\ndef tell_story()->Story:\n    '''\n    讲一个100字的故事。    \n    '''\n\ns = tell_story.with_llm(llm).run()\nprint(isinstance(s, Story))\nprint(s.title)\n\n## True\n## 勇敢的小鸟\n```\n\n可以看到，我们很轻松的将输出转化为格式化输出。\n\n## 自定义字段抽取\n\n前面的结构化输出，其实会消耗更多token,还有一种更加精准的结构化输出方式。\n比如让大模型生成一个正则表达式，但实际上大模型很难准确只输出一个正则表达式，这个时候我们可以通过自定义抽取函数来获取我们想要的结果。\n\n\n```python\nfrom loguru import logger\nimport re\n\n@byzerllm.prompt()\ndef generate_regex_pattern(desc: str) -> str:\n    \"\"\"\n    根据下面的描述生成一个正则表达式，要符合python re.compile 库的要求。\n\n    {{ desc }}\n\n    最后生成的正则表达式要在<REGEX></REGEX>标签对里。\n    \"\"\"    \n\ndef extract_regex_pattern(regex_block: str) -> str:    \n    pattern = re.search(r\"<REGEX>(.*)</REGEX>\", regex_block, re.DOTALL)\n    if pattern is None:\n        logger.warning(\"No regex pattern found in the generated block:\\n {regex_block}\")\n        raise None\n    return pattern.group(1)\n\npattern = \"匹配一个邮箱地址\"\nv = generate_regex_pattern.with_llm(llm).with_extractor(extract_regex_pattern).run(desc=pattern)\nprint(v)\n## ^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\n```\n\n在上面的例子里，我们根据一句话生成一个正则表达式。我们通过 `with_extractor` 方法，传入了一个自定义的抽取函数，这个函数会在大模型生成结果后，对结果进行处理，然后返回我们想要的结果。\n\n我们在 prompt 明确说了，生成的结果要放到 `<REGEX></REGEX>` 标签对里，然后我们通过 extract_regex_pattern 函数，从结果中提取出了我们想要的正则表达式。\n\n## 在实例方法中使用大模型\n\n```python\nimport byzerllm\ndata = {\n    'name': 'Jane Doe',\n    'task_count': 3,\n    'tasks': [\n        {'name': 'Submit report', 'due_date': '2024-03-10'},\n        {'name': 'Finish project', 'due_date': '2024-03-15'},\n        {'name': 'Reply to emails', 'due_date': '2024-03-08'}\n    ]\n}\n\n\nclass RAG():\n    def __init__(self):        \n        self.llm = byzerllm.ByzerLLM()\n        self.llm.setup_template(model=\"deepseek_chat\",template=\"auto\")\n        self.llm.setup_default_model_name(\"deepseek_chat\")        \n    \n    @byzerllm.prompt(lambda self:self.llm)\n    def generate_answer(self,name,task_count,tasks)->str:\n        '''\n        Hello {{ name }},\n\n        This is a reminder that you have {{ task_count }} pending tasks:\n        {% for task in tasks %}\n        - Task: {{ task.name }} | Due: {{ task.due_date }}\n        {% endfor %}\n\n        Best regards,\n        Your Reminder System\n        '''        \n\nt = RAG()\n\nresponse = t.generate_answer(**data)\nprint(response)\n\n## 输出:\n## Hello Jane Doe,\n##I hope this message finds you well. I wanted to remind you of your 3 pending tasks to ensure you stay on track:\n## 1. **Submit report** - This task is due on **2024-03-10**. Please ensure that you allocat\n```\n\n这里我们给了个比较复杂的例子，但我们可以看到，给一个实例prompt方法和普通prompt 方法差异不大。\n唯一的区别是如果你希望在定义的时候就指定大模型，使用一个lambda函数返回实例的 llm 对象即可。\n\n```python\n@byzerllm.prompt(lambda self:self.llm)\n```\n\n你也可以不返回，在调用的时候通过 `with_llm` 方法指定 llm 对象。\n\n此外，这个例子也展示了如何通过jinja2模板语法，来处理复杂的结构化数据。\n\n## 通过 Python 代码处理复杂入参\n\n上面的一个例子中，我们通过 jinja2 模板语法，来处理复杂的结构化数据，但是有时候我们可能需要更加复杂的处理，这个时候我们可以通过 Python 代码来处理。\n\n```python\nimport byzerllm\n\ndata = {\n    'name': 'Jane Doe',\n    'task_count': 3,\n    'tasks': [\n        {'name': 'Submit report', 'due_date': '2024-03-10'},\n        {'name': 'Finish project', 'due_date': '2024-03-15'},\n        {'name': 'Reply to emails', 'due_date': '2024-03-08'}\n    ]\n}\n\n\nclass RAG():\n    def __init__(self):        \n        self.llm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n    \n    @byzerllm.prompt()\n    def generate_answer(self,name,task_count,tasks)->str:\n        '''\n        Hello {{ name }},\n\n        This is a reminder that you have {{ task_count }} pending tasks:\n            \n        {{ tasks }}\n\n        Best regards,\n        Your Reminder System\n        '''\n        \n        tasks_str = \"\\n\".join([f\"- Task: {task['name']} | Due: { task['due_date'] }\" for task in tasks])\n        return {\"tasks\": tasks_str}\n\nt = RAG()\n\nresponse = t.generate_answer.with_llm(t.llm).run(**data)\nprint(response)\n\n## Just a gentle nudge to keep you on track with your pending tasks. Here's a quick recap:....\n```\n\n在这个例子里，我们直接把 tasks 在方法体里进行处理，然后作为一个字符串返回，最够构建一个字典，字典的key为 tasks,然后\n你就可以在 docstring 里使用 `{{ tasks }}` 来引用这个字符串。\n\n这样对于很复杂的入参，就不用谢繁琐的 jinja2 模板语法了。\n\n## 如何自动实现一个方法\n\n比如我定义一个签名，但是我不想自己实现里面的逻辑，让大模型来实现。这个在 byzerllm 中叫 function impl。我们来看看怎么\n实现:\n\n```python\nimport pydantic\nclass Time(pydantic.BaseModel):\n    time: str = pydantic.Field(...,description=\"时间，时间格式为 yyyy-MM-dd\")\n\n\n@llm.impl()\ndef calculate_current_time()->Time:\n    '''\n    计算当前时间\n    '''\n    pass \n\n\ncalculate_current_time()\n#output: Time(time='2024-06-14')\n```\n\n在这个例子里，我们定义了一个 calculate_current_time 方法，但是我们没有实现里面的逻辑，我们通过 `@llm.impl()` 装饰器，让大模型来实现这个方法。\n为了避免每次都要“生成”这个方法，导致无法适用，我们提供了缓存，用户可以按如下方式打印速度：\n\n```python\nstart = time.monotonic()\ncalculate_current_time()\nprint(f\"first time cost: {time.monotonic()-start}\")\n\nstart = time.monotonic()\ncalculate_current_time()\nprint(f\"second time cost: {time.monotonic()-start}\")\n\n# output:\n# first time cost: 6.067266260739416\n# second time cost: 4.347506910562515e-05\n```\n可以看到，第一次执行花费了6s,第二次几乎是瞬间完成的，这是因为第一次执行的时候，我们实际上是在生成这个方法，第二次执行的时候，我们是执行已经生成好的代码，所以速度会非常快。你可以显示的调用 `llm.clear_impl_cache()` 清理掉函数缓存。\n\n## Stream 模式\n\n前面的例子都是一次性生成结果，但是有时候我们可能需要一个流式的输出，这个时候我们可能需要用底层一点的API来完成了：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\nv = llm.stream_chat_oai(model=\"deepseek_chat\",conversations=[{\n    \"role\":\"user\",\n    \"content\":\"你好，你是谁\",\n}],delta_mode=True)\n\nfor t in v:\n    print(t,flush=True)  \n\n# 你好\n# ！\n# 我\n# 是一个\n# 人工智能\n# 助手\n# ，\n# 旨在\n# 提供\n# 信息\n# 、\n# 解答\n# 问题....\n```\n\n如果你不想要流式输出，但是想用底层一点的API，你可以使用 `llm.chat_oai` 方法：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\nv = llm.chat_oai(model=\"deepseek_chat\",conversations=[{\n    \"role\":\"user\",\n    \"content\":\"你好，你是谁\",\n}])\n\nprint(v[0].output)\n## 你好！我是一个人工智能助手，旨在提供信息、解答问题和帮助用户解决问题。如果你有任何问题或需要帮助，请随时告诉我。\n```\n\n## Function Calling \n\nbyzerllm 可以不依赖模型自身就能提供 function calling 支持，我们来看个例子：\n\n\n```python\nfrom typing import List,Dict,Any,Annotated\nimport pydantic \nimport datetime\nfrom dateutil.relativedelta import relativedelta\n\ndef compute_date_range(count:Annotated[int,\"时间跨度，数值类型\"],\n                       unit:Annotated[str,\"时间单位，字符串类型\",{\"enum\":[\"day\",\"week\",\"month\",\"year\"]}])->List[str]:\n    '''\n    计算日期范围\n\n    Args:\n        count: 时间跨度，数值类型\n        unit: 时间单位，字符串类型，可选值为 day,week,month,year\n    '''        \n    now = datetime.datetime.now()\n    now_str = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n    if unit == \"day\":\n        return [(now - relativedelta(days=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"week\":\n        return [(now - relativedelta(weeks=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"month\":\n        return [(now - relativedelta(months=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"year\":\n        return [(now - relativedelta(years=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    return [\"\",\"\"]\n\ndef compute_now()->str:\n    '''\n    计算当前时间\n    '''\n    return datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n```\n\n我们定义了两个方法，一个是计算日期范围，一个是计算当前时间。\n\n现在我么可以来测试下，系统如何根据自然语言决定调用哪个方法：\n\n```python\nt = llm.chat_oai([{\n    \"content\":'''计算当前时间''',\n    \"role\":\"user\"    \n}],tools=[compute_date_range,compute_now],execute_tool=True)\n\nt[0].values\n\n## output: ['2024-06-14 15:18:02']\n```\n\n我们可以看到，他正确的选择了 compute_now 方法。\n\n接着我们再试一个：\n\n```python\nt = llm.chat_oai([{\n    \"content\":'''最近三个月趋势''',\n    \"role\":\"user\"    \n}],tools=[compute_date_range,compute_now],execute_tool=True)\n\nt[0].values\n\n## output: [['2024-03-14 15:19:13', '2024-06-14 15:19:13']]\n```\n\n模型正确的选择了 compute_date_range 方法。\n\n## 多模态\n\nbyerllm 也能很好的支持多模态的交互，而且统一了多模态大模型的接口，比如你可以用一样的方式使用 openai 或者 claude 的图片转文字能力， 或者一致的方式使用火山，azuer, openai的语音合成接口。\n\n### 图生文\n\n```python\nimport byzerllm\nfrom byzerllm.types import ImagePath\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\n\n\n@byzerllm.prompt()\ndef what_in_image(image_path: ImagePath) -> str:\n    \"\"\"\n    {{ image_path }}\n    这个图片里有什么？\n    \"\"\"    \n\n\nv = what_in_image.with_llm(vl_llm).run(\n    ImagePath(value=\"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\")\n)\nv\n## OUTPUT: 这张图片展示了多只可爱的猫咪，采用了艺术风格的绘画。猫咪们有不同的颜色和花纹，背景是浅棕色，上面还点缀着一些红色的花朵。整体画面给人一种温馨和谐的感觉\n```\n\n可以看到，我们只需要把 prompt 函数的图片地址入参使用 byzerllm.types.ImagePath里进行包装，就可以直接在 prompt 函数体里\n带上图片。\n\n或者你可以这样：\n\n```python\nimport byzerllm\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\n\n\n@byzerllm.prompt()\ndef what_in_image(image_path: str) -> str:\n    \"\"\"\n    {{ image }}\n    这个图片里有什么？\n    \"\"\"\n    return {\"image\": byzerllm.Image.load_image_from_path(image_path)}\n\n\nv = what_in_image.with_llm(vl_llm).run(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv\n```\n\n通过 `image_path` 参数，然后通过 `byzerllm.Image.load_image_from_path` 方法，转化为一个图片对象 image，最后在 prompt 函数体里\n使用 `{{ image }}` 引用这个图片对象。\n\n另外我们也是可以支持配置多张图片的。\n\n另外，我们也可以使用基础的 `llm.chat_oai` 方法来实现：\n\n```python\nimport byzerllm\nimport json\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\nimage = byzerllm.Image.load_image_from_path(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv = vl_llm.chat_oai(\n    conversations=[\n        {\n            \"role\": \"user\",\n            \"content\": json.dumps(\n                [{\"image\": image, \"text\": \"这个图片里有什么？\"}], ensure_ascii=False\n            ),\n        }\n    ]\n)\nv[0].output\n```\n\n还可以这么写：\n\n```python\nimport byzerllm\nimport json\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\nimage = byzerllm.Image.load_image_from_path(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv = vl_llm.chat_oai(\n    conversations=[\n        {\n            \"role\": \"user\",\n            \"content\": json.dumps(\n                [\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\"url\": image, \"detail\": \"high\"},\n                    },\n                    {\"text\": \"这个图片里有什么？\", \"type\": \"text\"},\n                ],\n                ensure_ascii=False,\n            ),\n        }\n    ]\n)\nv[0].output\n```\n\n### 语音合成\n\n这里以 openai 的 tts 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=tts-1 \\\n--model openai_tts\n```\n\n此外，byzerllm 支持 azure,火山引擎等 tts 语音合成引擎。\n\n接着你可以这么用：\n\n```python\nimport byzerllm\nimport base64\nimport json\n\nllm = byzerllm.ByzerLLM.from_default_model(\"openai_tts\")\n\n\nt = llm.chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\": json.dumps({\n        \"input\":\"hello, open_tts\",\n        \"voice\": \"alloy\",\n        \"response_format\": \"mp3\"\n    },ensure_ascii=False)\n}])\n\nwith open(\"voice.mp3\",\"wb\") as f:\n    f.write(base64.b64decode(t[0].output))\n```\n\ntts 模型生成没有prompt函数可以用，你需要直接使用 chat_oai。\n\n\n### 语音识别\n\n这里以 openai 的 whisper-1 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--worker_concurrency 10 \\\n--infer_params saas.model=whisper-1 saas.api_key=${MODEL_OPENAI_TOKEN} \\\n--model speech_to_text\n```\n\n语音识别的使用方式和图生文类似，我们可以直接在 prompt 函数体里带上音频文件。\n\n```python\nimport byzerllm\nimport json\nimport base64\nfrom byzerllm.types import AudioPath\n\nllm = byzerllm.ByzerLLM.from_default_model(\"speech_to_text\")\n\naudio_file = \"/Users/allwefantasy/videos/output_audio.mp3\"\n\n@byzerllm.prompt(llm=llm)\ndef audio_to_text(audio_file: AudioPath):\n    \"\"\"\n    {{ audio_file }}\n    \"\"\"\n\nv = audio_to_text(AudioPath(value=audio_file))\njson.loads(v)\n```\n输出的数据格式略微复杂：\n\n```\n{'text': 'In the last chapter, you and I started to step through the internal workings of a transformer. This is one of the key pieces of technology inside large language models, and a lot of other tools in the modern wave of AI.',\n 'task': 'transcribe',\n 'language': 'english',\n 'duration': 10.0,\n 'segments': [{'id': 0,\n   'seek': 0,\n   'start': 0.0,\n   'end': 4.78000020980835,\n   'text': ' In the last chapter, you and I started to step through the internal workings of a transformer.',\n   'tokens': [50364,\n    682,\n    264,\n    1036,\n    7187,\n    11,\n    291,\n    293,\n    286,\n    1409,\n    281,\n    1823,\n    807,\n    264,\n    6920,\n    589,\n    1109,\n    295,\n    257,\n    31782,\n    13,\n    50586],\n   'temperature': 0.0,\n   'avg_logprob': -0.28872039914131165,\n   'compression_ratio': 1.4220778942108154,\n   'no_speech_prob': 0.016033057123422623},\n  ....\n  {'id': 2,\n   'seek': 0,\n   'start': 8.579999923706055,\n   'end': 9.979999542236328,\n   'text': ' and a lot of other tools in the modern wave of AI.',\n   'tokens': [50759,\n    293,\n    257,\n    688,\n    295,\n    661,\n    3873,\n    294,\n    264,\n    4363,\n    5772,\n    295,\n    7318,\n    13,\n    50867],\n   'temperature': 0.0,\n   'avg_logprob': -0.28872039914131165,\n   'compression_ratio': 1.4220778942108154,\n   'no_speech_prob': 0.016033057123422623}],\n 'words': [{'word': 'In', 'start': 0.0, 'end': 0.18000000715255737},\n  {'word': 'the', 'start': 0.18000000715255737, 'end': 0.23999999463558197},\n  {'word': 'last', 'start': 0.23999999463558197, 'end': 0.5400000214576721},\n  {'word': 'chapter', 'start': 0.5400000214576721, 'end': 0.800000011920929},\n  ....\n  {'word': 'AI', 'start': 9.920000076293945, 'end': 9.979999542236328}]}\n```\n\n会输出每一句话以及每一个字所在的起始时间和截止时间。你可以根据需要来使用。\n\n\n### 文生图\n\n文生图和语音合成类似，首先要启动合适的模型,以openai 的 dall-e-3 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=dall-e-3 \\\n--model openai_image_gen\n```\n\n启动模型后，只需要记住几个模板参数即可使用，这里直接使用 chat_oai 方法来使用：\n\n```python\n\nimport byzerllm\nimport json\nimport base64\n\nllm = byzerllm.ByzerLLM.from_default_model(\"openai_image_gen\")\nt = llm.chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\": json.dumps({\n        \"input\":\"a white siamese cat\",\n        \"size\": \"1024x1024\",\n        \"quality\": \"standard\"\n    },ensure_ascii=False)\n}])\n\nwith open(\"output1.jpg\",\"wb\") as f:\n    f.write(base64.b64decode(t[0].output))\n\n\nimport matplotlib.pyplot as plt\n\nimage_path = \"output1.jpg\"\nimage = plt.imread(image_path)\n\nplt.imshow(image)\nplt.axis('off')\nplt.show()\n```\n\n## Prompt 函数的流式输出\n\nbyzerllm 底层支持流式输出，非 prompt 函数的用法是这样的：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\n\nv = llm.stream_chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\":\"讲一个100字的故事\"\n}])\n\nfor s  in v:\n    print(s[0], end=\"\")\n```\n\n如果你像用 prompt 函数，可以这么用：\n\n\n```python\nimport byzerllm\nimport json\nimport base64\nfrom typing import Generator\n\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\n\n@byzerllm.prompt()\ndef tell_story() -> Generator[str, None, None]:\n    '''\n    给我讲一个一百多字的故事\n    '''\n\nv = tell_story.with_llm(llm).run()    \nfor i in v:\n    print(i, end=\"\")\n```\n\n可以看到，和普通的 prompt 函数的区别在于，返回值是一个生成器，然后你可以通过 for 循环来获取结果。\n\n## 向量化模型\n\nbyzerllm 支持向量化模型,你可以这样启动一个本地的模型：\n\n```bash\n!byzerllm deploy --pretrained_model_type custom/bge \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--worker_concurrency 10 \\\n--model_path /home/winubuntu/.auto-coder/storage/models/AI-ModelScope/bge-large-zh \\\n--infer_backend transformers \\\n--num_workers 1 \\\n--model emb\n```\n\n注意两个参数:\n\n1. --infer_backend transformers: 表示使用 transformers 作为推理后端。\n2. --model_path: 表示模型的路径。\n\n也可以启动一个 SaaS 的emb模型,比如 qwen 的 emb 模型：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/qianwen \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 2 \\\n--infer_params saas.api_key=${MODEL_QIANWEN_TOKEN}  saas.model=text-embedding-v2 \\\n--model qianwen_emb\n```\n\n或者 openai 的 emb 模型：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--worker_concurrency 10 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=text-embedding-3-small \\\n--model gpt_emb\n```\nSaaS 模型无需配置 `--infer_backend` 参数。\n\n无论是本地模型还是 SaaS 模型，我们都可以这样使用：\n\n```python\nimport byzerllm\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\nllm.setup_default_emb_model_name(\"emb\")\nllm.emb_query(\"你好\")\n```\n\n如果你配置 byzerllm 中的 Storage 使用，比如你这样启动了存储：\n\n```bash\nbyzerllm storage start --enable_emb\n```\n\n那么需要这么使用：\n\n```python\nfrom byzerllm.apps.byzer_storage.simple_api import ByzerStorage, DataType, FieldOption,SortOption\nstorage = ByzerStorage(\"byzerai_store\", \"memory\", \"memory\")\nstorage.emb(\"你好\")\n```\n\n## 一些辅助工具\n\n当调用 prompt 函数返回字符串的时候，如果想从里面抽取代码，可以使用如下方式：\n\n```python\nfrom byzerllm.utils.client import code_utils\ntext_with_markdown = '''\n```shell\nls -l\n```\n'''\ncode_blocks = code_utils.extract_code(text_with_markdown)\nfor code_block in code_blocks:\n    if code_block[0] == \"shell\":\n        print(code_block[1])\n##output: ls -l        \n```\n\nTagExtractor 工具用于对任意 `<_tag_></_tag_>` 标签对的抽取，下面是一个使用示例：\n\n```python\nfrom byzerllm.apps.utils import TagExtractor\n\nextractor = TagExtractor('''\n大家好\n<_image_>data:image/jpeg;base64,xxxxxx</_image_>\n                         大家好\n<_image_>data:image/jpeg;base64,xxxxxx2</_image_>\n''')\n\nv = extractor.extract()\nprint(v.content[0].content)\nprint(v.content[1].content)\n```\n输出为:\n\n```\ndata:image/jpeg;base64,xxxxxx\ndata:image/jpeg;base64,xxxxxx2\n```\n\n我们成功的将 <_image_></_image_> 标签对里的内容抽取出来了。\n\n\n## 注意事项\n\n1. prompt函数方法体返回只能是dict，实际的返回类型和方法签名可以不一样，但是方法体返回只能是dict。\n2. 大部分情况prompt函数体为空，如果一定要有方法体，可以返回一个空字典。\n3. 调用prompt方法的时候，如果在@byzerllm.prompt()里没有指定llm对象，那么需要在调用的时候通过with_llm方法指定llm对象。\n\n===========================\n\n# Byzer Storage: 最易用的AI存储引擎\n\nByzer Storage是一个为AI应用设计的高性能存储引擎,它提供了简单易用的API,支持向量搜索、全文检索以及结构化查询。本文将详细介绍Byzer Storage的使用方法和主要特性。\n\n## 0. 安装和启动\n\n```bash\npip install byzerllm\nbyzerllm storage start\n```\n\nThat's it! Byzer Storage已经安装并启动成功,现在我们可以开始使用它了。默认会启动一个 byzerai_store 的集群。\n\n注意，如果你是这样启动的：\n\n```bash\nbyzerllm storage start --enable_emb\n```\n\n那么会自动启动一个 emb 模型，名字就叫 emb， ByzerStorage 会自动使用该模型，无需做任何其他配置。\n\n## 1. 初始化\n\n创建一个 ByzerStorage 对象，链接 byzerai_store 集群，并且指定数据库和表名（可以不存在）。\n\n```python\nfrom byzerllm.apps.byzer_storage.simple_api import ByzerStorage, DataType, FieldOption, SortOption\n\nstorage = ByzerStorage(\"byzerai_store\", \"my_database1\", \"my_table4s\")\n```\n\n\n## 2. 创建库表（可选）\n\nByzer Storage使用Schema来定义数据结构。我们可以使用SchemaBuilder来构建Schema:\n\n```python\n_ = (\n    storage.schema_builder()\n    .add_field(\"_id\", DataType.STRING)\n    .add_field(\"name\", DataType.STRING)\n    .add_field(\"content\", DataType.STRING, [FieldOption.ANALYZE])\n    .add_field(\"raw_content\", DataType.STRING, [FieldOption.NO_INDEX])    \n    .add_array_field(\"summary\", DataType.FLOAT)    \n    .add_field(\"created_time\", DataType.LONG, [FieldOption.SORT])    \n    .execute()\n)\n```\n\n这个Schema定义了以下字段:\n- `_id`: 字符串类型的主键\n- `name`: 字符串类型,可用于过滤条件\n- `content`: 字符串类型,带有ANALYZE选项,用于全文搜索\n- `raw_content`: 字符串类型,带有NO_INDEX选项,不会被索引\n- `summary`: 浮点数组类型,用于存储向量\n- `created_time`: 长整型,带有SORT选项,可用于排序\n\n需要注意的是：\n\n1. 如果一个字段带有ANALYZE选项,则该字段会被分词,并且可以用于全文搜索，但是就无法返回原始的文本了。所以你需要添加一个新字段专门用来存储原文，比如在我们这个例子里，我们新增了 raw_content 字段，并且显示的指定了 NO_INDEX 选项，这样就不会被索引，也就不会被分词，可以后续被检索后用来获取原始文本。\n2. 对于需要作为向量检索和存储的字段，需要指定为数组类型，比如我们这个例子里的 summary 字段。\n3. 如果你需要拿到向量字段的原始文本，那么你也需要添加一个新字段专门用来存储原文，就像我们这个例子里的 raw_content 字段一样。\n\n\n## 3. 写入数据\n\n准备数据并使用WriteBuilder写入Storage:\n\n```python\ndata = [\n    {\"_id\": \"1\", \"name\": \"Hello\", \"content\": \"Hello, world!\", \"raw_content\": \"Hello, world!\", \"summary\": \"hello world\", \"created_time\": 1612137600},\n    {\"_id\": \"2\", \"name\": \"Byzer\", \"content\": \"Byzer, world!\", \"raw_content\": \"Byzer, world!\", \"summary\": \"byzer\", \"created_time\": 1612137601},\n    {\"_id\": \"3\", \"name\": \"AI\", \"content\": \"AI, world!\", \"raw_content\": \"AI, world!\", \"summary\": \"AI\", \"created_time\": 16121376002},\n    {\"_id\": \"4\", \"name\": \"ByzerAI\", \"content\": \"ByzerAI, world!\", \"raw_content\": \"ByzerAI, world!\", \"summary\": \"ByzerAi\", \"created_time\": 16121376003},\n]\n\nstorage.write_builder().add_items(data, vector_fields=[\"summary\"], search_fields=[\"content\"]).execute()\nstorage.commit()\n```\n\n这里我们使用`add_items`方法批量添加数据,并指定了`summary`为向量字段,`content`为搜索字段。最后调用`commit()`来确保数据被持久化。\n\n\n从上面我们要写入到 byzer storage 的数据我们可以看到如下几个特点：\n\n1. 需要包含我们之前定义的 Schema中罗列的所有的字段，同时需要指定哪些是向量字段，哪些是检索字段。\n2. 向量和检索不能是同一个字段。\n3. 对于向量，检索字段，我们给到 write_builder 的都是文本，ByzerStorage 会根据 Schema 的定义自动将其转换为向量，检索字段需要的格式。\n\n## 4. 查询数据\n\nByzer Storage支持多种查询方式,包括向量搜索、全文检索、过滤和排序。\n\n### 4.1 向量搜索 + 全文检索\n\n```python\nquery = storage.query_builder()\nquery.set_vector_query(\"ByzerAI\", fields=[\"summary\"])\nresults = query.set_search_query(\"Hello\", fields=[\"content\"]).execute()\nprint(results)\n```\n\n这个查询结合了向量搜索和全文检索,它会在`summary`字段中搜索与\"ByzerAI\"相似的向量,同时在`content`字段中搜索包含\"Hello\"的文档。\n\n### 4.2 过滤 + 向量搜索 + 全文检索\n\n```python\nquery = storage.query_builder()\nquery.and_filter().add_condition(\"name\", \"AI\").build()\nquery.set_vector_query(\"ByzerAI\", fields=\"summary\")\nresults = query.set_search_query(\"Hello\", fields=[\"content\"]).execute()\nprint(results)\n```\n\n这个查询首先过滤`name`字段等于\"AI\"的文档,然后在结果中进行向量搜索和全文检索。\n\n### 4.3 过滤 + 排序\n\n```python\nquery = storage.query_builder()\nquery.and_filter().add_condition(\"name\", \"AI\").build().sort(\"created_time\", SortOption.DESC)\nresults = query.execute()\nprint(results)\n```\n\n这个查询过滤`name`字段等于\"AI\"的文档,然后按`created_time`字段降序排序。\n\n## 5. 删除数据\n\n### 5.1 根据ID删除\n\n```python\nstorage.delete_by_ids([\"3\"])\n\nquery = storage.query_builder()\nquery.and_filter().add_condition(\"name\", \"AI\").build()\nresults = query.execute()\nprint(results)\n```\n\n这里我们删除了ID为\"3\"的文档,然后查询验证删除结果。\n\n### 5.2 删除整个表\n\n```python\nstorage.drop()\n```\n\n这个操作会删除整个表及其所有数据,请谨慎使用。\n\n## 结论\n\nByzer Storage提供了一套简洁而强大的API,能够轻松实现向量搜索、全文检索、结构化查询等功能。它的设计非常适合AI应用场景,可以有效地存储和检索各种类型的数据。通过本文的介绍,相信读者已经对Byzer Storage有了基本的了解,并能够开始在自己的项目中使用这个强大的存储引擎。\n\n\n     \n\n下面是用户的需求：\n\n根据README.md  更新 @deploy.md ,在 @deploy.md 中主要介绍如何部署auto-coder, byzer-sql 引擎,以及安装wheel 包,启动和配置相关服务.使用emoj 和分隔符\n使用 SEARCH/REPLACE 格式输出\n"}, {"role": "assistant", "content": "我理解了，我会使用 SEARCH/REPLACE 格式来输出更新后的 deploy.md 文件内容。这里是更新后的内容：\n\n```python\n##File: /Users/allwefantasy/projects/super_analysis/docs/deploy.md\n<<<<<<< SEARCH\n=======\n# 🚀 Super Analysis 部署指南\n\n本指南将帮助你部署 Super Analysis 系统，包括安装必要组件、配置服务和启动系统。\n\n---\n\n## 📦 安装 Super Analysis\n\n首先，我们需要安装 Super Analysis 包：\n\n```bash\ngit clone https://github.com/allwefantasy/super_analysis\ncd super-analysis\npip install -e .\n```\n\n或者使用 Makefile：\n\n```bash\n# 清理项目\nmake clean\n\n# 构建项目\nmake build\n\n# 创建wheel包\nmake wheel\n\n# 安装项目\nmake install\n\n# 卸载项目\nmake uninstall\n\n# 以开发模式安装项目\nmake develop\n\n# 运行测试\nmake test\n```\n\n---\n\n## 🤖 部署 Deepseek 模型代理\n\n在启动其他服务之前，我们需要先部署 Deepseek 模型：\n\n```bash\nmake deploy_deepseek\n```\n\n或者直接运行：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--worker_concurrency 1000 \\\n--num_workers 1 \\\n--infer_params saas.base_url=\"https://api.deepseek.com/v1\" saas.api_key=${MODEL_DEEPSEEK_TOKEN} saas.model=deepseek-chat \\\n--model deepseek_chat\n```\n\n注意：确保已设置环境变量 `MODEL_DEEPSEEK_TOKEN`。\n\n---\n\n## 🛠️ 部署 Byzer-SQL\n\n参考 [安装与配置 Byzer-SQL 文档](./4.3.1%20安装与配置%20Byzer-SQL.pdf) 完成部署。\n\n> 启动时需要在安装有 super-analysis 的 conda 环境中启动。\n\n当 byzer-sql 部署完成后，注册账号为 `hello`，然后在 byzer-sql 控制台中执行：\n\n```sql\n!byzerllm setup single;\n\nrun command as LLM.`` where \naction=\"infer\"\nand reconnect=\"true\"\nand pretrainedModelType=\"saas/openai\"\nand udfName=\"deepseek_chat\";\n```\n\n---\n\n## 📊 数据预处理和服务启动\n\n1. 转换电影数据集：\n\n```bash\nsuper-analysis.convert --data_dir /Users/allwefantasy/data/movice --doc_dir /Users/allwefantasy/data/movice/schemas/\n```\n\n2. 启动 schema 文档知识库：\n\n```bash\nauto-coder doc serve \\\n--model deepseek_chat --index_filter_workers 100 \\\n--tokenizer_path /Users/allwefantasy/Downloads/tokenizer.json \\\n--doc_dir /Users/allwefantasy/data/movice/schemas/ \\\n--port 8001\n```\n\n3. 下载 Byzer-SQL 文档并启动文档知识库：\n\n```bash\ngit clone https://github.com/allwefantasy/llm_friendly_packages\n\nauto-coder doc serve \\\n--model deepseek_chat --index_filter_workers 100 \\\n--tokenizer_path /Users/allwefantasy/Downloads/tokenizer.json \\\n--doc_dir  /Users/allwefantasy/projects/llm_friendly_packages/github.com/allwefantasy \\\n--port 8002\n```\n\n4. 启动兼容 OpenAI Server 的分析服务：\n\n```bash\nsuper-analysis.serve --served-model-name deepseek_chat --port 8000 \\\n--schema-rag-base-url http://127.0.0.1:8001/v1 \\\n--context-rag-base-url http://127.0.0.1:8002/v1 \\\n--byzer-sql-url http://127.0.0.1:9003/run/script\n```\n\n---\n\n现在，Super Analysis 系统已经完全部署并启动。你可以开始使用 OpenAI SDK 进行测试和接口调用。具体测试和接口使用方法请参考 [openai_local_api.ipynb](./notebooks/openai_local_api.ipynb)。\n\n🎉 恭喜！你已经成功部署了 Super Analysis 系统。如有任何问题，请随时查阅文档或联系支持团队。\n>>>>>>> REPLACE\n```\n\n这个 SEARCH/REPLACE 块将会完全替换 /Users/allwefantasy/projects/super_analysis/docs/deploy.md 文件的内容。新的内容包含了如何部署 auto-coder、byzer-sql 引擎，以及安装 wheel 包，启动和配置相关服务的详细说明，并使用了 emoji 和分隔符来增强可读性。"}], "model": "human", "yaml_file": "actions/144_chat_action.yml"}
{"instruction": "@Makefile 添加一个 dist 指令,运行 wheel 后然后将 @/Users/allwefantasy/projects/super_analysis/docs/4.3.1 安装与配置 Byzer-SQL.pdf 以及 @deploy.md 拷贝到 dist 目录下,然后将 dist 打包成一个 super_analysis.tar.gz\n使用 SEARCH/REPLACE 格式输出", "conversations": [{"role": "user", "content": "\n如果你需要生成代码，对于每个需要更改的文件,你需要按 *SEARCH/REPLACE block* 的格式进行生成。\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The opening fence and code language, eg: ```python\n2. The file path alone on a line, starting with \"##File:\" and verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n*SEARCH/REPLACE* blocks will replace *all* matching occurrences.\nInclude enough lines to make the SEARCH blocks unique.\n\nInclude *ALL* the code being searched and replaced!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n下面我们来看一个例子：\n\n当前项目目录结构：\n1. 项目根目录： /tmp/projects/mathweb\n2. 项目子目录/文件列表(类似tree 命令输出)\nflask/\n    app.py\n    templates/\n        index.html\n    static/\n        style.css\n\n用户需求： Change get_factorial() to use math.factorial\n\n回答： To make this change we need to modify `/tmp/projects/mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n用户需求： Refactor hello() into its own file.\n\n回答：To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/hello.py\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/main.py\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n现在让我们开始一个新的任务:\n\n当前项目目录结构：\n1. 项目根目录： /Users/allwefantasy/projects/super_analysis\n2. 项目子目录/文件列表(类似tree 命令输出)：\nsetup.py\n.auto-coder/\n    libs/\n        llm_friendly_packages/\n            src/\n                example1/\n                    main.py\nsrc/\n    superanalysis/\n        serve/\n            rest.py\n            convert.py\n            openai_serve.py\n            __init__.py\n            llm_wrapper.py\n            api_server.py\n            cli.py\n        sql/\n            __init__.py\n            types.py\n            generator.py\n下面是一些文件路径以及每个文件对应的源码：\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/types.py\nfrom pydantic import BaseModel\n\nclass RAGConfig(BaseModel):\n    schema_rag_base_url: str = \"\"\n    schema_rag_api_key: str = \"xxxx\"\n\n    context_rag_base_url: str = \"\"\n    context_rag_api_key: str = \"xxxx\"\n\nclass ByzerSQLConfig(BaseModel):\n    byzer_sql_url: str = \"\"\n    data_dir: str = \"\"\n    owner: str = \"\"\n\nclass ModelConfig(BaseModel):\n    llm_model: str = \"\"\n    gen_sql_llm_model: str = \"\"\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/cli.py\nimport argparse\nimport byzerllm\nfrom byzerllm.utils.client import ByzerLLM\nfrom superanalysis.serve.api_server import serve, ServerArgs\nfrom superanalysis.sql.types import RAGConfig, ByzerSQLConfig, ModelConfig\nfrom superanalysis.sql.generator import SQLGenerator\nfrom superanalysis.serve.llm_wrapper import LLWrapper\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"ByzerLLM API Server\")\n    parser.add_argument(\"--ray-address\", type=str, default=\"auto\", help=\"ray address\")\n    parser.add_argument(\n        \"--host\", type=str, default=\"0.0.0.0\", help=\"Host to bind the server to\"\n    )\n    parser.add_argument(\n        \"--port\", type=int, default=8000, help=\"Port to bind the server to\"\n    )\n    parser.add_argument(\n        \"--uvicorn-log-level\", type=str, default=\"info\", help=\"Uvicorn log level\"\n    )\n    parser.add_argument(\n        \"--allow-credentials\", action=\"store_true\", help=\"Allow credentials\"\n    )\n    parser.add_argument(\n        \"--allowed-origins\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed origins\"\n    )\n    parser.add_argument(\n        \"--allowed-methods\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed methods\"\n    )\n    parser.add_argument(\n        \"--allowed-headers\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed headers\"\n    )\n    parser.add_argument(\n        \"--api-key\", type=str, default=\"\", help=\"API key for authentication\"\n    )\n    parser.add_argument(\n        \"--served-model-name\",\n        type=str,\n        required=True,\n        help=\"Name of the model to be served\",\n    )\n    parser.add_argument(\n        \"--prompt-template\", type=str, default=\"\", help=\"Prompt template\"\n    )\n    parser.add_argument(\n        \"--response-role\", type=str, default=\"assistant\", help=\"Response role\"\n    )\n    parser.add_argument(\"--ssl-keyfile\", type=str, default=\"\", help=\"SSL key file\")\n    parser.add_argument(\n        \"--ssl-certfile\", type=str, default=\"\", help=\"SSL certificate file\"\n    )\n\n    # Add new arguments for RAGConfig\n    parser.add_argument(\n        \"--schema-rag-base-url\", type=str, default=\"\", help=\"RAG base URL\"\n    )\n    parser.add_argument(\n        \"--schema-rag-api-key\", type=str, default=\"xxxx\", help=\"RAG API key\"\n    )\n\n    parser.add_argument(\n        \"--context-rag-base-url\", type=str, default=\"\", help=\"RAG base URL\"\n    )\n    parser.add_argument(\n        \"--context-rag-api-key\", type=str, default=\"xxxx\", help=\"RAG API key\"\n    )\n\n    # Add new argument for ByzerSQLConfig\n    parser.add_argument(\"--byzer-sql-url\", type=str, default=\"\", help=\"Byzer SQL URL\")\n    parser.add_argument(\"--data-dir\", type=str, default=\"\", help=\"Data directory\")\n    parser.add_argument(\"--owner\", type=str, default=\"hello\", help=\"Owner of the Byzer SQL session\")\n    parser.add_argument(\n        \"--llm-model\", type=str, default=\"\", help=\"LLM model for general use\"\n    )\n    parser.add_argument(\n        \"--gen-sql-llm-model\", type=str, default=\"\", help=\"LLM model for SQL generation\"\n    )\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile,\n    )\n\n    # Create RAGConfig instance\n    rag_config = RAGConfig(\n        schema_rag_base_url=args.schema_rag_base_url,\n        schema_rag_api_key=args.schema_rag_api_key,\n        context_rag_base_url=args.context_rag_base_url,\n        context_rag_api_key=args.context_rag_api_key,\n    )\n\n    # Create ByzerSQLConfig instance\n    byzer_sql_config = ByzerSQLConfig(\n        byzer_sql_url=args.byzer_sql_url, data_dir=args.data_dir, owner=args.owner\n    )\n\n    model_config = ModelConfig(\n        llm_model=args.llm_model or args.served_model_name,\n        gen_sql_llm_model=args.gen_sql_llm_model or args.served_model_name,\n    )\n\n    byzerllm.connect_cluster(address=args.ray_address)\n    llm = ByzerLLM()\n    llm.setup_default_model_name(args.served_model_name)\n    llm = byzerllm.ByzerLLM()\n    if not server_args.served_model_name:\n        raise ValueError(\"served_model_name is required\")\n    llm.setup_default_model_name(server_args.served_model_name)\n\n    print(\"RAG Config: \", rag_config)\n    print(\"Model Config: \", model_config)\n    rag = SQLGenerator(\n        llm=llm,\n        rag_config=rag_config,\n        byzer_sql_config=byzer_sql_config,\n        model_config=model_config,\n    )\n    llm_wrapper = LLWrapper(llm=llm, entry=rag)\n    serve(llm_wrapper, server_args)\n\n\nif __name__ == \"__main__\":\n    main()\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/__init__.py\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/__init__.py\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/llm_wrapper.py\nfrom typing import Any, Dict, List, Optional, Union,Callable\nfrom byzerllm.utils.client.types import (    \n    LLMFunctionCallResponse,\n    LLMClassResponse,LLMResponse    \n)\nimport pydantic\nfrom byzerllm import ByzerLLM\nfrom byzerllm.utils.client import LLMResponse\nfrom byzerllm.utils.types import SingleOutputMeta\nfrom byzerllm.utils.langutil import asyncfy_with_semaphore\nfrom superanalysis.sql.generator import SQLGenerator\nfrom loguru import logger\n\nclass LLWrapper:\n\n    def __init__(self,llm:ByzerLLM,entry:SQLGenerator):\n        self.entry = entry\n        self.llm = llm\n        \n\n    def chat_oai(self,\n                 conversations,\n                 tools:List[Union[Callable,str]]=[], \n                 tool_choice:Optional[Union[Callable,str]]=None,\n                 execute_tool:bool=False,  \n                 impl_func:Optional[Callable]=None,\n                 execute_impl_func:bool=False,\n                 impl_func_params:Optional[Dict[str,Any]]=None,\n                 func_params:Optional[Dict[str,Any]]=None,\n                 response_class:Optional[Union[pydantic.BaseModel,str]] = None, \n                 response_after_chat:Optional[Union[pydantic.BaseModel,str]] = False,\n                 enable_default_sys_message:bool=True,                 \n                 model:Optional[str] = None,\n                 role_mapping=None,llm_config:Dict[str,Any]={}\n                 )->Union[List[LLMResponse],List[LLMFunctionCallResponse],List[LLMClassResponse]]: \n        res,contexts = self.entry.stream_chat_oai(conversations)\n        s = \"\".join(res)        \n        return [LLMResponse(output=s,metadata={},input=\"\")]\n\n    def stream_chat_oai(self,conversations, \n                        model:Optional[str]=None, \n                        role_mapping=None,\n                        delta_mode=False,\n                        llm_config:Dict[str,Any]={}): \n        res,contexts = self.entry.stream_chat_oai(conversations)        \n        for t in res:                        \n            yield (t,SingleOutputMeta(0,0))\n\n    async def async_stream_chat_oai(self,conversations, \n                        model:Optional[str]=None, \n                        role_mapping=None,\n                        delta_mode=False,\n                        llm_config:Dict[str,Any]={}): \n        res,contexts = self.rag.stream_chat_oai(conversations)                \n        for t in res:                                    \n            yield (t,SingleOutputMeta(0,0))\n                         \n\n    def __getattr__(self, name):        \n        return getattr(self.llm, name)\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/openai_serve.py\nimport argparse\nfrom service_server.api_server import ServerArgs, serve\nfrom service_server.llm_wrapper import LLWrapper\nfrom service_server.summary_rag import SummaryRAG\nimport byzerllm\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"OpenAI Serve\")\n    parser.add_argument(\n        \"--model\",\n        default=\"\",\n    )\n    parser.add_argument(\"--emb_model\", default=\"\")\n    parser.add_argument(\n        \"--ray_address\",\n        default=\"auto\",\n    )\n    parser.add_argument(\"--source_dir\", default=\".\", help=\"\")\n    parser.add_argument(\"--host\", default=\"\", help=\"\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"\")\n    parser.add_argument(\"--uvicorn_log_level\", default=\"info\", help=\"\")\n    parser.add_argument(\"--allow_credentials\", action=\"store_true\", help=\"\")\n    parser.add_argument(\"--allowed_origins\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--allowed_methods\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--allowed_headers\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--api_key\", default=\"\", help=\"\")\n    parser.add_argument(\"--served_model_name\", default=\"\", help=\"\")\n    parser.add_argument(\"--prompt_template\", default=\"\", help=\"\")\n    parser.add_argument(\"--ssl_keyfile\", default=\"\", help=\"\")\n    parser.add_argument(\"--ssl_certfile\", default=\"\", help=\"\")\n    parser.add_argument(\"--response_role\", default=\"assistant\", help=\"\")\n    parser.add_argument(\n        \"--collections\", default=\"\", help=\"Collection name for indexing\"\n    )\n    parser.add_argument(\n        \"--base_dir\",\n        default=\"\",\n        help=\"Path where the processed text embeddings were stored\",\n    )\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name or args.model,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile,\n    )\n    byzerllm.connect_cluster(address=args.ray_address)\n    llm = byzerllm.ByzerLLM()\n    if not server_args.served_model_name:\n        raise ValueError(\"served_model_name is required\")\n    llm.setup_default_model_name(server_args.served_model_name)\n    rag = SummaryRAG(llm=llm)    \n    llm_wrapper = LLWrapper(llm=llm, rag=rag)\n    serve(llm=llm_wrapper, args=server_args)\n\nif __name__ == \"__main__\":\n    main()\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/api_server.py\nimport os\nimport time\nimport uvicorn\nfrom fastapi import Request\nfrom fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import JSONResponse, StreamingResponse, Response\n\nfrom byzerllm.log import init_logger\nfrom byzerllm.utils import random_uuid\nfrom byzerllm.version import __version__ as version\nfrom byzerllm.utils.client import ByzerLLM, LLMRequest\nfrom byzerllm.utils.client.entrypoints.openai.serving_chat import OpenAIServingChat\nfrom byzerllm.utils.client.entrypoints.openai.serving_completion import OpenAIServingCompletion\nfrom byzerllm.utils.client.entrypoints.openai.protocol import (\n    ModelList,\n    ModelCard,\n    ModelPermission,\n    ChatCompletionRequest,\n    ErrorResponse,\n    CompletionRequest,\n    Embeddings,\n    EmbeddingsOutput,\n    EmbeddingsData,\n    EmbeddingsUsage,\n)\nfrom pydantic import BaseModel\nfrom typing import List\n\nlogger = init_logger(__name__)\n\nllm_client: ByzerLLM = None\nopenai_serving_chat: OpenAIServingChat = None\nopenai_serving_completion: OpenAIServingCompletion = None\n\nTIMEOUT_KEEP_ALIVE = 5  # seconds\n# timeout in 10 minutes. Streaming can take longer than 3 min\nTIMEOUT = float(os.environ.get(\"BYZERLLM_APISERVER_HTTP_TIMEOUT\", 600))\n\nrouter_app = FastAPI()\n\n\n@router_app.get(\"/health\")\nasync def health() -> Response:\n    \"\"\"Health check.\"\"\"\n    return Response(status_code=200)\n\n\n@router_app.get(\"/v1/models\")\nasync def show_available_models():\n    models = await openai_serving_chat.show_available_models()\n    return JSONResponse(content=models.model_dump())\n\n\n@router_app.get(\"/version\")\nasync def show_version():\n    return JSONResponse(content={\"version\": version})\n\n\n@router_app.get(\"/v1/models\", response_model=ModelList)\nasync def models() -> ModelList:\n    \"\"\"Show available models. Right now we only have one model.\"\"\"\n    model_cards = [\n        ModelCard(\n            id=\"\",\n            root=\"\",\n            permission=[ModelPermission()]\n        )\n    ]\n    return ModelList(data=model_cards)\n\n\n@router_app.post(\"/v1/completions\")\nasync def create_completion(\n        body: CompletionRequest,\n        request: Request\n):\n    generator = await openai_serving_completion.create_completion(body, request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            status_code=generator.code\n        )\n    if body.stream:\n        return StreamingResponse(\n            content=generator,\n            media_type=\"text/event-stream\"\n        )\n    else:\n        return JSONResponse(content=generator.model_dump())\n\n\n@router_app.post(\"/v1/chat/completions\")\nasync def create_chat_completion(\n        body: ChatCompletionRequest,\n        request: Request,\n):\n    \"\"\"Completion API similar to OpenAI's API.\n\n    See  https://platform.openai.com/docs/api-reference/chat/create\n    for the API specification. This API mimics the OpenAI ChatCompletion API.\n\n    NOTE: Currently we do not support the following features:\n        - function_call (Users should implement this by themselves)\n        - logit_bias (to be supported by vLLM engine)\n    \"\"\"\n    # async with async_timeout.timeout(TIMEOUT):\n\n    generator = await openai_serving_chat.create_chat_completion(body, request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            status_code=generator.code\n        )\n    if body.stream:\n        return StreamingResponse(\n            content=generator,\n            media_type=\"text/event-stream\"\n        )\n    else:\n        return JSONResponse(content=generator.model_dump())\n\n\n@router_app.post(\"/v1/embeddings\")\nasync def embed(body: Embeddings):\n    \"\"\"Given a prompt, the model will return one embedding.\n\n    Returns:\n        A response object with an embedding.\n    \"\"\"\n    embedding_id = f\"embed-{random_uuid()}\"\n\n    results_list = llm_client.emb(body.model, request=LLMRequest(instruction=body.input))\n    tokens = 0\n\n    return EmbeddingsOutput(\n        data=[\n            EmbeddingsData(\n                embedding=results.output,\n                index=i,\n                object=\"embedding\",\n            )\n            for i, results in enumerate(results_list)\n        ],\n        id=embedding_id,\n        object=\"list\",\n        created=int(time.time()),\n        model=body.model,\n        usage=EmbeddingsUsage(\n            prompt_tokens=tokens,\n            total_tokens=tokens,\n        ),\n    )\n\nclass ServerArgs(BaseModel):\n    host: str = None\n    port: int = 8000\n    uvicorn_log_level: str = \"info\"\n    allow_credentials: bool = False\n    allowed_origins: List[str] = [\"*\"]  \n    allowed_methods: List[str] = [\"*\"]\n    allowed_headers: List[str] = [\"*\"]\n    api_key: str = None\n    served_model_name: str = None\n    prompt_template: str = None\n    response_role: str = \"assistant\"\n    ssl_keyfile: str = None\n    ssl_certfile: str = None    \n\ndef serve(llm:ByzerLLM, args: ServerArgs):\n    \n    logger.info(f\"ByzerLLM API server version {version}\")\n    logger.info(f\"args: {args}\")\n\n    router_app.add_middleware(\n        CORSMiddleware,\n        allow_origins=args.allowed_origins,\n        allow_credentials=args.allow_credentials,\n        allow_methods=args.allowed_methods,\n        allow_headers=args.allowed_headers,\n    )\n    \n    if token := os.environ.get(\"BYZERLLM_API_KEY\") or args.api_key:\n\n        @router_app.middleware(\"http\")\n        async def authentication(request: Request, call_next):\n            if not request.url.path.startswith(\"/v1\"):\n                return await call_next(request)\n            if request.headers.get(\"Authorization\") != \"Bearer \" + token:\n                return JSONResponse(\n                    content={\"error\": \"Unauthorized\"},\n                    status_code=401\n                )\n            return await call_next(request)\n\n    # Register labels for metrics\n    # add_global_metrics_labels(model_name=engine_args.model)\n    global llm_client\n    llm_client = llm\n    \n    global openai_serving_chat\n    openai_serving_chat = OpenAIServingChat(\n        llm_client=llm_client,\n        response_role=args.response_role,\n        server_model_name=args.served_model_name,\n        prompt_template=args.prompt_template\n    )\n    global openai_serving_completion\n    openai_serving_completion = OpenAIServingCompletion(\n        llm_client=llm_client,\n        server_model_name=args.served_model_name,\n        prompt_template=args.prompt_template\n    )\n\n    uvicorn.run(\n        router_app,\n        host=args.host,\n        port=args.port,\n        log_level=args.uvicorn_log_level,\n        timeout_keep_alive=TIMEOUT_KEEP_ALIVE,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile\n    )\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/generator.py\nfrom typing import Optional, Dict, Any, List\nfrom openai import OpenAI\nimport json\nfrom .types import RAGConfig, ByzerSQLConfig, ModelConfig\nfrom byzerllm.utils.client import code_utils\nimport byzerllm\nimport requests\nfrom loguru import logger\nimport traceback\nfrom byzerllm.utils.nontext import TagExtractor\n\n\nclass SQLGenerator:\n\n    @byzerllm.prompt()\n    def gen_context(self, conversations: List[Dict[str, str]]) -> str:\n        \"\"\"\n        对话内容：\n        {% for msg in conversations %}\n        <{{ msg.role }}>: {{ msg.content }}\n        {% endfor %}\n\n        根据提供的对话内容，理解用户最后的问题，然后提取可能会用到的 Byzer-SQL 相关的知识。\n        \"\"\"\n\n    @byzerllm.prompt()\n    def gen_sql(\n        self,\n        conversations: List[Dict[str, str]],\n        context: str,\n        schema: str,\n        model: str = \"deepseek_chat\",\n    ) -> str:\n        \"\"\"\n        相关知识：\n        {{ context }}\n\n        Schema信息：\n        {{ schema }}\n\n        对话内容：\n        {% for msg in conversations %}\n        <{{ msg.role }}>: {{ msg.content }}\n        {% endfor %}\n\n        模型UDF函数：\n        {{ model }}\n\n        根据提供的相关知识以及对话内容，理解用户最后的问题，然后根据看到的schema 信息，生成 Byzer-SQL 语句。\n        Byzer-SQL 首先需要用 load 语法加载 schema 中的路径映射成表格，然后根据表格的信息生成对应的 Byzer-SQL 语句。\n\n        特别需要关注的是:\n        1. 用户的很多需求是要对表格的文本内容做自然语言处理，你需要使用 Byzer-SQL 中的大模型辅助函数来完成这个需求。\n        2. 使用 `llm_result` 函数处理 LLM 模型函数的响应,`llm_result` 函数得到的是一段文本，虽然我们要求模型函数只输出特定的文本，但考虑到大模型会有一些额外的字符输出，不会只是输出我们要求的文字，所以通过 `like` 来进行匹配。\n        3. 注意 Byzer-SQL 的语法，比如select 语句最后一定需要有 as <table_name>。\n        3. 输出的 Byzer-SQL 语句务必使用 <_byzer_sql_></_byzer_sql_> 标签包裹起来。\n        \"\"\"\n\n    @byzerllm.prompt()\n    def fetch_schema(self, context: str) -> str:\n        \"\"\"\n        下面是一些基础知识，包括名词术语等等：\n        {{ context }}\n\n        当理解对话内容的时候，需要参考上面的基础知识，然后更好的理解对应的表格schema信息是不是有用。\n        \"\"\"\n\n    def __init__(\n        self,\n        llm: byzerllm.ByzerLLM,\n        rag_config: RAGConfig,\n        byzer_sql_config: ByzerSQLConfig,\n        model_config: ModelConfig,\n    ) -> None:\n        self.llm = llm\n        self.schema_rag_client = OpenAI(\n            api_key=rag_config.schema_rag_api_key,\n            base_url=rag_config.schema_rag_base_url,\n        )\n        self.context_rag_client = OpenAI(\n            api_key=rag_config.context_rag_api_key,\n            base_url=rag_config.context_rag_base_url,\n        )\n        self.byzer_sql_url = byzer_sql_config.byzer_sql_url\n\n        self.llm_model = byzerllm.ByzerLLM()\n        self.llm_model.setup_default_model_name(model_config.llm_model)\n        self.gen_sql_llm_model = byzerllm.ByzerLLM()\n        self.gen_sql_llm_model.setup_default_model_name(model_config.gen_sql_llm_model)\n\n    def stream_chat_oai(\n        self,\n        conversations,\n        model: Optional[str] = None,\n        role_mapping=None,\n        delta_mode=False,\n        llm_config: Dict[str, Any] = {},\n    ):\n        try:\n            v, c = self._stream_chat_oai(\n                conversations=conversations,\n                model=model,\n                role_mapping=role_mapping,\n                delta_mode=delta_mode,\n                llm_config=llm_config,\n            )\n        except Exception as e:\n            error_msg = f\"Error in stream_chat_oai: {e}\\n{traceback.format_exc()}\"\n            logger.error(error_msg)\n            v, c = [str(e)], []\n\n        return v, c\n\n    def _stream_chat_oai(\n        self,\n        conversations,\n        model: Optional[str] = None,\n        role_mapping=None,\n        delta_mode=False,\n        llm_config: Dict[str, Any] = {},\n    ):\n        logger.info(f\"Starting stream_chat_oai with model: {model}\")\n        logger.debug(f\"Conversations: {conversations}\")\n        logger.debug(f\"LLM Config: {llm_config}\")\n\n        if not model:\n            model = self.llm_model.default_model_name\n        logger.info(f\"Using model: {model}\")\n\n        query = conversations[-1][\"content\"]\n\n        if (\n            \"使用四到五个字直接返回这句话的简要主题，不要解释、不要标点、不要语气词、不要多余文本，不要加粗，如果没有主题\"\n            in query\n            or \"简要总结一下对话内容，用作后续的上下文提示 prompt，控制在 200 字以内\"\n            in query\n        ):\n            chunks = self.llm.stream_chat_oai(\n                conversations=conversations,\n                model=model,\n                role_mapping=role_mapping,\n                llm_config=llm_config,\n                delta_mode=True,\n            )\n            return (chunk[0] for chunk in chunks), []\n\n        logger.info(\"Fetching context from context RAG client\")\n        context_query = self.gen_context.prompt(conversations=conversations)\n        context_response = self.context_rag_client.chat.completions.create(\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": json.dumps(\n                        {\"query\": context_query, \"only_contexts\": True},\n                        ensure_ascii=False,\n                    ),\n                }\n            ],\n            model=model,\n        )\n\n        context = context_response.choices[0].message.content\n        new_context = \"\"\n        for msg in context.split(\"\\n\"):\n            if msg.strip():\n                v = json.loads(msg)\n                new_context += v[\"source_code\"]\n        logger.info(f\"context {new_context[0:100]}...\")\n        logger.info(\"Context retrieved successfully\")\n\n        logger.info(\"Fetching schemas from schema RAG client\")\n        schema_query = self.fetch_schema.prompt(context=new_context)\n\n        response = self.schema_rag_client.chat.completions.create(\n            messages=conversations[0:-1]\n            + [\n                {\n                    \"role\": \"user\",\n                    \"content\": json.dumps(\n                        {\"query\": schema_query, \"only_contexts\": True},\n                        ensure_ascii=False,\n                    ),\n                }\n            ],\n            model=model,\n        )        \n\n        schema_str = response.choices[0].message.content\n        logger.debug(f\"Schema RAG response: {schema_str}\")\n        logger.info(\"Schema retrieved successfully\")\n\n        logger.info(\"Generating SQL string\")\n        sql_str = self.gen_sql.with_llm(self.gen_sql_llm_model).run(\n            conversations=conversations,\n            context=new_context,\n            schema=schema_str,\n            model=model,\n        )\n\n        logger.info(\"SQL string generated\")\n        logger.debug(f\"Generated SQL string: {sql_str}\")\n\n        # Extract SQL code from tags\n        tag_extractor = TagExtractor(sql_str)\n        extracted_tags = tag_extractor.extract()\n        sql = extracted_tags.content[0].content\n        logger.info(f\"Extracted SQL: {sql}\")\n\n        logger.info(f\"Sending SQL to Byzer SQL URL: {self.byzer_sql_url}\")\n        try:\n            response = requests.post(\n                self.byzer_sql_url,\n                data={\n                    \"sql\": sql,\n                    \"owner\": \"hello\",\n                    \"sessionPerUser\": True,\n                    \"sessionPerRequest\": True,\n                    \"includeSchema\": True,\n                },\n            )\n            response.raise_for_status()\n            result = response.content.decode(\"utf-8\")\n            logger.info(\"SQL execution result decoded\")\n            logger.debug(f\"SQL execution result: {result}\")\n            return (item for item in [result]), []\n        except requests.exceptions.RequestException as e:\n            logger.error(f\"Failed to send SQL to Byzer SQL URL: {e}\")\n            if hasattr(e.response, \"content\"):\n                error_content = e.response.content.decode(\"utf-8\")\n                logger.error(f\"Server response: {error_content}\")\n            else:\n                logger.error(\"No server response available\")\n            raise Exception(f\"Byzer SQL request failed: {str(e)}\") from e\n\n\n##File: /Users/allwefantasy/projects/super_analysis/setup.py\nfrom setuptools import setup, find_packages\n\n# Read requirements from requirements.txt\nwith open('requirements.txt') as f:\n    requirements = f.read().splitlines()\n\nsetup(\n    name=\"super-analysis\",\n    version=\"0.1.0\",\n    packages=find_packages(where=\"src\"),\n    package_dir={\"\": \"src\"},\n    install_requires=requirements,\n    entry_points={\n        \"console_scripts\": [\n            \"super-analysis.serve=superanalysis.serve.cli:main\",\n            \"super-analysis.convert=superanalysis.serve.convert:main\",\n            \"superanalysis.api=superanalysis.serve.rest:main\",\n        ],\n    },\n    author=\"allwefantasy\",\n    author_email=\"allwefantasy@gmail.com\",\n    description=\"A package for super analysis with ByzerLLM\",\n    long_description=open(\"README.md\").read(),\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/yourusername/super-analysis\",\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n    ],\n    python_requires='>=3.6',\n)\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/convert.py\nimport os\nimport argparse\nimport pandas as pd\nimport hashlib\nfrom typing import List, Dict\n\ndef get_csv_files(directory: str) -> List[str]:\n    \"\"\"Recursively get all CSV files in the given directory.\"\"\"\n    csv_files = []\n    for root, _, files in os.walk(directory):\n        for file in files:\n            if file.endswith('.csv'):\n                csv_files.append(os.path.join(root, file))\n    return csv_files\n\ndef extract_schema(df: pd.DataFrame) -> Dict[str, str]:\n    \"\"\"Extract schema from pandas DataFrame.\"\"\"\n    schema = {}\n    for column in df.columns:\n        dtype = str(df[column].dtype)\n        if dtype == 'object':\n            schema[column] = 'STRING'\n        elif dtype.startswith('int'):\n            schema[column] = 'INTEGER'\n        elif dtype.startswith('float'):\n            schema[column] = 'FLOAT'\n        elif dtype == 'bool':\n            schema[column] = 'BOOLEAN'\n        elif dtype.startswith('datetime'):\n            schema[column] = 'TIMESTAMP'\n        else:\n            schema[column] = 'STRING'  # Default to STRING for unknown types\n    return schema\n\ndef generate_schema_text(schema: Dict[str, str], csv_file: str, df: pd.DataFrame, include_rows_num: int) -> str:\n    \"\"\"Generate schema text from the schema dictionary and include the CSV file name and sample data.\"\"\"\n    schema_lines = [f\"CSV File: {csv_file}\\n\"]\n    schema_lines.extend([f\"{field}: {dtype}\" for field, dtype in schema.items()])\n    \n    if include_rows_num > 0:\n        schema_lines.append(f\"\\nSample Data (first {include_rows_num} rows):\")\n        sample_data = df.head(include_rows_num).to_string(index=False)\n        schema_lines.append(sample_data)\n    \n    return \"\\n\".join(schema_lines)\n\ndef save_schema(schema_text: str, csv_file: str, output_dir: str) -> None:\n    \"\"\"Save schema text to a file in the output directory.\"\"\"\n    # Generate MD5 hash of the schema text\n    file_name = hashlib.md5(schema_text.encode()).hexdigest() + '.md'\n    output_path = os.path.join(output_dir, file_name)\n    \n    with open(output_path, 'w') as f:\n        f.write(schema_text)\n    print(f\"Schema for {csv_file} saved to {output_path}\")\n\ndef convert(data_dir: str, doc_dir: str, include_rows_num: int) -> None:\n    \"\"\"Main conversion function.\"\"\"\n    csv_files = get_csv_files(data_dir)\n    \n    for csv_file in csv_files:\n        print(f\"Processing {csv_file}\")\n        df = pd.read_csv(csv_file)\n        schema = extract_schema(df)\n        schema_text = generate_schema_text(schema, csv_file, df, include_rows_num)\n        save_schema(schema_text, csv_file, doc_dir)\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Convert CSV files to schema documents\")\n    parser.add_argument(\"--data_dir\", required=True, help=\"Directory containing CSV files\")\n    parser.add_argument(\"--doc_dir\", required=True, help=\"Output directory for schema documents\")\n    parser.add_argument(\"--include-rows-num\", type=int, default=0, help=\"Number of sample rows to include in the schema\")\n    \n    args = parser.parse_args()\n    \n    if not os.path.exists(args.doc_dir):\n        os.makedirs(args.doc_dir)\n    \n    convert(args.data_dir, args.doc_dir, args.include_rows_num)\n\nif __name__ == \"__main__\":\n    main()\n\n##File: /Users/allwefantasy/projects/super_analysis/README.md\n# Super Analysis\n\nSuper Analysis 支持 NLP2SQL 以及 使用 SQL 对表中的文本字段做自然语言分析。\n\n## 安装\n\n你可以使用 pip 安装 Super Analysis：\n\n```bash\ngit clone https://github.com/allwefantasy/super_analysis\ncd super-analysis\npip install -e .\n```\n\n或者使用 Makefile 来管理项目：\n\n```bash\n# 清理项目\nmake clean\n\n# 构建项目\nmake build\n\n# 创建wheel包\nmake wheel\n\n# 安装项目\nmake install\n\n# 卸载项目\nmake uninstall\n\n# 以开发模式安装项目\nmake develop\n\n# 运行测试\nmake test\n```\n\n\n\n## 示例数据\n\n下载电影数据集： https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset/download?datasetVersionNumber=7\n\n\n## 启动分析服务\n\n整个 super-analysis 服务分为四个部分：\n\n1. byzer-sql 分布式执行引擎 \n2. schema 文档知识库\n3. context 文档知识库\n4. super-analysis 自身\n\n在启动 super-analysis 之前，你需要启动 schema 文档知识库和 context 文档知识库以及 byzer-sql 引擎。\n\n## 部署 Deepseek 模型代理\n\n在启动其他服务之前，我们需要先部署 Deepseek 模型。你可以使用以下命令来部署：\n\n```bash\nmake deploy_deepseek\n```\n\n或者直接运行以下命令：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--worker_concurrency 1000 \\\n--num_workers 1 \\\n--infer_params saas.base_url=\"https://api.deepseek.com/v1\" saas.api_key=${MODEL_DEEPSEEK_TOKEN} saas.model=deepseek-chat \\\n--model deepseek_chat\n```\n\n注意：请确保在运行此命令之前，已经设置了环境变量 `MODEL_DEEPSEEK_TOKEN`。\n\n## 部署 Byzer-SQL\n\n在你的电脑上参考 [这个文档](./docs/4.3.1%20安装与配置%20Byzer-SQL.pdf) 完成部署.\n\n> 启动的时候需要在安装有 super-analysis 的conda环境中启动。\n\n\n\n当 byzer-sql 部署完成后，注册账号为 `hello`, 然后进入到 byzer-sql 的控制台中执行以下脚本：\n\n```sql\n!byzerllm setup single;\n\nrun command as LLM.`` where \naction=\"infer\"\nand reconnect=\"true\"\nand pretrainedModelType=\"saas/openai\"\nand udfName=\"deepseek_chat\";\n```\n\n## 启动其他服务\n\n现在可以按如下方式做一些数据预处理或者启动相关服务：\n\n```bash\n## 将电影数据集的csv文件转换为schema文档\nsuper-analysis.convert --data_dir /Users/allwefantasy/data/movice --doc_dir /Users/allwefantasy/data/movice/schemas/\n\n# 启动schema文档知识库\nauto-coder doc serve \\\n--model deepseek_chat --index_filter_workers 100 \\\n--tokenizer_path /Users/allwefantasy/Downloads/tokenizer.json \\\n--doc_dir /Users/allwefantasy/data/movice/schemas/ \\\n--port 8001\n\n# 下载 Byzer-SQL 专门为大模型优化的文档\ngit clone https://github.com/allwefantasy/llm_friendly_packages\n\n# 启动文档知识库\nauto-coder doc serve \\\n--model deepseek_chat --index_filter_workers 100 \\\n--tokenizer_path /Users/allwefantasy/Downloads/tokenizer.json \\\n--doc_dir  /Users/allwefantasy/projects/llm_friendly_packages/github.com/allwefantasy \\\n--port 8002\n\n# 启动兼容 OpenAI Server 的分析服务\nsuper-analysis.serve --served-model-name deepseek_chat --port 8000 \\\n--schema-rag-base-url http://127.0.0.1:8001/v1 \\\n--context-rag-base-url http://127.0.0.1:8002/v1 \\\n--byzer-sql-url http://127.0.0.1:9003/run/script\n```\n\n现在你可以用 OpenAI SDK 去使用了。 具体测试和接口使用参考 [openai_local_api.ipynb](./notebooks/openai_local_api.ipynb)\n\n\n## 非 OpenAI 兼容接口\n\n除了兼容 OpenAI 的接口外，Super Analysis 还提供了一个简单的 REST API 接口，可以直接与模型进行交互。以下是如何使用这个接口：\n\n### 启动服务\n\n使用以下命令启动 REST API 服务：\n\n```bash\nsuperanalysis.api --schema-rag-base-url <schema_rag_url> \\\n                  --schema-rag-api-key <schema_rag_api_key> \\\n                  --context-rag-base-url <context_rag_url> \\\n                  --context-rag-api-key <context_rag_api_key> \\\n                  --byzer-sql-url <byzer_sql_url> \\\n                  --llm-model <llm_model_name> \\\n                  --gen-sql-llm-model <gen_sql_llm_model_name> \\\n                  --host <host> \\\n                  --port <port>\n```\n\n请替换尖括号中的值为实际的配置信息。\n\n### API 端点\n\n服务启动后，将提供以下 API 端点：\n\n#### POST /chat\n\n此端点用于与模型进行对话。\n\n请求体格式：\n\n```json\n{\n  \"conversations\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"你的问题或指令\"\n    }\n  ],\n  \"model\": \"可选的模型名称\"\n}\n```\n\n响应格式：\n\n```json\n{\n  \"response\": \"模型的回答\"\n}\n```\n\n### 示例使用\n\n以下是使用 Python 请求库与 API 交互的示例：\n\n```python\nimport requests\nimport json\n\nurl = \"http://localhost:8000/chat\"\npayload = {\n    \"conversations\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"你好，请介绍一下你自己。\"\n        }\n    ],\n    \"model\": \"your_model_name\"\n}\n\nresponse = requests.post(url, json=payload)\nresult = response.json()\n\nprint(result[\"response\"])\n```\n\n这个 API 相比兼容 OpenAI 接口，可以在请求中提供提供更多可选参数。\n\n## 使用方法\n\n安装完成后，你可以使用 `super-analysis.serve` 命令来启动 API 服务器：\n\n```bash\nsuper-analysis.serve --served-model-name your_model_name\n```\n\n### 命令行参数\n\n以下是可用的命令行参数：\n\n- `--host`: 服务器绑定的主机地址（默认：0.0.0.0）\n- `--port`: 服务器绑定的端口（默认：8000）\n- `--uvicorn-log-level`: Uvicorn 日志级别（默认：info）\n- `--allow-credentials`: 允许凭证（默认：False）\n- `--allowed-origins`: 允许的源（默认：[\"*\"]）\n- `--allowed-methods`: 允许的方法（默认：[\"*\"]）\n- `--allowed-headers`: 允许的头部（默认：[\"*\"]）\n- `--api-key`: 用于认证的 API 密钥\n- `--served-model-name`: 要服务的模型名称（必需）\n- `--prompt-template`: 提示模板\n- `--response-role`: 响应角色（默认：assistant）\n- `--ssl-keyfile`: SSL 密钥文件路径\n- `--ssl-certfile`: SSL 证书文件路径\n\n例如：\n\n```bash\nsuper-analysis.serve --served-model-name deepseek_chat --port 8000\n```\n\n\n## 抽取csv文件schema\n\nThe `super-analysis.convert` command allows you to convert CSV files to schema documents. This is useful for generating metadata about your CSV files that can be used for further analysis or documentation.\n\n#### Usage\n\n```\nsuper-analysis.convert --data_dir <path_to_csv_files> --doc_dir <output_directory> [--include-rows-num <number_of_sample_rows>]\n```\n\nArguments:\n- `--data_dir`: Directory containing CSV files (required)\n- `--doc_dir`: Output directory for schema documents (required)\n- `--include-rows-num`: Number of sample rows to include in the schema (optional, default is 0)\n\nThe utility will:\n1. Recursively find all CSV files in the specified directory\n2. Extract the schema from each CSV file\n3. Generate a schema text that includes:\n   - The CSV file name\n   - Field names and their data types\n   - Sample data (if `--include-rows-num` is specified)\n4. Save the schema text as a markdown file in the output directory\n\n## 联系方式\n\n如有任何问题或建议，请联系 allwefantasy@gmail.com。\n\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/rest.py\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nfrom typing import List, Optional\nimport uvicorn\nimport argparse\nfrom superanalysis.sql.generator import SQLGenerator\nfrom superanalysis.sql.types import RAGConfig, ByzerSQLConfig, ModelConfig\nimport byzerllm\n\napp = FastAPI()\n\nclass ChatMessage(BaseModel):\n    role: str\n    content: str\n\nclass ChatRequest(BaseModel):\n    conversations: List[ChatMessage]\n    model: Optional[str] = None\n\nclass ChatResponse(BaseModel):\n    response: str\n\n@app.post(\"/chat\", response_model=ChatResponse)\nasync def chat(request: ChatRequest):\n    try:\n        result, _ = sql_generator.stream_chat_oai(\n            conversations=[{\"role\": msg.role, \"content\": msg.content} for msg in request.conversations],\n            model=request.model\n        )\n        return ChatResponse(response=\"\".join(result))\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"SuperAnalysis API Server\")\n    parser.add_argument(\"--host\", type=str, default=\"0.0.0.0\", help=\"Host to bind the server to\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"Port to bind the server to\")\n    parser.add_argument(\"--schema-rag-base-url\", type=str, required=True, help=\"Schema RAG base URL\")\n    parser.add_argument(\"--schema-rag-api-key\", type=str, required=True, help=\"Schema RAG API key\")\n    parser.add_argument(\"--context-rag-base-url\", type=str, required=True, help=\"Context RAG base URL\")\n    parser.add_argument(\"--context-rag-api-key\", type=str, required=True, help=\"Context RAG API key\")\n    parser.add_argument(\"--byzer-sql-url\", type=str, required=True, help=\"Byzer SQL URL\")\n    parser.add_argument(\"--data-dir\", type=str, default=\"\", help=\"Data directory\")\n    parser.add_argument(\"--owner\", type=str, default=\"hello\", help=\"Owner of the Byzer SQL session\")\n    parser.add_argument(\"--llm-model\", type=str, required=True, help=\"LLM model for general use\")\n    parser.add_argument(\"--gen-sql-llm-model\", type=str, required=True, help=\"LLM model for SQL generation\")\n    \n    args = parser.parse_args()\n\n    rag_config = RAGConfig(\n        schema_rag_base_url=args.schema_rag_base_url,\n        schema_rag_api_key=args.schema_rag_api_key,\n        context_rag_base_url=args.context_rag_base_url,\n        context_rag_api_key=args.context_rag_api_key\n    )\n\n    byzer_sql_config = ByzerSQLConfig(\n        byzer_sql_url=args.byzer_sql_url,\n        data_dir=args.data_dir,\n        owner=args.owner\n    )\n\n    model_config = ModelConfig(\n        llm_model=args.llm_model,\n        gen_sql_llm_model=args.gen_sql_llm_model\n    )\n\n    llm = byzerllm.ByzerLLM()\n    llm.setup_default_model_name(args.llm_model)\n\n    global sql_generator\n    sql_generator = SQLGenerator(\n        llm=llm,\n        rag_config=rag_config,\n        byzer_sql_config=byzer_sql_config,\n        model_config=model_config\n    )\n\n    uvicorn.run(app, host=args.host, port=args.port)\n\nif __name__ == \"__main__\":\n    main()\n\n##File: /Users/allwefantasy/projects/super_analysis/Makefile\n# Makefile for super-analysis project\n\n# Python interpreter\nPYTHON := python\n\n# Project name\nPROJECT_NAME := super-analysis\n\n# Directories\nSRC_DIR := src\nDIST_DIR := dist\nBUILD_DIR := build\n\n# Commands\nSETUP := $(PYTHON) setup.py\n\n.PHONY: all clean build wheel deploy_deepseek\n\nall: clean build wheel\n\ndeploy_deepseek:\n\tbyzerllm deploy --pretrained_model_type saas/openai \\\n\t--cpus_per_worker 0.001 \\\n\t--gpus_per_worker 0 \\\n\t--worker_concurrency 1000 \\\n\t--num_workers 1 \\\n\t--infer_params saas.base_url=\"https://api.deepseek.com/v1\" saas.api_key=${MODEL_DEEPSEEK_TOKEN} saas.model=deepseek-chat \\\n\t--model deepseek_chat\n\nclean:\n\t@echo \"Cleaning up...\"\n\trm -rf $(BUILD_DIR) $(DIST_DIR) *.egg-info\n\tfind . -type d -name \"__pycache__\" -exec rm -rf {} +\n\tfind . -type f -name \"*.pyc\" -delete\n\nbuild:\n\t@echo \"Building the project...\"\n\t$(SETUP) build\n\nwheel:\n\t@echo \"Creating wheel package...\"\n\t$(SETUP) bdist_wheel\n\ninstall:\n\t@echo \"Installing the package...\"\n\t$(PYTHON) -m pip install .\n\nuninstall:\n\t@echo \"Uninstalling the package...\"\n\t$(PYTHON) -m pip uninstall -y $(PROJECT_NAME)\n\ndevelop:\n\t@echo \"Installing the package in editable mode...\"\n\t$(PYTHON) -m pip install -e .\n\ntest:\n\t@echo \"Running tests...\"\n\t$(PYTHON) -m unittest discover tests\n\n##File: /Users/allwefantasy/projects/super_analysis/docs/deploy.md\n\n# 🚀 Super Analysis 部署指南\n\n本指南将帮助你部署 Super Analysis 系统，包括安装必要组件、配置服务和启动系统。\n\n---\n\n## 📦 安装 Super Analysis\n\n\n```bash\npip install -U auto-coder\npip install super_analysis-0.1.0-py3-none-any.whl\n```\n\n\n---\n\n## 🤖 部署 Deepseek 模型代理\n\n在启动其他服务之前，我们需要先部署 Deepseek 模型：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--worker_concurrency 1000 \\\n--num_workers 1 \\\n--infer_params saas.base_url=\"https://api.deepseek.com/v1\" saas.api_key=${MODEL_DEEPSEEK_TOKEN} saas.model=deepseek-chat \\\n--model deepseek_chat\n```\n\n注意：确保已设置环境变量 `MODEL_DEEPSEEK_TOKEN`。\n\n---\n\n## 🛠️ 部署 Byzer-SQL\n\n参考 [安装与配置 Byzer-SQL 文档](./4.3.1%20安装与配置%20Byzer-SQL.pdf) 完成部署。\n\n> 启动时需要在安装有 super-analysis 的 conda 环境中启动。\n\n当 byzer-sql 部署完成后，注册账号为 `hello`，然后在 byzer-sql 控制台中执行：\n\n```sql\n!byzerllm setup single;\n\nrun command as LLM.`` where \naction=\"infer\"\nand reconnect=\"true\"\nand pretrainedModelType=\"saas/openai\"\nand udfName=\"deepseek_chat\";\n```\n\n---\n\n## 📊 数据预处理和服务启动\n\n1. 转换电影数据集：\n\n```bash\nsuper-analysis.convert --data_dir /Users/allwefantasy/data/movice --doc_dir /Users/allwefantasy/data/movice/schemas/\n```\n\n2. 启动 schema 文档知识库：\n\n```bash\nauto-coder doc serve \\\n--model deepseek_chat --index_filter_workers 100 \\\n--tokenizer_path /Users/allwefantasy/Downloads/tokenizer.json \\\n--doc_dir /Users/allwefantasy/data/movice/schemas/ \\\n--port 8001\n```\n\n3. 下载 Byzer-SQL 文档并启动文档知识库：\n\n```bash\ngit clone https://github.com/allwefantasy/llm_friendly_packages\n\nauto-coder doc serve \\\n--model deepseek_chat --index_filter_workers 100 \\\n--tokenizer_path /Users/allwefantasy/Downloads/tokenizer.json \\\n--doc_dir  /Users/allwefantasy/projects/llm_friendly_packages/github.com/allwefantasy \\\n--port 8002\n```\n\n4. 启动兼容 OpenAI Server 的分析服务：\n\n```bash\nsuper-analysis.serve --served-model-name deepseek_chat --port 8000 \\\n--schema-rag-base-url http://127.0.0.1:8001/v1 \\\n--context-rag-base-url http://127.0.0.1:8002/v1 \\\n--byzer-sql-url http://127.0.0.1:9003/run/script\n```\n\n---\n\n现在，Super Analysis 系统已经完全部署并启动。你可以开始使用 OpenAI SDK 进行测试和接口调用。具体测试和接口使用方法请参考 [openai_local_api.ipynb](./notebooks/openai_local_api.ipynb)。\n\n🎉 恭喜！你已经成功部署了 Super Analysis 系统。如有任何问题，请随时查阅文档或联系支持团队。\n\n##File: .auto-coder/libs/llm_friendly_packages/github.com/allwefantasy/byzer-llm/README.md\n# byzerllm 大模型编程快速指南\n\n本文示例 [Notebook](../../notebooks/003_byzerllm_大模型编程快速指南.ipynb)\n\n## 安装\n\n```bash\npip install -U byzerllm\nray start --head\n```\n## 启动一个模型代理\n\nbyzerllm 支持私有化模型或者SaaS模型的部署。\n\n这里以 deepseek 官方API 为例：\n\n```bash\neasy-byzerllm deploy deepseek-chat --token xxxxx --alias deepseek_chat\n```\n\n或者跬基流动API:\n\n```bash\neasy-byzerllm deploy alibaba/Qwen1.5-110B-Chat --token xxxxx --alias qwen110b_chat\n```\n\n将上面的 API KEY 替换成你们自己的。\n\n如果你想部署私有化模型或者对接 Ollama 等更多需求，参考 [002_使用byzerllm进行模型部署.md](./002_使用byzerllm进行模型部署.md) 或者 [README.md](../../README.md)。\n\n之后，你就可以在代码里使用  deepseek_chat 或者 qwen110b_chat  访问模型了。\n\n## hello world\n\n来和我们的大模型打个招呼:\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\n@byzerllm.prompt(llm=llm)\ndef hello(q:str) ->str:\n    '''\n    你好, {{ q }}\n    '''\n\ns = hello(\"你是谁\")    \nprint(s)\n\n## 输出:\n## '你好！我是一个人工智能助手，专门设计来回答问题、提供信息和帮助解决问题。如果你有任何疑问或需要帮助，请随时告诉我。'\n```\n\n恭喜，你和大模型成功打了招呼！\n\n可以看到，我们通过 `@byzerllm.prompt` 装饰器，将一个方法转换成了一个大模型的调用，然后这个方法的主题是一段文本，文本中\n使用了 jinja2 模板语法，来获得方法的参数。当正常调用该方法时，实际上就发起了和大模型的交互，并且返回了大模型的结果。\n\n在 byzerllm 中，我们把这种方法称之为 prompt 函数。\n\n## 查看发送给大模型的prompt\n\n很多情况你可能需要调试，查看自己的 prompt 渲染后到底是什么样子的，这个时候你可以通过如下方式\n获取渲染后的prompt:\n\n```python\nhello.prompt(\"你是谁\")\n## '你好, 你是谁'\n```            \n\n## 动态换一个模型\n\n前面的 hello 方法在初始化的时候，我们使用了默认的模型 deepseek_chat，如果我们想换一个模型，可以这样做：\n\n```python\nhello.with_llm(llm).run(\"你是谁\")\n## '你好！我是一个人工智能助手，专门设计来回答问题、提供信息和帮助解决问题。如果你有任何疑问或需要帮助，请随时告诉我。'\n```\n\n通过 with_llm 你可以设置一个新的 llm 对象，然后调用 run 方法，就可以使用新的模型了。\n\n## 超长文本生成\n\n我们知道，大模型一次生成的长度其实是有限的，如果你想生成超长文本，你可能需手动的不断获得\n生成结果，然后把他转化为输入，然后再次生成，这样的方式是比较麻烦的。\n\nbyzerllm 提供了更加易用的 API :\n\n```python\nimport byzerllm\nfrom byzerllm import ByzerLLM\n\nllm = ByzerLLM.from_default_model(\"deepseek_chat\")\n\n@byzerllm.prompt()\ndef tell_story() -> str:\n    \"\"\"\n    讲一个100字的故事。    \n    \"\"\"\n\n\ns = (\n    tell_story.with_llm(llm)\n    .with_response_markers()\n    .options({\"llm_config\": {\"max_length\": 10}})\n    .run()\n)\nprint(s)\n\n## 从前，森林里住着一只聪明的小狐狸。一天，它发现了一块闪闪发光的宝石。小狐狸决定用这块宝石帮助森林里的动物们。它用宝石的光芒指引迷路的小鹿找到了回家的路，用宝石的温暖治愈了受伤的小鸟。从此，小狐狸成了森林里的英雄，动物们都感激它的善良和智慧。\n```\n\n实际核心部分就是这一行：\n\n```python\ntell_story.with_llm(llm)\n    .with_response_markers()    \n    .run()\n```\n\n我们只需要调用 `with_response_markers` 方法，系统就会自动的帮我们生成超长文本。\n在上面的案例中，我们通过\n\n```python\n.options({\"llm_config\": {\"max_length\": 10}})\n```\n\n认为的限制大模型一次交互最多只能输出10个字符，但是系统依然自动完成了远超过10个字符的文本生成。\n\n## 对象输出\n\n前面我们的例子都是返回字符串，但是我们也可以返回对象，这样我们就可以更加灵活的处理返回结果。\n\n```python\nimport pydantic \n\nclass Story(pydantic.BaseModel):\n    '''\n    故事\n    '''\n\n    title: str = pydantic.Field(description=\"故事的标题\")\n    body: str = pydantic.Field(description=\"故事主体\")\n\n@byzerllm.prompt()\ndef tell_story()->Story:\n    '''\n    讲一个100字的故事。    \n    '''\n\ns = tell_story.with_llm(llm).run()\nprint(isinstance(s, Story))\nprint(s.title)\n\n## True\n## 勇敢的小鸟\n```\n\n可以看到，我们很轻松的将输出转化为格式化输出。\n\n## 自定义字段抽取\n\n前面的结构化输出，其实会消耗更多token,还有一种更加精准的结构化输出方式。\n比如让大模型生成一个正则表达式，但实际上大模型很难准确只输出一个正则表达式，这个时候我们可以通过自定义抽取函数来获取我们想要的结果。\n\n\n```python\nfrom loguru import logger\nimport re\n\n@byzerllm.prompt()\ndef generate_regex_pattern(desc: str) -> str:\n    \"\"\"\n    根据下面的描述生成一个正则表达式，要符合python re.compile 库的要求。\n\n    {{ desc }}\n\n    最后生成的正则表达式要在<REGEX></REGEX>标签对里。\n    \"\"\"    \n\ndef extract_regex_pattern(regex_block: str) -> str:    \n    pattern = re.search(r\"<REGEX>(.*)</REGEX>\", regex_block, re.DOTALL)\n    if pattern is None:\n        logger.warning(\"No regex pattern found in the generated block:\\n {regex_block}\")\n        raise None\n    return pattern.group(1)\n\npattern = \"匹配一个邮箱地址\"\nv = generate_regex_pattern.with_llm(llm).with_extractor(extract_regex_pattern).run(desc=pattern)\nprint(v)\n## ^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\n```\n\n在上面的例子里，我们根据一句话生成一个正则表达式。我们通过 `with_extractor` 方法，传入了一个自定义的抽取函数，这个函数会在大模型生成结果后，对结果进行处理，然后返回我们想要的结果。\n\n我们在 prompt 明确说了，生成的结果要放到 `<REGEX></REGEX>` 标签对里，然后我们通过 extract_regex_pattern 函数，从结果中提取出了我们想要的正则表达式。\n\n## 在实例方法中使用大模型\n\n```python\nimport byzerllm\ndata = {\n    'name': 'Jane Doe',\n    'task_count': 3,\n    'tasks': [\n        {'name': 'Submit report', 'due_date': '2024-03-10'},\n        {'name': 'Finish project', 'due_date': '2024-03-15'},\n        {'name': 'Reply to emails', 'due_date': '2024-03-08'}\n    ]\n}\n\n\nclass RAG():\n    def __init__(self):        \n        self.llm = byzerllm.ByzerLLM()\n        self.llm.setup_template(model=\"deepseek_chat\",template=\"auto\")\n        self.llm.setup_default_model_name(\"deepseek_chat\")        \n    \n    @byzerllm.prompt(lambda self:self.llm)\n    def generate_answer(self,name,task_count,tasks)->str:\n        '''\n        Hello {{ name }},\n\n        This is a reminder that you have {{ task_count }} pending tasks:\n        {% for task in tasks %}\n        - Task: {{ task.name }} | Due: {{ task.due_date }}\n        {% endfor %}\n\n        Best regards,\n        Your Reminder System\n        '''        \n\nt = RAG()\n\nresponse = t.generate_answer(**data)\nprint(response)\n\n## 输出:\n## Hello Jane Doe,\n##I hope this message finds you well. I wanted to remind you of your 3 pending tasks to ensure you stay on track:\n## 1. **Submit report** - This task is due on **2024-03-10**. Please ensure that you allocat\n```\n\n这里我们给了个比较复杂的例子，但我们可以看到，给一个实例prompt方法和普通prompt 方法差异不大。\n唯一的区别是如果你希望在定义的时候就指定大模型，使用一个lambda函数返回实例的 llm 对象即可。\n\n```python\n@byzerllm.prompt(lambda self:self.llm)\n```\n\n你也可以不返回，在调用的时候通过 `with_llm` 方法指定 llm 对象。\n\n此外，这个例子也展示了如何通过jinja2模板语法，来处理复杂的结构化数据。\n\n## 通过 Python 代码处理复杂入参\n\n上面的一个例子中，我们通过 jinja2 模板语法，来处理复杂的结构化数据，但是有时候我们可能需要更加复杂的处理，这个时候我们可以通过 Python 代码来处理。\n\n```python\nimport byzerllm\n\ndata = {\n    'name': 'Jane Doe',\n    'task_count': 3,\n    'tasks': [\n        {'name': 'Submit report', 'due_date': '2024-03-10'},\n        {'name': 'Finish project', 'due_date': '2024-03-15'},\n        {'name': 'Reply to emails', 'due_date': '2024-03-08'}\n    ]\n}\n\n\nclass RAG():\n    def __init__(self):        \n        self.llm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n    \n    @byzerllm.prompt()\n    def generate_answer(self,name,task_count,tasks)->str:\n        '''\n        Hello {{ name }},\n\n        This is a reminder that you have {{ task_count }} pending tasks:\n            \n        {{ tasks }}\n\n        Best regards,\n        Your Reminder System\n        '''\n        \n        tasks_str = \"\\n\".join([f\"- Task: {task['name']} | Due: { task['due_date'] }\" for task in tasks])\n        return {\"tasks\": tasks_str}\n\nt = RAG()\n\nresponse = t.generate_answer.with_llm(t.llm).run(**data)\nprint(response)\n\n## Just a gentle nudge to keep you on track with your pending tasks. Here's a quick recap:....\n```\n\n在这个例子里，我们直接把 tasks 在方法体里进行处理，然后作为一个字符串返回，最够构建一个字典，字典的key为 tasks,然后\n你就可以在 docstring 里使用 `{{ tasks }}` 来引用这个字符串。\n\n这样对于很复杂的入参，就不用谢繁琐的 jinja2 模板语法了。\n\n## 如何自动实现一个方法\n\n比如我定义一个签名，但是我不想自己实现里面的逻辑，让大模型来实现。这个在 byzerllm 中叫 function impl。我们来看看怎么\n实现:\n\n```python\nimport pydantic\nclass Time(pydantic.BaseModel):\n    time: str = pydantic.Field(...,description=\"时间，时间格式为 yyyy-MM-dd\")\n\n\n@llm.impl()\ndef calculate_current_time()->Time:\n    '''\n    计算当前时间\n    '''\n    pass \n\n\ncalculate_current_time()\n#output: Time(time='2024-06-14')\n```\n\n在这个例子里，我们定义了一个 calculate_current_time 方法，但是我们没有实现里面的逻辑，我们通过 `@llm.impl()` 装饰器，让大模型来实现这个方法。\n为了避免每次都要“生成”这个方法，导致无法适用，我们提供了缓存，用户可以按如下方式打印速度：\n\n```python\nstart = time.monotonic()\ncalculate_current_time()\nprint(f\"first time cost: {time.monotonic()-start}\")\n\nstart = time.monotonic()\ncalculate_current_time()\nprint(f\"second time cost: {time.monotonic()-start}\")\n\n# output:\n# first time cost: 6.067266260739416\n# second time cost: 4.347506910562515e-05\n```\n可以看到，第一次执行花费了6s,第二次几乎是瞬间完成的，这是因为第一次执行的时候，我们实际上是在生成这个方法，第二次执行的时候，我们是执行已经生成好的代码，所以速度会非常快。你可以显示的调用 `llm.clear_impl_cache()` 清理掉函数缓存。\n\n## Stream 模式\n\n前面的例子都是一次性生成结果，但是有时候我们可能需要一个流式的输出，这个时候我们可能需要用底层一点的API来完成了：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\nv = llm.stream_chat_oai(model=\"deepseek_chat\",conversations=[{\n    \"role\":\"user\",\n    \"content\":\"你好，你是谁\",\n}],delta_mode=True)\n\nfor t in v:\n    print(t,flush=True)  \n\n# 你好\n# ！\n# 我\n# 是一个\n# 人工智能\n# 助手\n# ，\n# 旨在\n# 提供\n# 信息\n# 、\n# 解答\n# 问题....\n```\n\n如果你不想要流式输出，但是想用底层一点的API，你可以使用 `llm.chat_oai` 方法：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\nv = llm.chat_oai(model=\"deepseek_chat\",conversations=[{\n    \"role\":\"user\",\n    \"content\":\"你好，你是谁\",\n}])\n\nprint(v[0].output)\n## 你好！我是一个人工智能助手，旨在提供信息、解答问题和帮助用户解决问题。如果你有任何问题或需要帮助，请随时告诉我。\n```\n\n## Function Calling \n\nbyzerllm 可以不依赖模型自身就能提供 function calling 支持，我们来看个例子：\n\n\n```python\nfrom typing import List,Dict,Any,Annotated\nimport pydantic \nimport datetime\nfrom dateutil.relativedelta import relativedelta\n\ndef compute_date_range(count:Annotated[int,\"时间跨度，数值类型\"],\n                       unit:Annotated[str,\"时间单位，字符串类型\",{\"enum\":[\"day\",\"week\",\"month\",\"year\"]}])->List[str]:\n    '''\n    计算日期范围\n\n    Args:\n        count: 时间跨度，数值类型\n        unit: 时间单位，字符串类型，可选值为 day,week,month,year\n    '''        \n    now = datetime.datetime.now()\n    now_str = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n    if unit == \"day\":\n        return [(now - relativedelta(days=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"week\":\n        return [(now - relativedelta(weeks=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"month\":\n        return [(now - relativedelta(months=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"year\":\n        return [(now - relativedelta(years=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    return [\"\",\"\"]\n\ndef compute_now()->str:\n    '''\n    计算当前时间\n    '''\n    return datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n```\n\n我们定义了两个方法，一个是计算日期范围，一个是计算当前时间。\n\n现在我么可以来测试下，系统如何根据自然语言决定调用哪个方法：\n\n```python\nt = llm.chat_oai([{\n    \"content\":'''计算当前时间''',\n    \"role\":\"user\"    \n}],tools=[compute_date_range,compute_now],execute_tool=True)\n\nt[0].values\n\n## output: ['2024-06-14 15:18:02']\n```\n\n我们可以看到，他正确的选择了 compute_now 方法。\n\n接着我们再试一个：\n\n```python\nt = llm.chat_oai([{\n    \"content\":'''最近三个月趋势''',\n    \"role\":\"user\"    \n}],tools=[compute_date_range,compute_now],execute_tool=True)\n\nt[0].values\n\n## output: [['2024-03-14 15:19:13', '2024-06-14 15:19:13']]\n```\n\n模型正确的选择了 compute_date_range 方法。\n\n## 多模态\n\nbyerllm 也能很好的支持多模态的交互，而且统一了多模态大模型的接口，比如你可以用一样的方式使用 openai 或者 claude 的图片转文字能力， 或者一致的方式使用火山，azuer, openai的语音合成接口。\n\n### 图生文\n\n```python\nimport byzerllm\nfrom byzerllm.types import ImagePath\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\n\n\n@byzerllm.prompt()\ndef what_in_image(image_path: ImagePath) -> str:\n    \"\"\"\n    {{ image_path }}\n    这个图片里有什么？\n    \"\"\"    \n\n\nv = what_in_image.with_llm(vl_llm).run(\n    ImagePath(value=\"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\")\n)\nv\n## OUTPUT: 这张图片展示了多只可爱的猫咪，采用了艺术风格的绘画。猫咪们有不同的颜色和花纹，背景是浅棕色，上面还点缀着一些红色的花朵。整体画面给人一种温馨和谐的感觉\n```\n\n可以看到，我们只需要把 prompt 函数的图片地址入参使用 byzerllm.types.ImagePath里进行包装，就可以直接在 prompt 函数体里\n带上图片。\n\n或者你可以这样：\n\n```python\nimport byzerllm\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\n\n\n@byzerllm.prompt()\ndef what_in_image(image_path: str) -> str:\n    \"\"\"\n    {{ image }}\n    这个图片里有什么？\n    \"\"\"\n    return {\"image\": byzerllm.Image.load_image_from_path(image_path)}\n\n\nv = what_in_image.with_llm(vl_llm).run(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv\n```\n\n通过 `image_path` 参数，然后通过 `byzerllm.Image.load_image_from_path` 方法，转化为一个图片对象 image，最后在 prompt 函数体里\n使用 `{{ image }}` 引用这个图片对象。\n\n另外我们也是可以支持配置多张图片的。\n\n另外，我们也可以使用基础的 `llm.chat_oai` 方法来实现：\n\n```python\nimport byzerllm\nimport json\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\nimage = byzerllm.Image.load_image_from_path(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv = vl_llm.chat_oai(\n    conversations=[\n        {\n            \"role\": \"user\",\n            \"content\": json.dumps(\n                [{\"image\": image, \"text\": \"这个图片里有什么？\"}], ensure_ascii=False\n            ),\n        }\n    ]\n)\nv[0].output\n```\n\n还可以这么写：\n\n```python\nimport byzerllm\nimport json\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\nimage = byzerllm.Image.load_image_from_path(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv = vl_llm.chat_oai(\n    conversations=[\n        {\n            \"role\": \"user\",\n            \"content\": json.dumps(\n                [\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\"url\": image, \"detail\": \"high\"},\n                    },\n                    {\"text\": \"这个图片里有什么？\", \"type\": \"text\"},\n                ],\n                ensure_ascii=False,\n            ),\n        }\n    ]\n)\nv[0].output\n```\n\n### 语音合成\n\n这里以 openai 的 tts 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=tts-1 \\\n--model openai_tts\n```\n\n此外，byzerllm 支持 azure,火山引擎等 tts 语音合成引擎。\n\n接着你可以这么用：\n\n```python\nimport byzerllm\nimport base64\nimport json\n\nllm = byzerllm.ByzerLLM.from_default_model(\"openai_tts\")\n\n\nt = llm.chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\": json.dumps({\n        \"input\":\"hello, open_tts\",\n        \"voice\": \"alloy\",\n        \"response_format\": \"mp3\"\n    },ensure_ascii=False)\n}])\n\nwith open(\"voice.mp3\",\"wb\") as f:\n    f.write(base64.b64decode(t[0].output))\n```\n\ntts 模型生成没有prompt函数可以用，你需要直接使用 chat_oai。\n\n\n### 语音识别\n\n这里以 openai 的 whisper-1 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--worker_concurrency 10 \\\n--infer_params saas.model=whisper-1 saas.api_key=${MODEL_OPENAI_TOKEN} \\\n--model speech_to_text\n```\n\n语音识别的使用方式和图生文类似，我们可以直接在 prompt 函数体里带上音频文件。\n\n```python\nimport byzerllm\nimport json\nimport base64\nfrom byzerllm.types import AudioPath\n\nllm = byzerllm.ByzerLLM.from_default_model(\"speech_to_text\")\n\naudio_file = \"/Users/allwefantasy/videos/output_audio.mp3\"\n\n@byzerllm.prompt(llm=llm)\ndef audio_to_text(audio_file: AudioPath):\n    \"\"\"\n    {{ audio_file }}\n    \"\"\"\n\nv = audio_to_text(AudioPath(value=audio_file))\njson.loads(v)\n```\n输出的数据格式略微复杂：\n\n```\n{'text': 'In the last chapter, you and I started to step through the internal workings of a transformer. This is one of the key pieces of technology inside large language models, and a lot of other tools in the modern wave of AI.',\n 'task': 'transcribe',\n 'language': 'english',\n 'duration': 10.0,\n 'segments': [{'id': 0,\n   'seek': 0,\n   'start': 0.0,\n   'end': 4.78000020980835,\n   'text': ' In the last chapter, you and I started to step through the internal workings of a transformer.',\n   'tokens': [50364,\n    682,\n    264,\n    1036,\n    7187,\n    11,\n    291,\n    293,\n    286,\n    1409,\n    281,\n    1823,\n    807,\n    264,\n    6920,\n    589,\n    1109,\n    295,\n    257,\n    31782,\n    13,\n    50586],\n   'temperature': 0.0,\n   'avg_logprob': -0.28872039914131165,\n   'compression_ratio': 1.4220778942108154,\n   'no_speech_prob': 0.016033057123422623},\n  ....\n  {'id': 2,\n   'seek': 0,\n   'start': 8.579999923706055,\n   'end': 9.979999542236328,\n   'text': ' and a lot of other tools in the modern wave of AI.',\n   'tokens': [50759,\n    293,\n    257,\n    688,\n    295,\n    661,\n    3873,\n    294,\n    264,\n    4363,\n    5772,\n    295,\n    7318,\n    13,\n    50867],\n   'temperature': 0.0,\n   'avg_logprob': -0.28872039914131165,\n   'compression_ratio': 1.4220778942108154,\n   'no_speech_prob': 0.016033057123422623}],\n 'words': [{'word': 'In', 'start': 0.0, 'end': 0.18000000715255737},\n  {'word': 'the', 'start': 0.18000000715255737, 'end': 0.23999999463558197},\n  {'word': 'last', 'start': 0.23999999463558197, 'end': 0.5400000214576721},\n  {'word': 'chapter', 'start': 0.5400000214576721, 'end': 0.800000011920929},\n  ....\n  {'word': 'AI', 'start': 9.920000076293945, 'end': 9.979999542236328}]}\n```\n\n会输出每一句话以及每一个字所在的起始时间和截止时间。你可以根据需要来使用。\n\n\n### 文生图\n\n文生图和语音合成类似，首先要启动合适的模型,以openai 的 dall-e-3 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=dall-e-3 \\\n--model openai_image_gen\n```\n\n启动模型后，只需要记住几个模板参数即可使用，这里直接使用 chat_oai 方法来使用：\n\n```python\n\nimport byzerllm\nimport json\nimport base64\n\nllm = byzerllm.ByzerLLM.from_default_model(\"openai_image_gen\")\nt = llm.chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\": json.dumps({\n        \"input\":\"a white siamese cat\",\n        \"size\": \"1024x1024\",\n        \"quality\": \"standard\"\n    },ensure_ascii=False)\n}])\n\nwith open(\"output1.jpg\",\"wb\") as f:\n    f.write(base64.b64decode(t[0].output))\n\n\nimport matplotlib.pyplot as plt\n\nimage_path = \"output1.jpg\"\nimage = plt.imread(image_path)\n\nplt.imshow(image)\nplt.axis('off')\nplt.show()\n```\n\n## Prompt 函数的流式输出\n\nbyzerllm 底层支持流式输出，非 prompt 函数的用法是这样的：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\n\nv = llm.stream_chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\":\"讲一个100字的故事\"\n}])\n\nfor s  in v:\n    print(s[0], end=\"\")\n```\n\n如果你像用 prompt 函数，可以这么用：\n\n\n```python\nimport byzerllm\nimport json\nimport base64\nfrom typing import Generator\n\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\n\n@byzerllm.prompt()\ndef tell_story() -> Generator[str, None, None]:\n    '''\n    给我讲一个一百多字的故事\n    '''\n\nv = tell_story.with_llm(llm).run()    \nfor i in v:\n    print(i, end=\"\")\n```\n\n可以看到，和普通的 prompt 函数的区别在于，返回值是一个生成器，然后你可以通过 for 循环来获取结果。\n\n## 向量化模型\n\nbyzerllm 支持向量化模型,你可以这样启动一个本地的模型：\n\n```bash\n!byzerllm deploy --pretrained_model_type custom/bge \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--worker_concurrency 10 \\\n--model_path /home/winubuntu/.auto-coder/storage/models/AI-ModelScope/bge-large-zh \\\n--infer_backend transformers \\\n--num_workers 1 \\\n--model emb\n```\n\n注意两个参数:\n\n1. --infer_backend transformers: 表示使用 transformers 作为推理后端。\n2. --model_path: 表示模型的路径。\n\n也可以启动一个 SaaS 的emb模型,比如 qwen 的 emb 模型：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/qianwen \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 2 \\\n--infer_params saas.api_key=${MODEL_QIANWEN_TOKEN}  saas.model=text-embedding-v2 \\\n--model qianwen_emb\n```\n\n或者 openai 的 emb 模型：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--worker_concurrency 10 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=text-embedding-3-small \\\n--model gpt_emb\n```\nSaaS 模型无需配置 `--infer_backend` 参数。\n\n无论是本地模型还是 SaaS 模型，我们都可以这样使用：\n\n```python\nimport byzerllm\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\nllm.setup_default_emb_model_name(\"emb\")\nllm.emb_query(\"你好\")\n```\n\n如果你配置 byzerllm 中的 Storage 使用，比如你这样启动了存储：\n\n```bash\nbyzerllm storage start --enable_emb\n```\n\n那么需要这么使用：\n\n```python\nfrom byzerllm.apps.byzer_storage.simple_api import ByzerStorage, DataType, FieldOption,SortOption\nstorage = ByzerStorage(\"byzerai_store\", \"memory\", \"memory\")\nstorage.emb(\"你好\")\n```\n\n## 一些辅助工具\n\n当调用 prompt 函数返回字符串的时候，如果想从里面抽取代码，可以使用如下方式：\n\n```python\nfrom byzerllm.utils.client import code_utils\ntext_with_markdown = '''\n```shell\nls -l\n```\n'''\ncode_blocks = code_utils.extract_code(text_with_markdown)\nfor code_block in code_blocks:\n    if code_block[0] == \"shell\":\n        print(code_block[1])\n##output: ls -l        \n```\n\nTagExtractor 工具用于对任意 `<_tag_></_tag_>` 标签对的抽取，下面是一个使用示例：\n\n```python\nfrom byzerllm.apps.utils import TagExtractor\n\nextractor = TagExtractor('''\n大家好\n<_image_>data:image/jpeg;base64,xxxxxx</_image_>\n                         大家好\n<_image_>data:image/jpeg;base64,xxxxxx2</_image_>\n''')\n\nv = extractor.extract()\nprint(v.content[0].content)\nprint(v.content[1].content)\n```\n输出为:\n\n```\ndata:image/jpeg;base64,xxxxxx\ndata:image/jpeg;base64,xxxxxx2\n```\n\n我们成功的将 <_image_></_image_> 标签对里的内容抽取出来了。\n\n\n## 注意事项\n\n1. prompt函数方法体返回只能是dict，实际的返回类型和方法签名可以不一样，但是方法体返回只能是dict。\n2. 大部分情况prompt函数体为空，如果一定要有方法体，可以返回一个空字典。\n3. 调用prompt方法的时候，如果在@byzerllm.prompt()里没有指定llm对象，那么需要在调用的时候通过with_llm方法指定llm对象。\n\n===========================\n\n# Byzer Storage: 最易用的AI存储引擎\n\nByzer Storage是一个为AI应用设计的高性能存储引擎,它提供了简单易用的API,支持向量搜索、全文检索以及结构化查询。本文将详细介绍Byzer Storage的使用方法和主要特性。\n\n## 0. 安装和启动\n\n```bash\npip install byzerllm\nbyzerllm storage start\n```\n\nThat's it! Byzer Storage已经安装并启动成功,现在我们可以开始使用它了。默认会启动一个 byzerai_store 的集群。\n\n注意，如果你是这样启动的：\n\n```bash\nbyzerllm storage start --enable_emb\n```\n\n那么会自动启动一个 emb 模型，名字就叫 emb， ByzerStorage 会自动使用该模型，无需做任何其他配置。\n\n## 1. 初始化\n\n创建一个 ByzerStorage 对象，链接 byzerai_store 集群，并且指定数据库和表名（可以不存在）。\n\n```python\nfrom byzerllm.apps.byzer_storage.simple_api import ByzerStorage, DataType, FieldOption, SortOption\n\nstorage = ByzerStorage(\"byzerai_store\", \"my_database1\", \"my_table4s\")\n```\n\n\n## 2. 创建库表（可选）\n\nByzer Storage使用Schema来定义数据结构。我们可以使用SchemaBuilder来构建Schema:\n\n```python\n_ = (\n    storage.schema_builder()\n    .add_field(\"_id\", DataType.STRING)\n    .add_field(\"name\", DataType.STRING)\n    .add_field(\"content\", DataType.STRING, [FieldOption.ANALYZE])\n    .add_field(\"raw_content\", DataType.STRING, [FieldOption.NO_INDEX])    \n    .add_array_field(\"summary\", DataType.FLOAT)    \n    .add_field(\"created_time\", DataType.LONG, [FieldOption.SORT])    \n    .execute()\n)\n```\n\n这个Schema定义了以下字段:\n- `_id`: 字符串类型的主键\n- `name`: 字符串类型,可用于过滤条件\n- `content`: 字符串类型,带有ANALYZE选项,用于全文搜索\n- `raw_content`: 字符串类型,带有NO_INDEX选项,不会被索引\n- `summary`: 浮点数组类型,用于存储向量\n- `created_time`: 长整型,带有SORT选项,可用于排序\n\n需要注意的是：\n\n1. 如果一个字段带有ANALYZE选项,则该字段会被分词,并且可以用于全文搜索，但是就无法返回原始的文本了。所以你需要添加一个新字段专门用来存储原文，比如在我们这个例子里，我们新增了 raw_content 字段，并且显示的指定了 NO_INDEX 选项，这样就不会被索引，也就不会被分词，可以后续被检索后用来获取原始文本。\n2. 对于需要作为向量检索和存储的字段，需要指定为数组类型，比如我们这个例子里的 summary 字段。\n3. 如果你需要拿到向量字段的原始文本，那么你也需要添加一个新字段专门用来存储原文，就像我们这个例子里的 raw_content 字段一样。\n\n\n## 3. 写入数据\n\n准备数据并使用WriteBuilder写入Storage:\n\n```python\ndata = [\n    {\"_id\": \"1\", \"name\": \"Hello\", \"content\": \"Hello, world!\", \"raw_content\": \"Hello, world!\", \"summary\": \"hello world\", \"created_time\": 1612137600},\n    {\"_id\": \"2\", \"name\": \"Byzer\", \"content\": \"Byzer, world!\", \"raw_content\": \"Byzer, world!\", \"summary\": \"byzer\", \"created_time\": 1612137601},\n    {\"_id\": \"3\", \"name\": \"AI\", \"content\": \"AI, world!\", \"raw_content\": \"AI, world!\", \"summary\": \"AI\", \"created_time\": 16121376002},\n    {\"_id\": \"4\", \"name\": \"ByzerAI\", \"content\": \"ByzerAI, world!\", \"raw_content\": \"ByzerAI, world!\", \"summary\": \"ByzerAi\", \"created_time\": 16121376003},\n]\n\nstorage.write_builder().add_items(data, vector_fields=[\"summary\"], search_fields=[\"content\"]).execute()\nstorage.commit()\n```\n\n这里我们使用`add_items`方法批量添加数据,并指定了`summary`为向量字段,`content`为搜索字段。最后调用`commit()`来确保数据被持久化。\n\n\n从上面我们要写入到 byzer storage 的数据我们可以看到如下几个特点：\n\n1. 需要包含我们之前定义的 Schema中罗列的所有的字段，同时需要指定哪些是向量字段，哪些是检索字段。\n2. 向量和检索不能是同一个字段。\n3. 对于向量，检索字段，我们给到 write_builder 的都是文本，ByzerStorage 会根据 Schema 的定义自动将其转换为向量，检索字段需要的格式。\n\n## 4. 查询数据\n\nByzer Storage支持多种查询方式,包括向量搜索、全文检索、过滤和排序。\n\n### 4.1 向量搜索 + 全文检索\n\n```python\nquery = storage.query_builder()\nquery.set_vector_query(\"ByzerAI\", fields=[\"summary\"])\nresults = query.set_search_query(\"Hello\", fields=[\"content\"]).execute()\nprint(results)\n```\n\n这个查询结合了向量搜索和全文检索,它会在`summary`字段中搜索与\"ByzerAI\"相似的向量,同时在`content`字段中搜索包含\"Hello\"的文档。\n\n### 4.2 过滤 + 向量搜索 + 全文检索\n\n```python\nquery = storage.query_builder()\nquery.and_filter().add_condition(\"name\", \"AI\").build()\nquery.set_vector_query(\"ByzerAI\", fields=\"summary\")\nresults = query.set_search_query(\"Hello\", fields=[\"content\"]).execute()\nprint(results)\n```\n\n这个查询首先过滤`name`字段等于\"AI\"的文档,然后在结果中进行向量搜索和全文检索。\n\n### 4.3 过滤 + 排序\n\n```python\nquery = storage.query_builder()\nquery.and_filter().add_condition(\"name\", \"AI\").build().sort(\"created_time\", SortOption.DESC)\nresults = query.execute()\nprint(results)\n```\n\n这个查询过滤`name`字段等于\"AI\"的文档,然后按`created_time`字段降序排序。\n\n## 5. 删除数据\n\n### 5.1 根据ID删除\n\n```python\nstorage.delete_by_ids([\"3\"])\n\nquery = storage.query_builder()\nquery.and_filter().add_condition(\"name\", \"AI\").build()\nresults = query.execute()\nprint(results)\n```\n\n这里我们删除了ID为\"3\"的文档,然后查询验证删除结果。\n\n### 5.2 删除整个表\n\n```python\nstorage.drop()\n```\n\n这个操作会删除整个表及其所有数据,请谨慎使用。\n\n## 结论\n\nByzer Storage提供了一套简洁而强大的API,能够轻松实现向量搜索、全文检索、结构化查询等功能。它的设计非常适合AI应用场景,可以有效地存储和检索各种类型的数据。通过本文的介绍,相信读者已经对Byzer Storage有了基本的了解,并能够开始在自己的项目中使用这个强大的存储引擎。\n\n\n     \n\n下面是用户的需求：\n\n@Makefile 添加一个 dist 指令,运行 wheel 后然后将 @/Users/allwefantasy/projects/super_analysis/docs/4.3.1 安装与配置 Byzer-SQL.pdf 以及 @deploy.md 拷贝到 dist 目录下,然后将 dist 打包成一个 super_analysis.tar.gz\n使用 SEARCH/REPLACE 格式输出\n"}, {"role": "assistant", "content": "Here's the modification to the Makefile to add a `dist` command as requested:\n\n```makefile\n##File: /Users/allwefantasy/projects/super_analysis/Makefile\n<<<<<<< SEARCH\n.PHONY: all clean build wheel deploy_deepseek\n\nall: clean build wheel\n=======\n.PHONY: all clean build wheel deploy_deepseek dist\n\nall: clean build wheel dist\n>>>>>>> REPLACE\n```\n\n```makefile\n##File: /Users/allwefantasy/projects/super_analysis/Makefile\n<<<<<<< SEARCH\ntest:\n\t@echo \"Running tests...\"\n\t$(PYTHON) -m unittest discover tests\n=======\ntest:\n\t@echo \"Running tests...\"\n\t$(PYTHON) -m unittest discover tests\n\ndist: wheel\n\t@echo \"Creating distribution package...\"\n\tmkdir -p $(DIST_DIR)\n\tcp \"/Users/allwefantasy/projects/super_analysis/docs/4.3.1 安装与配置 Byzer-SQL.pdf\" $(DIST_DIR)\n\tcp \"/Users/allwefantasy/projects/super_analysis/docs/deploy.md\" $(DIST_DIR)\n\ttar -czf super_analysis.tar.gz $(DIST_DIR)\n\t@echo \"Distribution package created: super_analysis.tar.gz\"\n>>>>>>> REPLACE\n```\n\nThese changes add a new `dist` target to the Makefile. This target depends on the `wheel` target, copies the specified PDF and markdown files to the `dist` directory, and then creates a tarball named `super_analysis.tar.gz` containing the `dist` directory.\n\nNote that this assumes the `DIST_DIR` variable is already defined in your Makefile. If it's not, you should add it at the top of the Makefile:\n\n```makefile\nDIST_DIR := dist\n```\n\nAlso, make sure the paths to the PDF and markdown files are correct. If they're not, adjust them accordingly in the `dist` target."}], "model": "human", "yaml_file": "actions/145_chat_action.yml"}
{"instruction": "dist: wheel 我们希望解压后目录名也是 super_analysis 而不是 dist\n使用 SEARCH/REPLACE 格式输出", "conversations": [{"role": "user", "content": "\n如果你需要生成代码，对于每个需要更改的文件,你需要按 *SEARCH/REPLACE block* 的格式进行生成。\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The opening fence and code language, eg: ```python\n2. The file path alone on a line, starting with \"##File:\" and verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n*SEARCH/REPLACE* blocks will replace *all* matching occurrences.\nInclude enough lines to make the SEARCH blocks unique.\n\nInclude *ALL* the code being searched and replaced!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n下面我们来看一个例子：\n\n当前项目目录结构：\n1. 项目根目录： /tmp/projects/mathweb\n2. 项目子目录/文件列表(类似tree 命令输出)\nflask/\n    app.py\n    templates/\n        index.html\n    static/\n        style.css\n\n用户需求： Change get_factorial() to use math.factorial\n\n回答： To make this change we need to modify `/tmp/projects/mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n用户需求： Refactor hello() into its own file.\n\n回答：To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/hello.py\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/main.py\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n现在让我们开始一个新的任务:\n\n当前项目目录结构：\n1. 项目根目录： /Users/allwefantasy/projects/super_analysis\n2. 项目子目录/文件列表(类似tree 命令输出)：\nsetup.py\n.auto-coder/\n    libs/\n        llm_friendly_packages/\n            src/\n                example1/\n                    main.py\nsrc/\n    superanalysis/\n        serve/\n            rest.py\n            convert.py\n            openai_serve.py\n            __init__.py\n            llm_wrapper.py\n            api_server.py\n            cli.py\n        sql/\n            __init__.py\n            types.py\n            generator.py\n下面是一些文件路径以及每个文件对应的源码：\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/types.py\nfrom pydantic import BaseModel\n\nclass RAGConfig(BaseModel):\n    schema_rag_base_url: str = \"\"\n    schema_rag_api_key: str = \"xxxx\"\n\n    context_rag_base_url: str = \"\"\n    context_rag_api_key: str = \"xxxx\"\n\nclass ByzerSQLConfig(BaseModel):\n    byzer_sql_url: str = \"\"\n    data_dir: str = \"\"\n    owner: str = \"\"\n\nclass ModelConfig(BaseModel):\n    llm_model: str = \"\"\n    gen_sql_llm_model: str = \"\"\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/cli.py\nimport argparse\nimport byzerllm\nfrom byzerllm.utils.client import ByzerLLM\nfrom superanalysis.serve.api_server import serve, ServerArgs\nfrom superanalysis.sql.types import RAGConfig, ByzerSQLConfig, ModelConfig\nfrom superanalysis.sql.generator import SQLGenerator\nfrom superanalysis.serve.llm_wrapper import LLWrapper\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"ByzerLLM API Server\")\n    parser.add_argument(\"--ray-address\", type=str, default=\"auto\", help=\"ray address\")\n    parser.add_argument(\n        \"--host\", type=str, default=\"0.0.0.0\", help=\"Host to bind the server to\"\n    )\n    parser.add_argument(\n        \"--port\", type=int, default=8000, help=\"Port to bind the server to\"\n    )\n    parser.add_argument(\n        \"--uvicorn-log-level\", type=str, default=\"info\", help=\"Uvicorn log level\"\n    )\n    parser.add_argument(\n        \"--allow-credentials\", action=\"store_true\", help=\"Allow credentials\"\n    )\n    parser.add_argument(\n        \"--allowed-origins\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed origins\"\n    )\n    parser.add_argument(\n        \"--allowed-methods\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed methods\"\n    )\n    parser.add_argument(\n        \"--allowed-headers\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed headers\"\n    )\n    parser.add_argument(\n        \"--api-key\", type=str, default=\"\", help=\"API key for authentication\"\n    )\n    parser.add_argument(\n        \"--served-model-name\",\n        type=str,\n        required=True,\n        help=\"Name of the model to be served\",\n    )\n    parser.add_argument(\n        \"--prompt-template\", type=str, default=\"\", help=\"Prompt template\"\n    )\n    parser.add_argument(\n        \"--response-role\", type=str, default=\"assistant\", help=\"Response role\"\n    )\n    parser.add_argument(\"--ssl-keyfile\", type=str, default=\"\", help=\"SSL key file\")\n    parser.add_argument(\n        \"--ssl-certfile\", type=str, default=\"\", help=\"SSL certificate file\"\n    )\n\n    # Add new arguments for RAGConfig\n    parser.add_argument(\n        \"--schema-rag-base-url\", type=str, default=\"\", help=\"RAG base URL\"\n    )\n    parser.add_argument(\n        \"--schema-rag-api-key\", type=str, default=\"xxxx\", help=\"RAG API key\"\n    )\n\n    parser.add_argument(\n        \"--context-rag-base-url\", type=str, default=\"\", help=\"RAG base URL\"\n    )\n    parser.add_argument(\n        \"--context-rag-api-key\", type=str, default=\"xxxx\", help=\"RAG API key\"\n    )\n\n    # Add new argument for ByzerSQLConfig\n    parser.add_argument(\"--byzer-sql-url\", type=str, default=\"\", help=\"Byzer SQL URL\")\n    parser.add_argument(\"--data-dir\", type=str, default=\"\", help=\"Data directory\")\n    parser.add_argument(\"--owner\", type=str, default=\"hello\", help=\"Owner of the Byzer SQL session\")\n    parser.add_argument(\n        \"--llm-model\", type=str, default=\"\", help=\"LLM model for general use\"\n    )\n    parser.add_argument(\n        \"--gen-sql-llm-model\", type=str, default=\"\", help=\"LLM model for SQL generation\"\n    )\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile,\n    )\n\n    # Create RAGConfig instance\n    rag_config = RAGConfig(\n        schema_rag_base_url=args.schema_rag_base_url,\n        schema_rag_api_key=args.schema_rag_api_key,\n        context_rag_base_url=args.context_rag_base_url,\n        context_rag_api_key=args.context_rag_api_key,\n    )\n\n    # Create ByzerSQLConfig instance\n    byzer_sql_config = ByzerSQLConfig(\n        byzer_sql_url=args.byzer_sql_url, data_dir=args.data_dir, owner=args.owner\n    )\n\n    model_config = ModelConfig(\n        llm_model=args.llm_model or args.served_model_name,\n        gen_sql_llm_model=args.gen_sql_llm_model or args.served_model_name,\n    )\n\n    byzerllm.connect_cluster(address=args.ray_address)\n    llm = ByzerLLM()\n    llm.setup_default_model_name(args.served_model_name)\n    llm = byzerllm.ByzerLLM()\n    if not server_args.served_model_name:\n        raise ValueError(\"served_model_name is required\")\n    llm.setup_default_model_name(server_args.served_model_name)\n\n    print(\"RAG Config: \", rag_config)\n    print(\"Model Config: \", model_config)\n    rag = SQLGenerator(\n        llm=llm,\n        rag_config=rag_config,\n        byzer_sql_config=byzer_sql_config,\n        model_config=model_config,\n    )\n    llm_wrapper = LLWrapper(llm=llm, entry=rag)\n    serve(llm_wrapper, server_args)\n\n\nif __name__ == \"__main__\":\n    main()\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/__init__.py\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/__init__.py\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/llm_wrapper.py\nfrom typing import Any, Dict, List, Optional, Union,Callable\nfrom byzerllm.utils.client.types import (    \n    LLMFunctionCallResponse,\n    LLMClassResponse,LLMResponse    \n)\nimport pydantic\nfrom byzerllm import ByzerLLM\nfrom byzerllm.utils.client import LLMResponse\nfrom byzerllm.utils.types import SingleOutputMeta\nfrom byzerllm.utils.langutil import asyncfy_with_semaphore\nfrom superanalysis.sql.generator import SQLGenerator\nfrom loguru import logger\n\nclass LLWrapper:\n\n    def __init__(self,llm:ByzerLLM,entry:SQLGenerator):\n        self.entry = entry\n        self.llm = llm\n        \n\n    def chat_oai(self,\n                 conversations,\n                 tools:List[Union[Callable,str]]=[], \n                 tool_choice:Optional[Union[Callable,str]]=None,\n                 execute_tool:bool=False,  \n                 impl_func:Optional[Callable]=None,\n                 execute_impl_func:bool=False,\n                 impl_func_params:Optional[Dict[str,Any]]=None,\n                 func_params:Optional[Dict[str,Any]]=None,\n                 response_class:Optional[Union[pydantic.BaseModel,str]] = None, \n                 response_after_chat:Optional[Union[pydantic.BaseModel,str]] = False,\n                 enable_default_sys_message:bool=True,                 \n                 model:Optional[str] = None,\n                 role_mapping=None,llm_config:Dict[str,Any]={}\n                 )->Union[List[LLMResponse],List[LLMFunctionCallResponse],List[LLMClassResponse]]: \n        res,contexts = self.entry.stream_chat_oai(conversations)\n        s = \"\".join(res)        \n        return [LLMResponse(output=s,metadata={},input=\"\")]\n\n    def stream_chat_oai(self,conversations, \n                        model:Optional[str]=None, \n                        role_mapping=None,\n                        delta_mode=False,\n                        llm_config:Dict[str,Any]={}): \n        res,contexts = self.entry.stream_chat_oai(conversations)        \n        for t in res:                        \n            yield (t,SingleOutputMeta(0,0))\n\n    async def async_stream_chat_oai(self,conversations, \n                        model:Optional[str]=None, \n                        role_mapping=None,\n                        delta_mode=False,\n                        llm_config:Dict[str,Any]={}): \n        res,contexts = self.rag.stream_chat_oai(conversations)                \n        for t in res:                                    \n            yield (t,SingleOutputMeta(0,0))\n                         \n\n    def __getattr__(self, name):        \n        return getattr(self.llm, name)\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/openai_serve.py\nimport argparse\nfrom service_server.api_server import ServerArgs, serve\nfrom service_server.llm_wrapper import LLWrapper\nfrom service_server.summary_rag import SummaryRAG\nimport byzerllm\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"OpenAI Serve\")\n    parser.add_argument(\n        \"--model\",\n        default=\"\",\n    )\n    parser.add_argument(\"--emb_model\", default=\"\")\n    parser.add_argument(\n        \"--ray_address\",\n        default=\"auto\",\n    )\n    parser.add_argument(\"--source_dir\", default=\".\", help=\"\")\n    parser.add_argument(\"--host\", default=\"\", help=\"\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"\")\n    parser.add_argument(\"--uvicorn_log_level\", default=\"info\", help=\"\")\n    parser.add_argument(\"--allow_credentials\", action=\"store_true\", help=\"\")\n    parser.add_argument(\"--allowed_origins\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--allowed_methods\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--allowed_headers\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--api_key\", default=\"\", help=\"\")\n    parser.add_argument(\"--served_model_name\", default=\"\", help=\"\")\n    parser.add_argument(\"--prompt_template\", default=\"\", help=\"\")\n    parser.add_argument(\"--ssl_keyfile\", default=\"\", help=\"\")\n    parser.add_argument(\"--ssl_certfile\", default=\"\", help=\"\")\n    parser.add_argument(\"--response_role\", default=\"assistant\", help=\"\")\n    parser.add_argument(\n        \"--collections\", default=\"\", help=\"Collection name for indexing\"\n    )\n    parser.add_argument(\n        \"--base_dir\",\n        default=\"\",\n        help=\"Path where the processed text embeddings were stored\",\n    )\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name or args.model,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile,\n    )\n    byzerllm.connect_cluster(address=args.ray_address)\n    llm = byzerllm.ByzerLLM()\n    if not server_args.served_model_name:\n        raise ValueError(\"served_model_name is required\")\n    llm.setup_default_model_name(server_args.served_model_name)\n    rag = SummaryRAG(llm=llm)    \n    llm_wrapper = LLWrapper(llm=llm, rag=rag)\n    serve(llm=llm_wrapper, args=server_args)\n\nif __name__ == \"__main__\":\n    main()\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/api_server.py\nimport os\nimport time\nimport uvicorn\nfrom fastapi import Request\nfrom fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import JSONResponse, StreamingResponse, Response\n\nfrom byzerllm.log import init_logger\nfrom byzerllm.utils import random_uuid\nfrom byzerllm.version import __version__ as version\nfrom byzerllm.utils.client import ByzerLLM, LLMRequest\nfrom byzerllm.utils.client.entrypoints.openai.serving_chat import OpenAIServingChat\nfrom byzerllm.utils.client.entrypoints.openai.serving_completion import OpenAIServingCompletion\nfrom byzerllm.utils.client.entrypoints.openai.protocol import (\n    ModelList,\n    ModelCard,\n    ModelPermission,\n    ChatCompletionRequest,\n    ErrorResponse,\n    CompletionRequest,\n    Embeddings,\n    EmbeddingsOutput,\n    EmbeddingsData,\n    EmbeddingsUsage,\n)\nfrom pydantic import BaseModel\nfrom typing import List\n\nlogger = init_logger(__name__)\n\nllm_client: ByzerLLM = None\nopenai_serving_chat: OpenAIServingChat = None\nopenai_serving_completion: OpenAIServingCompletion = None\n\nTIMEOUT_KEEP_ALIVE = 5  # seconds\n# timeout in 10 minutes. Streaming can take longer than 3 min\nTIMEOUT = float(os.environ.get(\"BYZERLLM_APISERVER_HTTP_TIMEOUT\", 600))\n\nrouter_app = FastAPI()\n\n\n@router_app.get(\"/health\")\nasync def health() -> Response:\n    \"\"\"Health check.\"\"\"\n    return Response(status_code=200)\n\n\n@router_app.get(\"/v1/models\")\nasync def show_available_models():\n    models = await openai_serving_chat.show_available_models()\n    return JSONResponse(content=models.model_dump())\n\n\n@router_app.get(\"/version\")\nasync def show_version():\n    return JSONResponse(content={\"version\": version})\n\n\n@router_app.get(\"/v1/models\", response_model=ModelList)\nasync def models() -> ModelList:\n    \"\"\"Show available models. Right now we only have one model.\"\"\"\n    model_cards = [\n        ModelCard(\n            id=\"\",\n            root=\"\",\n            permission=[ModelPermission()]\n        )\n    ]\n    return ModelList(data=model_cards)\n\n\n@router_app.post(\"/v1/completions\")\nasync def create_completion(\n        body: CompletionRequest,\n        request: Request\n):\n    generator = await openai_serving_completion.create_completion(body, request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            status_code=generator.code\n        )\n    if body.stream:\n        return StreamingResponse(\n            content=generator,\n            media_type=\"text/event-stream\"\n        )\n    else:\n        return JSONResponse(content=generator.model_dump())\n\n\n@router_app.post(\"/v1/chat/completions\")\nasync def create_chat_completion(\n        body: ChatCompletionRequest,\n        request: Request,\n):\n    \"\"\"Completion API similar to OpenAI's API.\n\n    See  https://platform.openai.com/docs/api-reference/chat/create\n    for the API specification. This API mimics the OpenAI ChatCompletion API.\n\n    NOTE: Currently we do not support the following features:\n        - function_call (Users should implement this by themselves)\n        - logit_bias (to be supported by vLLM engine)\n    \"\"\"\n    # async with async_timeout.timeout(TIMEOUT):\n\n    generator = await openai_serving_chat.create_chat_completion(body, request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            status_code=generator.code\n        )\n    if body.stream:\n        return StreamingResponse(\n            content=generator,\n            media_type=\"text/event-stream\"\n        )\n    else:\n        return JSONResponse(content=generator.model_dump())\n\n\n@router_app.post(\"/v1/embeddings\")\nasync def embed(body: Embeddings):\n    \"\"\"Given a prompt, the model will return one embedding.\n\n    Returns:\n        A response object with an embedding.\n    \"\"\"\n    embedding_id = f\"embed-{random_uuid()}\"\n\n    results_list = llm_client.emb(body.model, request=LLMRequest(instruction=body.input))\n    tokens = 0\n\n    return EmbeddingsOutput(\n        data=[\n            EmbeddingsData(\n                embedding=results.output,\n                index=i,\n                object=\"embedding\",\n            )\n            for i, results in enumerate(results_list)\n        ],\n        id=embedding_id,\n        object=\"list\",\n        created=int(time.time()),\n        model=body.model,\n        usage=EmbeddingsUsage(\n            prompt_tokens=tokens,\n            total_tokens=tokens,\n        ),\n    )\n\nclass ServerArgs(BaseModel):\n    host: str = None\n    port: int = 8000\n    uvicorn_log_level: str = \"info\"\n    allow_credentials: bool = False\n    allowed_origins: List[str] = [\"*\"]  \n    allowed_methods: List[str] = [\"*\"]\n    allowed_headers: List[str] = [\"*\"]\n    api_key: str = None\n    served_model_name: str = None\n    prompt_template: str = None\n    response_role: str = \"assistant\"\n    ssl_keyfile: str = None\n    ssl_certfile: str = None    \n\ndef serve(llm:ByzerLLM, args: ServerArgs):\n    \n    logger.info(f\"ByzerLLM API server version {version}\")\n    logger.info(f\"args: {args}\")\n\n    router_app.add_middleware(\n        CORSMiddleware,\n        allow_origins=args.allowed_origins,\n        allow_credentials=args.allow_credentials,\n        allow_methods=args.allowed_methods,\n        allow_headers=args.allowed_headers,\n    )\n    \n    if token := os.environ.get(\"BYZERLLM_API_KEY\") or args.api_key:\n\n        @router_app.middleware(\"http\")\n        async def authentication(request: Request, call_next):\n            if not request.url.path.startswith(\"/v1\"):\n                return await call_next(request)\n            if request.headers.get(\"Authorization\") != \"Bearer \" + token:\n                return JSONResponse(\n                    content={\"error\": \"Unauthorized\"},\n                    status_code=401\n                )\n            return await call_next(request)\n\n    # Register labels for metrics\n    # add_global_metrics_labels(model_name=engine_args.model)\n    global llm_client\n    llm_client = llm\n    \n    global openai_serving_chat\n    openai_serving_chat = OpenAIServingChat(\n        llm_client=llm_client,\n        response_role=args.response_role,\n        server_model_name=args.served_model_name,\n        prompt_template=args.prompt_template\n    )\n    global openai_serving_completion\n    openai_serving_completion = OpenAIServingCompletion(\n        llm_client=llm_client,\n        server_model_name=args.served_model_name,\n        prompt_template=args.prompt_template\n    )\n\n    uvicorn.run(\n        router_app,\n        host=args.host,\n        port=args.port,\n        log_level=args.uvicorn_log_level,\n        timeout_keep_alive=TIMEOUT_KEEP_ALIVE,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile\n    )\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/generator.py\nfrom typing import Optional, Dict, Any, List\nfrom openai import OpenAI\nimport json\nfrom .types import RAGConfig, ByzerSQLConfig, ModelConfig\nfrom byzerllm.utils.client import code_utils\nimport byzerllm\nimport requests\nfrom loguru import logger\nimport traceback\nfrom byzerllm.utils.nontext import TagExtractor\n\n\nclass SQLGenerator:\n\n    @byzerllm.prompt()\n    def gen_context(self, conversations: List[Dict[str, str]]) -> str:\n        \"\"\"\n        对话内容：\n        {% for msg in conversations %}\n        <{{ msg.role }}>: {{ msg.content }}\n        {% endfor %}\n\n        根据提供的对话内容，理解用户最后的问题，然后提取可能会用到的 Byzer-SQL 相关的知识。\n        \"\"\"\n\n    @byzerllm.prompt()\n    def gen_sql(\n        self,\n        conversations: List[Dict[str, str]],\n        context: str,\n        schema: str,\n        model: str = \"deepseek_chat\",\n    ) -> str:\n        \"\"\"\n        相关知识：\n        {{ context }}\n\n        Schema信息：\n        {{ schema }}\n\n        对话内容：\n        {% for msg in conversations %}\n        <{{ msg.role }}>: {{ msg.content }}\n        {% endfor %}\n\n        模型UDF函数：\n        {{ model }}\n\n        根据提供的相关知识以及对话内容，理解用户最后的问题，然后根据看到的schema 信息，生成 Byzer-SQL 语句。\n        Byzer-SQL 首先需要用 load 语法加载 schema 中的路径映射成表格，然后根据表格的信息生成对应的 Byzer-SQL 语句。\n\n        特别需要关注的是:\n        1. 用户的很多需求是要对表格的文本内容做自然语言处理，你需要使用 Byzer-SQL 中的大模型辅助函数来完成这个需求。\n        2. 使用 `llm_result` 函数处理 LLM 模型函数的响应,`llm_result` 函数得到的是一段文本，虽然我们要求模型函数只输出特定的文本，但考虑到大模型会有一些额外的字符输出，不会只是输出我们要求的文字，所以通过 `like` 来进行匹配。\n        3. 注意 Byzer-SQL 的语法，比如select 语句最后一定需要有 as <table_name>。\n        3. 输出的 Byzer-SQL 语句务必使用 <_byzer_sql_></_byzer_sql_> 标签包裹起来。\n        \"\"\"\n\n    @byzerllm.prompt()\n    def fetch_schema(self, context: str) -> str:\n        \"\"\"\n        下面是一些基础知识，包括名词术语等等：\n        {{ context }}\n\n        当理解对话内容的时候，需要参考上面的基础知识，然后更好的理解对应的表格schema信息是不是有用。\n        \"\"\"\n\n    def __init__(\n        self,\n        llm: byzerllm.ByzerLLM,\n        rag_config: RAGConfig,\n        byzer_sql_config: ByzerSQLConfig,\n        model_config: ModelConfig,\n    ) -> None:\n        self.llm = llm\n        self.schema_rag_client = OpenAI(\n            api_key=rag_config.schema_rag_api_key,\n            base_url=rag_config.schema_rag_base_url,\n        )\n        self.context_rag_client = OpenAI(\n            api_key=rag_config.context_rag_api_key,\n            base_url=rag_config.context_rag_base_url,\n        )\n        self.byzer_sql_url = byzer_sql_config.byzer_sql_url\n\n        self.llm_model = byzerllm.ByzerLLM()\n        self.llm_model.setup_default_model_name(model_config.llm_model)\n        self.gen_sql_llm_model = byzerllm.ByzerLLM()\n        self.gen_sql_llm_model.setup_default_model_name(model_config.gen_sql_llm_model)\n\n    def stream_chat_oai(\n        self,\n        conversations,\n        model: Optional[str] = None,\n        role_mapping=None,\n        delta_mode=False,\n        llm_config: Dict[str, Any] = {},\n    ):\n        try:\n            v, c = self._stream_chat_oai(\n                conversations=conversations,\n                model=model,\n                role_mapping=role_mapping,\n                delta_mode=delta_mode,\n                llm_config=llm_config,\n            )\n        except Exception as e:\n            error_msg = f\"Error in stream_chat_oai: {e}\\n{traceback.format_exc()}\"\n            logger.error(error_msg)\n            v, c = [str(e)], []\n\n        return v, c\n\n    def _stream_chat_oai(\n        self,\n        conversations,\n        model: Optional[str] = None,\n        role_mapping=None,\n        delta_mode=False,\n        llm_config: Dict[str, Any] = {},\n    ):\n        logger.info(f\"Starting stream_chat_oai with model: {model}\")\n        logger.debug(f\"Conversations: {conversations}\")\n        logger.debug(f\"LLM Config: {llm_config}\")\n\n        if not model:\n            model = self.llm_model.default_model_name\n        logger.info(f\"Using model: {model}\")\n\n        query = conversations[-1][\"content\"]\n\n        if (\n            \"使用四到五个字直接返回这句话的简要主题，不要解释、不要标点、不要语气词、不要多余文本，不要加粗，如果没有主题\"\n            in query\n            or \"简要总结一下对话内容，用作后续的上下文提示 prompt，控制在 200 字以内\"\n            in query\n        ):\n            chunks = self.llm.stream_chat_oai(\n                conversations=conversations,\n                model=model,\n                role_mapping=role_mapping,\n                llm_config=llm_config,\n                delta_mode=True,\n            )\n            return (chunk[0] for chunk in chunks), []\n\n        logger.info(\"Fetching context from context RAG client\")\n        context_query = self.gen_context.prompt(conversations=conversations)\n        context_response = self.context_rag_client.chat.completions.create(\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": json.dumps(\n                        {\"query\": context_query, \"only_contexts\": True},\n                        ensure_ascii=False,\n                    ),\n                }\n            ],\n            model=model,\n        )\n\n        context = context_response.choices[0].message.content\n        new_context = \"\"\n        for msg in context.split(\"\\n\"):\n            if msg.strip():\n                v = json.loads(msg)\n                new_context += v[\"source_code\"]\n        logger.info(f\"context {new_context[0:100]}...\")\n        logger.info(\"Context retrieved successfully\")\n\n        logger.info(\"Fetching schemas from schema RAG client\")\n        schema_query = self.fetch_schema.prompt(context=new_context)\n\n        response = self.schema_rag_client.chat.completions.create(\n            messages=conversations[0:-1]\n            + [\n                {\n                    \"role\": \"user\",\n                    \"content\": json.dumps(\n                        {\"query\": schema_query, \"only_contexts\": True},\n                        ensure_ascii=False,\n                    ),\n                }\n            ],\n            model=model,\n        )        \n\n        schema_str = response.choices[0].message.content\n        logger.debug(f\"Schema RAG response: {schema_str}\")\n        logger.info(\"Schema retrieved successfully\")\n\n        logger.info(\"Generating SQL string\")\n        sql_str = self.gen_sql.with_llm(self.gen_sql_llm_model).run(\n            conversations=conversations,\n            context=new_context,\n            schema=schema_str,\n            model=model,\n        )\n\n        logger.info(\"SQL string generated\")\n        logger.debug(f\"Generated SQL string: {sql_str}\")\n\n        # Extract SQL code from tags\n        tag_extractor = TagExtractor(sql_str)\n        extracted_tags = tag_extractor.extract()\n        sql = extracted_tags.content[0].content\n        logger.info(f\"Extracted SQL: {sql}\")\n\n        logger.info(f\"Sending SQL to Byzer SQL URL: {self.byzer_sql_url}\")\n        try:\n            response = requests.post(\n                self.byzer_sql_url,\n                data={\n                    \"sql\": sql,\n                    \"owner\": \"hello\",\n                    \"sessionPerUser\": True,\n                    \"sessionPerRequest\": True,\n                    \"includeSchema\": True,\n                },\n            )\n            response.raise_for_status()\n            result = response.content.decode(\"utf-8\")\n            logger.info(\"SQL execution result decoded\")\n            logger.debug(f\"SQL execution result: {result}\")\n            return (item for item in [result]), []\n        except requests.exceptions.RequestException as e:\n            logger.error(f\"Failed to send SQL to Byzer SQL URL: {e}\")\n            if hasattr(e.response, \"content\"):\n                error_content = e.response.content.decode(\"utf-8\")\n                logger.error(f\"Server response: {error_content}\")\n            else:\n                logger.error(\"No server response available\")\n            raise Exception(f\"Byzer SQL request failed: {str(e)}\") from e\n\n\n##File: /Users/allwefantasy/projects/super_analysis/setup.py\nfrom setuptools import setup, find_packages\n\n# Read requirements from requirements.txt\nwith open('requirements.txt') as f:\n    requirements = f.read().splitlines()\n\nsetup(\n    name=\"super-analysis\",\n    version=\"0.1.0\",\n    packages=find_packages(where=\"src\"),\n    package_dir={\"\": \"src\"},\n    install_requires=requirements,\n    entry_points={\n        \"console_scripts\": [\n            \"super-analysis.serve=superanalysis.serve.cli:main\",\n            \"super-analysis.convert=superanalysis.serve.convert:main\",\n            \"superanalysis.api=superanalysis.serve.rest:main\",\n        ],\n    },\n    author=\"allwefantasy\",\n    author_email=\"allwefantasy@gmail.com\",\n    description=\"A package for super analysis with ByzerLLM\",\n    long_description=open(\"README.md\").read(),\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/yourusername/super-analysis\",\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n    ],\n    python_requires='>=3.6',\n)\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/convert.py\nimport os\nimport argparse\nimport pandas as pd\nimport hashlib\nfrom typing import List, Dict\n\ndef get_csv_files(directory: str) -> List[str]:\n    \"\"\"Recursively get all CSV files in the given directory.\"\"\"\n    csv_files = []\n    for root, _, files in os.walk(directory):\n        for file in files:\n            if file.endswith('.csv'):\n                csv_files.append(os.path.join(root, file))\n    return csv_files\n\ndef extract_schema(df: pd.DataFrame) -> Dict[str, str]:\n    \"\"\"Extract schema from pandas DataFrame.\"\"\"\n    schema = {}\n    for column in df.columns:\n        dtype = str(df[column].dtype)\n        if dtype == 'object':\n            schema[column] = 'STRING'\n        elif dtype.startswith('int'):\n            schema[column] = 'INTEGER'\n        elif dtype.startswith('float'):\n            schema[column] = 'FLOAT'\n        elif dtype == 'bool':\n            schema[column] = 'BOOLEAN'\n        elif dtype.startswith('datetime'):\n            schema[column] = 'TIMESTAMP'\n        else:\n            schema[column] = 'STRING'  # Default to STRING for unknown types\n    return schema\n\ndef generate_schema_text(schema: Dict[str, str], csv_file: str, df: pd.DataFrame, include_rows_num: int) -> str:\n    \"\"\"Generate schema text from the schema dictionary and include the CSV file name and sample data.\"\"\"\n    schema_lines = [f\"CSV File: {csv_file}\\n\"]\n    schema_lines.extend([f\"{field}: {dtype}\" for field, dtype in schema.items()])\n    \n    if include_rows_num > 0:\n        schema_lines.append(f\"\\nSample Data (first {include_rows_num} rows):\")\n        sample_data = df.head(include_rows_num).to_string(index=False)\n        schema_lines.append(sample_data)\n    \n    return \"\\n\".join(schema_lines)\n\ndef save_schema(schema_text: str, csv_file: str, output_dir: str) -> None:\n    \"\"\"Save schema text to a file in the output directory.\"\"\"\n    # Generate MD5 hash of the schema text\n    file_name = hashlib.md5(schema_text.encode()).hexdigest() + '.md'\n    output_path = os.path.join(output_dir, file_name)\n    \n    with open(output_path, 'w') as f:\n        f.write(schema_text)\n    print(f\"Schema for {csv_file} saved to {output_path}\")\n\ndef convert(data_dir: str, doc_dir: str, include_rows_num: int) -> None:\n    \"\"\"Main conversion function.\"\"\"\n    csv_files = get_csv_files(data_dir)\n    \n    for csv_file in csv_files:\n        print(f\"Processing {csv_file}\")\n        df = pd.read_csv(csv_file)\n        schema = extract_schema(df)\n        schema_text = generate_schema_text(schema, csv_file, df, include_rows_num)\n        save_schema(schema_text, csv_file, doc_dir)\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Convert CSV files to schema documents\")\n    parser.add_argument(\"--data_dir\", required=True, help=\"Directory containing CSV files\")\n    parser.add_argument(\"--doc_dir\", required=True, help=\"Output directory for schema documents\")\n    parser.add_argument(\"--include-rows-num\", type=int, default=0, help=\"Number of sample rows to include in the schema\")\n    \n    args = parser.parse_args()\n    \n    if not os.path.exists(args.doc_dir):\n        os.makedirs(args.doc_dir)\n    \n    convert(args.data_dir, args.doc_dir, args.include_rows_num)\n\nif __name__ == \"__main__\":\n    main()\n\n##File: /Users/allwefantasy/projects/super_analysis/README.md\n# Super Analysis\n\nSuper Analysis 支持 NLP2SQL 以及 使用 SQL 对表中的文本字段做自然语言分析。\n\n## 安装\n\n你可以使用 pip 安装 Super Analysis：\n\n```bash\ngit clone https://github.com/allwefantasy/super_analysis\ncd super-analysis\npip install -e .\n```\n\n或者使用 Makefile 来管理项目：\n\n```bash\n# 清理项目\nmake clean\n\n# 构建项目\nmake build\n\n# 创建wheel包\nmake wheel\n\n# 安装项目\nmake install\n\n# 卸载项目\nmake uninstall\n\n# 以开发模式安装项目\nmake develop\n\n# 运行测试\nmake test\n```\n\n\n\n## 示例数据\n\n下载电影数据集： https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset/download?datasetVersionNumber=7\n\n\n## 启动分析服务\n\n整个 super-analysis 服务分为四个部分：\n\n1. byzer-sql 分布式执行引擎 \n2. schema 文档知识库\n3. context 文档知识库\n4. super-analysis 自身\n\n在启动 super-analysis 之前，你需要启动 schema 文档知识库和 context 文档知识库以及 byzer-sql 引擎。\n\n## 部署 Deepseek 模型代理\n\n在启动其他服务之前，我们需要先部署 Deepseek 模型。你可以使用以下命令来部署：\n\n```bash\nmake deploy_deepseek\n```\n\n或者直接运行以下命令：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--worker_concurrency 1000 \\\n--num_workers 1 \\\n--infer_params saas.base_url=\"https://api.deepseek.com/v1\" saas.api_key=${MODEL_DEEPSEEK_TOKEN} saas.model=deepseek-chat \\\n--model deepseek_chat\n```\n\n注意：请确保在运行此命令之前，已经设置了环境变量 `MODEL_DEEPSEEK_TOKEN`。\n\n## 部署 Byzer-SQL\n\n在你的电脑上参考 [这个文档](./docs/4.3.1%20安装与配置%20Byzer-SQL.pdf) 完成部署.\n\n> 启动的时候需要在安装有 super-analysis 的conda环境中启动。\n\n\n\n当 byzer-sql 部署完成后，注册账号为 `hello`, 然后进入到 byzer-sql 的控制台中执行以下脚本：\n\n```sql\n!byzerllm setup single;\n\nrun command as LLM.`` where \naction=\"infer\"\nand reconnect=\"true\"\nand pretrainedModelType=\"saas/openai\"\nand udfName=\"deepseek_chat\";\n```\n\n## 启动其他服务\n\n现在可以按如下方式做一些数据预处理或者启动相关服务：\n\n```bash\n## 将电影数据集的csv文件转换为schema文档\nsuper-analysis.convert --data_dir /Users/allwefantasy/data/movice --doc_dir /Users/allwefantasy/data/movice/schemas/\n\n# 启动schema文档知识库\nauto-coder doc serve \\\n--model deepseek_chat --index_filter_workers 100 \\\n--tokenizer_path /Users/allwefantasy/Downloads/tokenizer.json \\\n--doc_dir /Users/allwefantasy/data/movice/schemas/ \\\n--port 8001\n\n# 下载 Byzer-SQL 专门为大模型优化的文档\ngit clone https://github.com/allwefantasy/llm_friendly_packages\n\n# 启动文档知识库\nauto-coder doc serve \\\n--model deepseek_chat --index_filter_workers 100 \\\n--tokenizer_path /Users/allwefantasy/Downloads/tokenizer.json \\\n--doc_dir  /Users/allwefantasy/projects/llm_friendly_packages/github.com/allwefantasy \\\n--port 8002\n\n# 启动兼容 OpenAI Server 的分析服务\nsuper-analysis.serve --served-model-name deepseek_chat --port 8000 \\\n--schema-rag-base-url http://127.0.0.1:8001/v1 \\\n--context-rag-base-url http://127.0.0.1:8002/v1 \\\n--byzer-sql-url http://127.0.0.1:9003/run/script\n```\n\n现在你可以用 OpenAI SDK 去使用了。 具体测试和接口使用参考 [openai_local_api.ipynb](./notebooks/openai_local_api.ipynb)\n\n\n## 非 OpenAI 兼容接口\n\n除了兼容 OpenAI 的接口外，Super Analysis 还提供了一个简单的 REST API 接口，可以直接与模型进行交互。以下是如何使用这个接口：\n\n### 启动服务\n\n使用以下命令启动 REST API 服务：\n\n```bash\nsuperanalysis.api --schema-rag-base-url <schema_rag_url> \\\n                  --schema-rag-api-key <schema_rag_api_key> \\\n                  --context-rag-base-url <context_rag_url> \\\n                  --context-rag-api-key <context_rag_api_key> \\\n                  --byzer-sql-url <byzer_sql_url> \\\n                  --llm-model <llm_model_name> \\\n                  --gen-sql-llm-model <gen_sql_llm_model_name> \\\n                  --host <host> \\\n                  --port <port>\n```\n\n请替换尖括号中的值为实际的配置信息。\n\n### API 端点\n\n服务启动后，将提供以下 API 端点：\n\n#### POST /chat\n\n此端点用于与模型进行对话。\n\n请求体格式：\n\n```json\n{\n  \"conversations\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"你的问题或指令\"\n    }\n  ],\n  \"model\": \"可选的模型名称\"\n}\n```\n\n响应格式：\n\n```json\n{\n  \"response\": \"模型的回答\"\n}\n```\n\n### 示例使用\n\n以下是使用 Python 请求库与 API 交互的示例：\n\n```python\nimport requests\nimport json\n\nurl = \"http://localhost:8000/chat\"\npayload = {\n    \"conversations\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"你好，请介绍一下你自己。\"\n        }\n    ],\n    \"model\": \"your_model_name\"\n}\n\nresponse = requests.post(url, json=payload)\nresult = response.json()\n\nprint(result[\"response\"])\n```\n\n这个 API 相比兼容 OpenAI 接口，可以在请求中提供提供更多可选参数。\n\n## 使用方法\n\n安装完成后，你可以使用 `super-analysis.serve` 命令来启动 API 服务器：\n\n```bash\nsuper-analysis.serve --served-model-name your_model_name\n```\n\n### 命令行参数\n\n以下是可用的命令行参数：\n\n- `--host`: 服务器绑定的主机地址（默认：0.0.0.0）\n- `--port`: 服务器绑定的端口（默认：8000）\n- `--uvicorn-log-level`: Uvicorn 日志级别（默认：info）\n- `--allow-credentials`: 允许凭证（默认：False）\n- `--allowed-origins`: 允许的源（默认：[\"*\"]）\n- `--allowed-methods`: 允许的方法（默认：[\"*\"]）\n- `--allowed-headers`: 允许的头部（默认：[\"*\"]）\n- `--api-key`: 用于认证的 API 密钥\n- `--served-model-name`: 要服务的模型名称（必需）\n- `--prompt-template`: 提示模板\n- `--response-role`: 响应角色（默认：assistant）\n- `--ssl-keyfile`: SSL 密钥文件路径\n- `--ssl-certfile`: SSL 证书文件路径\n\n例如：\n\n```bash\nsuper-analysis.serve --served-model-name deepseek_chat --port 8000\n```\n\n\n## 抽取csv文件schema\n\nThe `super-analysis.convert` command allows you to convert CSV files to schema documents. This is useful for generating metadata about your CSV files that can be used for further analysis or documentation.\n\n#### Usage\n\n```\nsuper-analysis.convert --data_dir <path_to_csv_files> --doc_dir <output_directory> [--include-rows-num <number_of_sample_rows>]\n```\n\nArguments:\n- `--data_dir`: Directory containing CSV files (required)\n- `--doc_dir`: Output directory for schema documents (required)\n- `--include-rows-num`: Number of sample rows to include in the schema (optional, default is 0)\n\nThe utility will:\n1. Recursively find all CSV files in the specified directory\n2. Extract the schema from each CSV file\n3. Generate a schema text that includes:\n   - The CSV file name\n   - Field names and their data types\n   - Sample data (if `--include-rows-num` is specified)\n4. Save the schema text as a markdown file in the output directory\n\n## 联系方式\n\n如有任何问题或建议，请联系 allwefantasy@gmail.com。\n\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/rest.py\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nfrom typing import List, Optional\nimport uvicorn\nimport argparse\nfrom superanalysis.sql.generator import SQLGenerator\nfrom superanalysis.sql.types import RAGConfig, ByzerSQLConfig, ModelConfig\nimport byzerllm\n\napp = FastAPI()\n\nclass ChatMessage(BaseModel):\n    role: str\n    content: str\n\nclass ChatRequest(BaseModel):\n    conversations: List[ChatMessage]\n    model: Optional[str] = None\n\nclass ChatResponse(BaseModel):\n    response: str\n\n@app.post(\"/chat\", response_model=ChatResponse)\nasync def chat(request: ChatRequest):\n    try:\n        result, _ = sql_generator.stream_chat_oai(\n            conversations=[{\"role\": msg.role, \"content\": msg.content} for msg in request.conversations],\n            model=request.model\n        )\n        return ChatResponse(response=\"\".join(result))\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"SuperAnalysis API Server\")\n    parser.add_argument(\"--host\", type=str, default=\"0.0.0.0\", help=\"Host to bind the server to\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"Port to bind the server to\")\n    parser.add_argument(\"--schema-rag-base-url\", type=str, required=True, help=\"Schema RAG base URL\")\n    parser.add_argument(\"--schema-rag-api-key\", type=str, required=True, help=\"Schema RAG API key\")\n    parser.add_argument(\"--context-rag-base-url\", type=str, required=True, help=\"Context RAG base URL\")\n    parser.add_argument(\"--context-rag-api-key\", type=str, required=True, help=\"Context RAG API key\")\n    parser.add_argument(\"--byzer-sql-url\", type=str, required=True, help=\"Byzer SQL URL\")\n    parser.add_argument(\"--data-dir\", type=str, default=\"\", help=\"Data directory\")\n    parser.add_argument(\"--owner\", type=str, default=\"hello\", help=\"Owner of the Byzer SQL session\")\n    parser.add_argument(\"--llm-model\", type=str, required=True, help=\"LLM model for general use\")\n    parser.add_argument(\"--gen-sql-llm-model\", type=str, required=True, help=\"LLM model for SQL generation\")\n    \n    args = parser.parse_args()\n\n    rag_config = RAGConfig(\n        schema_rag_base_url=args.schema_rag_base_url,\n        schema_rag_api_key=args.schema_rag_api_key,\n        context_rag_base_url=args.context_rag_base_url,\n        context_rag_api_key=args.context_rag_api_key\n    )\n\n    byzer_sql_config = ByzerSQLConfig(\n        byzer_sql_url=args.byzer_sql_url,\n        data_dir=args.data_dir,\n        owner=args.owner\n    )\n\n    model_config = ModelConfig(\n        llm_model=args.llm_model,\n        gen_sql_llm_model=args.gen_sql_llm_model\n    )\n\n    llm = byzerllm.ByzerLLM()\n    llm.setup_default_model_name(args.llm_model)\n\n    global sql_generator\n    sql_generator = SQLGenerator(\n        llm=llm,\n        rag_config=rag_config,\n        byzer_sql_config=byzer_sql_config,\n        model_config=model_config\n    )\n\n    uvicorn.run(app, host=args.host, port=args.port)\n\nif __name__ == \"__main__\":\n    main()\n\n##File: /Users/allwefantasy/projects/super_analysis/Makefile\n# Makefile for super-analysis project\n\n# Python interpreter\nPYTHON := python\n\n# Project name\nPROJECT_NAME := super-analysis\n\n# Directories\nSRC_DIR := src\nDIST_DIR := dist\nBUILD_DIR := build\n\n# Commands\nSETUP := $(PYTHON) setup.py\n\n.PHONY: all clean build wheel deploy_deepseek dist\n\nall: clean build wheel dist\n\ndeploy_deepseek:\n\tbyzerllm deploy --pretrained_model_type saas/openai \\\n\t--cpus_per_worker 0.001 \\\n\t--gpus_per_worker 0 \\\n\t--worker_concurrency 1000 \\\n\t--num_workers 1 \\\n\t--infer_params saas.base_url=\"https://api.deepseek.com/v1\" saas.api_key=${MODEL_DEEPSEEK_TOKEN} saas.model=deepseek-chat \\\n\t--model deepseek_chat\n\nclean:\n\t@echo \"Cleaning up...\"\n\trm -rf $(BUILD_DIR) $(DIST_DIR) *.egg-info\n\tfind . -type d -name \"__pycache__\" -exec rm -rf {} +\n\tfind . -type f -name \"*.pyc\" -delete\n\nbuild:\n\t@echo \"Building the project...\"\n\t$(SETUP) build\n\nwheel:\n\t@echo \"Creating wheel package...\"\n\t$(SETUP) bdist_wheel\n\ninstall:\n\t@echo \"Installing the package...\"\n\t$(PYTHON) -m pip install .\n\nuninstall:\n\t@echo \"Uninstalling the package...\"\n\t$(PYTHON) -m pip uninstall -y $(PROJECT_NAME)\n\ndevelop:\n\t@echo \"Installing the package in editable mode...\"\n\t$(PYTHON) -m pip install -e .\n\ntest:\n\t@echo \"Running tests...\"\n\t$(PYTHON) -m unittest discover tests\n\ndist: wheel\n\t@echo \"Creating distribution package...\"\n\tmkdir -p $(DIST_DIR)\n\tcp \"/Users/allwefantasy/projects/super_analysis/docs/4.3.1 安装与配置 Byzer-SQL.pdf\" $(DIST_DIR)\n\tcp \"/Users/allwefantasy/projects/super_analysis/docs/deploy.md\" $(DIST_DIR)/README.md\n\ttar -czf super_analysis.tar.gz $(DIST_DIR)\n\t@echo \"Distribution package created: super_analysis.tar.gz\"\n\n##File: /Users/allwefantasy/projects/super_analysis/docs/deploy.md\n\n# 🚀 Super Analysis 部署指南\n\n本指南将帮助你部署 Super Analysis 系统，包括安装必要组件、配置服务和启动系统。\n\n---\n\n## 📦 安装 Super Analysis\n\n\n```bash\npip install -U auto-coder\npip install super_analysis-0.1.0-py3-none-any.whl\n```\n\n\n---\n\n## 🤖 部署 Deepseek 模型代理\n\n在启动其他服务之前，我们需要先部署 Deepseek 模型：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--worker_concurrency 1000 \\\n--num_workers 1 \\\n--infer_params saas.base_url=\"https://api.deepseek.com/v1\" saas.api_key=${MODEL_DEEPSEEK_TOKEN} saas.model=deepseek-chat \\\n--model deepseek_chat\n```\n\n注意：确保已设置环境变量 `MODEL_DEEPSEEK_TOKEN`。\n\n---\n\n## 🛠️ 部署 Byzer-SQL\n\n参考 [安装与配置 Byzer-SQL 文档](./4.3.1%20安装与配置%20Byzer-SQL.pdf) 完成部署。\n\n> 启动时需要在安装有 super-analysis 的 conda 环境中启动。\n\n当 byzer-sql 部署完成后，注册账号为 `hello`，然后在 byzer-sql 控制台中执行：\n\n```sql\n!byzerllm setup single;\n\nrun command as LLM.`` where \naction=\"infer\"\nand reconnect=\"true\"\nand pretrainedModelType=\"saas/openai\"\nand udfName=\"deepseek_chat\";\n```\n\n---\n\n## 📊 数据预处理和服务启动\n\n1. 转换电影数据集：\n\n```bash\nsuper-analysis.convert --data_dir /Users/allwefantasy/data/movice --doc_dir /Users/allwefantasy/data/movice/schemas/\n```\n\n2. 启动 schema 文档知识库：\n\n```bash\nauto-coder doc serve \\\n--model deepseek_chat --index_filter_workers 100 \\\n--tokenizer_path /Users/allwefantasy/Downloads/tokenizer.json \\\n--doc_dir /Users/allwefantasy/data/movice/schemas/ \\\n--port 8001\n```\n\n3. 下载 Byzer-SQL 文档并启动文档知识库：\n\n```bash\ngit clone https://github.com/allwefantasy/llm_friendly_packages\n\nauto-coder doc serve \\\n--model deepseek_chat --index_filter_workers 100 \\\n--tokenizer_path /Users/allwefantasy/Downloads/tokenizer.json \\\n--doc_dir  /Users/allwefantasy/projects/llm_friendly_packages/github.com/allwefantasy \\\n--port 8002\n```\n\n4. 启动兼容 OpenAI Server 的分析服务：\n\n```bash\nsuper-analysis.serve --served-model-name deepseek_chat --port 8000 \\\n--schema-rag-base-url http://127.0.0.1:8001/v1 \\\n--context-rag-base-url http://127.0.0.1:8002/v1 \\\n--byzer-sql-url http://127.0.0.1:9003/run/script\n```\n\n---\n\n现在，Super Analysis 系统已经完全部署并启动。你可以开始使用 OpenAI SDK 进行测试和接口调用。具体测试和接口使用方法请参考 [openai_local_api.ipynb](./notebooks/openai_local_api.ipynb)。\n\n🎉 恭喜！你已经成功部署了 Super Analysis 系统。如有任何问题，请随时查阅文档或联系支持团队。\n\n##File: .auto-coder/libs/llm_friendly_packages/github.com/allwefantasy/byzer-llm/README.md\n# byzerllm 大模型编程快速指南\n\n本文示例 [Notebook](../../notebooks/003_byzerllm_大模型编程快速指南.ipynb)\n\n## 安装\n\n```bash\npip install -U byzerllm\nray start --head\n```\n## 启动一个模型代理\n\nbyzerllm 支持私有化模型或者SaaS模型的部署。\n\n这里以 deepseek 官方API 为例：\n\n```bash\neasy-byzerllm deploy deepseek-chat --token xxxxx --alias deepseek_chat\n```\n\n或者跬基流动API:\n\n```bash\neasy-byzerllm deploy alibaba/Qwen1.5-110B-Chat --token xxxxx --alias qwen110b_chat\n```\n\n将上面的 API KEY 替换成你们自己的。\n\n如果你想部署私有化模型或者对接 Ollama 等更多需求，参考 [002_使用byzerllm进行模型部署.md](./002_使用byzerllm进行模型部署.md) 或者 [README.md](../../README.md)。\n\n之后，你就可以在代码里使用  deepseek_chat 或者 qwen110b_chat  访问模型了。\n\n## hello world\n\n来和我们的大模型打个招呼:\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\n@byzerllm.prompt(llm=llm)\ndef hello(q:str) ->str:\n    '''\n    你好, {{ q }}\n    '''\n\ns = hello(\"你是谁\")    \nprint(s)\n\n## 输出:\n## '你好！我是一个人工智能助手，专门设计来回答问题、提供信息和帮助解决问题。如果你有任何疑问或需要帮助，请随时告诉我。'\n```\n\n恭喜，你和大模型成功打了招呼！\n\n可以看到，我们通过 `@byzerllm.prompt` 装饰器，将一个方法转换成了一个大模型的调用，然后这个方法的主题是一段文本，文本中\n使用了 jinja2 模板语法，来获得方法的参数。当正常调用该方法时，实际上就发起了和大模型的交互，并且返回了大模型的结果。\n\n在 byzerllm 中，我们把这种方法称之为 prompt 函数。\n\n## 查看发送给大模型的prompt\n\n很多情况你可能需要调试，查看自己的 prompt 渲染后到底是什么样子的，这个时候你可以通过如下方式\n获取渲染后的prompt:\n\n```python\nhello.prompt(\"你是谁\")\n## '你好, 你是谁'\n```            \n\n## 动态换一个模型\n\n前面的 hello 方法在初始化的时候，我们使用了默认的模型 deepseek_chat，如果我们想换一个模型，可以这样做：\n\n```python\nhello.with_llm(llm).run(\"你是谁\")\n## '你好！我是一个人工智能助手，专门设计来回答问题、提供信息和帮助解决问题。如果你有任何疑问或需要帮助，请随时告诉我。'\n```\n\n通过 with_llm 你可以设置一个新的 llm 对象，然后调用 run 方法，就可以使用新的模型了。\n\n## 超长文本生成\n\n我们知道，大模型一次生成的长度其实是有限的，如果你想生成超长文本，你可能需手动的不断获得\n生成结果，然后把他转化为输入，然后再次生成，这样的方式是比较麻烦的。\n\nbyzerllm 提供了更加易用的 API :\n\n```python\nimport byzerllm\nfrom byzerllm import ByzerLLM\n\nllm = ByzerLLM.from_default_model(\"deepseek_chat\")\n\n@byzerllm.prompt()\ndef tell_story() -> str:\n    \"\"\"\n    讲一个100字的故事。    \n    \"\"\"\n\n\ns = (\n    tell_story.with_llm(llm)\n    .with_response_markers()\n    .options({\"llm_config\": {\"max_length\": 10}})\n    .run()\n)\nprint(s)\n\n## 从前，森林里住着一只聪明的小狐狸。一天，它发现了一块闪闪发光的宝石。小狐狸决定用这块宝石帮助森林里的动物们。它用宝石的光芒指引迷路的小鹿找到了回家的路，用宝石的温暖治愈了受伤的小鸟。从此，小狐狸成了森林里的英雄，动物们都感激它的善良和智慧。\n```\n\n实际核心部分就是这一行：\n\n```python\ntell_story.with_llm(llm)\n    .with_response_markers()    \n    .run()\n```\n\n我们只需要调用 `with_response_markers` 方法，系统就会自动的帮我们生成超长文本。\n在上面的案例中，我们通过\n\n```python\n.options({\"llm_config\": {\"max_length\": 10}})\n```\n\n认为的限制大模型一次交互最多只能输出10个字符，但是系统依然自动完成了远超过10个字符的文本生成。\n\n## 对象输出\n\n前面我们的例子都是返回字符串，但是我们也可以返回对象，这样我们就可以更加灵活的处理返回结果。\n\n```python\nimport pydantic \n\nclass Story(pydantic.BaseModel):\n    '''\n    故事\n    '''\n\n    title: str = pydantic.Field(description=\"故事的标题\")\n    body: str = pydantic.Field(description=\"故事主体\")\n\n@byzerllm.prompt()\ndef tell_story()->Story:\n    '''\n    讲一个100字的故事。    \n    '''\n\ns = tell_story.with_llm(llm).run()\nprint(isinstance(s, Story))\nprint(s.title)\n\n## True\n## 勇敢的小鸟\n```\n\n可以看到，我们很轻松的将输出转化为格式化输出。\n\n## 自定义字段抽取\n\n前面的结构化输出，其实会消耗更多token,还有一种更加精准的结构化输出方式。\n比如让大模型生成一个正则表达式，但实际上大模型很难准确只输出一个正则表达式，这个时候我们可以通过自定义抽取函数来获取我们想要的结果。\n\n\n```python\nfrom loguru import logger\nimport re\n\n@byzerllm.prompt()\ndef generate_regex_pattern(desc: str) -> str:\n    \"\"\"\n    根据下面的描述生成一个正则表达式，要符合python re.compile 库的要求。\n\n    {{ desc }}\n\n    最后生成的正则表达式要在<REGEX></REGEX>标签对里。\n    \"\"\"    \n\ndef extract_regex_pattern(regex_block: str) -> str:    \n    pattern = re.search(r\"<REGEX>(.*)</REGEX>\", regex_block, re.DOTALL)\n    if pattern is None:\n        logger.warning(\"No regex pattern found in the generated block:\\n {regex_block}\")\n        raise None\n    return pattern.group(1)\n\npattern = \"匹配一个邮箱地址\"\nv = generate_regex_pattern.with_llm(llm).with_extractor(extract_regex_pattern).run(desc=pattern)\nprint(v)\n## ^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\n```\n\n在上面的例子里，我们根据一句话生成一个正则表达式。我们通过 `with_extractor` 方法，传入了一个自定义的抽取函数，这个函数会在大模型生成结果后，对结果进行处理，然后返回我们想要的结果。\n\n我们在 prompt 明确说了，生成的结果要放到 `<REGEX></REGEX>` 标签对里，然后我们通过 extract_regex_pattern 函数，从结果中提取出了我们想要的正则表达式。\n\n## 在实例方法中使用大模型\n\n```python\nimport byzerllm\ndata = {\n    'name': 'Jane Doe',\n    'task_count': 3,\n    'tasks': [\n        {'name': 'Submit report', 'due_date': '2024-03-10'},\n        {'name': 'Finish project', 'due_date': '2024-03-15'},\n        {'name': 'Reply to emails', 'due_date': '2024-03-08'}\n    ]\n}\n\n\nclass RAG():\n    def __init__(self):        \n        self.llm = byzerllm.ByzerLLM()\n        self.llm.setup_template(model=\"deepseek_chat\",template=\"auto\")\n        self.llm.setup_default_model_name(\"deepseek_chat\")        \n    \n    @byzerllm.prompt(lambda self:self.llm)\n    def generate_answer(self,name,task_count,tasks)->str:\n        '''\n        Hello {{ name }},\n\n        This is a reminder that you have {{ task_count }} pending tasks:\n        {% for task in tasks %}\n        - Task: {{ task.name }} | Due: {{ task.due_date }}\n        {% endfor %}\n\n        Best regards,\n        Your Reminder System\n        '''        \n\nt = RAG()\n\nresponse = t.generate_answer(**data)\nprint(response)\n\n## 输出:\n## Hello Jane Doe,\n##I hope this message finds you well. I wanted to remind you of your 3 pending tasks to ensure you stay on track:\n## 1. **Submit report** - This task is due on **2024-03-10**. Please ensure that you allocat\n```\n\n这里我们给了个比较复杂的例子，但我们可以看到，给一个实例prompt方法和普通prompt 方法差异不大。\n唯一的区别是如果你希望在定义的时候就指定大模型，使用一个lambda函数返回实例的 llm 对象即可。\n\n```python\n@byzerllm.prompt(lambda self:self.llm)\n```\n\n你也可以不返回，在调用的时候通过 `with_llm` 方法指定 llm 对象。\n\n此外，这个例子也展示了如何通过jinja2模板语法，来处理复杂的结构化数据。\n\n## 通过 Python 代码处理复杂入参\n\n上面的一个例子中，我们通过 jinja2 模板语法，来处理复杂的结构化数据，但是有时候我们可能需要更加复杂的处理，这个时候我们可以通过 Python 代码来处理。\n\n```python\nimport byzerllm\n\ndata = {\n    'name': 'Jane Doe',\n    'task_count': 3,\n    'tasks': [\n        {'name': 'Submit report', 'due_date': '2024-03-10'},\n        {'name': 'Finish project', 'due_date': '2024-03-15'},\n        {'name': 'Reply to emails', 'due_date': '2024-03-08'}\n    ]\n}\n\n\nclass RAG():\n    def __init__(self):        \n        self.llm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n    \n    @byzerllm.prompt()\n    def generate_answer(self,name,task_count,tasks)->str:\n        '''\n        Hello {{ name }},\n\n        This is a reminder that you have {{ task_count }} pending tasks:\n            \n        {{ tasks }}\n\n        Best regards,\n        Your Reminder System\n        '''\n        \n        tasks_str = \"\\n\".join([f\"- Task: {task['name']} | Due: { task['due_date'] }\" for task in tasks])\n        return {\"tasks\": tasks_str}\n\nt = RAG()\n\nresponse = t.generate_answer.with_llm(t.llm).run(**data)\nprint(response)\n\n## Just a gentle nudge to keep you on track with your pending tasks. Here's a quick recap:....\n```\n\n在这个例子里，我们直接把 tasks 在方法体里进行处理，然后作为一个字符串返回，最够构建一个字典，字典的key为 tasks,然后\n你就可以在 docstring 里使用 `{{ tasks }}` 来引用这个字符串。\n\n这样对于很复杂的入参，就不用谢繁琐的 jinja2 模板语法了。\n\n## 如何自动实现一个方法\n\n比如我定义一个签名，但是我不想自己实现里面的逻辑，让大模型来实现。这个在 byzerllm 中叫 function impl。我们来看看怎么\n实现:\n\n```python\nimport pydantic\nclass Time(pydantic.BaseModel):\n    time: str = pydantic.Field(...,description=\"时间，时间格式为 yyyy-MM-dd\")\n\n\n@llm.impl()\ndef calculate_current_time()->Time:\n    '''\n    计算当前时间\n    '''\n    pass \n\n\ncalculate_current_time()\n#output: Time(time='2024-06-14')\n```\n\n在这个例子里，我们定义了一个 calculate_current_time 方法，但是我们没有实现里面的逻辑，我们通过 `@llm.impl()` 装饰器，让大模型来实现这个方法。\n为了避免每次都要“生成”这个方法，导致无法适用，我们提供了缓存，用户可以按如下方式打印速度：\n\n```python\nstart = time.monotonic()\ncalculate_current_time()\nprint(f\"first time cost: {time.monotonic()-start}\")\n\nstart = time.monotonic()\ncalculate_current_time()\nprint(f\"second time cost: {time.monotonic()-start}\")\n\n# output:\n# first time cost: 6.067266260739416\n# second time cost: 4.347506910562515e-05\n```\n可以看到，第一次执行花费了6s,第二次几乎是瞬间完成的，这是因为第一次执行的时候，我们实际上是在生成这个方法，第二次执行的时候，我们是执行已经生成好的代码，所以速度会非常快。你可以显示的调用 `llm.clear_impl_cache()` 清理掉函数缓存。\n\n## Stream 模式\n\n前面的例子都是一次性生成结果，但是有时候我们可能需要一个流式的输出，这个时候我们可能需要用底层一点的API来完成了：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\nv = llm.stream_chat_oai(model=\"deepseek_chat\",conversations=[{\n    \"role\":\"user\",\n    \"content\":\"你好，你是谁\",\n}],delta_mode=True)\n\nfor t in v:\n    print(t,flush=True)  \n\n# 你好\n# ！\n# 我\n# 是一个\n# 人工智能\n# 助手\n# ，\n# 旨在\n# 提供\n# 信息\n# 、\n# 解答\n# 问题....\n```\n\n如果你不想要流式输出，但是想用底层一点的API，你可以使用 `llm.chat_oai` 方法：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\nv = llm.chat_oai(model=\"deepseek_chat\",conversations=[{\n    \"role\":\"user\",\n    \"content\":\"你好，你是谁\",\n}])\n\nprint(v[0].output)\n## 你好！我是一个人工智能助手，旨在提供信息、解答问题和帮助用户解决问题。如果你有任何问题或需要帮助，请随时告诉我。\n```\n\n## Function Calling \n\nbyzerllm 可以不依赖模型自身就能提供 function calling 支持，我们来看个例子：\n\n\n```python\nfrom typing import List,Dict,Any,Annotated\nimport pydantic \nimport datetime\nfrom dateutil.relativedelta import relativedelta\n\ndef compute_date_range(count:Annotated[int,\"时间跨度，数值类型\"],\n                       unit:Annotated[str,\"时间单位，字符串类型\",{\"enum\":[\"day\",\"week\",\"month\",\"year\"]}])->List[str]:\n    '''\n    计算日期范围\n\n    Args:\n        count: 时间跨度，数值类型\n        unit: 时间单位，字符串类型，可选值为 day,week,month,year\n    '''        \n    now = datetime.datetime.now()\n    now_str = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n    if unit == \"day\":\n        return [(now - relativedelta(days=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"week\":\n        return [(now - relativedelta(weeks=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"month\":\n        return [(now - relativedelta(months=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"year\":\n        return [(now - relativedelta(years=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    return [\"\",\"\"]\n\ndef compute_now()->str:\n    '''\n    计算当前时间\n    '''\n    return datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n```\n\n我们定义了两个方法，一个是计算日期范围，一个是计算当前时间。\n\n现在我么可以来测试下，系统如何根据自然语言决定调用哪个方法：\n\n```python\nt = llm.chat_oai([{\n    \"content\":'''计算当前时间''',\n    \"role\":\"user\"    \n}],tools=[compute_date_range,compute_now],execute_tool=True)\n\nt[0].values\n\n## output: ['2024-06-14 15:18:02']\n```\n\n我们可以看到，他正确的选择了 compute_now 方法。\n\n接着我们再试一个：\n\n```python\nt = llm.chat_oai([{\n    \"content\":'''最近三个月趋势''',\n    \"role\":\"user\"    \n}],tools=[compute_date_range,compute_now],execute_tool=True)\n\nt[0].values\n\n## output: [['2024-03-14 15:19:13', '2024-06-14 15:19:13']]\n```\n\n模型正确的选择了 compute_date_range 方法。\n\n## 多模态\n\nbyerllm 也能很好的支持多模态的交互，而且统一了多模态大模型的接口，比如你可以用一样的方式使用 openai 或者 claude 的图片转文字能力， 或者一致的方式使用火山，azuer, openai的语音合成接口。\n\n### 图生文\n\n```python\nimport byzerllm\nfrom byzerllm.types import ImagePath\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\n\n\n@byzerllm.prompt()\ndef what_in_image(image_path: ImagePath) -> str:\n    \"\"\"\n    {{ image_path }}\n    这个图片里有什么？\n    \"\"\"    \n\n\nv = what_in_image.with_llm(vl_llm).run(\n    ImagePath(value=\"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\")\n)\nv\n## OUTPUT: 这张图片展示了多只可爱的猫咪，采用了艺术风格的绘画。猫咪们有不同的颜色和花纹，背景是浅棕色，上面还点缀着一些红色的花朵。整体画面给人一种温馨和谐的感觉\n```\n\n可以看到，我们只需要把 prompt 函数的图片地址入参使用 byzerllm.types.ImagePath里进行包装，就可以直接在 prompt 函数体里\n带上图片。\n\n或者你可以这样：\n\n```python\nimport byzerllm\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\n\n\n@byzerllm.prompt()\ndef what_in_image(image_path: str) -> str:\n    \"\"\"\n    {{ image }}\n    这个图片里有什么？\n    \"\"\"\n    return {\"image\": byzerllm.Image.load_image_from_path(image_path)}\n\n\nv = what_in_image.with_llm(vl_llm).run(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv\n```\n\n通过 `image_path` 参数，然后通过 `byzerllm.Image.load_image_from_path` 方法，转化为一个图片对象 image，最后在 prompt 函数体里\n使用 `{{ image }}` 引用这个图片对象。\n\n另外我们也是可以支持配置多张图片的。\n\n另外，我们也可以使用基础的 `llm.chat_oai` 方法来实现：\n\n```python\nimport byzerllm\nimport json\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\nimage = byzerllm.Image.load_image_from_path(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv = vl_llm.chat_oai(\n    conversations=[\n        {\n            \"role\": \"user\",\n            \"content\": json.dumps(\n                [{\"image\": image, \"text\": \"这个图片里有什么？\"}], ensure_ascii=False\n            ),\n        }\n    ]\n)\nv[0].output\n```\n\n还可以这么写：\n\n```python\nimport byzerllm\nimport json\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\nimage = byzerllm.Image.load_image_from_path(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv = vl_llm.chat_oai(\n    conversations=[\n        {\n            \"role\": \"user\",\n            \"content\": json.dumps(\n                [\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\"url\": image, \"detail\": \"high\"},\n                    },\n                    {\"text\": \"这个图片里有什么？\", \"type\": \"text\"},\n                ],\n                ensure_ascii=False,\n            ),\n        }\n    ]\n)\nv[0].output\n```\n\n### 语音合成\n\n这里以 openai 的 tts 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=tts-1 \\\n--model openai_tts\n```\n\n此外，byzerllm 支持 azure,火山引擎等 tts 语音合成引擎。\n\n接着你可以这么用：\n\n```python\nimport byzerllm\nimport base64\nimport json\n\nllm = byzerllm.ByzerLLM.from_default_model(\"openai_tts\")\n\n\nt = llm.chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\": json.dumps({\n        \"input\":\"hello, open_tts\",\n        \"voice\": \"alloy\",\n        \"response_format\": \"mp3\"\n    },ensure_ascii=False)\n}])\n\nwith open(\"voice.mp3\",\"wb\") as f:\n    f.write(base64.b64decode(t[0].output))\n```\n\ntts 模型生成没有prompt函数可以用，你需要直接使用 chat_oai。\n\n\n### 语音识别\n\n这里以 openai 的 whisper-1 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--worker_concurrency 10 \\\n--infer_params saas.model=whisper-1 saas.api_key=${MODEL_OPENAI_TOKEN} \\\n--model speech_to_text\n```\n\n语音识别的使用方式和图生文类似，我们可以直接在 prompt 函数体里带上音频文件。\n\n```python\nimport byzerllm\nimport json\nimport base64\nfrom byzerllm.types import AudioPath\n\nllm = byzerllm.ByzerLLM.from_default_model(\"speech_to_text\")\n\naudio_file = \"/Users/allwefantasy/videos/output_audio.mp3\"\n\n@byzerllm.prompt(llm=llm)\ndef audio_to_text(audio_file: AudioPath):\n    \"\"\"\n    {{ audio_file }}\n    \"\"\"\n\nv = audio_to_text(AudioPath(value=audio_file))\njson.loads(v)\n```\n输出的数据格式略微复杂：\n\n```\n{'text': 'In the last chapter, you and I started to step through the internal workings of a transformer. This is one of the key pieces of technology inside large language models, and a lot of other tools in the modern wave of AI.',\n 'task': 'transcribe',\n 'language': 'english',\n 'duration': 10.0,\n 'segments': [{'id': 0,\n   'seek': 0,\n   'start': 0.0,\n   'end': 4.78000020980835,\n   'text': ' In the last chapter, you and I started to step through the internal workings of a transformer.',\n   'tokens': [50364,\n    682,\n    264,\n    1036,\n    7187,\n    11,\n    291,\n    293,\n    286,\n    1409,\n    281,\n    1823,\n    807,\n    264,\n    6920,\n    589,\n    1109,\n    295,\n    257,\n    31782,\n    13,\n    50586],\n   'temperature': 0.0,\n   'avg_logprob': -0.28872039914131165,\n   'compression_ratio': 1.4220778942108154,\n   'no_speech_prob': 0.016033057123422623},\n  ....\n  {'id': 2,\n   'seek': 0,\n   'start': 8.579999923706055,\n   'end': 9.979999542236328,\n   'text': ' and a lot of other tools in the modern wave of AI.',\n   'tokens': [50759,\n    293,\n    257,\n    688,\n    295,\n    661,\n    3873,\n    294,\n    264,\n    4363,\n    5772,\n    295,\n    7318,\n    13,\n    50867],\n   'temperature': 0.0,\n   'avg_logprob': -0.28872039914131165,\n   'compression_ratio': 1.4220778942108154,\n   'no_speech_prob': 0.016033057123422623}],\n 'words': [{'word': 'In', 'start': 0.0, 'end': 0.18000000715255737},\n  {'word': 'the', 'start': 0.18000000715255737, 'end': 0.23999999463558197},\n  {'word': 'last', 'start': 0.23999999463558197, 'end': 0.5400000214576721},\n  {'word': 'chapter', 'start': 0.5400000214576721, 'end': 0.800000011920929},\n  ....\n  {'word': 'AI', 'start': 9.920000076293945, 'end': 9.979999542236328}]}\n```\n\n会输出每一句话以及每一个字所在的起始时间和截止时间。你可以根据需要来使用。\n\n\n### 文生图\n\n文生图和语音合成类似，首先要启动合适的模型,以openai 的 dall-e-3 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=dall-e-3 \\\n--model openai_image_gen\n```\n\n启动模型后，只需要记住几个模板参数即可使用，这里直接使用 chat_oai 方法来使用：\n\n```python\n\nimport byzerllm\nimport json\nimport base64\n\nllm = byzerllm.ByzerLLM.from_default_model(\"openai_image_gen\")\nt = llm.chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\": json.dumps({\n        \"input\":\"a white siamese cat\",\n        \"size\": \"1024x1024\",\n        \"quality\": \"standard\"\n    },ensure_ascii=False)\n}])\n\nwith open(\"output1.jpg\",\"wb\") as f:\n    f.write(base64.b64decode(t[0].output))\n\n\nimport matplotlib.pyplot as plt\n\nimage_path = \"output1.jpg\"\nimage = plt.imread(image_path)\n\nplt.imshow(image)\nplt.axis('off')\nplt.show()\n```\n\n## Prompt 函数的流式输出\n\nbyzerllm 底层支持流式输出，非 prompt 函数的用法是这样的：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\n\nv = llm.stream_chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\":\"讲一个100字的故事\"\n}])\n\nfor s  in v:\n    print(s[0], end=\"\")\n```\n\n如果你像用 prompt 函数，可以这么用：\n\n\n```python\nimport byzerllm\nimport json\nimport base64\nfrom typing import Generator\n\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\n\n@byzerllm.prompt()\ndef tell_story() -> Generator[str, None, None]:\n    '''\n    给我讲一个一百多字的故事\n    '''\n\nv = tell_story.with_llm(llm).run()    \nfor i in v:\n    print(i, end=\"\")\n```\n\n可以看到，和普通的 prompt 函数的区别在于，返回值是一个生成器，然后你可以通过 for 循环来获取结果。\n\n## 向量化模型\n\nbyzerllm 支持向量化模型,你可以这样启动一个本地的模型：\n\n```bash\n!byzerllm deploy --pretrained_model_type custom/bge \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--worker_concurrency 10 \\\n--model_path /home/winubuntu/.auto-coder/storage/models/AI-ModelScope/bge-large-zh \\\n--infer_backend transformers \\\n--num_workers 1 \\\n--model emb\n```\n\n注意两个参数:\n\n1. --infer_backend transformers: 表示使用 transformers 作为推理后端。\n2. --model_path: 表示模型的路径。\n\n也可以启动一个 SaaS 的emb模型,比如 qwen 的 emb 模型：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/qianwen \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 2 \\\n--infer_params saas.api_key=${MODEL_QIANWEN_TOKEN}  saas.model=text-embedding-v2 \\\n--model qianwen_emb\n```\n\n或者 openai 的 emb 模型：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--worker_concurrency 10 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=text-embedding-3-small \\\n--model gpt_emb\n```\nSaaS 模型无需配置 `--infer_backend` 参数。\n\n无论是本地模型还是 SaaS 模型，我们都可以这样使用：\n\n```python\nimport byzerllm\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\nllm.setup_default_emb_model_name(\"emb\")\nllm.emb_query(\"你好\")\n```\n\n如果你配置 byzerllm 中的 Storage 使用，比如你这样启动了存储：\n\n```bash\nbyzerllm storage start --enable_emb\n```\n\n那么需要这么使用：\n\n```python\nfrom byzerllm.apps.byzer_storage.simple_api import ByzerStorage, DataType, FieldOption,SortOption\nstorage = ByzerStorage(\"byzerai_store\", \"memory\", \"memory\")\nstorage.emb(\"你好\")\n```\n\n## 一些辅助工具\n\n当调用 prompt 函数返回字符串的时候，如果想从里面抽取代码，可以使用如下方式：\n\n```python\nfrom byzerllm.utils.client import code_utils\ntext_with_markdown = '''\n```shell\nls -l\n```\n'''\ncode_blocks = code_utils.extract_code(text_with_markdown)\nfor code_block in code_blocks:\n    if code_block[0] == \"shell\":\n        print(code_block[1])\n##output: ls -l        \n```\n\nTagExtractor 工具用于对任意 `<_tag_></_tag_>` 标签对的抽取，下面是一个使用示例：\n\n```python\nfrom byzerllm.apps.utils import TagExtractor\n\nextractor = TagExtractor('''\n大家好\n<_image_>data:image/jpeg;base64,xxxxxx</_image_>\n                         大家好\n<_image_>data:image/jpeg;base64,xxxxxx2</_image_>\n''')\n\nv = extractor.extract()\nprint(v.content[0].content)\nprint(v.content[1].content)\n```\n输出为:\n\n```\ndata:image/jpeg;base64,xxxxxx\ndata:image/jpeg;base64,xxxxxx2\n```\n\n我们成功的将 <_image_></_image_> 标签对里的内容抽取出来了。\n\n\n## 注意事项\n\n1. prompt函数方法体返回只能是dict，实际的返回类型和方法签名可以不一样，但是方法体返回只能是dict。\n2. 大部分情况prompt函数体为空，如果一定要有方法体，可以返回一个空字典。\n3. 调用prompt方法的时候，如果在@byzerllm.prompt()里没有指定llm对象，那么需要在调用的时候通过with_llm方法指定llm对象。\n\n===========================\n\n# Byzer Storage: 最易用的AI存储引擎\n\nByzer Storage是一个为AI应用设计的高性能存储引擎,它提供了简单易用的API,支持向量搜索、全文检索以及结构化查询。本文将详细介绍Byzer Storage的使用方法和主要特性。\n\n## 0. 安装和启动\n\n```bash\npip install byzerllm\nbyzerllm storage start\n```\n\nThat's it! Byzer Storage已经安装并启动成功,现在我们可以开始使用它了。默认会启动一个 byzerai_store 的集群。\n\n注意，如果你是这样启动的：\n\n```bash\nbyzerllm storage start --enable_emb\n```\n\n那么会自动启动一个 emb 模型，名字就叫 emb， ByzerStorage 会自动使用该模型，无需做任何其他配置。\n\n## 1. 初始化\n\n创建一个 ByzerStorage 对象，链接 byzerai_store 集群，并且指定数据库和表名（可以不存在）。\n\n```python\nfrom byzerllm.apps.byzer_storage.simple_api import ByzerStorage, DataType, FieldOption, SortOption\n\nstorage = ByzerStorage(\"byzerai_store\", \"my_database1\", \"my_table4s\")\n```\n\n\n## 2. 创建库表（可选）\n\nByzer Storage使用Schema来定义数据结构。我们可以使用SchemaBuilder来构建Schema:\n\n```python\n_ = (\n    storage.schema_builder()\n    .add_field(\"_id\", DataType.STRING)\n    .add_field(\"name\", DataType.STRING)\n    .add_field(\"content\", DataType.STRING, [FieldOption.ANALYZE])\n    .add_field(\"raw_content\", DataType.STRING, [FieldOption.NO_INDEX])    \n    .add_array_field(\"summary\", DataType.FLOAT)    \n    .add_field(\"created_time\", DataType.LONG, [FieldOption.SORT])    \n    .execute()\n)\n```\n\n这个Schema定义了以下字段:\n- `_id`: 字符串类型的主键\n- `name`: 字符串类型,可用于过滤条件\n- `content`: 字符串类型,带有ANALYZE选项,用于全文搜索\n- `raw_content`: 字符串类型,带有NO_INDEX选项,不会被索引\n- `summary`: 浮点数组类型,用于存储向量\n- `created_time`: 长整型,带有SORT选项,可用于排序\n\n需要注意的是：\n\n1. 如果一个字段带有ANALYZE选项,则该字段会被分词,并且可以用于全文搜索，但是就无法返回原始的文本了。所以你需要添加一个新字段专门用来存储原文，比如在我们这个例子里，我们新增了 raw_content 字段，并且显示的指定了 NO_INDEX 选项，这样就不会被索引，也就不会被分词，可以后续被检索后用来获取原始文本。\n2. 对于需要作为向量检索和存储的字段，需要指定为数组类型，比如我们这个例子里的 summary 字段。\n3. 如果你需要拿到向量字段的原始文本，那么你也需要添加一个新字段专门用来存储原文，就像我们这个例子里的 raw_content 字段一样。\n\n\n## 3. 写入数据\n\n准备数据并使用WriteBuilder写入Storage:\n\n```python\ndata = [\n    {\"_id\": \"1\", \"name\": \"Hello\", \"content\": \"Hello, world!\", \"raw_content\": \"Hello, world!\", \"summary\": \"hello world\", \"created_time\": 1612137600},\n    {\"_id\": \"2\", \"name\": \"Byzer\", \"content\": \"Byzer, world!\", \"raw_content\": \"Byzer, world!\", \"summary\": \"byzer\", \"created_time\": 1612137601},\n    {\"_id\": \"3\", \"name\": \"AI\", \"content\": \"AI, world!\", \"raw_content\": \"AI, world!\", \"summary\": \"AI\", \"created_time\": 16121376002},\n    {\"_id\": \"4\", \"name\": \"ByzerAI\", \"content\": \"ByzerAI, world!\", \"raw_content\": \"ByzerAI, world!\", \"summary\": \"ByzerAi\", \"created_time\": 16121376003},\n]\n\nstorage.write_builder().add_items(data, vector_fields=[\"summary\"], search_fields=[\"content\"]).execute()\nstorage.commit()\n```\n\n这里我们使用`add_items`方法批量添加数据,并指定了`summary`为向量字段,`content`为搜索字段。最后调用`commit()`来确保数据被持久化。\n\n\n从上面我们要写入到 byzer storage 的数据我们可以看到如下几个特点：\n\n1. 需要包含我们之前定义的 Schema中罗列的所有的字段，同时需要指定哪些是向量字段，哪些是检索字段。\n2. 向量和检索不能是同一个字段。\n3. 对于向量，检索字段，我们给到 write_builder 的都是文本，ByzerStorage 会根据 Schema 的定义自动将其转换为向量，检索字段需要的格式。\n\n## 4. 查询数据\n\nByzer Storage支持多种查询方式,包括向量搜索、全文检索、过滤和排序。\n\n### 4.1 向量搜索 + 全文检索\n\n```python\nquery = storage.query_builder()\nquery.set_vector_query(\"ByzerAI\", fields=[\"summary\"])\nresults = query.set_search_query(\"Hello\", fields=[\"content\"]).execute()\nprint(results)\n```\n\n这个查询结合了向量搜索和全文检索,它会在`summary`字段中搜索与\"ByzerAI\"相似的向量,同时在`content`字段中搜索包含\"Hello\"的文档。\n\n### 4.2 过滤 + 向量搜索 + 全文检索\n\n```python\nquery = storage.query_builder()\nquery.and_filter().add_condition(\"name\", \"AI\").build()\nquery.set_vector_query(\"ByzerAI\", fields=\"summary\")\nresults = query.set_search_query(\"Hello\", fields=[\"content\"]).execute()\nprint(results)\n```\n\n这个查询首先过滤`name`字段等于\"AI\"的文档,然后在结果中进行向量搜索和全文检索。\n\n### 4.3 过滤 + 排序\n\n```python\nquery = storage.query_builder()\nquery.and_filter().add_condition(\"name\", \"AI\").build().sort(\"created_time\", SortOption.DESC)\nresults = query.execute()\nprint(results)\n```\n\n这个查询过滤`name`字段等于\"AI\"的文档,然后按`created_time`字段降序排序。\n\n## 5. 删除数据\n\n### 5.1 根据ID删除\n\n```python\nstorage.delete_by_ids([\"3\"])\n\nquery = storage.query_builder()\nquery.and_filter().add_condition(\"name\", \"AI\").build()\nresults = query.execute()\nprint(results)\n```\n\n这里我们删除了ID为\"3\"的文档,然后查询验证删除结果。\n\n### 5.2 删除整个表\n\n```python\nstorage.drop()\n```\n\n这个操作会删除整个表及其所有数据,请谨慎使用。\n\n## 结论\n\nByzer Storage提供了一套简洁而强大的API,能够轻松实现向量搜索、全文检索、结构化查询等功能。它的设计非常适合AI应用场景,可以有效地存储和检索各种类型的数据。通过本文的介绍,相信读者已经对Byzer Storage有了基本的了解,并能够开始在自己的项目中使用这个强大的存储引擎。\n\n\n     \n\n下面是用户的需求：\n\ndist: wheel 我们希望解压后目录名也是 super_analysis 而不是 dist\n使用 SEARCH/REPLACE 格式输出\n"}, {"role": "assistant", "content": "好的，我会按照您的要求修改 Makefile 中的 dist 目标，以确保解压后的目录名是 super_analysis 而不是 dist。我将使用 SEARCH/REPLACE 格式输出修改后的内容。\n\n```makefile\n##File: /Users/allwefantasy/projects/super_analysis/Makefile\n<<<<<<< SEARCH\ndist: wheel\n\t@echo \"Creating distribution package...\"\n\tmkdir -p $(DIST_DIR)\n\tcp \"/Users/allwefantasy/projects/super_analysis/docs/4.3.1 安装与配置 Byzer-SQL.pdf\" $(DIST_DIR)\n\tcp \"/Users/allwefantasy/projects/super_analysis/docs/deploy.md\" $(DIST_DIR)/README.md\n\ttar -czf super_analysis.tar.gz $(DIST_DIR)\n\t@echo \"Distribution package created: super_analysis.tar.gz\"\n=======\ndist: wheel\n\t@echo \"Creating distribution package...\"\n\tmkdir -p super_analysis\n\tcp -r $(DIST_DIR)/* super_analysis/\n\tcp \"/Users/allwefantasy/projects/super_analysis/docs/4.3.1 安装与配置 Byzer-SQL.pdf\" super_analysis/\n\tcp \"/Users/allwefantasy/projects/super_analysis/docs/deploy.md\" super_analysis/README.md\n\ttar -czf super_analysis.tar.gz super_analysis\n\trm -rf super_analysis\n\t@echo \"Distribution package created: super_analysis.tar.gz\"\n>>>>>>> REPLACE\n```\n\n这个修改做了以下更改：\n\n1. 创建一个名为 `super_analysis` 的临时目录。\n2. 将 `$(DIST_DIR)` 中的所有内容复制到 `super_analysis` 目录。\n3. 将 PDF 文件和 README.md 复制到 `super_analysis` 目录。\n4. 使用 `super_analysis` 目录创建 tar 包。\n5. 删除临时的 `super_analysis` 目录。\n\n这样，当解压 `super_analysis.tar.gz` 时，得到的目录名将是 `super_analysis`。"}], "model": "human", "yaml_file": "actions/146_chat_action.yml"}
{"instruction": "ModelConfig 添加一个 sql_func_llm_model 参数, cli.py 要添加该命令行\n使用 SEARCH/REPLACE 格式输出", "conversations": [{"role": "user", "content": "\n如果你需要生成代码，对于每个需要更改的文件,你需要按 *SEARCH/REPLACE block* 的格式进行生成。\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The opening fence and code language, eg: ```python\n2. The file path alone on a line, starting with \"##File:\" and verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n*SEARCH/REPLACE* blocks will replace *all* matching occurrences.\nInclude enough lines to make the SEARCH blocks unique.\n\nInclude *ALL* the code being searched and replaced!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n下面我们来看一个例子：\n\n当前项目目录结构：\n1. 项目根目录： /tmp/projects/mathweb\n2. 项目子目录/文件列表(类似tree 命令输出)\nflask/\n    app.py\n    templates/\n        index.html\n    static/\n        style.css\n\n用户需求： Change get_factorial() to use math.factorial\n\n回答： To make this change we need to modify `/tmp/projects/mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n用户需求： Refactor hello() into its own file.\n\n回答：To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/hello.py\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/main.py\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n现在让我们开始一个新的任务:\n\n当前项目目录结构：\n1. 项目根目录： /Users/allwefantasy/projects/super_analysis\n2. 项目子目录/文件列表(类似tree 命令输出)：\nsetup.py\n.auto-coder/\n    libs/\n        llm_friendly_packages/\n            src/\n                example1/\n                    main.py\nsrc/\n    superanalysis/\n        serve/\n            rest.py\n            convert.py\n            openai_serve.py\n            __init__.py\n            llm_wrapper.py\n            api_server.py\n            cli.py\n        sql/\n            __init__.py\n            types.py\n            generator.py\n下面是一些文件路径以及每个文件对应的源码：\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/types.py\nfrom pydantic import BaseModel\n\nclass RAGConfig(BaseModel):\n    schema_rag_base_url: str = \"\"\n    schema_rag_api_key: str = \"xxxx\"\n\n    context_rag_base_url: str = \"\"\n    context_rag_api_key: str = \"xxxx\"\n\nclass ByzerSQLConfig(BaseModel):\n    byzer_sql_url: str = \"\"\n    data_dir: str = \"\"\n    owner: str = \"\"\n\nclass ModelConfig(BaseModel):\n    llm_model: str = \"\"\n    gen_sql_llm_model: str = \"\"\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/cli.py\nimport argparse\nimport byzerllm\nfrom byzerllm.utils.client import ByzerLLM\nfrom superanalysis.serve.api_server import serve, ServerArgs\nfrom superanalysis.sql.types import RAGConfig, ByzerSQLConfig, ModelConfig\nfrom superanalysis.sql.generator import SQLGenerator\nfrom superanalysis.serve.llm_wrapper import LLWrapper\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"ByzerLLM API Server\")\n    parser.add_argument(\"--ray-address\", type=str, default=\"auto\", help=\"ray address\")\n    parser.add_argument(\n        \"--host\", type=str, default=\"0.0.0.0\", help=\"Host to bind the server to\"\n    )\n    parser.add_argument(\n        \"--port\", type=int, default=8000, help=\"Port to bind the server to\"\n    )\n    parser.add_argument(\n        \"--uvicorn-log-level\", type=str, default=\"info\", help=\"Uvicorn log level\"\n    )\n    parser.add_argument(\n        \"--allow-credentials\", action=\"store_true\", help=\"Allow credentials\"\n    )\n    parser.add_argument(\n        \"--allowed-origins\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed origins\"\n    )\n    parser.add_argument(\n        \"--allowed-methods\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed methods\"\n    )\n    parser.add_argument(\n        \"--allowed-headers\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed headers\"\n    )\n    parser.add_argument(\n        \"--api-key\", type=str, default=\"\", help=\"API key for authentication\"\n    )\n    parser.add_argument(\n        \"--served-model-name\",\n        type=str,\n        required=True,\n        help=\"Name of the model to be served\",\n    )\n    parser.add_argument(\n        \"--prompt-template\", type=str, default=\"\", help=\"Prompt template\"\n    )\n    parser.add_argument(\n        \"--response-role\", type=str, default=\"assistant\", help=\"Response role\"\n    )\n    parser.add_argument(\"--ssl-keyfile\", type=str, default=\"\", help=\"SSL key file\")\n    parser.add_argument(\n        \"--ssl-certfile\", type=str, default=\"\", help=\"SSL certificate file\"\n    )\n\n    # Add new arguments for RAGConfig\n    parser.add_argument(\n        \"--schema-rag-base-url\", type=str, default=\"\", help=\"RAG base URL\"\n    )\n    parser.add_argument(\n        \"--schema-rag-api-key\", type=str, default=\"xxxx\", help=\"RAG API key\"\n    )\n\n    parser.add_argument(\n        \"--context-rag-base-url\", type=str, default=\"\", help=\"RAG base URL\"\n    )\n    parser.add_argument(\n        \"--context-rag-api-key\", type=str, default=\"xxxx\", help=\"RAG API key\"\n    )\n\n    # Add new argument for ByzerSQLConfig\n    parser.add_argument(\"--byzer-sql-url\", type=str, default=\"\", help=\"Byzer SQL URL\")\n    parser.add_argument(\"--data-dir\", type=str, default=\"\", help=\"Data directory\")\n    parser.add_argument(\"--owner\", type=str, default=\"hello\", help=\"Owner of the Byzer SQL session\")\n    parser.add_argument(\n        \"--llm-model\", type=str, default=\"\", help=\"LLM model for general use\"\n    )\n    parser.add_argument(\n        \"--gen-sql-llm-model\", type=str, default=\"\", help=\"LLM model for SQL generation\"\n    )\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile,\n    )\n\n    # Create RAGConfig instance\n    rag_config = RAGConfig(\n        schema_rag_base_url=args.schema_rag_base_url,\n        schema_rag_api_key=args.schema_rag_api_key,\n        context_rag_base_url=args.context_rag_base_url,\n        context_rag_api_key=args.context_rag_api_key,\n    )\n\n    # Create ByzerSQLConfig instance\n    byzer_sql_config = ByzerSQLConfig(\n        byzer_sql_url=args.byzer_sql_url, data_dir=args.data_dir, owner=args.owner\n    )\n\n    model_config = ModelConfig(\n        llm_model=args.llm_model or args.served_model_name,\n        gen_sql_llm_model=args.gen_sql_llm_model or args.served_model_name,\n    )\n\n    byzerllm.connect_cluster(address=args.ray_address)\n    llm = ByzerLLM()\n    llm.setup_default_model_name(args.served_model_name)\n    llm = byzerllm.ByzerLLM()\n    if not server_args.served_model_name:\n        raise ValueError(\"served_model_name is required\")\n    llm.setup_default_model_name(server_args.served_model_name)\n\n    print(\"RAG Config: \", rag_config)\n    print(\"Model Config: \", model_config)\n    rag = SQLGenerator(\n        llm=llm,\n        rag_config=rag_config,\n        byzer_sql_config=byzer_sql_config,\n        model_config=model_config,\n    )\n    llm_wrapper = LLWrapper(llm=llm, entry=rag)\n    serve(llm_wrapper, server_args)\n\n\nif __name__ == \"__main__\":\n    main()\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/__init__.py\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/__init__.py\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/llm_wrapper.py\nfrom typing import Any, Dict, List, Optional, Union,Callable\nfrom byzerllm.utils.client.types import (    \n    LLMFunctionCallResponse,\n    LLMClassResponse,LLMResponse    \n)\nimport pydantic\nfrom byzerllm import ByzerLLM\nfrom byzerllm.utils.client import LLMResponse\nfrom byzerllm.utils.types import SingleOutputMeta\nfrom byzerllm.utils.langutil import asyncfy_with_semaphore\nfrom superanalysis.sql.generator import SQLGenerator\nfrom loguru import logger\n\nclass LLWrapper:\n\n    def __init__(self,llm:ByzerLLM,entry:SQLGenerator):\n        self.entry = entry\n        self.llm = llm\n        \n\n    def chat_oai(self,\n                 conversations,\n                 tools:List[Union[Callable,str]]=[], \n                 tool_choice:Optional[Union[Callable,str]]=None,\n                 execute_tool:bool=False,  \n                 impl_func:Optional[Callable]=None,\n                 execute_impl_func:bool=False,\n                 impl_func_params:Optional[Dict[str,Any]]=None,\n                 func_params:Optional[Dict[str,Any]]=None,\n                 response_class:Optional[Union[pydantic.BaseModel,str]] = None, \n                 response_after_chat:Optional[Union[pydantic.BaseModel,str]] = False,\n                 enable_default_sys_message:bool=True,                 \n                 model:Optional[str] = None,\n                 role_mapping=None,llm_config:Dict[str,Any]={}\n                 )->Union[List[LLMResponse],List[LLMFunctionCallResponse],List[LLMClassResponse]]: \n        res,contexts = self.entry.stream_chat_oai(conversations)\n        s = \"\".join(res)        \n        return [LLMResponse(output=s,metadata={},input=\"\")]\n\n    def stream_chat_oai(self,conversations, \n                        model:Optional[str]=None, \n                        role_mapping=None,\n                        delta_mode=False,\n                        llm_config:Dict[str,Any]={}): \n        res,contexts = self.entry.stream_chat_oai(conversations)        \n        for t in res:                        \n            yield (t,SingleOutputMeta(0,0))\n\n    async def async_stream_chat_oai(self,conversations, \n                        model:Optional[str]=None, \n                        role_mapping=None,\n                        delta_mode=False,\n                        llm_config:Dict[str,Any]={}): \n        res,contexts = self.rag.stream_chat_oai(conversations)                \n        for t in res:                                    \n            yield (t,SingleOutputMeta(0,0))\n                         \n\n    def __getattr__(self, name):        \n        return getattr(self.llm, name)\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/openai_serve.py\nimport argparse\nfrom service_server.api_server import ServerArgs, serve\nfrom service_server.llm_wrapper import LLWrapper\nfrom service_server.summary_rag import SummaryRAG\nimport byzerllm\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"OpenAI Serve\")\n    parser.add_argument(\n        \"--model\",\n        default=\"\",\n    )\n    parser.add_argument(\"--emb_model\", default=\"\")\n    parser.add_argument(\n        \"--ray_address\",\n        default=\"auto\",\n    )\n    parser.add_argument(\"--source_dir\", default=\".\", help=\"\")\n    parser.add_argument(\"--host\", default=\"\", help=\"\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"\")\n    parser.add_argument(\"--uvicorn_log_level\", default=\"info\", help=\"\")\n    parser.add_argument(\"--allow_credentials\", action=\"store_true\", help=\"\")\n    parser.add_argument(\"--allowed_origins\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--allowed_methods\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--allowed_headers\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--api_key\", default=\"\", help=\"\")\n    parser.add_argument(\"--served_model_name\", default=\"\", help=\"\")\n    parser.add_argument(\"--prompt_template\", default=\"\", help=\"\")\n    parser.add_argument(\"--ssl_keyfile\", default=\"\", help=\"\")\n    parser.add_argument(\"--ssl_certfile\", default=\"\", help=\"\")\n    parser.add_argument(\"--response_role\", default=\"assistant\", help=\"\")\n    parser.add_argument(\n        \"--collections\", default=\"\", help=\"Collection name for indexing\"\n    )\n    parser.add_argument(\n        \"--base_dir\",\n        default=\"\",\n        help=\"Path where the processed text embeddings were stored\",\n    )\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name or args.model,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile,\n    )\n    byzerllm.connect_cluster(address=args.ray_address)\n    llm = byzerllm.ByzerLLM()\n    if not server_args.served_model_name:\n        raise ValueError(\"served_model_name is required\")\n    llm.setup_default_model_name(server_args.served_model_name)\n    rag = SummaryRAG(llm=llm)    \n    llm_wrapper = LLWrapper(llm=llm, rag=rag)\n    serve(llm=llm_wrapper, args=server_args)\n\nif __name__ == \"__main__\":\n    main()\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/api_server.py\nimport os\nimport time\nimport uvicorn\nfrom fastapi import Request\nfrom fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import JSONResponse, StreamingResponse, Response\n\nfrom byzerllm.log import init_logger\nfrom byzerllm.utils import random_uuid\nfrom byzerllm.version import __version__ as version\nfrom byzerllm.utils.client import ByzerLLM, LLMRequest\nfrom byzerllm.utils.client.entrypoints.openai.serving_chat import OpenAIServingChat\nfrom byzerllm.utils.client.entrypoints.openai.serving_completion import OpenAIServingCompletion\nfrom byzerllm.utils.client.entrypoints.openai.protocol import (\n    ModelList,\n    ModelCard,\n    ModelPermission,\n    ChatCompletionRequest,\n    ErrorResponse,\n    CompletionRequest,\n    Embeddings,\n    EmbeddingsOutput,\n    EmbeddingsData,\n    EmbeddingsUsage,\n)\nfrom pydantic import BaseModel\nfrom typing import List\n\nlogger = init_logger(__name__)\n\nllm_client: ByzerLLM = None\nopenai_serving_chat: OpenAIServingChat = None\nopenai_serving_completion: OpenAIServingCompletion = None\n\nTIMEOUT_KEEP_ALIVE = 5  # seconds\n# timeout in 10 minutes. Streaming can take longer than 3 min\nTIMEOUT = float(os.environ.get(\"BYZERLLM_APISERVER_HTTP_TIMEOUT\", 600))\n\nrouter_app = FastAPI()\n\n\n@router_app.get(\"/health\")\nasync def health() -> Response:\n    \"\"\"Health check.\"\"\"\n    return Response(status_code=200)\n\n\n@router_app.get(\"/v1/models\")\nasync def show_available_models():\n    models = await openai_serving_chat.show_available_models()\n    return JSONResponse(content=models.model_dump())\n\n\n@router_app.get(\"/version\")\nasync def show_version():\n    return JSONResponse(content={\"version\": version})\n\n\n@router_app.get(\"/v1/models\", response_model=ModelList)\nasync def models() -> ModelList:\n    \"\"\"Show available models. Right now we only have one model.\"\"\"\n    model_cards = [\n        ModelCard(\n            id=\"\",\n            root=\"\",\n            permission=[ModelPermission()]\n        )\n    ]\n    return ModelList(data=model_cards)\n\n\n@router_app.post(\"/v1/completions\")\nasync def create_completion(\n        body: CompletionRequest,\n        request: Request\n):\n    generator = await openai_serving_completion.create_completion(body, request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            status_code=generator.code\n        )\n    if body.stream:\n        return StreamingResponse(\n            content=generator,\n            media_type=\"text/event-stream\"\n        )\n    else:\n        return JSONResponse(content=generator.model_dump())\n\n\n@router_app.post(\"/v1/chat/completions\")\nasync def create_chat_completion(\n        body: ChatCompletionRequest,\n        request: Request,\n):\n    \"\"\"Completion API similar to OpenAI's API.\n\n    See  https://platform.openai.com/docs/api-reference/chat/create\n    for the API specification. This API mimics the OpenAI ChatCompletion API.\n\n    NOTE: Currently we do not support the following features:\n        - function_call (Users should implement this by themselves)\n        - logit_bias (to be supported by vLLM engine)\n    \"\"\"\n    # async with async_timeout.timeout(TIMEOUT):\n\n    generator = await openai_serving_chat.create_chat_completion(body, request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            status_code=generator.code\n        )\n    if body.stream:\n        return StreamingResponse(\n            content=generator,\n            media_type=\"text/event-stream\"\n        )\n    else:\n        return JSONResponse(content=generator.model_dump())\n\n\n@router_app.post(\"/v1/embeddings\")\nasync def embed(body: Embeddings):\n    \"\"\"Given a prompt, the model will return one embedding.\n\n    Returns:\n        A response object with an embedding.\n    \"\"\"\n    embedding_id = f\"embed-{random_uuid()}\"\n\n    results_list = llm_client.emb(body.model, request=LLMRequest(instruction=body.input))\n    tokens = 0\n\n    return EmbeddingsOutput(\n        data=[\n            EmbeddingsData(\n                embedding=results.output,\n                index=i,\n                object=\"embedding\",\n            )\n            for i, results in enumerate(results_list)\n        ],\n        id=embedding_id,\n        object=\"list\",\n        created=int(time.time()),\n        model=body.model,\n        usage=EmbeddingsUsage(\n            prompt_tokens=tokens,\n            total_tokens=tokens,\n        ),\n    )\n\nclass ServerArgs(BaseModel):\n    host: str = None\n    port: int = 8000\n    uvicorn_log_level: str = \"info\"\n    allow_credentials: bool = False\n    allowed_origins: List[str] = [\"*\"]  \n    allowed_methods: List[str] = [\"*\"]\n    allowed_headers: List[str] = [\"*\"]\n    api_key: str = None\n    served_model_name: str = None\n    prompt_template: str = None\n    response_role: str = \"assistant\"\n    ssl_keyfile: str = None\n    ssl_certfile: str = None    \n\ndef serve(llm:ByzerLLM, args: ServerArgs):\n    \n    logger.info(f\"ByzerLLM API server version {version}\")\n    logger.info(f\"args: {args}\")\n\n    router_app.add_middleware(\n        CORSMiddleware,\n        allow_origins=args.allowed_origins,\n        allow_credentials=args.allow_credentials,\n        allow_methods=args.allowed_methods,\n        allow_headers=args.allowed_headers,\n    )\n    \n    if token := os.environ.get(\"BYZERLLM_API_KEY\") or args.api_key:\n\n        @router_app.middleware(\"http\")\n        async def authentication(request: Request, call_next):\n            if not request.url.path.startswith(\"/v1\"):\n                return await call_next(request)\n            if request.headers.get(\"Authorization\") != \"Bearer \" + token:\n                return JSONResponse(\n                    content={\"error\": \"Unauthorized\"},\n                    status_code=401\n                )\n            return await call_next(request)\n\n    # Register labels for metrics\n    # add_global_metrics_labels(model_name=engine_args.model)\n    global llm_client\n    llm_client = llm\n    \n    global openai_serving_chat\n    openai_serving_chat = OpenAIServingChat(\n        llm_client=llm_client,\n        response_role=args.response_role,\n        server_model_name=args.served_model_name,\n        prompt_template=args.prompt_template\n    )\n    global openai_serving_completion\n    openai_serving_completion = OpenAIServingCompletion(\n        llm_client=llm_client,\n        server_model_name=args.served_model_name,\n        prompt_template=args.prompt_template\n    )\n\n    uvicorn.run(\n        router_app,\n        host=args.host,\n        port=args.port,\n        log_level=args.uvicorn_log_level,\n        timeout_keep_alive=TIMEOUT_KEEP_ALIVE,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile\n    )\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/generator.py\nfrom typing import Optional, Dict, Any, List\nfrom openai import OpenAI\nimport json\nfrom .types import RAGConfig, ByzerSQLConfig, ModelConfig\nfrom byzerllm.utils.client import code_utils\nimport byzerllm\nimport requests\nfrom loguru import logger\nimport traceback\nfrom byzerllm.utils.nontext import TagExtractor\n\n\nclass SQLGenerator:\n\n    @byzerllm.prompt()\n    def gen_context(self, conversations: List[Dict[str, str]]) -> str:\n        \"\"\"\n        对话内容：\n        {% for msg in conversations %}\n        <{{ msg.role }}>: {{ msg.content }}\n        {% endfor %}\n\n        根据提供的对话内容，理解用户最后的问题，然后提取可能会用到的 Byzer-SQL 相关的知识。\n        \"\"\"\n\n    @byzerllm.prompt()\n    def gen_sql(\n        self,\n        conversations: List[Dict[str, str]],\n        context: str,\n        schema: str,\n        model: str = \"deepseek_chat\",\n    ) -> str:\n        \"\"\"\n        相关知识：\n        {{ context }}\n\n        Schema信息：\n        {{ schema }}\n\n        对话内容：\n        {% for msg in conversations %}\n        <{{ msg.role }}>: {{ msg.content }}\n        {% endfor %}\n\n        模型UDF函数：\n        {{ model }}\n\n        根据提供的相关知识以及对话内容，理解用户最后的问题，然后根据看到的schema 信息，生成 Byzer-SQL 语句。\n        Byzer-SQL 首先需要用 load 语法加载 schema 中的路径映射成表格，然后根据表格的信息生成对应的 Byzer-SQL 语句。\n\n        特别需要关注的是:\n        1. 用户的很多需求是要对表格的文本内容做自然语言处理，你需要使用 Byzer-SQL 中的大模型辅助函数来完成这个需求。\n        2. 使用 `llm_result` 函数处理 LLM 模型函数的响应,`llm_result` 函数得到的是一段文本，虽然我们要求模型函数只输出特定的文本，但考虑到大模型会有一些额外的字符输出，不会只是输出我们要求的文字，所以通过 `like` 来进行匹配。\n        3. 注意 Byzer-SQL 的语法，比如select 语句最后一定需要有 as <table_name>。\n        3. 输出的 Byzer-SQL 语句务必使用 <_byzer_sql_></_byzer_sql_> 标签包裹起来。\n        \"\"\"\n\n    @byzerllm.prompt()\n    def fetch_schema(self, context: str) -> str:\n        \"\"\"\n        下面是一些基础知识，包括名词术语等等：\n        {{ context }}\n\n        当理解对话内容的时候，需要参考上面的基础知识，然后更好的理解对应的表格schema信息是不是有用。\n        \"\"\"\n\n    def __init__(\n        self,\n        llm: byzerllm.ByzerLLM,\n        rag_config: RAGConfig,\n        byzer_sql_config: ByzerSQLConfig,\n        model_config: ModelConfig,\n    ) -> None:\n        self.llm = llm\n        self.schema_rag_client = OpenAI(\n            api_key=rag_config.schema_rag_api_key,\n            base_url=rag_config.schema_rag_base_url,\n        )\n        self.context_rag_client = OpenAI(\n            api_key=rag_config.context_rag_api_key,\n            base_url=rag_config.context_rag_base_url,\n        )\n        self.byzer_sql_url = byzer_sql_config.byzer_sql_url\n\n        self.llm_model = byzerllm.ByzerLLM()\n        self.llm_model.setup_default_model_name(model_config.llm_model)\n        self.gen_sql_llm_model = byzerllm.ByzerLLM()\n        self.gen_sql_llm_model.setup_default_model_name(model_config.gen_sql_llm_model)\n\n    def stream_chat_oai(\n        self,\n        conversations,\n        model: Optional[str] = None,\n        role_mapping=None,\n        delta_mode=False,\n        llm_config: Dict[str, Any] = {},\n    ):\n        try:\n            v, c = self._stream_chat_oai(\n                conversations=conversations,\n                model=model,\n                role_mapping=role_mapping,\n                delta_mode=delta_mode,\n                llm_config=llm_config,\n            )\n        except Exception as e:\n            error_msg = f\"Error in stream_chat_oai: {e}\\n{traceback.format_exc()}\"\n            logger.error(error_msg)\n            v, c = [str(e)], []\n\n        return v, c\n\n    def _stream_chat_oai(\n        self,\n        conversations,\n        model: Optional[str] = None,\n        role_mapping=None,\n        delta_mode=False,\n        llm_config: Dict[str, Any] = {},\n    ):\n        logger.info(f\"Starting stream_chat_oai with model: {model}\")\n        logger.debug(f\"Conversations: {conversations}\")\n        logger.debug(f\"LLM Config: {llm_config}\")\n\n        if not model:\n            model = self.llm_model.default_model_name\n        logger.info(f\"Using model: {model}\")\n\n        query = conversations[-1][\"content\"]\n\n        if (\n            \"使用四到五个字直接返回这句话的简要主题，不要解释、不要标点、不要语气词、不要多余文本，不要加粗，如果没有主题\"\n            in query\n            or \"简要总结一下对话内容，用作后续的上下文提示 prompt，控制在 200 字以内\"\n            in query\n        ):\n            chunks = self.llm.stream_chat_oai(\n                conversations=conversations,\n                model=model,\n                role_mapping=role_mapping,\n                llm_config=llm_config,\n                delta_mode=True,\n            )\n            return (chunk[0] for chunk in chunks), []\n\n        logger.info(\"Fetching context from context RAG client\")\n        context_query = self.gen_context.prompt(conversations=conversations)\n        context_response = self.context_rag_client.chat.completions.create(\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": json.dumps(\n                        {\"query\": context_query, \"only_contexts\": True},\n                        ensure_ascii=False,\n                    ),\n                }\n            ],\n            model=model,\n        )\n\n        context = context_response.choices[0].message.content\n        new_context = \"\"\n        for msg in context.split(\"\\n\"):\n            if msg.strip():\n                v = json.loads(msg)\n                new_context += v[\"source_code\"]\n        logger.info(f\"context {new_context[0:100]}...\")\n        logger.info(\"Context retrieved successfully\")\n\n        logger.info(\"Fetching schemas from schema RAG client\")\n        schema_query = self.fetch_schema.prompt(context=new_context)\n\n        response = self.schema_rag_client.chat.completions.create(\n            messages=conversations[0:-1]\n            + [\n                {\n                    \"role\": \"user\",\n                    \"content\": json.dumps(\n                        {\"query\": schema_query, \"only_contexts\": True},\n                        ensure_ascii=False,\n                    ),\n                }\n            ],\n            model=model,\n        )        \n\n        schema_str = response.choices[0].message.content\n        logger.debug(f\"Schema RAG response: {schema_str}\")\n        logger.info(\"Schema retrieved successfully\")\n\n        logger.info(\"Generating SQL string\")\n        sql_str = self.gen_sql.with_llm(self.gen_sql_llm_model).run(\n            conversations=conversations,\n            context=new_context,\n            schema=schema_str,\n            model=model,\n        )\n\n        logger.info(\"SQL string generated\")\n        logger.debug(f\"Generated SQL string: {sql_str}\")\n\n        # Extract SQL code from tags\n        tag_extractor = TagExtractor(sql_str)\n        extracted_tags = tag_extractor.extract()\n        sql = extracted_tags.content[0].content\n        logger.info(f\"Extracted SQL: {sql}\")\n\n        logger.info(f\"Sending SQL to Byzer SQL URL: {self.byzer_sql_url}\")\n        try:\n            response = requests.post(\n                self.byzer_sql_url,\n                data={\n                    \"sql\": sql,\n                    \"owner\": \"hello\",\n                    \"sessionPerUser\": True,\n                    \"sessionPerRequest\": True,\n                    \"includeSchema\": True,\n                },\n            )\n            response.raise_for_status()\n            result = response.content.decode(\"utf-8\")\n            logger.info(\"SQL execution result decoded\")\n            logger.debug(f\"SQL execution result: {result}\")\n            return (item for item in [result]), []\n        except requests.exceptions.RequestException as e:\n            logger.error(f\"Failed to send SQL to Byzer SQL URL: {e}\")\n            if hasattr(e.response, \"content\"):\n                error_content = e.response.content.decode(\"utf-8\")\n                logger.error(f\"Server response: {error_content}\")\n            else:\n                logger.error(\"No server response available\")\n            raise Exception(f\"Byzer SQL request failed: {str(e)}\") from e\n\n\n##File: /Users/allwefantasy/projects/super_analysis/setup.py\nfrom setuptools import setup, find_packages\n\n# Read requirements from requirements.txt\nwith open('requirements.txt') as f:\n    requirements = f.read().splitlines()\n\nsetup(\n    name=\"super-analysis\",\n    version=\"0.1.0\",\n    packages=find_packages(where=\"src\"),\n    package_dir={\"\": \"src\"},\n    install_requires=requirements,\n    entry_points={\n        \"console_scripts\": [\n            \"super-analysis.serve=superanalysis.serve.cli:main\",\n            \"super-analysis.convert=superanalysis.serve.convert:main\",\n            \"superanalysis.api=superanalysis.serve.rest:main\",\n        ],\n    },\n    author=\"allwefantasy\",\n    author_email=\"allwefantasy@gmail.com\",\n    description=\"A package for super analysis with ByzerLLM\",\n    long_description=open(\"README.md\").read(),\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/yourusername/super-analysis\",\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n    ],\n    python_requires='>=3.6',\n)\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/convert.py\nimport os\nimport argparse\nimport pandas as pd\nimport hashlib\nfrom typing import List, Dict\n\ndef get_csv_files(directory: str) -> List[str]:\n    \"\"\"Recursively get all CSV files in the given directory.\"\"\"\n    csv_files = []\n    for root, _, files in os.walk(directory):\n        for file in files:\n            if file.endswith('.csv'):\n                csv_files.append(os.path.join(root, file))\n    return csv_files\n\ndef extract_schema(df: pd.DataFrame) -> Dict[str, str]:\n    \"\"\"Extract schema from pandas DataFrame.\"\"\"\n    schema = {}\n    for column in df.columns:\n        dtype = str(df[column].dtype)\n        if dtype == 'object':\n            schema[column] = 'STRING'\n        elif dtype.startswith('int'):\n            schema[column] = 'INTEGER'\n        elif dtype.startswith('float'):\n            schema[column] = 'FLOAT'\n        elif dtype == 'bool':\n            schema[column] = 'BOOLEAN'\n        elif dtype.startswith('datetime'):\n            schema[column] = 'TIMESTAMP'\n        else:\n            schema[column] = 'STRING'  # Default to STRING for unknown types\n    return schema\n\ndef generate_schema_text(schema: Dict[str, str], csv_file: str, df: pd.DataFrame, include_rows_num: int) -> str:\n    \"\"\"Generate schema text from the schema dictionary and include the CSV file name and sample data.\"\"\"\n    schema_lines = [f\"CSV File: {csv_file}\\n\"]\n    schema_lines.extend([f\"{field}: {dtype}\" for field, dtype in schema.items()])\n    \n    if include_rows_num > 0:\n        schema_lines.append(f\"\\nSample Data (first {include_rows_num} rows):\")\n        sample_data = df.head(include_rows_num).to_string(index=False)\n        schema_lines.append(sample_data)\n    \n    return \"\\n\".join(schema_lines)\n\ndef save_schema(schema_text: str, csv_file: str, output_dir: str) -> None:\n    \"\"\"Save schema text to a file in the output directory.\"\"\"\n    # Generate MD5 hash of the schema text\n    file_name = hashlib.md5(schema_text.encode()).hexdigest() + '.md'\n    output_path = os.path.join(output_dir, file_name)\n    \n    with open(output_path, 'w') as f:\n        f.write(schema_text)\n    print(f\"Schema for {csv_file} saved to {output_path}\")\n\ndef convert(data_dir: str, doc_dir: str, include_rows_num: int) -> None:\n    \"\"\"Main conversion function.\"\"\"\n    csv_files = get_csv_files(data_dir)\n    \n    for csv_file in csv_files:\n        print(f\"Processing {csv_file}\")\n        df = pd.read_csv(csv_file)\n        schema = extract_schema(df)\n        schema_text = generate_schema_text(schema, csv_file, df, include_rows_num)\n        save_schema(schema_text, csv_file, doc_dir)\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Convert CSV files to schema documents\")\n    parser.add_argument(\"--data_dir\", required=True, help=\"Directory containing CSV files\")\n    parser.add_argument(\"--doc_dir\", required=True, help=\"Output directory for schema documents\")\n    parser.add_argument(\"--include-rows-num\", type=int, default=0, help=\"Number of sample rows to include in the schema\")\n    \n    args = parser.parse_args()\n    \n    if not os.path.exists(args.doc_dir):\n        os.makedirs(args.doc_dir)\n    \n    convert(args.data_dir, args.doc_dir, args.include_rows_num)\n\nif __name__ == \"__main__\":\n    main()\n\n##File: /Users/allwefantasy/projects/super_analysis/README.md\n# Super Analysis\n\nSuper Analysis 支持 NLP2SQL 以及 使用 SQL 对表中的文本字段做自然语言分析。\n\n## 安装\n\n你可以使用 pip 安装 Super Analysis：\n\n```bash\ngit clone https://github.com/allwefantasy/super_analysis\ncd super-analysis\npip install -e .\n```\n\n或者使用 Makefile 来管理项目：\n\n```bash\n# 清理项目\nmake clean\n\n# 构建项目\nmake build\n\n# 创建wheel包\nmake wheel\n\n# 安装项目\nmake install\n\n# 卸载项目\nmake uninstall\n\n# 以开发模式安装项目\nmake develop\n\n# 运行测试\nmake test\n```\n\n\n\n## 示例数据\n\n下载电影数据集： https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset/download?datasetVersionNumber=7\n\n\n## 启动分析服务\n\n整个 super-analysis 服务分为四个部分：\n\n1. byzer-sql 分布式执行引擎 \n2. schema 文档知识库\n3. context 文档知识库\n4. super-analysis 自身\n\n在启动 super-analysis 之前，你需要启动 schema 文档知识库和 context 文档知识库以及 byzer-sql 引擎。\n\n## 部署 Deepseek 模型代理\n\n在启动其他服务之前，我们需要先部署 Deepseek 模型。你可以使用以下命令来部署：\n\n```bash\nmake deploy_deepseek\n```\n\n或者直接运行以下命令：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--worker_concurrency 1000 \\\n--num_workers 1 \\\n--infer_params saas.base_url=\"https://api.deepseek.com/v1\" saas.api_key=${MODEL_DEEPSEEK_TOKEN} saas.model=deepseek-chat \\\n--model deepseek_chat\n```\n\n注意：请确保在运行此命令之前，已经设置了环境变量 `MODEL_DEEPSEEK_TOKEN`。\n\n## 部署 Byzer-SQL\n\n在你的电脑上参考 [这个文档](./docs/4.3.1%20安装与配置%20Byzer-SQL.pdf) 完成部署.\n\n> 启动的时候需要在安装有 super-analysis 的conda环境中启动。\n\n\n\n当 byzer-sql 部署完成后，注册账号为 `hello`, 然后进入到 byzer-sql 的控制台中执行以下脚本：\n\n```sql\n!byzerllm setup single;\n\nrun command as LLM.`` where \naction=\"infer\"\nand reconnect=\"true\"\nand pretrainedModelType=\"saas/openai\"\nand udfName=\"deepseek_chat\";\n```\n\n## 启动其他服务\n\n现在可以按如下方式做一些数据预处理或者启动相关服务：\n\n```bash\n## 将电影数据集的csv文件转换为schema文档\nsuper-analysis.convert --data_dir /Users/allwefantasy/data/movice --doc_dir /Users/allwefantasy/data/movice/schemas/\n\n# 启动schema文档知识库\nauto-coder doc serve \\\n--model deepseek_chat --index_filter_workers 100 \\\n--tokenizer_path /Users/allwefantasy/Downloads/tokenizer.json \\\n--doc_dir /Users/allwefantasy/data/movice/schemas/ \\\n--port 8001\n\n# 下载 Byzer-SQL 专门为大模型优化的文档\ngit clone https://github.com/allwefantasy/llm_friendly_packages\n\n# 启动文档知识库\nauto-coder doc serve \\\n--model deepseek_chat --index_filter_workers 100 \\\n--tokenizer_path /Users/allwefantasy/Downloads/tokenizer.json \\\n--doc_dir  /Users/allwefantasy/projects/llm_friendly_packages/github.com/allwefantasy \\\n--port 8002\n\n# 启动兼容 OpenAI Server 的分析服务\nsuper-analysis.serve --served-model-name deepseek_chat --port 8000 \\\n--schema-rag-base-url http://127.0.0.1:8001/v1 \\\n--context-rag-base-url http://127.0.0.1:8002/v1 \\\n--byzer-sql-url http://127.0.0.1:9003/run/script\n```\n\n现在你可以用 OpenAI SDK 去使用了。 具体测试和接口使用参考 [openai_local_api.ipynb](./notebooks/openai_local_api.ipynb)\n\n\n## 非 OpenAI 兼容接口\n\n除了兼容 OpenAI 的接口外，Super Analysis 还提供了一个简单的 REST API 接口，可以直接与模型进行交互。以下是如何使用这个接口：\n\n### 启动服务\n\n使用以下命令启动 REST API 服务：\n\n```bash\nsuperanalysis.api --schema-rag-base-url <schema_rag_url> \\\n                  --schema-rag-api-key <schema_rag_api_key> \\\n                  --context-rag-base-url <context_rag_url> \\\n                  --context-rag-api-key <context_rag_api_key> \\\n                  --byzer-sql-url <byzer_sql_url> \\\n                  --llm-model <llm_model_name> \\\n                  --gen-sql-llm-model <gen_sql_llm_model_name> \\\n                  --host <host> \\\n                  --port <port>\n```\n\n请替换尖括号中的值为实际的配置信息。\n\n### API 端点\n\n服务启动后，将提供以下 API 端点：\n\n#### POST /chat\n\n此端点用于与模型进行对话。\n\n请求体格式：\n\n```json\n{\n  \"conversations\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"你的问题或指令\"\n    }\n  ],\n  \"model\": \"可选的模型名称\"\n}\n```\n\n响应格式：\n\n```json\n{\n  \"response\": \"模型的回答\"\n}\n```\n\n### 示例使用\n\n以下是使用 Python 请求库与 API 交互的示例：\n\n```python\nimport requests\nimport json\n\nurl = \"http://localhost:8000/chat\"\npayload = {\n    \"conversations\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"你好，请介绍一下你自己。\"\n        }\n    ],\n    \"model\": \"your_model_name\"\n}\n\nresponse = requests.post(url, json=payload)\nresult = response.json()\n\nprint(result[\"response\"])\n```\n\n这个 API 相比兼容 OpenAI 接口，可以在请求中提供提供更多可选参数。\n\n## 使用方法\n\n安装完成后，你可以使用 `super-analysis.serve` 命令来启动 API 服务器：\n\n```bash\nsuper-analysis.serve --served-model-name your_model_name\n```\n\n### 命令行参数\n\n以下是可用的命令行参数：\n\n- `--host`: 服务器绑定的主机地址（默认：0.0.0.0）\n- `--port`: 服务器绑定的端口（默认：8000）\n- `--uvicorn-log-level`: Uvicorn 日志级别（默认：info）\n- `--allow-credentials`: 允许凭证（默认：False）\n- `--allowed-origins`: 允许的源（默认：[\"*\"]）\n- `--allowed-methods`: 允许的方法（默认：[\"*\"]）\n- `--allowed-headers`: 允许的头部（默认：[\"*\"]）\n- `--api-key`: 用于认证的 API 密钥\n- `--served-model-name`: 要服务的模型名称（必需）\n- `--prompt-template`: 提示模板\n- `--response-role`: 响应角色（默认：assistant）\n- `--ssl-keyfile`: SSL 密钥文件路径\n- `--ssl-certfile`: SSL 证书文件路径\n\n例如：\n\n```bash\nsuper-analysis.serve --served-model-name deepseek_chat --port 8000\n```\n\n\n## 抽取csv文件schema\n\nThe `super-analysis.convert` command allows you to convert CSV files to schema documents. This is useful for generating metadata about your CSV files that can be used for further analysis or documentation.\n\n#### Usage\n\n```\nsuper-analysis.convert --data_dir <path_to_csv_files> --doc_dir <output_directory> [--include-rows-num <number_of_sample_rows>]\n```\n\nArguments:\n- `--data_dir`: Directory containing CSV files (required)\n- `--doc_dir`: Output directory for schema documents (required)\n- `--include-rows-num`: Number of sample rows to include in the schema (optional, default is 0)\n\nThe utility will:\n1. Recursively find all CSV files in the specified directory\n2. Extract the schema from each CSV file\n3. Generate a schema text that includes:\n   - The CSV file name\n   - Field names and their data types\n   - Sample data (if `--include-rows-num` is specified)\n4. Save the schema text as a markdown file in the output directory\n\n## 联系方式\n\n如有任何问题或建议，请联系 allwefantasy@gmail.com。\n\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/rest.py\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nfrom typing import List, Optional\nimport uvicorn\nimport argparse\nfrom superanalysis.sql.generator import SQLGenerator\nfrom superanalysis.sql.types import RAGConfig, ByzerSQLConfig, ModelConfig\nimport byzerllm\n\napp = FastAPI()\n\nclass ChatMessage(BaseModel):\n    role: str\n    content: str\n\nclass ChatRequest(BaseModel):\n    conversations: List[ChatMessage]\n    model: Optional[str] = None\n\nclass ChatResponse(BaseModel):\n    response: str\n\n@app.post(\"/chat\", response_model=ChatResponse)\nasync def chat(request: ChatRequest):\n    try:\n        result, _ = sql_generator.stream_chat_oai(\n            conversations=[{\"role\": msg.role, \"content\": msg.content} for msg in request.conversations],\n            model=request.model\n        )\n        return ChatResponse(response=\"\".join(result))\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"SuperAnalysis API Server\")\n    parser.add_argument(\"--host\", type=str, default=\"0.0.0.0\", help=\"Host to bind the server to\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"Port to bind the server to\")\n    parser.add_argument(\"--schema-rag-base-url\", type=str, required=True, help=\"Schema RAG base URL\")\n    parser.add_argument(\"--schema-rag-api-key\", type=str, required=True, help=\"Schema RAG API key\")\n    parser.add_argument(\"--context-rag-base-url\", type=str, required=True, help=\"Context RAG base URL\")\n    parser.add_argument(\"--context-rag-api-key\", type=str, required=True, help=\"Context RAG API key\")\n    parser.add_argument(\"--byzer-sql-url\", type=str, required=True, help=\"Byzer SQL URL\")\n    parser.add_argument(\"--data-dir\", type=str, default=\"\", help=\"Data directory\")\n    parser.add_argument(\"--owner\", type=str, default=\"hello\", help=\"Owner of the Byzer SQL session\")\n    parser.add_argument(\"--llm-model\", type=str, required=True, help=\"LLM model for general use\")\n    parser.add_argument(\"--gen-sql-llm-model\", type=str, required=True, help=\"LLM model for SQL generation\")\n    \n    args = parser.parse_args()\n\n    rag_config = RAGConfig(\n        schema_rag_base_url=args.schema_rag_base_url,\n        schema_rag_api_key=args.schema_rag_api_key,\n        context_rag_base_url=args.context_rag_base_url,\n        context_rag_api_key=args.context_rag_api_key\n    )\n\n    byzer_sql_config = ByzerSQLConfig(\n        byzer_sql_url=args.byzer_sql_url,\n        data_dir=args.data_dir,\n        owner=args.owner\n    )\n\n    model_config = ModelConfig(\n        llm_model=args.llm_model,\n        gen_sql_llm_model=args.gen_sql_llm_model\n    )\n\n    llm = byzerllm.ByzerLLM()\n    llm.setup_default_model_name(args.llm_model)\n\n    global sql_generator\n    sql_generator = SQLGenerator(\n        llm=llm,\n        rag_config=rag_config,\n        byzer_sql_config=byzer_sql_config,\n        model_config=model_config\n    )\n\n    uvicorn.run(app, host=args.host, port=args.port)\n\nif __name__ == \"__main__\":\n    main()\n\n##File: /Users/allwefantasy/projects/super_analysis/Makefile\n# Makefile for super-analysis project\n\n# Python interpreter\nPYTHON := python\n\n# Project name\nPROJECT_NAME := super-analysis\n\n# Directories\nSRC_DIR := src\nDIST_DIR := dist\nBUILD_DIR := build\n\n# Commands\nSETUP := $(PYTHON) setup.py\n\n.PHONY: all clean build wheel deploy_deepseek dist\n\nall: clean build wheel dist\n\ndeploy_deepseek:\n\tbyzerllm deploy --pretrained_model_type saas/openai \\\n\t--cpus_per_worker 0.001 \\\n\t--gpus_per_worker 0 \\\n\t--worker_concurrency 1000 \\\n\t--num_workers 1 \\\n\t--infer_params saas.base_url=\"https://api.deepseek.com/v1\" saas.api_key=${MODEL_DEEPSEEK_TOKEN} saas.model=deepseek-chat \\\n\t--model deepseek_chat\n\nclean:\n\t@echo \"Cleaning up...\"\n\trm -rf $(BUILD_DIR) $(DIST_DIR) *.egg-info\n\tfind . -type d -name \"__pycache__\" -exec rm -rf {} +\n\tfind . -type f -name \"*.pyc\" -delete\n\nbuild:\n\t@echo \"Building the project...\"\n\t$(SETUP) build\n\nwheel:\n\t@echo \"Creating wheel package...\"\n\t$(SETUP) bdist_wheel\n\ninstall:\n\t@echo \"Installing the package...\"\n\t$(PYTHON) -m pip install .\n\nuninstall:\n\t@echo \"Uninstalling the package...\"\n\t$(PYTHON) -m pip uninstall -y $(PROJECT_NAME)\n\ndevelop:\n\t@echo \"Installing the package in editable mode...\"\n\t$(PYTHON) -m pip install -e .\n\ntest:\n\t@echo \"Running tests...\"\n\t$(PYTHON) -m unittest discover tests\n\ndist: wheel\n\t@echo \"Creating distribution package...\"\n\trm -rf super_analysis.tar.gz\n\tmkdir -p super_analysis\n\tcp -r $(DIST_DIR)/* super_analysis/\n\tcp \"/Users/allwefantasy/projects/super_analysis/docs/4.3.1 安装与配置 Byzer-SQL.pdf\" super_analysis/\n\tcp \"/Users/allwefantasy/projects/super_analysis/docs/deploy.md\" super_analysis/README.md\n\tcp \"/Users/allwefantasy/projects/super_analysis/notebooks/openai_local_api.ipynb\" super_analysis/\n\ttar -czf super_analysis.tar.gz super_analysis\n\trm -rf super_analysis\n\t@echo \"Distribution package created: super_analysis.tar.gz\"\n\n##File: /Users/allwefantasy/projects/super_analysis/docs/deploy.md\n\n# 🚀 Super Analysis 部署指南\n\n本指南将帮助你部署 Super Analysis 系统，包括安装必要组件、配置服务和启动系统。\n\n---\n\n## 📦 安装 Super Analysis\n\n\n```bash\npip install -U auto-coder\npip install super_analysis-0.1.0-py3-none-any.whl\n```\n\n\n---\n\n## 🤖 部署 Deepseek 模型代理\n\n在启动其他服务之前，我们需要先部署 Deepseek 模型：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--worker_concurrency 1000 \\\n--num_workers 1 \\\n--infer_params saas.base_url=\"https://api.deepseek.com/v1\" saas.api_key=${MODEL_DEEPSEEK_TOKEN} saas.model=deepseek-chat \\\n--model deepseek_chat\n```\n\n注意：确保已设置环境变量 `MODEL_DEEPSEEK_TOKEN`。\n\n---\n\n## 🛠️ 部署 Byzer-SQL\n\n参考 [安装与配置 Byzer-SQL 文档](./4.3.1%20安装与配置%20Byzer-SQL.pdf) 完成部署。\n\n> 启动时需要在安装有 super-analysis 的 conda 环境中启动。\n\n当 byzer-sql 部署完成后，注册账号为 `hello`，然后在 byzer-sql 控制台中执行：\n\n```sql\n!byzerllm setup single;\n\nrun command as LLM.`` where \naction=\"infer\"\nand reconnect=\"true\"\nand pretrainedModelType=\"saas/openai\"\nand udfName=\"deepseek_chat\";\n```\n\n---\n\n## 示例数据\n\n下载电影数据集： https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset/download?datasetVersionNumber=7\n\n---\n\n## 📊 数据预处理和服务启动\n\n1. 抽取电影数据集schema：\n\n```bash\nsuper-analysis.convert --data_dir /Users/allwefantasy/data/movice --doc_dir /Users/allwefantasy/data/movice/schemas/\n```\n\n你还可以添加 --include-rows-num 5 让系统在生成 schema 文档时同时提供一些示例数据。方便大模型更好的对这个表进行认知。\n\n2. 启动 schema 文档知识库：\n\n```bash\nauto-coder doc serve \\\n--model deepseek_chat --index_filter_workers 100 \\\n--tokenizer_path /Users/allwefantasy/Downloads/tokenizer.json \\\n--doc_dir /Users/allwefantasy/data/movice/schemas/ \\\n--port 8001\n```\n\n3. 下载 Byzer-SQL 文档并启动文档知识库：\n\n```bash\ngit clone https://github.com/allwefantasy/llm_friendly_packages\n\nauto-coder doc serve \\\n--model deepseek_chat --index_filter_workers 100 \\\n--tokenizer_path /Users/allwefantasy/Downloads/tokenizer.json \\\n--doc_dir  /Users/allwefantasy/projects/llm_friendly_packages/github.com/allwefantasy \\\n--port 8002\n```\n\n4. 启动兼容 OpenAI Server 的分析服务：\n\n```bash\nsuper-analysis.serve --served-model-name deepseek_chat --port 8000 \\\n--schema-rag-base-url http://127.0.0.1:8001/v1 \\\n--context-rag-base-url http://127.0.0.1:8002/v1 \\\n--byzer-sql-url http://127.0.0.1:9003/run/script\n```\n\n---\n\n现在，Super Analysis 系统已经完全部署并启动。你可以开始使用 OpenAI SDK 进行测试和接口调用。具体测试和接口使用方法请参考 [openai_local_api.ipynb](./openai_local_api.ipynb)。\n\n🎉 恭喜！你已经成功部署了 Super Analysis 系统。如有任何问题，请随时查阅文档或联系支持团队。\n\n##File: .auto-coder/libs/llm_friendly_packages/github.com/allwefantasy/byzer-llm/README.md\n# byzerllm 大模型编程快速指南\n\n本文示例 [Notebook](../../notebooks/003_byzerllm_大模型编程快速指南.ipynb)\n\n## 安装\n\n```bash\npip install -U byzerllm\nray start --head\n```\n## 启动一个模型代理\n\nbyzerllm 支持私有化模型或者SaaS模型的部署。\n\n这里以 deepseek 官方API 为例：\n\n```bash\neasy-byzerllm deploy deepseek-chat --token xxxxx --alias deepseek_chat\n```\n\n或者跬基流动API:\n\n```bash\neasy-byzerllm deploy alibaba/Qwen1.5-110B-Chat --token xxxxx --alias qwen110b_chat\n```\n\n将上面的 API KEY 替换成你们自己的。\n\n如果你想部署私有化模型或者对接 Ollama 等更多需求，参考 [002_使用byzerllm进行模型部署.md](./002_使用byzerllm进行模型部署.md) 或者 [README.md](../../README.md)。\n\n之后，你就可以在代码里使用  deepseek_chat 或者 qwen110b_chat  访问模型了。\n\n## hello world\n\n来和我们的大模型打个招呼:\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\n@byzerllm.prompt(llm=llm)\ndef hello(q:str) ->str:\n    '''\n    你好, {{ q }}\n    '''\n\ns = hello(\"你是谁\")    \nprint(s)\n\n## 输出:\n## '你好！我是一个人工智能助手，专门设计来回答问题、提供信息和帮助解决问题。如果你有任何疑问或需要帮助，请随时告诉我。'\n```\n\n恭喜，你和大模型成功打了招呼！\n\n可以看到，我们通过 `@byzerllm.prompt` 装饰器，将一个方法转换成了一个大模型的调用，然后这个方法的主题是一段文本，文本中\n使用了 jinja2 模板语法，来获得方法的参数。当正常调用该方法时，实际上就发起了和大模型的交互，并且返回了大模型的结果。\n\n在 byzerllm 中，我们把这种方法称之为 prompt 函数。\n\n## 查看发送给大模型的prompt\n\n很多情况你可能需要调试，查看自己的 prompt 渲染后到底是什么样子的，这个时候你可以通过如下方式\n获取渲染后的prompt:\n\n```python\nhello.prompt(\"你是谁\")\n## '你好, 你是谁'\n```            \n\n## 动态换一个模型\n\n前面的 hello 方法在初始化的时候，我们使用了默认的模型 deepseek_chat，如果我们想换一个模型，可以这样做：\n\n```python\nhello.with_llm(llm).run(\"你是谁\")\n## '你好！我是一个人工智能助手，专门设计来回答问题、提供信息和帮助解决问题。如果你有任何疑问或需要帮助，请随时告诉我。'\n```\n\n通过 with_llm 你可以设置一个新的 llm 对象，然后调用 run 方法，就可以使用新的模型了。\n\n## 超长文本生成\n\n我们知道，大模型一次生成的长度其实是有限的，如果你想生成超长文本，你可能需手动的不断获得\n生成结果，然后把他转化为输入，然后再次生成，这样的方式是比较麻烦的。\n\nbyzerllm 提供了更加易用的 API :\n\n```python\nimport byzerllm\nfrom byzerllm import ByzerLLM\n\nllm = ByzerLLM.from_default_model(\"deepseek_chat\")\n\n@byzerllm.prompt()\ndef tell_story() -> str:\n    \"\"\"\n    讲一个100字的故事。    \n    \"\"\"\n\n\ns = (\n    tell_story.with_llm(llm)\n    .with_response_markers()\n    .options({\"llm_config\": {\"max_length\": 10}})\n    .run()\n)\nprint(s)\n\n## 从前，森林里住着一只聪明的小狐狸。一天，它发现了一块闪闪发光的宝石。小狐狸决定用这块宝石帮助森林里的动物们。它用宝石的光芒指引迷路的小鹿找到了回家的路，用宝石的温暖治愈了受伤的小鸟。从此，小狐狸成了森林里的英雄，动物们都感激它的善良和智慧。\n```\n\n实际核心部分就是这一行：\n\n```python\ntell_story.with_llm(llm)\n    .with_response_markers()    \n    .run()\n```\n\n我们只需要调用 `with_response_markers` 方法，系统就会自动的帮我们生成超长文本。\n在上面的案例中，我们通过\n\n```python\n.options({\"llm_config\": {\"max_length\": 10}})\n```\n\n认为的限制大模型一次交互最多只能输出10个字符，但是系统依然自动完成了远超过10个字符的文本生成。\n\n## 对象输出\n\n前面我们的例子都是返回字符串，但是我们也可以返回对象，这样我们就可以更加灵活的处理返回结果。\n\n```python\nimport pydantic \n\nclass Story(pydantic.BaseModel):\n    '''\n    故事\n    '''\n\n    title: str = pydantic.Field(description=\"故事的标题\")\n    body: str = pydantic.Field(description=\"故事主体\")\n\n@byzerllm.prompt()\ndef tell_story()->Story:\n    '''\n    讲一个100字的故事。    \n    '''\n\ns = tell_story.with_llm(llm).run()\nprint(isinstance(s, Story))\nprint(s.title)\n\n## True\n## 勇敢的小鸟\n```\n\n可以看到，我们很轻松的将输出转化为格式化输出。\n\n## 自定义字段抽取\n\n前面的结构化输出，其实会消耗更多token,还有一种更加精准的结构化输出方式。\n比如让大模型生成一个正则表达式，但实际上大模型很难准确只输出一个正则表达式，这个时候我们可以通过自定义抽取函数来获取我们想要的结果。\n\n\n```python\nfrom loguru import logger\nimport re\n\n@byzerllm.prompt()\ndef generate_regex_pattern(desc: str) -> str:\n    \"\"\"\n    根据下面的描述生成一个正则表达式，要符合python re.compile 库的要求。\n\n    {{ desc }}\n\n    最后生成的正则表达式要在<REGEX></REGEX>标签对里。\n    \"\"\"    \n\ndef extract_regex_pattern(regex_block: str) -> str:    \n    pattern = re.search(r\"<REGEX>(.*)</REGEX>\", regex_block, re.DOTALL)\n    if pattern is None:\n        logger.warning(\"No regex pattern found in the generated block:\\n {regex_block}\")\n        raise None\n    return pattern.group(1)\n\npattern = \"匹配一个邮箱地址\"\nv = generate_regex_pattern.with_llm(llm).with_extractor(extract_regex_pattern).run(desc=pattern)\nprint(v)\n## ^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\n```\n\n在上面的例子里，我们根据一句话生成一个正则表达式。我们通过 `with_extractor` 方法，传入了一个自定义的抽取函数，这个函数会在大模型生成结果后，对结果进行处理，然后返回我们想要的结果。\n\n我们在 prompt 明确说了，生成的结果要放到 `<REGEX></REGEX>` 标签对里，然后我们通过 extract_regex_pattern 函数，从结果中提取出了我们想要的正则表达式。\n\n## 在实例方法中使用大模型\n\n```python\nimport byzerllm\ndata = {\n    'name': 'Jane Doe',\n    'task_count': 3,\n    'tasks': [\n        {'name': 'Submit report', 'due_date': '2024-03-10'},\n        {'name': 'Finish project', 'due_date': '2024-03-15'},\n        {'name': 'Reply to emails', 'due_date': '2024-03-08'}\n    ]\n}\n\n\nclass RAG():\n    def __init__(self):        \n        self.llm = byzerllm.ByzerLLM()\n        self.llm.setup_template(model=\"deepseek_chat\",template=\"auto\")\n        self.llm.setup_default_model_name(\"deepseek_chat\")        \n    \n    @byzerllm.prompt(lambda self:self.llm)\n    def generate_answer(self,name,task_count,tasks)->str:\n        '''\n        Hello {{ name }},\n\n        This is a reminder that you have {{ task_count }} pending tasks:\n        {% for task in tasks %}\n        - Task: {{ task.name }} | Due: {{ task.due_date }}\n        {% endfor %}\n\n        Best regards,\n        Your Reminder System\n        '''        \n\nt = RAG()\n\nresponse = t.generate_answer(**data)\nprint(response)\n\n## 输出:\n## Hello Jane Doe,\n##I hope this message finds you well. I wanted to remind you of your 3 pending tasks to ensure you stay on track:\n## 1. **Submit report** - This task is due on **2024-03-10**. Please ensure that you allocat\n```\n\n这里我们给了个比较复杂的例子，但我们可以看到，给一个实例prompt方法和普通prompt 方法差异不大。\n唯一的区别是如果你希望在定义的时候就指定大模型，使用一个lambda函数返回实例的 llm 对象即可。\n\n```python\n@byzerllm.prompt(lambda self:self.llm)\n```\n\n你也可以不返回，在调用的时候通过 `with_llm` 方法指定 llm 对象。\n\n此外，这个例子也展示了如何通过jinja2模板语法，来处理复杂的结构化数据。\n\n## 通过 Python 代码处理复杂入参\n\n上面的一个例子中，我们通过 jinja2 模板语法，来处理复杂的结构化数据，但是有时候我们可能需要更加复杂的处理，这个时候我们可以通过 Python 代码来处理。\n\n```python\nimport byzerllm\n\ndata = {\n    'name': 'Jane Doe',\n    'task_count': 3,\n    'tasks': [\n        {'name': 'Submit report', 'due_date': '2024-03-10'},\n        {'name': 'Finish project', 'due_date': '2024-03-15'},\n        {'name': 'Reply to emails', 'due_date': '2024-03-08'}\n    ]\n}\n\n\nclass RAG():\n    def __init__(self):        \n        self.llm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n    \n    @byzerllm.prompt()\n    def generate_answer(self,name,task_count,tasks)->str:\n        '''\n        Hello {{ name }},\n\n        This is a reminder that you have {{ task_count }} pending tasks:\n            \n        {{ tasks }}\n\n        Best regards,\n        Your Reminder System\n        '''\n        \n        tasks_str = \"\\n\".join([f\"- Task: {task['name']} | Due: { task['due_date'] }\" for task in tasks])\n        return {\"tasks\": tasks_str}\n\nt = RAG()\n\nresponse = t.generate_answer.with_llm(t.llm).run(**data)\nprint(response)\n\n## Just a gentle nudge to keep you on track with your pending tasks. Here's a quick recap:....\n```\n\n在这个例子里，我们直接把 tasks 在方法体里进行处理，然后作为一个字符串返回，最够构建一个字典，字典的key为 tasks,然后\n你就可以在 docstring 里使用 `{{ tasks }}` 来引用这个字符串。\n\n这样对于很复杂的入参，就不用谢繁琐的 jinja2 模板语法了。\n\n## 如何自动实现一个方法\n\n比如我定义一个签名，但是我不想自己实现里面的逻辑，让大模型来实现。这个在 byzerllm 中叫 function impl。我们来看看怎么\n实现:\n\n```python\nimport pydantic\nclass Time(pydantic.BaseModel):\n    time: str = pydantic.Field(...,description=\"时间，时间格式为 yyyy-MM-dd\")\n\n\n@llm.impl()\ndef calculate_current_time()->Time:\n    '''\n    计算当前时间\n    '''\n    pass \n\n\ncalculate_current_time()\n#output: Time(time='2024-06-14')\n```\n\n在这个例子里，我们定义了一个 calculate_current_time 方法，但是我们没有实现里面的逻辑，我们通过 `@llm.impl()` 装饰器，让大模型来实现这个方法。\n为了避免每次都要“生成”这个方法，导致无法适用，我们提供了缓存，用户可以按如下方式打印速度：\n\n```python\nstart = time.monotonic()\ncalculate_current_time()\nprint(f\"first time cost: {time.monotonic()-start}\")\n\nstart = time.monotonic()\ncalculate_current_time()\nprint(f\"second time cost: {time.monotonic()-start}\")\n\n# output:\n# first time cost: 6.067266260739416\n# second time cost: 4.347506910562515e-05\n```\n可以看到，第一次执行花费了6s,第二次几乎是瞬间完成的，这是因为第一次执行的时候，我们实际上是在生成这个方法，第二次执行的时候，我们是执行已经生成好的代码，所以速度会非常快。你可以显示的调用 `llm.clear_impl_cache()` 清理掉函数缓存。\n\n## Stream 模式\n\n前面的例子都是一次性生成结果，但是有时候我们可能需要一个流式的输出，这个时候我们可能需要用底层一点的API来完成了：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\nv = llm.stream_chat_oai(model=\"deepseek_chat\",conversations=[{\n    \"role\":\"user\",\n    \"content\":\"你好，你是谁\",\n}],delta_mode=True)\n\nfor t in v:\n    print(t,flush=True)  \n\n# 你好\n# ！\n# 我\n# 是一个\n# 人工智能\n# 助手\n# ，\n# 旨在\n# 提供\n# 信息\n# 、\n# 解答\n# 问题....\n```\n\n如果你不想要流式输出，但是想用底层一点的API，你可以使用 `llm.chat_oai` 方法：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\nv = llm.chat_oai(model=\"deepseek_chat\",conversations=[{\n    \"role\":\"user\",\n    \"content\":\"你好，你是谁\",\n}])\n\nprint(v[0].output)\n## 你好！我是一个人工智能助手，旨在提供信息、解答问题和帮助用户解决问题。如果你有任何问题或需要帮助，请随时告诉我。\n```\n\n## Function Calling \n\nbyzerllm 可以不依赖模型自身就能提供 function calling 支持，我们来看个例子：\n\n\n```python\nfrom typing import List,Dict,Any,Annotated\nimport pydantic \nimport datetime\nfrom dateutil.relativedelta import relativedelta\n\ndef compute_date_range(count:Annotated[int,\"时间跨度，数值类型\"],\n                       unit:Annotated[str,\"时间单位，字符串类型\",{\"enum\":[\"day\",\"week\",\"month\",\"year\"]}])->List[str]:\n    '''\n    计算日期范围\n\n    Args:\n        count: 时间跨度，数值类型\n        unit: 时间单位，字符串类型，可选值为 day,week,month,year\n    '''        \n    now = datetime.datetime.now()\n    now_str = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n    if unit == \"day\":\n        return [(now - relativedelta(days=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"week\":\n        return [(now - relativedelta(weeks=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"month\":\n        return [(now - relativedelta(months=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"year\":\n        return [(now - relativedelta(years=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    return [\"\",\"\"]\n\ndef compute_now()->str:\n    '''\n    计算当前时间\n    '''\n    return datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n```\n\n我们定义了两个方法，一个是计算日期范围，一个是计算当前时间。\n\n现在我么可以来测试下，系统如何根据自然语言决定调用哪个方法：\n\n```python\nt = llm.chat_oai([{\n    \"content\":'''计算当前时间''',\n    \"role\":\"user\"    \n}],tools=[compute_date_range,compute_now],execute_tool=True)\n\nt[0].values\n\n## output: ['2024-06-14 15:18:02']\n```\n\n我们可以看到，他正确的选择了 compute_now 方法。\n\n接着我们再试一个：\n\n```python\nt = llm.chat_oai([{\n    \"content\":'''最近三个月趋势''',\n    \"role\":\"user\"    \n}],tools=[compute_date_range,compute_now],execute_tool=True)\n\nt[0].values\n\n## output: [['2024-03-14 15:19:13', '2024-06-14 15:19:13']]\n```\n\n模型正确的选择了 compute_date_range 方法。\n\n## 多模态\n\nbyerllm 也能很好的支持多模态的交互，而且统一了多模态大模型的接口，比如你可以用一样的方式使用 openai 或者 claude 的图片转文字能力， 或者一致的方式使用火山，azuer, openai的语音合成接口。\n\n### 图生文\n\n```python\nimport byzerllm\nfrom byzerllm.types import ImagePath\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\n\n\n@byzerllm.prompt()\ndef what_in_image(image_path: ImagePath) -> str:\n    \"\"\"\n    {{ image_path }}\n    这个图片里有什么？\n    \"\"\"    \n\n\nv = what_in_image.with_llm(vl_llm).run(\n    ImagePath(value=\"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\")\n)\nv\n## OUTPUT: 这张图片展示了多只可爱的猫咪，采用了艺术风格的绘画。猫咪们有不同的颜色和花纹，背景是浅棕色，上面还点缀着一些红色的花朵。整体画面给人一种温馨和谐的感觉\n```\n\n可以看到，我们只需要把 prompt 函数的图片地址入参使用 byzerllm.types.ImagePath里进行包装，就可以直接在 prompt 函数体里\n带上图片。\n\n或者你可以这样：\n\n```python\nimport byzerllm\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\n\n\n@byzerllm.prompt()\ndef what_in_image(image_path: str) -> str:\n    \"\"\"\n    {{ image }}\n    这个图片里有什么？\n    \"\"\"\n    return {\"image\": byzerllm.Image.load_image_from_path(image_path)}\n\n\nv = what_in_image.with_llm(vl_llm).run(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv\n```\n\n通过 `image_path` 参数，然后通过 `byzerllm.Image.load_image_from_path` 方法，转化为一个图片对象 image，最后在 prompt 函数体里\n使用 `{{ image }}` 引用这个图片对象。\n\n另外我们也是可以支持配置多张图片的。\n\n另外，我们也可以使用基础的 `llm.chat_oai` 方法来实现：\n\n```python\nimport byzerllm\nimport json\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\nimage = byzerllm.Image.load_image_from_path(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv = vl_llm.chat_oai(\n    conversations=[\n        {\n            \"role\": \"user\",\n            \"content\": json.dumps(\n                [{\"image\": image, \"text\": \"这个图片里有什么？\"}], ensure_ascii=False\n            ),\n        }\n    ]\n)\nv[0].output\n```\n\n还可以这么写：\n\n```python\nimport byzerllm\nimport json\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\nimage = byzerllm.Image.load_image_from_path(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv = vl_llm.chat_oai(\n    conversations=[\n        {\n            \"role\": \"user\",\n            \"content\": json.dumps(\n                [\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\"url\": image, \"detail\": \"high\"},\n                    },\n                    {\"text\": \"这个图片里有什么？\", \"type\": \"text\"},\n                ],\n                ensure_ascii=False,\n            ),\n        }\n    ]\n)\nv[0].output\n```\n\n### 语音合成\n\n这里以 openai 的 tts 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=tts-1 \\\n--model openai_tts\n```\n\n此外，byzerllm 支持 azure,火山引擎等 tts 语音合成引擎。\n\n接着你可以这么用：\n\n```python\nimport byzerllm\nimport base64\nimport json\n\nllm = byzerllm.ByzerLLM.from_default_model(\"openai_tts\")\n\n\nt = llm.chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\": json.dumps({\n        \"input\":\"hello, open_tts\",\n        \"voice\": \"alloy\",\n        \"response_format\": \"mp3\"\n    },ensure_ascii=False)\n}])\n\nwith open(\"voice.mp3\",\"wb\") as f:\n    f.write(base64.b64decode(t[0].output))\n```\n\ntts 模型生成没有prompt函数可以用，你需要直接使用 chat_oai。\n\n\n### 语音识别\n\n这里以 openai 的 whisper-1 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--worker_concurrency 10 \\\n--infer_params saas.model=whisper-1 saas.api_key=${MODEL_OPENAI_TOKEN} \\\n--model speech_to_text\n```\n\n语音识别的使用方式和图生文类似，我们可以直接在 prompt 函数体里带上音频文件。\n\n```python\nimport byzerllm\nimport json\nimport base64\nfrom byzerllm.types import AudioPath\n\nllm = byzerllm.ByzerLLM.from_default_model(\"speech_to_text\")\n\naudio_file = \"/Users/allwefantasy/videos/output_audio.mp3\"\n\n@byzerllm.prompt(llm=llm)\ndef audio_to_text(audio_file: AudioPath):\n    \"\"\"\n    {{ audio_file }}\n    \"\"\"\n\nv = audio_to_text(AudioPath(value=audio_file))\njson.loads(v)\n```\n输出的数据格式略微复杂：\n\n```\n{'text': 'In the last chapter, you and I started to step through the internal workings of a transformer. This is one of the key pieces of technology inside large language models, and a lot of other tools in the modern wave of AI.',\n 'task': 'transcribe',\n 'language': 'english',\n 'duration': 10.0,\n 'segments': [{'id': 0,\n   'seek': 0,\n   'start': 0.0,\n   'end': 4.78000020980835,\n   'text': ' In the last chapter, you and I started to step through the internal workings of a transformer.',\n   'tokens': [50364,\n    682,\n    264,\n    1036,\n    7187,\n    11,\n    291,\n    293,\n    286,\n    1409,\n    281,\n    1823,\n    807,\n    264,\n    6920,\n    589,\n    1109,\n    295,\n    257,\n    31782,\n    13,\n    50586],\n   'temperature': 0.0,\n   'avg_logprob': -0.28872039914131165,\n   'compression_ratio': 1.4220778942108154,\n   'no_speech_prob': 0.016033057123422623},\n  ....\n  {'id': 2,\n   'seek': 0,\n   'start': 8.579999923706055,\n   'end': 9.979999542236328,\n   'text': ' and a lot of other tools in the modern wave of AI.',\n   'tokens': [50759,\n    293,\n    257,\n    688,\n    295,\n    661,\n    3873,\n    294,\n    264,\n    4363,\n    5772,\n    295,\n    7318,\n    13,\n    50867],\n   'temperature': 0.0,\n   'avg_logprob': -0.28872039914131165,\n   'compression_ratio': 1.4220778942108154,\n   'no_speech_prob': 0.016033057123422623}],\n 'words': [{'word': 'In', 'start': 0.0, 'end': 0.18000000715255737},\n  {'word': 'the', 'start': 0.18000000715255737, 'end': 0.23999999463558197},\n  {'word': 'last', 'start': 0.23999999463558197, 'end': 0.5400000214576721},\n  {'word': 'chapter', 'start': 0.5400000214576721, 'end': 0.800000011920929},\n  ....\n  {'word': 'AI', 'start': 9.920000076293945, 'end': 9.979999542236328}]}\n```\n\n会输出每一句话以及每一个字所在的起始时间和截止时间。你可以根据需要来使用。\n\n\n### 文生图\n\n文生图和语音合成类似，首先要启动合适的模型,以openai 的 dall-e-3 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=dall-e-3 \\\n--model openai_image_gen\n```\n\n启动模型后，只需要记住几个模板参数即可使用，这里直接使用 chat_oai 方法来使用：\n\n```python\n\nimport byzerllm\nimport json\nimport base64\n\nllm = byzerllm.ByzerLLM.from_default_model(\"openai_image_gen\")\nt = llm.chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\": json.dumps({\n        \"input\":\"a white siamese cat\",\n        \"size\": \"1024x1024\",\n        \"quality\": \"standard\"\n    },ensure_ascii=False)\n}])\n\nwith open(\"output1.jpg\",\"wb\") as f:\n    f.write(base64.b64decode(t[0].output))\n\n\nimport matplotlib.pyplot as plt\n\nimage_path = \"output1.jpg\"\nimage = plt.imread(image_path)\n\nplt.imshow(image)\nplt.axis('off')\nplt.show()\n```\n\n## Prompt 函数的流式输出\n\nbyzerllm 底层支持流式输出，非 prompt 函数的用法是这样的：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\n\nv = llm.stream_chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\":\"讲一个100字的故事\"\n}])\n\nfor s  in v:\n    print(s[0], end=\"\")\n```\n\n如果你像用 prompt 函数，可以这么用：\n\n\n```python\nimport byzerllm\nimport json\nimport base64\nfrom typing import Generator\n\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\n\n@byzerllm.prompt()\ndef tell_story() -> Generator[str, None, None]:\n    '''\n    给我讲一个一百多字的故事\n    '''\n\nv = tell_story.with_llm(llm).run()    \nfor i in v:\n    print(i, end=\"\")\n```\n\n可以看到，和普通的 prompt 函数的区别在于，返回值是一个生成器，然后你可以通过 for 循环来获取结果。\n\n## 向量化模型\n\nbyzerllm 支持向量化模型,你可以这样启动一个本地的模型：\n\n```bash\n!byzerllm deploy --pretrained_model_type custom/bge \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--worker_concurrency 10 \\\n--model_path /home/winubuntu/.auto-coder/storage/models/AI-ModelScope/bge-large-zh \\\n--infer_backend transformers \\\n--num_workers 1 \\\n--model emb\n```\n\n注意两个参数:\n\n1. --infer_backend transformers: 表示使用 transformers 作为推理后端。\n2. --model_path: 表示模型的路径。\n\n也可以启动一个 SaaS 的emb模型,比如 qwen 的 emb 模型：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/qianwen \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 2 \\\n--infer_params saas.api_key=${MODEL_QIANWEN_TOKEN}  saas.model=text-embedding-v2 \\\n--model qianwen_emb\n```\n\n或者 openai 的 emb 模型：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--worker_concurrency 10 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=text-embedding-3-small \\\n--model gpt_emb\n```\nSaaS 模型无需配置 `--infer_backend` 参数。\n\n无论是本地模型还是 SaaS 模型，我们都可以这样使用：\n\n```python\nimport byzerllm\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\nllm.setup_default_emb_model_name(\"emb\")\nllm.emb_query(\"你好\")\n```\n\n如果你配置 byzerllm 中的 Storage 使用，比如你这样启动了存储：\n\n```bash\nbyzerllm storage start --enable_emb\n```\n\n那么需要这么使用：\n\n```python\nfrom byzerllm.apps.byzer_storage.simple_api import ByzerStorage, DataType, FieldOption,SortOption\nstorage = ByzerStorage(\"byzerai_store\", \"memory\", \"memory\")\nstorage.emb(\"你好\")\n```\n\n## 一些辅助工具\n\n当调用 prompt 函数返回字符串的时候，如果想从里面抽取代码，可以使用如下方式：\n\n```python\nfrom byzerllm.utils.client import code_utils\ntext_with_markdown = '''\n```shell\nls -l\n```\n'''\ncode_blocks = code_utils.extract_code(text_with_markdown)\nfor code_block in code_blocks:\n    if code_block[0] == \"shell\":\n        print(code_block[1])\n##output: ls -l        \n```\n\nTagExtractor 工具用于对任意 `<_tag_></_tag_>` 标签对的抽取，下面是一个使用示例：\n\n```python\nfrom byzerllm.apps.utils import TagExtractor\n\nextractor = TagExtractor('''\n大家好\n<_image_>data:image/jpeg;base64,xxxxxx</_image_>\n                         大家好\n<_image_>data:image/jpeg;base64,xxxxxx2</_image_>\n''')\n\nv = extractor.extract()\nprint(v.content[0].content)\nprint(v.content[1].content)\n```\n输出为:\n\n```\ndata:image/jpeg;base64,xxxxxx\ndata:image/jpeg;base64,xxxxxx2\n```\n\n我们成功的将 <_image_></_image_> 标签对里的内容抽取出来了。\n\n\n## 注意事项\n\n1. prompt函数方法体返回只能是dict，实际的返回类型和方法签名可以不一样，但是方法体返回只能是dict。\n2. 大部分情况prompt函数体为空，如果一定要有方法体，可以返回一个空字典。\n3. 调用prompt方法的时候，如果在@byzerllm.prompt()里没有指定llm对象，那么需要在调用的时候通过with_llm方法指定llm对象。\n\n===========================\n\n# Byzer Storage: 最易用的AI存储引擎\n\nByzer Storage是一个为AI应用设计的高性能存储引擎,它提供了简单易用的API,支持向量搜索、全文检索以及结构化查询。本文将详细介绍Byzer Storage的使用方法和主要特性。\n\n## 0. 安装和启动\n\n```bash\npip install byzerllm\nbyzerllm storage start\n```\n\nThat's it! Byzer Storage已经安装并启动成功,现在我们可以开始使用它了。默认会启动一个 byzerai_store 的集群。\n\n注意，如果你是这样启动的：\n\n```bash\nbyzerllm storage start --enable_emb\n```\n\n那么会自动启动一个 emb 模型，名字就叫 emb， ByzerStorage 会自动使用该模型，无需做任何其他配置。\n\n## 1. 初始化\n\n创建一个 ByzerStorage 对象，链接 byzerai_store 集群，并且指定数据库和表名（可以不存在）。\n\n```python\nfrom byzerllm.apps.byzer_storage.simple_api import ByzerStorage, DataType, FieldOption, SortOption\n\nstorage = ByzerStorage(\"byzerai_store\", \"my_database1\", \"my_table4s\")\n```\n\n\n## 2. 创建库表（可选）\n\nByzer Storage使用Schema来定义数据结构。我们可以使用SchemaBuilder来构建Schema:\n\n```python\n_ = (\n    storage.schema_builder()\n    .add_field(\"_id\", DataType.STRING)\n    .add_field(\"name\", DataType.STRING)\n    .add_field(\"content\", DataType.STRING, [FieldOption.ANALYZE])\n    .add_field(\"raw_content\", DataType.STRING, [FieldOption.NO_INDEX])    \n    .add_array_field(\"summary\", DataType.FLOAT)    \n    .add_field(\"created_time\", DataType.LONG, [FieldOption.SORT])    \n    .execute()\n)\n```\n\n这个Schema定义了以下字段:\n- `_id`: 字符串类型的主键\n- `name`: 字符串类型,可用于过滤条件\n- `content`: 字符串类型,带有ANALYZE选项,用于全文搜索\n- `raw_content`: 字符串类型,带有NO_INDEX选项,不会被索引\n- `summary`: 浮点数组类型,用于存储向量\n- `created_time`: 长整型,带有SORT选项,可用于排序\n\n需要注意的是：\n\n1. 如果一个字段带有ANALYZE选项,则该字段会被分词,并且可以用于全文搜索，但是就无法返回原始的文本了。所以你需要添加一个新字段专门用来存储原文，比如在我们这个例子里，我们新增了 raw_content 字段，并且显示的指定了 NO_INDEX 选项，这样就不会被索引，也就不会被分词，可以后续被检索后用来获取原始文本。\n2. 对于需要作为向量检索和存储的字段，需要指定为数组类型，比如我们这个例子里的 summary 字段。\n3. 如果你需要拿到向量字段的原始文本，那么你也需要添加一个新字段专门用来存储原文，就像我们这个例子里的 raw_content 字段一样。\n\n\n## 3. 写入数据\n\n准备数据并使用WriteBuilder写入Storage:\n\n```python\ndata = [\n    {\"_id\": \"1\", \"name\": \"Hello\", \"content\": \"Hello, world!\", \"raw_content\": \"Hello, world!\", \"summary\": \"hello world\", \"created_time\": 1612137600},\n    {\"_id\": \"2\", \"name\": \"Byzer\", \"content\": \"Byzer, world!\", \"raw_content\": \"Byzer, world!\", \"summary\": \"byzer\", \"created_time\": 1612137601},\n    {\"_id\": \"3\", \"name\": \"AI\", \"content\": \"AI, world!\", \"raw_content\": \"AI, world!\", \"summary\": \"AI\", \"created_time\": 16121376002},\n    {\"_id\": \"4\", \"name\": \"ByzerAI\", \"content\": \"ByzerAI, world!\", \"raw_content\": \"ByzerAI, world!\", \"summary\": \"ByzerAi\", \"created_time\": 16121376003},\n]\n\nstorage.write_builder().add_items(data, vector_fields=[\"summary\"], search_fields=[\"content\"]).execute()\nstorage.commit()\n```\n\n这里我们使用`add_items`方法批量添加数据,并指定了`summary`为向量字段,`content`为搜索字段。最后调用`commit()`来确保数据被持久化。\n\n\n从上面我们要写入到 byzer storage 的数据我们可以看到如下几个特点：\n\n1. 需要包含我们之前定义的 Schema中罗列的所有的字段，同时需要指定哪些是向量字段，哪些是检索字段。\n2. 向量和检索不能是同一个字段。\n3. 对于向量，检索字段，我们给到 write_builder 的都是文本，ByzerStorage 会根据 Schema 的定义自动将其转换为向量，检索字段需要的格式。\n\n## 4. 查询数据\n\nByzer Storage支持多种查询方式,包括向量搜索、全文检索、过滤和排序。\n\n### 4.1 向量搜索 + 全文检索\n\n```python\nquery = storage.query_builder()\nquery.set_vector_query(\"ByzerAI\", fields=[\"summary\"])\nresults = query.set_search_query(\"Hello\", fields=[\"content\"]).execute()\nprint(results)\n```\n\n这个查询结合了向量搜索和全文检索,它会在`summary`字段中搜索与\"ByzerAI\"相似的向量,同时在`content`字段中搜索包含\"Hello\"的文档。\n\n### 4.2 过滤 + 向量搜索 + 全文检索\n\n```python\nquery = storage.query_builder()\nquery.and_filter().add_condition(\"name\", \"AI\").build()\nquery.set_vector_query(\"ByzerAI\", fields=\"summary\")\nresults = query.set_search_query(\"Hello\", fields=[\"content\"]).execute()\nprint(results)\n```\n\n这个查询首先过滤`name`字段等于\"AI\"的文档,然后在结果中进行向量搜索和全文检索。\n\n### 4.3 过滤 + 排序\n\n```python\nquery = storage.query_builder()\nquery.and_filter().add_condition(\"name\", \"AI\").build().sort(\"created_time\", SortOption.DESC)\nresults = query.execute()\nprint(results)\n```\n\n这个查询过滤`name`字段等于\"AI\"的文档,然后按`created_time`字段降序排序。\n\n## 5. 删除数据\n\n### 5.1 根据ID删除\n\n```python\nstorage.delete_by_ids([\"3\"])\n\nquery = storage.query_builder()\nquery.and_filter().add_condition(\"name\", \"AI\").build()\nresults = query.execute()\nprint(results)\n```\n\n这里我们删除了ID为\"3\"的文档,然后查询验证删除结果。\n\n### 5.2 删除整个表\n\n```python\nstorage.drop()\n```\n\n这个操作会删除整个表及其所有数据,请谨慎使用。\n\n## 结论\n\nByzer Storage提供了一套简洁而强大的API,能够轻松实现向量搜索、全文检索、结构化查询等功能。它的设计非常适合AI应用场景,可以有效地存储和检索各种类型的数据。通过本文的介绍,相信读者已经对Byzer Storage有了基本的了解,并能够开始在自己的项目中使用这个强大的存储引擎。\n\n\n     \n\n下面是用户的需求：\n\nModelConfig 添加一个 sql_func_llm_model 参数, cli.py 要添加该命令行\n使用 SEARCH/REPLACE 格式输出\n"}, {"role": "assistant", "content": "根据您的需求，我将对ModelConfig和cli.py文件进行修改。以下是相应的SEARCH/REPLACE块：\n\n```python\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/types.py\n<<<<<<< SEARCH\nclass ModelConfig(BaseModel):\n    llm_model: str = \"\"\n    gen_sql_llm_model: str = \"\"\n=======\nclass ModelConfig(BaseModel):\n    llm_model: str = \"\"\n    gen_sql_llm_model: str = \"\"\n    sql_func_llm_model: str = \"\"\n>>>>>>> REPLACE\n```\n\n这个修改为ModelConfig添加了sql_func_llm_model参数。\n\n现在，让我们修改cli.py文件以添加相应的命令行参数：\n\n```python\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/cli.py\n<<<<<<< SEARCH\n    parser.add_argument(\n        \"--gen-sql-llm-model\", type=str, default=\"\", help=\"LLM model for SQL generation\"\n    )\n\n    args = parser.parse_args()\n=======\n    parser.add_argument(\n        \"--gen-sql-llm-model\", type=str, default=\"\", help=\"LLM model for SQL generation\"\n    )\n    parser.add_argument(\n        \"--sql-func-llm-model\", type=str, default=\"\", help=\"LLM model for SQL functions\"\n    )\n\n    args = parser.parse_args()\n>>>>>>> REPLACE\n```\n\n```python\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/cli.py\n<<<<<<< SEARCH\n    model_config = ModelConfig(\n        llm_model=args.llm_model or args.served_model_name,\n        gen_sql_llm_model=args.gen_sql_llm_model or args.served_model_name,\n    )\n=======\n    model_config = ModelConfig(\n        llm_model=args.llm_model or args.served_model_name,\n        gen_sql_llm_model=args.gen_sql_llm_model or args.served_model_name,\n        sql_func_llm_model=args.sql_func_llm_model or args.served_model_name,\n    )\n>>>>>>> REPLACE\n```\n\n这些修改添加了--sql-func-llm-model命令行参数，并在创建ModelConfig实例时使用该参数。如果未提供sql_func_llm_model，它将默认使用served_model_name。"}], "model": "human", "yaml_file": "actions/148_chat_action.yml"}
{"instruction": "@setup.py 和 makefile 中的 dist 要增加版本号\n使用 SEARCH/REPLACE 格式输出", "conversations": [{"role": "user", "content": "\n如果你需要生成代码，对于每个需要更改的文件,你需要按 *SEARCH/REPLACE block* 的格式进行生成。\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The opening fence and code language, eg: ```python\n2. The file path alone on a line, starting with \"##File:\" and verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n*SEARCH/REPLACE* blocks will replace *all* matching occurrences.\nInclude enough lines to make the SEARCH blocks unique.\n\nInclude *ALL* the code being searched and replaced!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n下面我们来看一个例子：\n\n当前项目目录结构：\n1. 项目根目录： /tmp/projects/mathweb\n2. 项目子目录/文件列表(类似tree 命令输出)\nflask/\n    app.py\n    templates/\n        index.html\n    static/\n        style.css\n\n用户需求： Change get_factorial() to use math.factorial\n\n回答： To make this change we need to modify `/tmp/projects/mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n用户需求： Refactor hello() into its own file.\n\n回答：To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/hello.py\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/main.py\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n现在让我们开始一个新的任务:\n\n当前项目目录结构：\n1. 项目根目录： /Users/allwefantasy/projects/super_analysis\n2. 项目子目录/文件列表(类似tree 命令输出)：\nsetup.py\n.auto-coder/\n    libs/\n        llm_friendly_packages/\n            src/\n                example1/\n                    main.py\nsrc/\n    superanalysis/\n        version.py\n        serve/\n            rest.py\n            convert.py\n            openai_serve.py\n            __init__.py\n            llm_wrapper.py\n            api_server.py\n            cli.py\n        sql/\n            __init__.py\n            types.py\n            generator.py\n下面是一些文件路径以及每个文件对应的源码：\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/types.py\nfrom pydantic import BaseModel\n\nclass RAGConfig(BaseModel):\n    schema_rag_base_url: str = \"\"\n    schema_rag_api_key: str = \"xxxx\"\n\n    context_rag_base_url: str = \"\"\n    context_rag_api_key: str = \"xxxx\"\n\nclass ByzerSQLConfig(BaseModel):\n    byzer_sql_url: str = \"\"\n    data_dir: str = \"\"\n    owner: str = \"\"\n\nclass ModelConfig(BaseModel):\n    llm_model: str = \"\"\n    gen_sql_llm_model: str = \"\"\n    sql_func_llm_model: str = \"\"\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/cli.py\nimport argparse\nimport byzerllm\nfrom byzerllm.utils.client import ByzerLLM\nfrom superanalysis.serve.api_server import serve, ServerArgs\nfrom superanalysis.sql.types import RAGConfig, ByzerSQLConfig, ModelConfig\nfrom superanalysis.sql.generator import SQLGenerator\nfrom superanalysis.serve.llm_wrapper import LLWrapper\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"ByzerLLM API Server\")\n    parser.add_argument(\"--ray-address\", type=str, default=\"auto\", help=\"ray address\")\n    parser.add_argument(\n        \"--host\", type=str, default=\"0.0.0.0\", help=\"Host to bind the server to\"\n    )\n    parser.add_argument(\n        \"--port\", type=int, default=8000, help=\"Port to bind the server to\"\n    )\n    parser.add_argument(\n        \"--uvicorn-log-level\", type=str, default=\"info\", help=\"Uvicorn log level\"\n    )\n    parser.add_argument(\n        \"--allow-credentials\", action=\"store_true\", help=\"Allow credentials\"\n    )\n    parser.add_argument(\n        \"--allowed-origins\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed origins\"\n    )\n    parser.add_argument(\n        \"--allowed-methods\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed methods\"\n    )\n    parser.add_argument(\n        \"--allowed-headers\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed headers\"\n    )\n    parser.add_argument(\n        \"--api-key\", type=str, default=\"\", help=\"API key for authentication\"\n    )\n    parser.add_argument(\n        \"--served-model-name\",\n        type=str,\n        required=True,\n        help=\"Name of the model to be served\",\n    )\n    parser.add_argument(\n        \"--prompt-template\", type=str, default=\"\", help=\"Prompt template\"\n    )\n    parser.add_argument(\n        \"--response-role\", type=str, default=\"assistant\", help=\"Response role\"\n    )\n    parser.add_argument(\"--ssl-keyfile\", type=str, default=\"\", help=\"SSL key file\")\n    parser.add_argument(\n        \"--ssl-certfile\", type=str, default=\"\", help=\"SSL certificate file\"\n    )\n\n    # Add new arguments for RAGConfig\n    parser.add_argument(\n        \"--schema-rag-base-url\", type=str, default=\"\", help=\"RAG base URL\"\n    )\n    parser.add_argument(\n        \"--schema-rag-api-key\", type=str, default=\"xxxx\", help=\"RAG API key\"\n    )\n\n    parser.add_argument(\n        \"--context-rag-base-url\", type=str, default=\"\", help=\"RAG base URL\"\n    )\n    parser.add_argument(\n        \"--context-rag-api-key\", type=str, default=\"xxxx\", help=\"RAG API key\"\n    )\n\n    # Add new argument for ByzerSQLConfig\n    parser.add_argument(\"--byzer-sql-url\", type=str, default=\"\", help=\"Byzer SQL URL\")\n    parser.add_argument(\"--data-dir\", type=str, default=\"\", help=\"Data directory\")\n    parser.add_argument(\"--owner\", type=str, default=\"hello\", help=\"Owner of the Byzer SQL session\")\n    parser.add_argument(\n        \"--llm-model\", type=str, default=\"\", help=\"LLM model for general use\"\n    )\n    parser.add_argument(\n        \"--gen-sql-llm-model\", type=str, default=\"\", help=\"LLM model for SQL generation\"\n    )\n    parser.add_argument(\n        \"--sql-func-llm-model\", type=str, default=\"\", help=\"LLM model for SQL functions\"\n    )\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile,\n    )\n\n    # Create RAGConfig instance\n    rag_config = RAGConfig(\n        schema_rag_base_url=args.schema_rag_base_url,\n        schema_rag_api_key=args.schema_rag_api_key,\n        context_rag_base_url=args.context_rag_base_url,\n        context_rag_api_key=args.context_rag_api_key,\n    )\n\n    # Create ByzerSQLConfig instance\n    byzer_sql_config = ByzerSQLConfig(\n        byzer_sql_url=args.byzer_sql_url, data_dir=args.data_dir, owner=args.owner\n    )\n\n    model_config = ModelConfig(\n        llm_model=args.llm_model or args.served_model_name,\n        gen_sql_llm_model=args.gen_sql_llm_model or args.served_model_name,\n        sql_func_llm_model=args.sql_func_llm_model or args.served_model_name,\n    )\n\n    byzerllm.connect_cluster(address=args.ray_address)\n    llm = ByzerLLM()\n    llm.setup_default_model_name(args.served_model_name)\n    llm = byzerllm.ByzerLLM()\n    if not server_args.served_model_name:\n        raise ValueError(\"served_model_name is required\")\n    llm.setup_default_model_name(server_args.served_model_name)\n\n    print(\"RAG Config: \", rag_config)\n    print(\"Model Config: \", model_config)\n    rag = SQLGenerator(\n        llm=llm,\n        rag_config=rag_config,\n        byzer_sql_config=byzer_sql_config,\n        model_config=model_config,\n    )\n    llm_wrapper = LLWrapper(llm=llm, entry=rag)\n    serve(llm_wrapper, server_args)\n\n\nif __name__ == \"__main__\":\n    main()\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/__init__.py\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/__init__.py\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/llm_wrapper.py\nfrom typing import Any, Dict, List, Optional, Union,Callable\nfrom byzerllm.utils.client.types import (    \n    LLMFunctionCallResponse,\n    LLMClassResponse,LLMResponse    \n)\nimport pydantic\nfrom byzerllm import ByzerLLM\nfrom byzerllm.utils.client import LLMResponse\nfrom byzerllm.utils.types import SingleOutputMeta\nfrom byzerllm.utils.langutil import asyncfy_with_semaphore\nfrom superanalysis.sql.generator import SQLGenerator\nfrom loguru import logger\n\nclass LLWrapper:\n\n    def __init__(self,llm:ByzerLLM,entry:SQLGenerator):\n        self.entry = entry\n        self.llm = llm\n        \n\n    def chat_oai(self,\n                 conversations,\n                 tools:List[Union[Callable,str]]=[], \n                 tool_choice:Optional[Union[Callable,str]]=None,\n                 execute_tool:bool=False,  \n                 impl_func:Optional[Callable]=None,\n                 execute_impl_func:bool=False,\n                 impl_func_params:Optional[Dict[str,Any]]=None,\n                 func_params:Optional[Dict[str,Any]]=None,\n                 response_class:Optional[Union[pydantic.BaseModel,str]] = None, \n                 response_after_chat:Optional[Union[pydantic.BaseModel,str]] = False,\n                 enable_default_sys_message:bool=True,                 \n                 model:Optional[str] = None,\n                 role_mapping=None,llm_config:Dict[str,Any]={}\n                 )->Union[List[LLMResponse],List[LLMFunctionCallResponse],List[LLMClassResponse]]: \n        res,contexts = self.entry.stream_chat_oai(conversations)\n        s = \"\".join(res)        \n        return [LLMResponse(output=s,metadata={},input=\"\")]\n\n    def stream_chat_oai(self,conversations, \n                        model:Optional[str]=None, \n                        role_mapping=None,\n                        delta_mode=False,\n                        llm_config:Dict[str,Any]={}): \n        res,contexts = self.entry.stream_chat_oai(conversations)        \n        for t in res:                        \n            yield (t,SingleOutputMeta(0,0))\n\n    async def async_stream_chat_oai(self,conversations, \n                        model:Optional[str]=None, \n                        role_mapping=None,\n                        delta_mode=False,\n                        llm_config:Dict[str,Any]={}): \n        res,contexts = self.rag.stream_chat_oai(conversations)                \n        for t in res:                                    \n            yield (t,SingleOutputMeta(0,0))\n                         \n\n    def __getattr__(self, name):        \n        return getattr(self.llm, name)\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/openai_serve.py\nimport argparse\nfrom service_server.api_server import ServerArgs, serve\nfrom service_server.llm_wrapper import LLWrapper\nfrom service_server.summary_rag import SummaryRAG\nimport byzerllm\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"OpenAI Serve\")\n    parser.add_argument(\n        \"--model\",\n        default=\"\",\n    )\n    parser.add_argument(\"--emb_model\", default=\"\")\n    parser.add_argument(\n        \"--ray_address\",\n        default=\"auto\",\n    )\n    parser.add_argument(\"--source_dir\", default=\".\", help=\"\")\n    parser.add_argument(\"--host\", default=\"\", help=\"\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"\")\n    parser.add_argument(\"--uvicorn_log_level\", default=\"info\", help=\"\")\n    parser.add_argument(\"--allow_credentials\", action=\"store_true\", help=\"\")\n    parser.add_argument(\"--allowed_origins\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--allowed_methods\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--allowed_headers\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--api_key\", default=\"\", help=\"\")\n    parser.add_argument(\"--served_model_name\", default=\"\", help=\"\")\n    parser.add_argument(\"--prompt_template\", default=\"\", help=\"\")\n    parser.add_argument(\"--ssl_keyfile\", default=\"\", help=\"\")\n    parser.add_argument(\"--ssl_certfile\", default=\"\", help=\"\")\n    parser.add_argument(\"--response_role\", default=\"assistant\", help=\"\")\n    parser.add_argument(\n        \"--collections\", default=\"\", help=\"Collection name for indexing\"\n    )\n    parser.add_argument(\n        \"--base_dir\",\n        default=\"\",\n        help=\"Path where the processed text embeddings were stored\",\n    )\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name or args.model,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile,\n    )\n    byzerllm.connect_cluster(address=args.ray_address)\n    llm = byzerllm.ByzerLLM()\n    if not server_args.served_model_name:\n        raise ValueError(\"served_model_name is required\")\n    llm.setup_default_model_name(server_args.served_model_name)\n    rag = SummaryRAG(llm=llm)    \n    llm_wrapper = LLWrapper(llm=llm, rag=rag)\n    serve(llm=llm_wrapper, args=server_args)\n\nif __name__ == \"__main__\":\n    main()\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/api_server.py\nimport os\nimport time\nimport uvicorn\nfrom fastapi import Request\nfrom fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import JSONResponse, StreamingResponse, Response\n\nfrom byzerllm.log import init_logger\nfrom byzerllm.utils import random_uuid\nfrom byzerllm.version import __version__ as version\nfrom byzerllm.utils.client import ByzerLLM, LLMRequest\nfrom byzerllm.utils.client.entrypoints.openai.serving_chat import OpenAIServingChat\nfrom byzerllm.utils.client.entrypoints.openai.serving_completion import OpenAIServingCompletion\nfrom byzerllm.utils.client.entrypoints.openai.protocol import (\n    ModelList,\n    ModelCard,\n    ModelPermission,\n    ChatCompletionRequest,\n    ErrorResponse,\n    CompletionRequest,\n    Embeddings,\n    EmbeddingsOutput,\n    EmbeddingsData,\n    EmbeddingsUsage,\n)\nfrom pydantic import BaseModel\nfrom typing import List\n\nlogger = init_logger(__name__)\n\nllm_client: ByzerLLM = None\nopenai_serving_chat: OpenAIServingChat = None\nopenai_serving_completion: OpenAIServingCompletion = None\n\nTIMEOUT_KEEP_ALIVE = 5  # seconds\n# timeout in 10 minutes. Streaming can take longer than 3 min\nTIMEOUT = float(os.environ.get(\"BYZERLLM_APISERVER_HTTP_TIMEOUT\", 600))\n\nrouter_app = FastAPI()\n\n\n@router_app.get(\"/health\")\nasync def health() -> Response:\n    \"\"\"Health check.\"\"\"\n    return Response(status_code=200)\n\n\n@router_app.get(\"/v1/models\")\nasync def show_available_models():\n    models = await openai_serving_chat.show_available_models()\n    return JSONResponse(content=models.model_dump())\n\n\n@router_app.get(\"/version\")\nasync def show_version():\n    return JSONResponse(content={\"version\": version})\n\n\n@router_app.get(\"/v1/models\", response_model=ModelList)\nasync def models() -> ModelList:\n    \"\"\"Show available models. Right now we only have one model.\"\"\"\n    model_cards = [\n        ModelCard(\n            id=\"\",\n            root=\"\",\n            permission=[ModelPermission()]\n        )\n    ]\n    return ModelList(data=model_cards)\n\n\n@router_app.post(\"/v1/completions\")\nasync def create_completion(\n        body: CompletionRequest,\n        request: Request\n):\n    generator = await openai_serving_completion.create_completion(body, request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            status_code=generator.code\n        )\n    if body.stream:\n        return StreamingResponse(\n            content=generator,\n            media_type=\"text/event-stream\"\n        )\n    else:\n        return JSONResponse(content=generator.model_dump())\n\n\n@router_app.post(\"/v1/chat/completions\")\nasync def create_chat_completion(\n        body: ChatCompletionRequest,\n        request: Request,\n):\n    \"\"\"Completion API similar to OpenAI's API.\n\n    See  https://platform.openai.com/docs/api-reference/chat/create\n    for the API specification. This API mimics the OpenAI ChatCompletion API.\n\n    NOTE: Currently we do not support the following features:\n        - function_call (Users should implement this by themselves)\n        - logit_bias (to be supported by vLLM engine)\n    \"\"\"\n    # async with async_timeout.timeout(TIMEOUT):\n\n    generator = await openai_serving_chat.create_chat_completion(body, request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            status_code=generator.code\n        )\n    if body.stream:\n        return StreamingResponse(\n            content=generator,\n            media_type=\"text/event-stream\"\n        )\n    else:\n        return JSONResponse(content=generator.model_dump())\n\n\n@router_app.post(\"/v1/embeddings\")\nasync def embed(body: Embeddings):\n    \"\"\"Given a prompt, the model will return one embedding.\n\n    Returns:\n        A response object with an embedding.\n    \"\"\"\n    embedding_id = f\"embed-{random_uuid()}\"\n\n    results_list = llm_client.emb(body.model, request=LLMRequest(instruction=body.input))\n    tokens = 0\n\n    return EmbeddingsOutput(\n        data=[\n            EmbeddingsData(\n                embedding=results.output,\n                index=i,\n                object=\"embedding\",\n            )\n            for i, results in enumerate(results_list)\n        ],\n        id=embedding_id,\n        object=\"list\",\n        created=int(time.time()),\n        model=body.model,\n        usage=EmbeddingsUsage(\n            prompt_tokens=tokens,\n            total_tokens=tokens,\n        ),\n    )\n\nclass ServerArgs(BaseModel):\n    host: str = None\n    port: int = 8000\n    uvicorn_log_level: str = \"info\"\n    allow_credentials: bool = False\n    allowed_origins: List[str] = [\"*\"]  \n    allowed_methods: List[str] = [\"*\"]\n    allowed_headers: List[str] = [\"*\"]\n    api_key: str = None\n    served_model_name: str = None\n    prompt_template: str = None\n    response_role: str = \"assistant\"\n    ssl_keyfile: str = None\n    ssl_certfile: str = None    \n\ndef serve(llm:ByzerLLM, args: ServerArgs):\n    \n    logger.info(f\"ByzerLLM API server version {version}\")\n    logger.info(f\"args: {args}\")\n\n    router_app.add_middleware(\n        CORSMiddleware,\n        allow_origins=args.allowed_origins,\n        allow_credentials=args.allow_credentials,\n        allow_methods=args.allowed_methods,\n        allow_headers=args.allowed_headers,\n    )\n    \n    if token := os.environ.get(\"BYZERLLM_API_KEY\") or args.api_key:\n\n        @router_app.middleware(\"http\")\n        async def authentication(request: Request, call_next):\n            if not request.url.path.startswith(\"/v1\"):\n                return await call_next(request)\n            if request.headers.get(\"Authorization\") != \"Bearer \" + token:\n                return JSONResponse(\n                    content={\"error\": \"Unauthorized\"},\n                    status_code=401\n                )\n            return await call_next(request)\n\n    # Register labels for metrics\n    # add_global_metrics_labels(model_name=engine_args.model)\n    global llm_client\n    llm_client = llm\n    \n    global openai_serving_chat\n    openai_serving_chat = OpenAIServingChat(\n        llm_client=llm_client,\n        response_role=args.response_role,\n        server_model_name=args.served_model_name,\n        prompt_template=args.prompt_template\n    )\n    global openai_serving_completion\n    openai_serving_completion = OpenAIServingCompletion(\n        llm_client=llm_client,\n        server_model_name=args.served_model_name,\n        prompt_template=args.prompt_template\n    )\n\n    uvicorn.run(\n        router_app,\n        host=args.host,\n        port=args.port,\n        log_level=args.uvicorn_log_level,\n        timeout_keep_alive=TIMEOUT_KEEP_ALIVE,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile\n    )\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/generator.py\nfrom typing import Optional, Dict, Any, List\nfrom openai import OpenAI\nimport json\nfrom .types import RAGConfig, ByzerSQLConfig, ModelConfig\nfrom byzerllm.utils.client import code_utils\nimport byzerllm\nimport requests\nfrom loguru import logger\nimport traceback\nfrom byzerllm.utils.nontext import TagExtractor\n\n\nclass SQLGenerator:\n\n    @byzerllm.prompt()\n    def gen_context(self, conversations: List[Dict[str, str]]) -> str:\n        \"\"\"\n        对话内容：\n        {% for msg in conversations %}\n        <{{ msg.role }}>: {{ msg.content }}\n        {% endfor %}\n\n        根据提供的对话内容，理解用户最后的问题，然后提取可能会用到的 Byzer-SQL 相关的知识。\n        \"\"\"\n\n    @byzerllm.prompt()\n    def gen_sql(\n        self,\n        conversations: List[Dict[str, str]],\n        context: str,\n        schema: str,\n        model: str = \"deepseek_chat\",\n    ) -> str:\n        \"\"\"\n        相关知识：\n        {{ context }}\n\n        Schema信息：\n        {{ schema }}\n\n        对话内容：\n        {% for msg in conversations %}\n        <{{ msg.role }}>: {{ msg.content }}\n        {% endfor %}\n\n        模型UDF函数：\n        {{ model }}\n\n        根据提供的相关知识以及对话内容，理解用户最后的问题，然后根据看到的schema 信息，生成 Byzer-SQL 语句。\n        Byzer-SQL 首先需要用 load 语法加载 schema 中的路径映射成表格，然后根据表格的信息生成对应的 Byzer-SQL 语句。\n\n        特别需要关注的是:\n        1. 用户的很多需求是要对表格的文本内容做自然语言处理，你需要使用 Byzer-SQL 中的大模型辅助函数来完成这个需求。\n        2. 使用 `llm_result` 函数处理 LLM 模型函数的响应,`llm_result` 函数得到的是一段文本，虽然我们要求模型函数只输出特定的文本，但考虑到大模型会有一些额外的字符输出，不会只是输出我们要求的文字，所以通过 `like` 来进行匹配。\n        3. 注意 Byzer-SQL 的语法，比如select 语句最后一定需要有 as <table_name>。\n        3. 输出的 Byzer-SQL 语句务必使用 <_byzer_sql_></_byzer_sql_> 标签包裹起来。\n        \"\"\"\n\n    @byzerllm.prompt()\n    def fetch_schema(self, context: str) -> str:\n        \"\"\"\n        下面是一些基础知识，包括名词术语等等：\n        {{ context }}\n\n        当理解对话内容的时候，需要参考上面的基础知识，然后更好的理解对应的表格schema信息是不是有用。\n        \"\"\"\n\n    def __init__(\n        self,\n        llm: byzerllm.ByzerLLM,\n        rag_config: RAGConfig,\n        byzer_sql_config: ByzerSQLConfig,\n        model_config: ModelConfig,\n    ) -> None:\n        self.llm = llm\n        self.schema_rag_client = OpenAI(\n            api_key=rag_config.schema_rag_api_key,\n            base_url=rag_config.schema_rag_base_url,\n        )\n        self.context_rag_client = OpenAI(\n            api_key=rag_config.context_rag_api_key,\n            base_url=rag_config.context_rag_base_url,\n        )\n        self.byzer_sql_url = byzer_sql_config.byzer_sql_url\n\n        self.llm_model = byzerllm.ByzerLLM()\n        self.llm_model.setup_default_model_name(model_config.llm_model)\n        self.gen_sql_llm_model = byzerllm.ByzerLLM()\n        self.gen_sql_llm_model.setup_default_model_name(model_config.gen_sql_llm_model)\n        self.model_config = model_config\n\n\n    def stream_chat_oai(\n        self,\n        conversations,\n        model: Optional[str] = None,\n        role_mapping=None,\n        delta_mode=False,\n        llm_config: Dict[str, Any] = {},\n    ):\n        try:\n            v, c = self._stream_chat_oai(\n                conversations=conversations,\n                model=model,\n                role_mapping=role_mapping,\n                delta_mode=delta_mode,\n                llm_config=llm_config,\n            )\n        except Exception as e:\n            error_msg = f\"Error in stream_chat_oai: {e}\\n{traceback.format_exc()}\"\n            logger.error(error_msg)\n            v, c = [str(e)], []\n\n        return v, c\n\n    def _stream_chat_oai(\n        self,\n        conversations,\n        model: Optional[str] = None,\n        role_mapping=None,\n        delta_mode=False,\n        llm_config: Dict[str, Any] = {},\n    ):\n        logger.info(f\"Starting stream_chat_oai with model: {model}\")\n        logger.debug(f\"Conversations: {conversations}\")\n        logger.debug(f\"LLM Config: {llm_config}\")\n\n        if not model:\n            model = self.llm_model.default_model_name\n        logger.info(f\"Using model: {model}\")\n\n        query = conversations[-1][\"content\"]\n\n        if (\n            \"使用四到五个字直接返回这句话的简要主题，不要解释、不要标点、不要语气词、不要多余文本，不要加粗，如果没有主题\"\n            in query\n            or \"简要总结一下对话内容，用作后续的上下文提示 prompt，控制在 200 字以内\"\n            in query\n        ):\n            chunks = self.llm.stream_chat_oai(\n                conversations=conversations,\n                model=model,\n                role_mapping=role_mapping,\n                llm_config=llm_config,\n                delta_mode=True,\n            )\n            return (chunk[0] for chunk in chunks), []\n\n        logger.info(\"Fetching context from context RAG client\")\n        context_query = self.gen_context.prompt(conversations=conversations)\n        context_response = self.context_rag_client.chat.completions.create(\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": json.dumps(\n                        {\"query\": context_query, \"only_contexts\": True},\n                        ensure_ascii=False,\n                    ),\n                }\n            ],\n            model=model,\n        )\n\n        context = context_response.choices[0].message.content\n        new_context = \"\"\n        for msg in context.split(\"\\n\"):\n            if msg.strip():\n                v = json.loads(msg)\n                new_context += v[\"source_code\"]\n        logger.info(f\"context {new_context[0:100]}...\")\n        logger.info(\"Context retrieved successfully\")\n\n        logger.info(\"Fetching schemas from schema RAG client\")\n        schema_query = self.fetch_schema.prompt(context=new_context)\n\n        response = self.schema_rag_client.chat.completions.create(\n            messages=conversations[0:-1]\n            + [\n                {\n                    \"role\": \"user\",\n                    \"content\": json.dumps(\n                        {\"query\": schema_query, \"only_contexts\": True},\n                        ensure_ascii=False,\n                    ),\n                }\n            ],\n            model=model,\n        )        \n\n        schema_str = response.choices[0].message.content\n        logger.debug(f\"Schema RAG response: {schema_str}\")\n        logger.info(\"Schema retrieved successfully\")\n\n        logger.info(\"Generating SQL string\")\n        sql_str = self.gen_sql.with_llm(self.gen_sql_llm_model).run(\n            conversations=conversations,\n            context=new_context,\n            schema=schema_str,\n            model=self.model_config.sql_func_llm_model,\n        )\n\n        logger.info(\"SQL string generated\")\n        logger.debug(f\"Generated SQL string: {sql_str}\")\n\n        # Extract SQL code from tags\n        tag_extractor = TagExtractor(sql_str)\n        extracted_tags = tag_extractor.extract()\n        sql = extracted_tags.content[0].content\n        logger.info(f\"Extracted SQL: {sql}\")\n\n        logger.info(f\"Sending SQL to Byzer SQL URL: {self.byzer_sql_url}\")\n        try:\n            response = requests.post(\n                self.byzer_sql_url,\n                data={\n                    \"sql\": sql,\n                    \"owner\": \"hello\",\n                    \"sessionPerUser\": True,\n                    \"sessionPerRequest\": True,\n                    \"includeSchema\": True,\n                },\n            )\n            response.raise_for_status()\n            result = response.content.decode(\"utf-8\")\n            logger.info(\"SQL execution result decoded\")\n            logger.debug(f\"SQL execution result: {result}\")\n            return (item for item in [result]), []\n        except requests.exceptions.RequestException as e:\n            logger.error(f\"Failed to send SQL to Byzer SQL URL: {e}\")\n            if hasattr(e.response, \"content\"):\n                error_content = e.response.content.decode(\"utf-8\")\n                logger.error(f\"Server response: {error_content}\")\n            else:\n                logger.error(\"No server response available\")\n            raise Exception(f\"Byzer SQL request failed: {str(e)}\") from e\n\n\n##File: /Users/allwefantasy/projects/super_analysis/setup.py\nfrom setuptools import setup, find_packages\n\n# Read requirements from requirements.txt\nwith open('requirements.txt') as f:\n    requirements = f.read().splitlines()\n\nsetup(\n    name=\"super-analysis\",\n    version=\"0.1.0\",\n    packages=find_packages(where=\"src\"),\n    package_dir={\"\": \"src\"},\n    install_requires=requirements,\n    entry_points={\n        \"console_scripts\": [\n            \"super-analysis.serve=superanalysis.serve.cli:main\",\n            \"super-analysis.convert=superanalysis.serve.convert:main\",\n            \"superanalysis.api=superanalysis.serve.rest:main\",\n        ],\n    },\n    author=\"allwefantasy\",\n    author_email=\"allwefantasy@gmail.com\",\n    description=\"A package for super analysis with ByzerLLM\",\n    long_description=open(\"README.md\").read(),\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/yourusername/super-analysis\",\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n    ],\n    python_requires='>=3.6',\n)\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/convert.py\nimport os\nimport argparse\nimport pandas as pd\nimport hashlib\nfrom typing import List, Dict\n\ndef get_csv_files(directory: str) -> List[str]:\n    \"\"\"Recursively get all CSV files in the given directory.\"\"\"\n    csv_files = []\n    for root, _, files in os.walk(directory):\n        for file in files:\n            if file.endswith('.csv'):\n                csv_files.append(os.path.join(root, file))\n    return csv_files\n\ndef extract_schema(df: pd.DataFrame) -> Dict[str, str]:\n    \"\"\"Extract schema from pandas DataFrame.\"\"\"\n    schema = {}\n    for column in df.columns:\n        dtype = str(df[column].dtype)\n        if dtype == 'object':\n            schema[column] = 'STRING'\n        elif dtype.startswith('int'):\n            schema[column] = 'INTEGER'\n        elif dtype.startswith('float'):\n            schema[column] = 'FLOAT'\n        elif dtype == 'bool':\n            schema[column] = 'BOOLEAN'\n        elif dtype.startswith('datetime'):\n            schema[column] = 'TIMESTAMP'\n        else:\n            schema[column] = 'STRING'  # Default to STRING for unknown types\n    return schema\n\ndef generate_schema_text(schema: Dict[str, str], csv_file: str, df: pd.DataFrame, include_rows_num: int) -> str:\n    \"\"\"Generate schema text from the schema dictionary and include the CSV file name and sample data.\"\"\"\n    schema_lines = [f\"CSV File: {csv_file}\\n\"]\n    schema_lines.extend([f\"{field}: {dtype}\" for field, dtype in schema.items()])\n    \n    if include_rows_num > 0:\n        schema_lines.append(f\"\\nSample Data (first {include_rows_num} rows):\")\n        sample_data = df.head(include_rows_num).to_string(index=False)\n        schema_lines.append(sample_data)\n    \n    return \"\\n\".join(schema_lines)\n\ndef save_schema(schema_text: str, csv_file: str, output_dir: str) -> None:\n    \"\"\"Save schema text to a file in the output directory.\"\"\"\n    # Generate MD5 hash of the schema text\n    file_name = hashlib.md5(schema_text.encode()).hexdigest() + '.md'\n    output_path = os.path.join(output_dir, file_name)\n    \n    with open(output_path, 'w') as f:\n        f.write(schema_text)\n    print(f\"Schema for {csv_file} saved to {output_path}\")\n\ndef convert(data_dir: str, doc_dir: str, include_rows_num: int) -> None:\n    \"\"\"Main conversion function.\"\"\"\n    csv_files = get_csv_files(data_dir)\n    \n    for csv_file in csv_files:\n        print(f\"Processing {csv_file}\")\n        df = pd.read_csv(csv_file)\n        schema = extract_schema(df)\n        schema_text = generate_schema_text(schema, csv_file, df, include_rows_num)\n        save_schema(schema_text, csv_file, doc_dir)\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Convert CSV files to schema documents\")\n    parser.add_argument(\"--data_dir\", required=True, help=\"Directory containing CSV files\")\n    parser.add_argument(\"--doc_dir\", required=True, help=\"Output directory for schema documents\")\n    parser.add_argument(\"--include-rows-num\", type=int, default=0, help=\"Number of sample rows to include in the schema\")\n    \n    args = parser.parse_args()\n    \n    if not os.path.exists(args.doc_dir):\n        os.makedirs(args.doc_dir)\n    \n    convert(args.data_dir, args.doc_dir, args.include_rows_num)\n\nif __name__ == \"__main__\":\n    main()\n\n##File: /Users/allwefantasy/projects/super_analysis/README.md\n# Super Analysis\n\nSuper Analysis 支持 NLP2SQL 以及 使用 SQL 对表中的文本字段做自然语言分析。\n\n## 安装\n\n你可以使用 pip 安装 Super Analysis：\n\n```bash\ngit clone https://github.com/allwefantasy/super_analysis\ncd super-analysis\npip install -e .\n```\n\n或者使用 Makefile 来管理项目：\n\n```bash\n# 清理项目\nmake clean\n\n# 构建项目\nmake build\n\n# 创建wheel包\nmake wheel\n\n# 安装项目\nmake install\n\n# 卸载项目\nmake uninstall\n\n# 以开发模式安装项目\nmake develop\n\n# 运行测试\nmake test\n```\n\n\n\n## 示例数据\n\n下载电影数据集： https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset/download?datasetVersionNumber=7\n\n\n## 启动分析服务\n\n整个 super-analysis 服务分为四个部分：\n\n1. byzer-sql 分布式执行引擎 \n2. schema 文档知识库\n3. context 文档知识库\n4. super-analysis 自身\n\n在启动 super-analysis 之前，你需要启动 schema 文档知识库和 context 文档知识库以及 byzer-sql 引擎。\n\n## 部署 Deepseek 模型代理\n\n在启动其他服务之前，我们需要先部署 Deepseek 模型。你可以使用以下命令来部署：\n\n```bash\nmake deploy_deepseek\n```\n\n或者直接运行以下命令：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--worker_concurrency 1000 \\\n--num_workers 1 \\\n--infer_params saas.base_url=\"https://api.deepseek.com/v1\" saas.api_key=${MODEL_DEEPSEEK_TOKEN} saas.model=deepseek-chat \\\n--model deepseek_chat\n```\n\n注意：请确保在运行此命令之前，已经设置了环境变量 `MODEL_DEEPSEEK_TOKEN`。\n\n## 部署 Byzer-SQL\n\n在你的电脑上参考 [这个文档](./docs/4.3.1%20安装与配置%20Byzer-SQL.pdf) 完成部署.\n\n> 启动的时候需要在安装有 super-analysis 的conda环境中启动。\n\n\n\n当 byzer-sql 部署完成后，注册账号为 `hello`, 然后进入到 byzer-sql 的控制台中执行以下脚本：\n\n```sql\n!byzerllm setup single;\n\nrun command as LLM.`` where \naction=\"infer\"\nand reconnect=\"true\"\nand pretrainedModelType=\"saas/openai\"\nand udfName=\"deepseek_chat\";\n```\n\n## 启动其他服务\n\n现在可以按如下方式做一些数据预处理或者启动相关服务：\n\n```bash\n## 将电影数据集的csv文件转换为schema文档\nsuper-analysis.convert --data_dir /Users/allwefantasy/data/movice --doc_dir /Users/allwefantasy/data/movice/schemas/\n\n# 启动schema文档知识库\nauto-coder doc serve \\\n--model deepseek_chat --index_filter_workers 100 \\\n--tokenizer_path /Users/allwefantasy/Downloads/tokenizer.json \\\n--doc_dir /Users/allwefantasy/data/movice/schemas/ \\\n--port 8001\n\n# 下载 Byzer-SQL 专门为大模型优化的文档\ngit clone https://github.com/allwefantasy/llm_friendly_packages\n\n# 启动文档知识库\nauto-coder doc serve \\\n--model deepseek_chat --index_filter_workers 100 \\\n--tokenizer_path /Users/allwefantasy/Downloads/tokenizer.json \\\n--doc_dir  /Users/allwefantasy/projects/llm_friendly_packages/github.com/allwefantasy \\\n--port 8002\n\n# 启动兼容 OpenAI Server 的分析服务\nsuper-analysis.serve --served-model-name deepseek_chat --port 8000 \\\n--schema-rag-base-url http://127.0.0.1:8001/v1 \\\n--context-rag-base-url http://127.0.0.1:8002/v1 \\\n--byzer-sql-url http://127.0.0.1:9003/run/script\n```\n\n现在你可以用 OpenAI SDK 去使用了。 具体测试和接口使用参考 [openai_local_api.ipynb](./notebooks/openai_local_api.ipynb)\n\n\n## 非 OpenAI 兼容接口\n\n除了兼容 OpenAI 的接口外，Super Analysis 还提供了一个简单的 REST API 接口，可以直接与模型进行交互。以下是如何使用这个接口：\n\n### 启动服务\n\n使用以下命令启动 REST API 服务：\n\n```bash\nsuperanalysis.api --schema-rag-base-url <schema_rag_url> \\\n                  --schema-rag-api-key <schema_rag_api_key> \\\n                  --context-rag-base-url <context_rag_url> \\\n                  --context-rag-api-key <context_rag_api_key> \\\n                  --byzer-sql-url <byzer_sql_url> \\\n                  --llm-model <llm_model_name> \\\n                  --gen-sql-llm-model <gen_sql_llm_model_name> \\\n                  --host <host> \\\n                  --port <port>\n```\n\n请替换尖括号中的值为实际的配置信息。\n\n### API 端点\n\n服务启动后，将提供以下 API 端点：\n\n#### POST /chat\n\n此端点用于与模型进行对话。\n\n请求体格式：\n\n```json\n{\n  \"conversations\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"你的问题或指令\"\n    }\n  ],\n  \"model\": \"可选的模型名称\"\n}\n```\n\n响应格式：\n\n```json\n{\n  \"response\": \"模型的回答\"\n}\n```\n\n### 示例使用\n\n以下是使用 Python 请求库与 API 交互的示例：\n\n```python\nimport requests\nimport json\n\nurl = \"http://localhost:8000/chat\"\npayload = {\n    \"conversations\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"你好，请介绍一下你自己。\"\n        }\n    ],\n    \"model\": \"your_model_name\"\n}\n\nresponse = requests.post(url, json=payload)\nresult = response.json()\n\nprint(result[\"response\"])\n```\n\n这个 API 相比兼容 OpenAI 接口，可以在请求中提供提供更多可选参数。\n\n## 使用方法\n\n安装完成后，你可以使用 `super-analysis.serve` 命令来启动 API 服务器：\n\n```bash\nsuper-analysis.serve --served-model-name your_model_name\n```\n\n### 命令行参数\n\n以下是可用的命令行参数：\n\n- `--host`: 服务器绑定的主机地址（默认：0.0.0.0）\n- `--port`: 服务器绑定的端口（默认：8000）\n- `--uvicorn-log-level`: Uvicorn 日志级别（默认：info）\n- `--allow-credentials`: 允许凭证（默认：False）\n- `--allowed-origins`: 允许的源（默认：[\"*\"]）\n- `--allowed-methods`: 允许的方法（默认：[\"*\"]）\n- `--allowed-headers`: 允许的头部（默认：[\"*\"]）\n- `--api-key`: 用于认证的 API 密钥\n- `--served-model-name`: 要服务的模型名称（必需）\n- `--prompt-template`: 提示模板\n- `--response-role`: 响应角色（默认：assistant）\n- `--ssl-keyfile`: SSL 密钥文件路径\n- `--ssl-certfile`: SSL 证书文件路径\n\n例如：\n\n```bash\nsuper-analysis.serve --served-model-name deepseek_chat --port 8000\n```\n\n\n## 抽取csv文件schema\n\nThe `super-analysis.convert` command allows you to convert CSV files to schema documents. This is useful for generating metadata about your CSV files that can be used for further analysis or documentation.\n\n#### Usage\n\n```\nsuper-analysis.convert --data_dir <path_to_csv_files> --doc_dir <output_directory> [--include-rows-num <number_of_sample_rows>]\n```\n\nArguments:\n- `--data_dir`: Directory containing CSV files (required)\n- `--doc_dir`: Output directory for schema documents (required)\n- `--include-rows-num`: Number of sample rows to include in the schema (optional, default is 0)\n\nThe utility will:\n1. Recursively find all CSV files in the specified directory\n2. Extract the schema from each CSV file\n3. Generate a schema text that includes:\n   - The CSV file name\n   - Field names and their data types\n   - Sample data (if `--include-rows-num` is specified)\n4. Save the schema text as a markdown file in the output directory\n\n## 联系方式\n\n如有任何问题或建议，请联系 allwefantasy@gmail.com。\n\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/rest.py\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nfrom typing import List, Optional\nimport uvicorn\nimport argparse\nfrom superanalysis.sql.generator import SQLGenerator\nfrom superanalysis.sql.types import RAGConfig, ByzerSQLConfig, ModelConfig\nimport byzerllm\n\napp = FastAPI()\n\nclass ChatMessage(BaseModel):\n    role: str\n    content: str\n\nclass ChatRequest(BaseModel):\n    conversations: List[ChatMessage]    \n    model_config: Optional[ModelConfig] = None\n\nclass ChatResponse(BaseModel):\n    response: str\n\n@app.post(\"/chat\", response_model=ChatResponse)\nasync def chat(request: ChatRequest):\n    try:\n        sql_generator = SQLGenerator(\n            llm=llm,\n            rag_config=rag_config,\n            byzer_sql_config=byzer_sql_config,\n            model_config=request.model_config\n        )\n        result, _ = sql_generator.stream_chat_oai(\n            conversations=[{\"role\": msg.role, \"content\": msg.content} for msg in request.conversations],\n            model=request.model_config.llm_model\n        )\n        return ChatResponse(response=\"\".join(result))\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"SuperAnalysis API Server\")\n    parser.add_argument(\"--host\", type=str, default=\"0.0.0.0\", help=\"Host to bind the server to\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"Port to bind the server to\")\n    parser.add_argument(\"--schema-rag-base-url\", type=str, required=True, help=\"Schema RAG base URL\")\n    parser.add_argument(\"--schema-rag-api-key\", type=str, required=True, help=\"Schema RAG API key\")\n    parser.add_argument(\"--context-rag-base-url\", type=str, required=True, help=\"Context RAG base URL\")\n    parser.add_argument(\"--context-rag-api-key\", type=str, required=True, help=\"Context RAG API key\")\n    parser.add_argument(\"--byzer-sql-url\", type=str, required=True, help=\"Byzer SQL URL\")\n    parser.add_argument(\"--data-dir\", type=str, default=\"\", help=\"Data directory\")\n    parser.add_argument(\"--owner\", type=str, default=\"hello\", help=\"Owner of the Byzer SQL session\")\n    parser.add_argument(\"--llm-model\", type=str, required=True, help=\"LLM model for general use\")\n    parser.add_argument(\"--gen-sql-llm-model\", type=str, required=True, help=\"LLM model for SQL generation\")\n    \n    args = parser.parse_args()\n\n    global rag_config\n\n    rag_config = RAGConfig(\n        schema_rag_base_url=args.schema_rag_base_url,\n        schema_rag_api_key=args.schema_rag_api_key,\n        context_rag_base_url=args.context_rag_base_url,\n        context_rag_api_key=args.context_rag_api_key\n    )\n    global byzer_sql_config\n    byzer_sql_config = ByzerSQLConfig(\n        byzer_sql_url=args.byzer_sql_url,\n        data_dir=args.data_dir,\n        owner=args.owner\n    )\n\n    global llm\n    llm = byzerllm.ByzerLLM()\n    llm.setup_default_model_name(args.llm_model)\n\n    uvicorn.run(app, host=args.host, port=args.port)\n\nif __name__ == \"__main__\":\n    main()\n\n##File: /Users/allwefantasy/projects/super_analysis/Makefile\n# Makefile for super-analysis project\n\n# Python interpreter\nPYTHON := python\n\n# Project name\nPROJECT_NAME := super-analysis\n\n# Directories\nSRC_DIR := src\nDIST_DIR := dist\nBUILD_DIR := build\n\n# Commands\nSETUP := $(PYTHON) setup.py\n\n.PHONY: all clean build wheel deploy_deepseek dist\n\nall: clean build wheel dist\n\ndeploy_deepseek:\n\tbyzerllm deploy --pretrained_model_type saas/openai \\\n\t--cpus_per_worker 0.001 \\\n\t--gpus_per_worker 0 \\\n\t--worker_concurrency 1000 \\\n\t--num_workers 1 \\\n\t--infer_params saas.base_url=\"https://api.deepseek.com/v1\" saas.api_key=${MODEL_DEEPSEEK_TOKEN} saas.model=deepseek-chat \\\n\t--model deepseek_chat\n\nclean:\n\t@echo \"Cleaning up...\"\n\trm -rf $(BUILD_DIR) $(DIST_DIR) *.egg-info\n\tfind . -type d -name \"__pycache__\" -exec rm -rf {} +\n\tfind . -type f -name \"*.pyc\" -delete\n\nbuild:\n\t@echo \"Building the project...\"\n\t$(SETUP) build\n\nwheel:\n\t@echo \"Creating wheel package...\"\n\t$(SETUP) bdist_wheel\n\ninstall:\n\t@echo \"Installing the package...\"\n\t$(PYTHON) -m pip install .\n\nuninstall:\n\t@echo \"Uninstalling the package...\"\n\t$(PYTHON) -m pip uninstall -y $(PROJECT_NAME)\n\ndevelop:\n\t@echo \"Installing the package in editable mode...\"\n\t$(PYTHON) -m pip install -e .\n\ntest:\n\t@echo \"Running tests...\"\n\t$(PYTHON) -m unittest discover tests\n\ndist: wheel\n\t@echo \"Creating distribution package...\"\n\trm -rf super_analysis.tar.gz\n\tmkdir -p super_analysis\n\tcp -r $(DIST_DIR)/* super_analysis/\n\tcp \"/Users/allwefantasy/projects/super_analysis/docs/4.3.1 安装与配置 Byzer-SQL.pdf\" super_analysis/\n\tcp \"/Users/allwefantasy/projects/super_analysis/docs/deploy.md\" super_analysis/README.md\n\tcp \"/Users/allwefantasy/projects/super_analysis/notebooks/openai_local_api.ipynb\" super_analysis/\n\ttar -czf super_analysis.tar.gz super_analysis\n\trm -rf super_analysis\n\t@echo \"Distribution package created: super_analysis.tar.gz\"\n\n##File: /Users/allwefantasy/projects/super_analysis/docs/deploy.md\n\n# 🚀 Super Analysis 部署指南\n\n本指南将帮助你部署 Super Analysis 系统，包括安装必要组件、配置服务和启动系统。\n\n---\n\n## 📦 安装 Super Analysis\n\n\n```bash\npip install -U auto-coder\npip install super_analysis-0.1.0-py3-none-any.whl\n```\n\n\n---\n\n## 🤖 部署 Deepseek 模型代理\n\n在启动其他服务之前，我们需要先部署 Deepseek 模型：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--worker_concurrency 1000 \\\n--num_workers 1 \\\n--infer_params saas.base_url=\"https://api.deepseek.com/v1\" saas.api_key=${MODEL_DEEPSEEK_TOKEN} saas.model=deepseek-chat \\\n--model deepseek_chat\n```\n\n注意：确保已设置环境变量 `MODEL_DEEPSEEK_TOKEN`。\n\n---\n\n## 🛠️ 部署 Byzer-SQL\n\n参考 [安装与配置 Byzer-SQL 文档](./4.3.1%20安装与配置%20Byzer-SQL.pdf) 完成部署。\n\n> 启动时需要在安装有 super-analysis 的 conda 环境中启动。\n\n当 byzer-sql 部署完成后，注册账号为 `hello`，然后在 byzer-sql 控制台中执行：\n\n```sql\n!byzerllm setup single;\n\nrun command as LLM.`` where \naction=\"infer\"\nand reconnect=\"true\"\nand pretrainedModelType=\"saas/openai\"\nand udfName=\"deepseek_chat\";\n```\n\n---\n\n## 示例数据\n\n下载电影数据集： https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset/download?datasetVersionNumber=7\n\n---\n\n## 📊 数据预处理和服务启动\n\n1. 抽取电影数据集schema：\n\n```bash\nsuper-analysis.convert --data_dir /Users/allwefantasy/data/movice --doc_dir /Users/allwefantasy/data/movice/schemas/\n```\n\n你还可以添加 --include-rows-num 5 让系统在生成 schema 文档时同时提供一些示例数据。方便大模型更好的对这个表进行认知。\n\n\n2. 启动 schema 文档知识库：\n\n```bash\nauto-coder doc serve \\\n--model deepseek_chat --index_filter_workers 100 \\\n--tokenizer_path /Users/allwefantasy/Downloads/tokenizer.json \\\n--doc_dir /Users/allwefantasy/data/movice/schemas/ \\\n--port 8001\n```\n\n3. 下载 Byzer-SQL 文档并启动文档知识库：\n\n```bash\ngit clone https://github.com/allwefantasy/llm_friendly_packages\n\nauto-coder doc serve \\\n--model deepseek_chat --index_filter_workers 100 \\\n--tokenizer_path /Users/allwefantasy/Downloads/tokenizer.json \\\n--doc_dir  /Users/allwefantasy/projects/llm_friendly_packages/github.com/allwefantasy \\\n--port 8002\n```\n\n4. 启动兼容 OpenAI Server 的分析服务：\n\n```bash\nsuper-analysis.serve --served-model-name deepseek_chat --port 8000 \\\n--schema-rag-base-url http://127.0.0.1:8001/v1 \\\n--context-rag-base-url http://127.0.0.1:8002/v1 \\\n--byzer-sql-url http://127.0.0.1:9003/run/script\n```\n\n---\n\n现在，Super Analysis 系统已经完全部署并启动。你可以开始使用 OpenAI SDK 进行测试和接口调用。具体测试和接口使用方法请参考 [openai_local_api.ipynb](./openai_local_api.ipynb)。\n\n🎉 恭喜！你已经成功部署了 Super Analysis 系统。如有任何问题，请随时查阅文档或联系支持团队。\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/version.py\n__version__ = '0.1.0'\n\n##File: .auto-coder/libs/llm_friendly_packages/github.com/allwefantasy/byzer-llm/README.md\n# byzerllm 大模型编程快速指南\n\n本文示例 [Notebook](../../notebooks/003_byzerllm_大模型编程快速指南.ipynb)\n\n## 安装\n\n```bash\npip install -U byzerllm\nray start --head\n```\n## 启动一个模型代理\n\nbyzerllm 支持私有化模型或者SaaS模型的部署。\n\n这里以 deepseek 官方API 为例：\n\n```bash\neasy-byzerllm deploy deepseek-chat --token xxxxx --alias deepseek_chat\n```\n\n或者跬基流动API:\n\n```bash\neasy-byzerllm deploy alibaba/Qwen1.5-110B-Chat --token xxxxx --alias qwen110b_chat\n```\n\n将上面的 API KEY 替换成你们自己的。\n\n如果你想部署私有化模型或者对接 Ollama 等更多需求，参考 [002_使用byzerllm进行模型部署.md](./002_使用byzerllm进行模型部署.md) 或者 [README.md](../../README.md)。\n\n之后，你就可以在代码里使用  deepseek_chat 或者 qwen110b_chat  访问模型了。\n\n## hello world\n\n来和我们的大模型打个招呼:\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\n@byzerllm.prompt(llm=llm)\ndef hello(q:str) ->str:\n    '''\n    你好, {{ q }}\n    '''\n\ns = hello(\"你是谁\")    \nprint(s)\n\n## 输出:\n## '你好！我是一个人工智能助手，专门设计来回答问题、提供信息和帮助解决问题。如果你有任何疑问或需要帮助，请随时告诉我。'\n```\n\n恭喜，你和大模型成功打了招呼！\n\n可以看到，我们通过 `@byzerllm.prompt` 装饰器，将一个方法转换成了一个大模型的调用，然后这个方法的主题是一段文本，文本中\n使用了 jinja2 模板语法，来获得方法的参数。当正常调用该方法时，实际上就发起了和大模型的交互，并且返回了大模型的结果。\n\n在 byzerllm 中，我们把这种方法称之为 prompt 函数。\n\n## 查看发送给大模型的prompt\n\n很多情况你可能需要调试，查看自己的 prompt 渲染后到底是什么样子的，这个时候你可以通过如下方式\n获取渲染后的prompt:\n\n```python\nhello.prompt(\"你是谁\")\n## '你好, 你是谁'\n```            \n\n## 动态换一个模型\n\n前面的 hello 方法在初始化的时候，我们使用了默认的模型 deepseek_chat，如果我们想换一个模型，可以这样做：\n\n```python\nhello.with_llm(llm).run(\"你是谁\")\n## '你好！我是一个人工智能助手，专门设计来回答问题、提供信息和帮助解决问题。如果你有任何疑问或需要帮助，请随时告诉我。'\n```\n\n通过 with_llm 你可以设置一个新的 llm 对象，然后调用 run 方法，就可以使用新的模型了。\n\n## 超长文本生成\n\n我们知道，大模型一次生成的长度其实是有限的，如果你想生成超长文本，你可能需手动的不断获得\n生成结果，然后把他转化为输入，然后再次生成，这样的方式是比较麻烦的。\n\nbyzerllm 提供了更加易用的 API :\n\n```python\nimport byzerllm\nfrom byzerllm import ByzerLLM\n\nllm = ByzerLLM.from_default_model(\"deepseek_chat\")\n\n@byzerllm.prompt()\ndef tell_story() -> str:\n    \"\"\"\n    讲一个100字的故事。    \n    \"\"\"\n\n\ns = (\n    tell_story.with_llm(llm)\n    .with_response_markers()\n    .options({\"llm_config\": {\"max_length\": 10}})\n    .run()\n)\nprint(s)\n\n## 从前，森林里住着一只聪明的小狐狸。一天，它发现了一块闪闪发光的宝石。小狐狸决定用这块宝石帮助森林里的动物们。它用宝石的光芒指引迷路的小鹿找到了回家的路，用宝石的温暖治愈了受伤的小鸟。从此，小狐狸成了森林里的英雄，动物们都感激它的善良和智慧。\n```\n\n实际核心部分就是这一行：\n\n```python\ntell_story.with_llm(llm)\n    .with_response_markers()    \n    .run()\n```\n\n我们只需要调用 `with_response_markers` 方法，系统就会自动的帮我们生成超长文本。\n在上面的案例中，我们通过\n\n```python\n.options({\"llm_config\": {\"max_length\": 10}})\n```\n\n认为的限制大模型一次交互最多只能输出10个字符，但是系统依然自动完成了远超过10个字符的文本生成。\n\n## 对象输出\n\n前面我们的例子都是返回字符串，但是我们也可以返回对象，这样我们就可以更加灵活的处理返回结果。\n\n```python\nimport pydantic \n\nclass Story(pydantic.BaseModel):\n    '''\n    故事\n    '''\n\n    title: str = pydantic.Field(description=\"故事的标题\")\n    body: str = pydantic.Field(description=\"故事主体\")\n\n@byzerllm.prompt()\ndef tell_story()->Story:\n    '''\n    讲一个100字的故事。    \n    '''\n\ns = tell_story.with_llm(llm).run()\nprint(isinstance(s, Story))\nprint(s.title)\n\n## True\n## 勇敢的小鸟\n```\n\n可以看到，我们很轻松的将输出转化为格式化输出。\n\n## 自定义字段抽取\n\n前面的结构化输出，其实会消耗更多token,还有一种更加精准的结构化输出方式。\n比如让大模型生成一个正则表达式，但实际上大模型很难准确只输出一个正则表达式，这个时候我们可以通过自定义抽取函数来获取我们想要的结果。\n\n\n```python\nfrom loguru import logger\nimport re\n\n@byzerllm.prompt()\ndef generate_regex_pattern(desc: str) -> str:\n    \"\"\"\n    根据下面的描述生成一个正则表达式，要符合python re.compile 库的要求。\n\n    {{ desc }}\n\n    最后生成的正则表达式要在<REGEX></REGEX>标签对里。\n    \"\"\"    \n\ndef extract_regex_pattern(regex_block: str) -> str:    \n    pattern = re.search(r\"<REGEX>(.*)</REGEX>\", regex_block, re.DOTALL)\n    if pattern is None:\n        logger.warning(\"No regex pattern found in the generated block:\\n {regex_block}\")\n        raise None\n    return pattern.group(1)\n\npattern = \"匹配一个邮箱地址\"\nv = generate_regex_pattern.with_llm(llm).with_extractor(extract_regex_pattern).run(desc=pattern)\nprint(v)\n## ^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\n```\n\n在上面的例子里，我们根据一句话生成一个正则表达式。我们通过 `with_extractor` 方法，传入了一个自定义的抽取函数，这个函数会在大模型生成结果后，对结果进行处理，然后返回我们想要的结果。\n\n我们在 prompt 明确说了，生成的结果要放到 `<REGEX></REGEX>` 标签对里，然后我们通过 extract_regex_pattern 函数，从结果中提取出了我们想要的正则表达式。\n\n## 在实例方法中使用大模型\n\n```python\nimport byzerllm\ndata = {\n    'name': 'Jane Doe',\n    'task_count': 3,\n    'tasks': [\n        {'name': 'Submit report', 'due_date': '2024-03-10'},\n        {'name': 'Finish project', 'due_date': '2024-03-15'},\n        {'name': 'Reply to emails', 'due_date': '2024-03-08'}\n    ]\n}\n\n\nclass RAG():\n    def __init__(self):        \n        self.llm = byzerllm.ByzerLLM()\n        self.llm.setup_template(model=\"deepseek_chat\",template=\"auto\")\n        self.llm.setup_default_model_name(\"deepseek_chat\")        \n    \n    @byzerllm.prompt(lambda self:self.llm)\n    def generate_answer(self,name,task_count,tasks)->str:\n        '''\n        Hello {{ name }},\n\n        This is a reminder that you have {{ task_count }} pending tasks:\n        {% for task in tasks %}\n        - Task: {{ task.name }} | Due: {{ task.due_date }}\n        {% endfor %}\n\n        Best regards,\n        Your Reminder System\n        '''        \n\nt = RAG()\n\nresponse = t.generate_answer(**data)\nprint(response)\n\n## 输出:\n## Hello Jane Doe,\n##I hope this message finds you well. I wanted to remind you of your 3 pending tasks to ensure you stay on track:\n## 1. **Submit report** - This task is due on **2024-03-10**. Please ensure that you allocat\n```\n\n这里我们给了个比较复杂的例子，但我们可以看到，给一个实例prompt方法和普通prompt 方法差异不大。\n唯一的区别是如果你希望在定义的时候就指定大模型，使用一个lambda函数返回实例的 llm 对象即可。\n\n```python\n@byzerllm.prompt(lambda self:self.llm)\n```\n\n你也可以不返回，在调用的时候通过 `with_llm` 方法指定 llm 对象。\n\n此外，这个例子也展示了如何通过jinja2模板语法，来处理复杂的结构化数据。\n\n## 通过 Python 代码处理复杂入参\n\n上面的一个例子中，我们通过 jinja2 模板语法，来处理复杂的结构化数据，但是有时候我们可能需要更加复杂的处理，这个时候我们可以通过 Python 代码来处理。\n\n```python\nimport byzerllm\n\ndata = {\n    'name': 'Jane Doe',\n    'task_count': 3,\n    'tasks': [\n        {'name': 'Submit report', 'due_date': '2024-03-10'},\n        {'name': 'Finish project', 'due_date': '2024-03-15'},\n        {'name': 'Reply to emails', 'due_date': '2024-03-08'}\n    ]\n}\n\n\nclass RAG():\n    def __init__(self):        \n        self.llm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n    \n    @byzerllm.prompt()\n    def generate_answer(self,name,task_count,tasks)->str:\n        '''\n        Hello {{ name }},\n\n        This is a reminder that you have {{ task_count }} pending tasks:\n            \n        {{ tasks }}\n\n        Best regards,\n        Your Reminder System\n        '''\n        \n        tasks_str = \"\\n\".join([f\"- Task: {task['name']} | Due: { task['due_date'] }\" for task in tasks])\n        return {\"tasks\": tasks_str}\n\nt = RAG()\n\nresponse = t.generate_answer.with_llm(t.llm).run(**data)\nprint(response)\n\n## Just a gentle nudge to keep you on track with your pending tasks. Here's a quick recap:....\n```\n\n在这个例子里，我们直接把 tasks 在方法体里进行处理，然后作为一个字符串返回，最够构建一个字典，字典的key为 tasks,然后\n你就可以在 docstring 里使用 `{{ tasks }}` 来引用这个字符串。\n\n这样对于很复杂的入参，就不用谢繁琐的 jinja2 模板语法了。\n\n## 如何自动实现一个方法\n\n比如我定义一个签名，但是我不想自己实现里面的逻辑，让大模型来实现。这个在 byzerllm 中叫 function impl。我们来看看怎么\n实现:\n\n```python\nimport pydantic\nclass Time(pydantic.BaseModel):\n    time: str = pydantic.Field(...,description=\"时间，时间格式为 yyyy-MM-dd\")\n\n\n@llm.impl()\ndef calculate_current_time()->Time:\n    '''\n    计算当前时间\n    '''\n    pass \n\n\ncalculate_current_time()\n#output: Time(time='2024-06-14')\n```\n\n在这个例子里，我们定义了一个 calculate_current_time 方法，但是我们没有实现里面的逻辑，我们通过 `@llm.impl()` 装饰器，让大模型来实现这个方法。\n为了避免每次都要“生成”这个方法，导致无法适用，我们提供了缓存，用户可以按如下方式打印速度：\n\n```python\nstart = time.monotonic()\ncalculate_current_time()\nprint(f\"first time cost: {time.monotonic()-start}\")\n\nstart = time.monotonic()\ncalculate_current_time()\nprint(f\"second time cost: {time.monotonic()-start}\")\n\n# output:\n# first time cost: 6.067266260739416\n# second time cost: 4.347506910562515e-05\n```\n可以看到，第一次执行花费了6s,第二次几乎是瞬间完成的，这是因为第一次执行的时候，我们实际上是在生成这个方法，第二次执行的时候，我们是执行已经生成好的代码，所以速度会非常快。你可以显示的调用 `llm.clear_impl_cache()` 清理掉函数缓存。\n\n## Stream 模式\n\n前面的例子都是一次性生成结果，但是有时候我们可能需要一个流式的输出，这个时候我们可能需要用底层一点的API来完成了：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\nv = llm.stream_chat_oai(model=\"deepseek_chat\",conversations=[{\n    \"role\":\"user\",\n    \"content\":\"你好，你是谁\",\n}],delta_mode=True)\n\nfor t in v:\n    print(t,flush=True)  \n\n# 你好\n# ！\n# 我\n# 是一个\n# 人工智能\n# 助手\n# ，\n# 旨在\n# 提供\n# 信息\n# 、\n# 解答\n# 问题....\n```\n\n如果你不想要流式输出，但是想用底层一点的API，你可以使用 `llm.chat_oai` 方法：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\nv = llm.chat_oai(model=\"deepseek_chat\",conversations=[{\n    \"role\":\"user\",\n    \"content\":\"你好，你是谁\",\n}])\n\nprint(v[0].output)\n## 你好！我是一个人工智能助手，旨在提供信息、解答问题和帮助用户解决问题。如果你有任何问题或需要帮助，请随时告诉我。\n```\n\n## Function Calling \n\nbyzerllm 可以不依赖模型自身就能提供 function calling 支持，我们来看个例子：\n\n\n```python\nfrom typing import List,Dict,Any,Annotated\nimport pydantic \nimport datetime\nfrom dateutil.relativedelta import relativedelta\n\ndef compute_date_range(count:Annotated[int,\"时间跨度，数值类型\"],\n                       unit:Annotated[str,\"时间单位，字符串类型\",{\"enum\":[\"day\",\"week\",\"month\",\"year\"]}])->List[str]:\n    '''\n    计算日期范围\n\n    Args:\n        count: 时间跨度，数值类型\n        unit: 时间单位，字符串类型，可选值为 day,week,month,year\n    '''        \n    now = datetime.datetime.now()\n    now_str = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n    if unit == \"day\":\n        return [(now - relativedelta(days=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"week\":\n        return [(now - relativedelta(weeks=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"month\":\n        return [(now - relativedelta(months=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"year\":\n        return [(now - relativedelta(years=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    return [\"\",\"\"]\n\ndef compute_now()->str:\n    '''\n    计算当前时间\n    '''\n    return datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n```\n\n我们定义了两个方法，一个是计算日期范围，一个是计算当前时间。\n\n现在我么可以来测试下，系统如何根据自然语言决定调用哪个方法：\n\n```python\nt = llm.chat_oai([{\n    \"content\":'''计算当前时间''',\n    \"role\":\"user\"    \n}],tools=[compute_date_range,compute_now],execute_tool=True)\n\nt[0].values\n\n## output: ['2024-06-14 15:18:02']\n```\n\n我们可以看到，他正确的选择了 compute_now 方法。\n\n接着我们再试一个：\n\n```python\nt = llm.chat_oai([{\n    \"content\":'''最近三个月趋势''',\n    \"role\":\"user\"    \n}],tools=[compute_date_range,compute_now],execute_tool=True)\n\nt[0].values\n\n## output: [['2024-03-14 15:19:13', '2024-06-14 15:19:13']]\n```\n\n模型正确的选择了 compute_date_range 方法。\n\n## 多模态\n\nbyerllm 也能很好的支持多模态的交互，而且统一了多模态大模型的接口，比如你可以用一样的方式使用 openai 或者 claude 的图片转文字能力， 或者一致的方式使用火山，azuer, openai的语音合成接口。\n\n### 图生文\n\n```python\nimport byzerllm\nfrom byzerllm.types import ImagePath\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\n\n\n@byzerllm.prompt()\ndef what_in_image(image_path: ImagePath) -> str:\n    \"\"\"\n    {{ image_path }}\n    这个图片里有什么？\n    \"\"\"    \n\n\nv = what_in_image.with_llm(vl_llm).run(\n    ImagePath(value=\"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\")\n)\nv\n## OUTPUT: 这张图片展示了多只可爱的猫咪，采用了艺术风格的绘画。猫咪们有不同的颜色和花纹，背景是浅棕色，上面还点缀着一些红色的花朵。整体画面给人一种温馨和谐的感觉\n```\n\n可以看到，我们只需要把 prompt 函数的图片地址入参使用 byzerllm.types.ImagePath里进行包装，就可以直接在 prompt 函数体里\n带上图片。\n\n或者你可以这样：\n\n```python\nimport byzerllm\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\n\n\n@byzerllm.prompt()\ndef what_in_image(image_path: str) -> str:\n    \"\"\"\n    {{ image }}\n    这个图片里有什么？\n    \"\"\"\n    return {\"image\": byzerllm.Image.load_image_from_path(image_path)}\n\n\nv = what_in_image.with_llm(vl_llm).run(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv\n```\n\n通过 `image_path` 参数，然后通过 `byzerllm.Image.load_image_from_path` 方法，转化为一个图片对象 image，最后在 prompt 函数体里\n使用 `{{ image }}` 引用这个图片对象。\n\n另外我们也是可以支持配置多张图片的。\n\n另外，我们也可以使用基础的 `llm.chat_oai` 方法来实现：\n\n```python\nimport byzerllm\nimport json\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\nimage = byzerllm.Image.load_image_from_path(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv = vl_llm.chat_oai(\n    conversations=[\n        {\n            \"role\": \"user\",\n            \"content\": json.dumps(\n                [{\"image\": image, \"text\": \"这个图片里有什么？\"}], ensure_ascii=False\n            ),\n        }\n    ]\n)\nv[0].output\n```\n\n还可以这么写：\n\n```python\nimport byzerllm\nimport json\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\nimage = byzerllm.Image.load_image_from_path(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv = vl_llm.chat_oai(\n    conversations=[\n        {\n            \"role\": \"user\",\n            \"content\": json.dumps(\n                [\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\"url\": image, \"detail\": \"high\"},\n                    },\n                    {\"text\": \"这个图片里有什么？\", \"type\": \"text\"},\n                ],\n                ensure_ascii=False,\n            ),\n        }\n    ]\n)\nv[0].output\n```\n\n### 语音合成\n\n这里以 openai 的 tts 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=tts-1 \\\n--model openai_tts\n```\n\n此外，byzerllm 支持 azure,火山引擎等 tts 语音合成引擎。\n\n接着你可以这么用：\n\n```python\nimport byzerllm\nimport base64\nimport json\n\nllm = byzerllm.ByzerLLM.from_default_model(\"openai_tts\")\n\n\nt = llm.chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\": json.dumps({\n        \"input\":\"hello, open_tts\",\n        \"voice\": \"alloy\",\n        \"response_format\": \"mp3\"\n    },ensure_ascii=False)\n}])\n\nwith open(\"voice.mp3\",\"wb\") as f:\n    f.write(base64.b64decode(t[0].output))\n```\n\ntts 模型生成没有prompt函数可以用，你需要直接使用 chat_oai。\n\n\n### 语音识别\n\n这里以 openai 的 whisper-1 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--worker_concurrency 10 \\\n--infer_params saas.model=whisper-1 saas.api_key=${MODEL_OPENAI_TOKEN} \\\n--model speech_to_text\n```\n\n语音识别的使用方式和图生文类似，我们可以直接在 prompt 函数体里带上音频文件。\n\n```python\nimport byzerllm\nimport json\nimport base64\nfrom byzerllm.types import AudioPath\n\nllm = byzerllm.ByzerLLM.from_default_model(\"speech_to_text\")\n\naudio_file = \"/Users/allwefantasy/videos/output_audio.mp3\"\n\n@byzerllm.prompt(llm=llm)\ndef audio_to_text(audio_file: AudioPath):\n    \"\"\"\n    {{ audio_file }}\n    \"\"\"\n\nv = audio_to_text(AudioPath(value=audio_file))\njson.loads(v)\n```\n输出的数据格式略微复杂：\n\n```\n{'text': 'In the last chapter, you and I started to step through the internal workings of a transformer. This is one of the key pieces of technology inside large language models, and a lot of other tools in the modern wave of AI.',\n 'task': 'transcribe',\n 'language': 'english',\n 'duration': 10.0,\n 'segments': [{'id': 0,\n   'seek': 0,\n   'start': 0.0,\n   'end': 4.78000020980835,\n   'text': ' In the last chapter, you and I started to step through the internal workings of a transformer.',\n   'tokens': [50364,\n    682,\n    264,\n    1036,\n    7187,\n    11,\n    291,\n    293,\n    286,\n    1409,\n    281,\n    1823,\n    807,\n    264,\n    6920,\n    589,\n    1109,\n    295,\n    257,\n    31782,\n    13,\n    50586],\n   'temperature': 0.0,\n   'avg_logprob': -0.28872039914131165,\n   'compression_ratio': 1.4220778942108154,\n   'no_speech_prob': 0.016033057123422623},\n  ....\n  {'id': 2,\n   'seek': 0,\n   'start': 8.579999923706055,\n   'end': 9.979999542236328,\n   'text': ' and a lot of other tools in the modern wave of AI.',\n   'tokens': [50759,\n    293,\n    257,\n    688,\n    295,\n    661,\n    3873,\n    294,\n    264,\n    4363,\n    5772,\n    295,\n    7318,\n    13,\n    50867],\n   'temperature': 0.0,\n   'avg_logprob': -0.28872039914131165,\n   'compression_ratio': 1.4220778942108154,\n   'no_speech_prob': 0.016033057123422623}],\n 'words': [{'word': 'In', 'start': 0.0, 'end': 0.18000000715255737},\n  {'word': 'the', 'start': 0.18000000715255737, 'end': 0.23999999463558197},\n  {'word': 'last', 'start': 0.23999999463558197, 'end': 0.5400000214576721},\n  {'word': 'chapter', 'start': 0.5400000214576721, 'end': 0.800000011920929},\n  ....\n  {'word': 'AI', 'start': 9.920000076293945, 'end': 9.979999542236328}]}\n```\n\n会输出每一句话以及每一个字所在的起始时间和截止时间。你可以根据需要来使用。\n\n\n### 文生图\n\n文生图和语音合成类似，首先要启动合适的模型,以openai 的 dall-e-3 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=dall-e-3 \\\n--model openai_image_gen\n```\n\n启动模型后，只需要记住几个模板参数即可使用，这里直接使用 chat_oai 方法来使用：\n\n```python\n\nimport byzerllm\nimport json\nimport base64\n\nllm = byzerllm.ByzerLLM.from_default_model(\"openai_image_gen\")\nt = llm.chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\": json.dumps({\n        \"input\":\"a white siamese cat\",\n        \"size\": \"1024x1024\",\n        \"quality\": \"standard\"\n    },ensure_ascii=False)\n}])\n\nwith open(\"output1.jpg\",\"wb\") as f:\n    f.write(base64.b64decode(t[0].output))\n\n\nimport matplotlib.pyplot as plt\n\nimage_path = \"output1.jpg\"\nimage = plt.imread(image_path)\n\nplt.imshow(image)\nplt.axis('off')\nplt.show()\n```\n\n## Prompt 函数的流式输出\n\nbyzerllm 底层支持流式输出，非 prompt 函数的用法是这样的：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\n\nv = llm.stream_chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\":\"讲一个100字的故事\"\n}])\n\nfor s  in v:\n    print(s[0], end=\"\")\n```\n\n如果你像用 prompt 函数，可以这么用：\n\n\n```python\nimport byzerllm\nimport json\nimport base64\nfrom typing import Generator\n\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\n\n@byzerllm.prompt()\ndef tell_story() -> Generator[str, None, None]:\n    '''\n    给我讲一个一百多字的故事\n    '''\n\nv = tell_story.with_llm(llm).run()    \nfor i in v:\n    print(i, end=\"\")\n```\n\n可以看到，和普通的 prompt 函数的区别在于，返回值是一个生成器，然后你可以通过 for 循环来获取结果。\n\n## 向量化模型\n\nbyzerllm 支持向量化模型,你可以这样启动一个本地的模型：\n\n```bash\n!byzerllm deploy --pretrained_model_type custom/bge \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--worker_concurrency 10 \\\n--model_path /home/winubuntu/.auto-coder/storage/models/AI-ModelScope/bge-large-zh \\\n--infer_backend transformers \\\n--num_workers 1 \\\n--model emb\n```\n\n注意两个参数:\n\n1. --infer_backend transformers: 表示使用 transformers 作为推理后端。\n2. --model_path: 表示模型的路径。\n\n也可以启动一个 SaaS 的emb模型,比如 qwen 的 emb 模型：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/qianwen \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 2 \\\n--infer_params saas.api_key=${MODEL_QIANWEN_TOKEN}  saas.model=text-embedding-v2 \\\n--model qianwen_emb\n```\n\n或者 openai 的 emb 模型：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--worker_concurrency 10 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=text-embedding-3-small \\\n--model gpt_emb\n```\nSaaS 模型无需配置 `--infer_backend` 参数。\n\n无论是本地模型还是 SaaS 模型，我们都可以这样使用：\n\n```python\nimport byzerllm\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\nllm.setup_default_emb_model_name(\"emb\")\nllm.emb_query(\"你好\")\n```\n\n如果你配置 byzerllm 中的 Storage 使用，比如你这样启动了存储：\n\n```bash\nbyzerllm storage start --enable_emb\n```\n\n那么需要这么使用：\n\n```python\nfrom byzerllm.apps.byzer_storage.simple_api import ByzerStorage, DataType, FieldOption,SortOption\nstorage = ByzerStorage(\"byzerai_store\", \"memory\", \"memory\")\nstorage.emb(\"你好\")\n```\n\n## 一些辅助工具\n\n当调用 prompt 函数返回字符串的时候，如果想从里面抽取代码，可以使用如下方式：\n\n```python\nfrom byzerllm.utils.client import code_utils\ntext_with_markdown = '''\n```shell\nls -l\n```\n'''\ncode_blocks = code_utils.extract_code(text_with_markdown)\nfor code_block in code_blocks:\n    if code_block[0] == \"shell\":\n        print(code_block[1])\n##output: ls -l        \n```\n\nTagExtractor 工具用于对任意 `<_tag_></_tag_>` 标签对的抽取，下面是一个使用示例：\n\n```python\nfrom byzerllm.apps.utils import TagExtractor\n\nextractor = TagExtractor('''\n大家好\n<_image_>data:image/jpeg;base64,xxxxxx</_image_>\n                         大家好\n<_image_>data:image/jpeg;base64,xxxxxx2</_image_>\n''')\n\nv = extractor.extract()\nprint(v.content[0].content)\nprint(v.content[1].content)\n```\n输出为:\n\n```\ndata:image/jpeg;base64,xxxxxx\ndata:image/jpeg;base64,xxxxxx2\n```\n\n我们成功的将 <_image_></_image_> 标签对里的内容抽取出来了。\n\n\n## 注意事项\n\n1. prompt函数方法体返回只能是dict，实际的返回类型和方法签名可以不一样，但是方法体返回只能是dict。\n2. 大部分情况prompt函数体为空，如果一定要有方法体，可以返回一个空字典。\n3. 调用prompt方法的时候，如果在@byzerllm.prompt()里没有指定llm对象，那么需要在调用的时候通过with_llm方法指定llm对象。\n\n===========================\n\n# Byzer Storage: 最易用的AI存储引擎\n\nByzer Storage是一个为AI应用设计的高性能存储引擎,它提供了简单易用的API,支持向量搜索、全文检索以及结构化查询。本文将详细介绍Byzer Storage的使用方法和主要特性。\n\n## 0. 安装和启动\n\n```bash\npip install byzerllm\nbyzerllm storage start\n```\n\nThat's it! Byzer Storage已经安装并启动成功,现在我们可以开始使用它了。默认会启动一个 byzerai_store 的集群。\n\n注意，如果你是这样启动的：\n\n```bash\nbyzerllm storage start --enable_emb\n```\n\n那么会自动启动一个 emb 模型，名字就叫 emb， ByzerStorage 会自动使用该模型，无需做任何其他配置。\n\n## 1. 初始化\n\n创建一个 ByzerStorage 对象，链接 byzerai_store 集群，并且指定数据库和表名（可以不存在）。\n\n```python\nfrom byzerllm.apps.byzer_storage.simple_api import ByzerStorage, DataType, FieldOption, SortOption\n\nstorage = ByzerStorage(\"byzerai_store\", \"my_database1\", \"my_table4s\")\n```\n\n\n## 2. 创建库表（可选）\n\nByzer Storage使用Schema来定义数据结构。我们可以使用SchemaBuilder来构建Schema:\n\n```python\n_ = (\n    storage.schema_builder()\n    .add_field(\"_id\", DataType.STRING)\n    .add_field(\"name\", DataType.STRING)\n    .add_field(\"content\", DataType.STRING, [FieldOption.ANALYZE])\n    .add_field(\"raw_content\", DataType.STRING, [FieldOption.NO_INDEX])    \n    .add_array_field(\"summary\", DataType.FLOAT)    \n    .add_field(\"created_time\", DataType.LONG, [FieldOption.SORT])    \n    .execute()\n)\n```\n\n这个Schema定义了以下字段:\n- `_id`: 字符串类型的主键\n- `name`: 字符串类型,可用于过滤条件\n- `content`: 字符串类型,带有ANALYZE选项,用于全文搜索\n- `raw_content`: 字符串类型,带有NO_INDEX选项,不会被索引\n- `summary`: 浮点数组类型,用于存储向量\n- `created_time`: 长整型,带有SORT选项,可用于排序\n\n需要注意的是：\n\n1. 如果一个字段带有ANALYZE选项,则该字段会被分词,并且可以用于全文搜索，但是就无法返回原始的文本了。所以你需要添加一个新字段专门用来存储原文，比如在我们这个例子里，我们新增了 raw_content 字段，并且显示的指定了 NO_INDEX 选项，这样就不会被索引，也就不会被分词，可以后续被检索后用来获取原始文本。\n2. 对于需要作为向量检索和存储的字段，需要指定为数组类型，比如我们这个例子里的 summary 字段。\n3. 如果你需要拿到向量字段的原始文本，那么你也需要添加一个新字段专门用来存储原文，就像我们这个例子里的 raw_content 字段一样。\n\n\n## 3. 写入数据\n\n准备数据并使用WriteBuilder写入Storage:\n\n```python\ndata = [\n    {\"_id\": \"1\", \"name\": \"Hello\", \"content\": \"Hello, world!\", \"raw_content\": \"Hello, world!\", \"summary\": \"hello world\", \"created_time\": 1612137600},\n    {\"_id\": \"2\", \"name\": \"Byzer\", \"content\": \"Byzer, world!\", \"raw_content\": \"Byzer, world!\", \"summary\": \"byzer\", \"created_time\": 1612137601},\n    {\"_id\": \"3\", \"name\": \"AI\", \"content\": \"AI, world!\", \"raw_content\": \"AI, world!\", \"summary\": \"AI\", \"created_time\": 16121376002},\n    {\"_id\": \"4\", \"name\": \"ByzerAI\", \"content\": \"ByzerAI, world!\", \"raw_content\": \"ByzerAI, world!\", \"summary\": \"ByzerAi\", \"created_time\": 16121376003},\n]\n\nstorage.write_builder().add_items(data, vector_fields=[\"summary\"], search_fields=[\"content\"]).execute()\nstorage.commit()\n```\n\n这里我们使用`add_items`方法批量添加数据,并指定了`summary`为向量字段,`content`为搜索字段。最后调用`commit()`来确保数据被持久化。\n\n\n从上面我们要写入到 byzer storage 的数据我们可以看到如下几个特点：\n\n1. 需要包含我们之前定义的 Schema中罗列的所有的字段，同时需要指定哪些是向量字段，哪些是检索字段。\n2. 向量和检索不能是同一个字段。\n3. 对于向量，检索字段，我们给到 write_builder 的都是文本，ByzerStorage 会根据 Schema 的定义自动将其转换为向量，检索字段需要的格式。\n\n## 4. 查询数据\n\nByzer Storage支持多种查询方式,包括向量搜索、全文检索、过滤和排序。\n\n### 4.1 向量搜索 + 全文检索\n\n```python\nquery = storage.query_builder()\nquery.set_vector_query(\"ByzerAI\", fields=[\"summary\"])\nresults = query.set_search_query(\"Hello\", fields=[\"content\"]).execute()\nprint(results)\n```\n\n这个查询结合了向量搜索和全文检索,它会在`summary`字段中搜索与\"ByzerAI\"相似的向量,同时在`content`字段中搜索包含\"Hello\"的文档。\n\n### 4.2 过滤 + 向量搜索 + 全文检索\n\n```python\nquery = storage.query_builder()\nquery.and_filter().add_condition(\"name\", \"AI\").build()\nquery.set_vector_query(\"ByzerAI\", fields=\"summary\")\nresults = query.set_search_query(\"Hello\", fields=[\"content\"]).execute()\nprint(results)\n```\n\n这个查询首先过滤`name`字段等于\"AI\"的文档,然后在结果中进行向量搜索和全文检索。\n\n### 4.3 过滤 + 排序\n\n```python\nquery = storage.query_builder()\nquery.and_filter().add_condition(\"name\", \"AI\").build().sort(\"created_time\", SortOption.DESC)\nresults = query.execute()\nprint(results)\n```\n\n这个查询过滤`name`字段等于\"AI\"的文档,然后按`created_time`字段降序排序。\n\n## 5. 删除数据\n\n### 5.1 根据ID删除\n\n```python\nstorage.delete_by_ids([\"3\"])\n\nquery = storage.query_builder()\nquery.and_filter().add_condition(\"name\", \"AI\").build()\nresults = query.execute()\nprint(results)\n```\n\n这里我们删除了ID为\"3\"的文档,然后查询验证删除结果。\n\n### 5.2 删除整个表\n\n```python\nstorage.drop()\n```\n\n这个操作会删除整个表及其所有数据,请谨慎使用。\n\n## 结论\n\nByzer Storage提供了一套简洁而强大的API,能够轻松实现向量搜索、全文检索、结构化查询等功能。它的设计非常适合AI应用场景,可以有效地存储和检索各种类型的数据。通过本文的介绍,相信读者已经对Byzer Storage有了基本的了解,并能够开始在自己的项目中使用这个强大的存储引擎。\n\n\n     \n\n下面是用户的需求：\n\n@setup.py 和 makefile 中的 dist 要增加版本号\n使用 SEARCH/REPLACE 格式输出\n"}, {"role": "assistant", "content": "好的，我会按照要求使用SEARCH/REPLACE格式来修改setup.py和Makefile文件，增加版本号。\n\n首先是setup.py文件：\n\n```python\n##File: /Users/allwefantasy/projects/super_analysis/setup.py\n<<<<<<< SEARCH\nfrom setuptools import setup, find_packages\n\n# Read requirements from requirements.txt\nwith open('requirements.txt') as f:\n    requirements = f.read().splitlines()\n\nsetup(\n    name=\"super-analysis\",\n    version=\"0.1.0\",\n=======\nfrom setuptools import setup, find_packages\nimport os\n\n# Read requirements from requirements.txt\nwith open('requirements.txt') as f:\n    requirements = f.read().splitlines()\n\n# Read version from version.py\nwith open(os.path.join('src', 'superanalysis', 'version.py'), 'r') as f:\n    exec(f.read())\n\nsetup(\n    name=\"super-analysis\",\n    version=__version__,\n>>>>>>> REPLACE\n```\n\n接下来是Makefile文件：\n\n```\n##File: /Users/allwefantasy/projects/super_analysis/Makefile\n<<<<<<< SEARCH\ndist: wheel\n\t@echo \"Creating distribution package...\"\n\trm -rf super_analysis.tar.gz\n\tmkdir -p super_analysis\n\tcp -r $(DIST_DIR)/* super_analysis/\n\tcp \"/Users/allwefantasy/projects/super_analysis/docs/4.3.1 安装与配置 Byzer-SQL.pdf\" super_analysis/\n\tcp \"/Users/allwefantasy/projects/super_analysis/docs/deploy.md\" super_analysis/README.md\n\tcp \"/Users/allwefantasy/projects/super_analysis/notebooks/openai_local_api.ipynb\" super_analysis/\n\ttar -czf super_analysis.tar.gz super_analysis\n\trm -rf super_analysis\n\t@echo \"Distribution package created: super_analysis.tar.gz\"\n=======\ndist: wheel\n\t@echo \"Creating distribution package...\"\n\t$(eval VERSION := $(shell python -c \"import sys; sys.path.append('src'); from superanalysis.version import __version__; print(__version__)\"))\n\trm -rf super_analysis-$(VERSION).tar.gz\n\tmkdir -p super_analysis-$(VERSION)\n\tcp -r $(DIST_DIR)/* super_analysis-$(VERSION)/\n\tcp \"/Users/allwefantasy/projects/super_analysis/docs/4.3.1 安装与配置 Byzer-SQL.pdf\" super_analysis-$(VERSION)/\n\tcp \"/Users/allwefantasy/projects/super_analysis/docs/deploy.md\" super_analysis-$(VERSION)/README.md\n\tcp \"/Users/allwefantasy/projects/super_analysis/notebooks/openai_local_api.ipynb\" super_analysis-$(VERSION)/\n\ttar -czf super_analysis-$(VERSION).tar.gz super_analysis-$(VERSION)\n\trm -rf super_analysis-$(VERSION)\n\t@echo \"Distribution package created: super_analysis-$(VERSION).tar.gz\"\n>>>>>>> REPLACE\n```\n\n这些修改将会：\n\n1. 在setup.py中，我们现在从version.py文件中读取版本号，而不是硬编码。\n\n2. 在Makefile中，我们添加了一个步骤来从version.py文件中读取版本号，并在创建分发包时使用这个版本号。\n\n这样，当你更新src/superanalysis/version.py中的版本号时，它会自动反映在setup.py和分发包的名称中。"}], "model": "human", "yaml_file": "actions/149_chat_action.yml"}
{"instruction": "makefile 中的disk 最好打的包要放到 releases 目录下\n使用 SEARCH/REPLACE 格式输出", "conversations": [{"role": "user", "content": "\n如果你需要生成代码，对于每个需要更改的文件,你需要按 *SEARCH/REPLACE block* 的格式进行生成。\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The opening fence and code language, eg: ```python\n2. The file path alone on a line, starting with \"##File:\" and verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n*SEARCH/REPLACE* blocks will replace *all* matching occurrences.\nInclude enough lines to make the SEARCH blocks unique.\n\nInclude *ALL* the code being searched and replaced!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n下面我们来看一个例子：\n\n当前项目目录结构：\n1. 项目根目录： /tmp/projects/mathweb\n2. 项目子目录/文件列表(类似tree 命令输出)\nflask/\n    app.py\n    templates/\n        index.html\n    static/\n        style.css\n\n用户需求： Change get_factorial() to use math.factorial\n\n回答： To make this change we need to modify `/tmp/projects/mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n用户需求： Refactor hello() into its own file.\n\n回答：To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/hello.py\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/main.py\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n现在让我们开始一个新的任务:\n\n当前项目目录结构：\n1. 项目根目录： /Users/allwefantasy/projects/super_analysis\n2. 项目子目录/文件列表(类似tree 命令输出)：\nsetup.py\n.auto-coder/\n    libs/\n        llm_friendly_packages/\n            src/\n                example1/\n                    main.py\nsrc/\n    superanalysis/\n        version.py\n        serve/\n            rest.py\n            convert.py\n            openai_serve.py\n            __init__.py\n            llm_wrapper.py\n            api_server.py\n            cli.py\n        sql/\n            __init__.py\n            types.py\n            generator.py\n下面是一些文件路径以及每个文件对应的源码：\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/types.py\nfrom pydantic import BaseModel\n\nclass RAGConfig(BaseModel):\n    schema_rag_base_url: str = \"\"\n    schema_rag_api_key: str = \"xxxx\"\n\n    context_rag_base_url: str = \"\"\n    context_rag_api_key: str = \"xxxx\"\n\nclass ByzerSQLConfig(BaseModel):\n    byzer_sql_url: str = \"\"\n    data_dir: str = \"\"\n    owner: str = \"\"\n\nclass ModelConfig(BaseModel):\n    llm_model: str = \"\"\n    gen_sql_llm_model: str = \"\"\n    sql_func_llm_model: str = \"\"\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/cli.py\nimport argparse\nimport byzerllm\nfrom byzerllm.utils.client import ByzerLLM\nfrom superanalysis.serve.api_server import serve, ServerArgs\nfrom superanalysis.sql.types import RAGConfig, ByzerSQLConfig, ModelConfig\nfrom superanalysis.sql.generator import SQLGenerator\nfrom superanalysis.serve.llm_wrapper import LLWrapper\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"ByzerLLM API Server\")\n    parser.add_argument(\"--ray-address\", type=str, default=\"auto\", help=\"ray address\")\n    parser.add_argument(\n        \"--host\", type=str, default=\"0.0.0.0\", help=\"Host to bind the server to\"\n    )\n    parser.add_argument(\n        \"--port\", type=int, default=8000, help=\"Port to bind the server to\"\n    )\n    parser.add_argument(\n        \"--uvicorn-log-level\", type=str, default=\"info\", help=\"Uvicorn log level\"\n    )\n    parser.add_argument(\n        \"--allow-credentials\", action=\"store_true\", help=\"Allow credentials\"\n    )\n    parser.add_argument(\n        \"--allowed-origins\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed origins\"\n    )\n    parser.add_argument(\n        \"--allowed-methods\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed methods\"\n    )\n    parser.add_argument(\n        \"--allowed-headers\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed headers\"\n    )\n    parser.add_argument(\n        \"--api-key\", type=str, default=\"\", help=\"API key for authentication\"\n    )\n    parser.add_argument(\n        \"--served-model-name\",\n        type=str,\n        required=True,\n        help=\"Name of the model to be served\",\n    )\n    parser.add_argument(\n        \"--prompt-template\", type=str, default=\"\", help=\"Prompt template\"\n    )\n    parser.add_argument(\n        \"--response-role\", type=str, default=\"assistant\", help=\"Response role\"\n    )\n    parser.add_argument(\"--ssl-keyfile\", type=str, default=\"\", help=\"SSL key file\")\n    parser.add_argument(\n        \"--ssl-certfile\", type=str, default=\"\", help=\"SSL certificate file\"\n    )\n\n    # Add new arguments for RAGConfig\n    parser.add_argument(\n        \"--schema-rag-base-url\", type=str, default=\"\", help=\"RAG base URL\"\n    )\n    parser.add_argument(\n        \"--schema-rag-api-key\", type=str, default=\"xxxx\", help=\"RAG API key\"\n    )\n\n    parser.add_argument(\n        \"--context-rag-base-url\", type=str, default=\"\", help=\"RAG base URL\"\n    )\n    parser.add_argument(\n        \"--context-rag-api-key\", type=str, default=\"xxxx\", help=\"RAG API key\"\n    )\n\n    # Add new argument for ByzerSQLConfig\n    parser.add_argument(\"--byzer-sql-url\", type=str, default=\"\", help=\"Byzer SQL URL\")\n    parser.add_argument(\"--data-dir\", type=str, default=\"\", help=\"Data directory\")\n    parser.add_argument(\"--owner\", type=str, default=\"hello\", help=\"Owner of the Byzer SQL session\")\n    parser.add_argument(\n        \"--llm-model\", type=str, default=\"\", help=\"LLM model for general use\"\n    )\n    parser.add_argument(\n        \"--gen-sql-llm-model\", type=str, default=\"\", help=\"LLM model for SQL generation\"\n    )\n    parser.add_argument(\n        \"--sql-func-llm-model\", type=str, default=\"\", help=\"LLM model for SQL functions\"\n    )\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile,\n    )\n\n    # Create RAGConfig instance\n    rag_config = RAGConfig(\n        schema_rag_base_url=args.schema_rag_base_url,\n        schema_rag_api_key=args.schema_rag_api_key,\n        context_rag_base_url=args.context_rag_base_url,\n        context_rag_api_key=args.context_rag_api_key,\n    )\n\n    # Create ByzerSQLConfig instance\n    byzer_sql_config = ByzerSQLConfig(\n        byzer_sql_url=args.byzer_sql_url, data_dir=args.data_dir, owner=args.owner\n    )\n\n    model_config = ModelConfig(\n        llm_model=args.llm_model or args.served_model_name,\n        gen_sql_llm_model=args.gen_sql_llm_model or args.served_model_name,\n        sql_func_llm_model=args.sql_func_llm_model or args.served_model_name,\n    )\n\n    byzerllm.connect_cluster(address=args.ray_address)\n    llm = ByzerLLM()\n    llm.setup_default_model_name(args.served_model_name)\n    llm = byzerllm.ByzerLLM()\n    if not server_args.served_model_name:\n        raise ValueError(\"served_model_name is required\")\n    llm.setup_default_model_name(server_args.served_model_name)\n\n    print(\"RAG Config: \", rag_config)\n    print(\"Model Config: \", model_config)\n    rag = SQLGenerator(\n        llm=llm,\n        rag_config=rag_config,\n        byzer_sql_config=byzer_sql_config,\n        model_config=model_config,\n    )\n    llm_wrapper = LLWrapper(llm=llm, entry=rag)\n    serve(llm_wrapper, server_args)\n\n\nif __name__ == \"__main__\":\n    main()\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/__init__.py\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/__init__.py\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/llm_wrapper.py\nfrom typing import Any, Dict, List, Optional, Union,Callable\nfrom byzerllm.utils.client.types import (    \n    LLMFunctionCallResponse,\n    LLMClassResponse,LLMResponse    \n)\nimport pydantic\nfrom byzerllm import ByzerLLM\nfrom byzerllm.utils.client import LLMResponse\nfrom byzerllm.utils.types import SingleOutputMeta\nfrom byzerllm.utils.langutil import asyncfy_with_semaphore\nfrom superanalysis.sql.generator import SQLGenerator\nfrom loguru import logger\n\nclass LLWrapper:\n\n    def __init__(self,llm:ByzerLLM,entry:SQLGenerator):\n        self.entry = entry\n        self.llm = llm\n        \n\n    def chat_oai(self,\n                 conversations,\n                 tools:List[Union[Callable,str]]=[], \n                 tool_choice:Optional[Union[Callable,str]]=None,\n                 execute_tool:bool=False,  \n                 impl_func:Optional[Callable]=None,\n                 execute_impl_func:bool=False,\n                 impl_func_params:Optional[Dict[str,Any]]=None,\n                 func_params:Optional[Dict[str,Any]]=None,\n                 response_class:Optional[Union[pydantic.BaseModel,str]] = None, \n                 response_after_chat:Optional[Union[pydantic.BaseModel,str]] = False,\n                 enable_default_sys_message:bool=True,                 \n                 model:Optional[str] = None,\n                 role_mapping=None,llm_config:Dict[str,Any]={}\n                 )->Union[List[LLMResponse],List[LLMFunctionCallResponse],List[LLMClassResponse]]: \n        res,contexts = self.entry.stream_chat_oai(conversations)\n        s = \"\".join(res)        \n        return [LLMResponse(output=s,metadata={},input=\"\")]\n\n    def stream_chat_oai(self,conversations, \n                        model:Optional[str]=None, \n                        role_mapping=None,\n                        delta_mode=False,\n                        llm_config:Dict[str,Any]={}): \n        res,contexts = self.entry.stream_chat_oai(conversations)        \n        for t in res:                        \n            yield (t,SingleOutputMeta(0,0))\n\n    async def async_stream_chat_oai(self,conversations, \n                        model:Optional[str]=None, \n                        role_mapping=None,\n                        delta_mode=False,\n                        llm_config:Dict[str,Any]={}): \n        res,contexts = self.rag.stream_chat_oai(conversations)                \n        for t in res:                                    \n            yield (t,SingleOutputMeta(0,0))\n                         \n\n    def __getattr__(self, name):        \n        return getattr(self.llm, name)\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/openai_serve.py\nimport argparse\nfrom service_server.api_server import ServerArgs, serve\nfrom service_server.llm_wrapper import LLWrapper\nfrom service_server.summary_rag import SummaryRAG\nimport byzerllm\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"OpenAI Serve\")\n    parser.add_argument(\n        \"--model\",\n        default=\"\",\n    )\n    parser.add_argument(\"--emb_model\", default=\"\")\n    parser.add_argument(\n        \"--ray_address\",\n        default=\"auto\",\n    )\n    parser.add_argument(\"--source_dir\", default=\".\", help=\"\")\n    parser.add_argument(\"--host\", default=\"\", help=\"\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"\")\n    parser.add_argument(\"--uvicorn_log_level\", default=\"info\", help=\"\")\n    parser.add_argument(\"--allow_credentials\", action=\"store_true\", help=\"\")\n    parser.add_argument(\"--allowed_origins\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--allowed_methods\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--allowed_headers\", default=[\"*\"], help=\"\")\n    parser.add_argument(\"--api_key\", default=\"\", help=\"\")\n    parser.add_argument(\"--served_model_name\", default=\"\", help=\"\")\n    parser.add_argument(\"--prompt_template\", default=\"\", help=\"\")\n    parser.add_argument(\"--ssl_keyfile\", default=\"\", help=\"\")\n    parser.add_argument(\"--ssl_certfile\", default=\"\", help=\"\")\n    parser.add_argument(\"--response_role\", default=\"assistant\", help=\"\")\n    parser.add_argument(\n        \"--collections\", default=\"\", help=\"Collection name for indexing\"\n    )\n    parser.add_argument(\n        \"--base_dir\",\n        default=\"\",\n        help=\"Path where the processed text embeddings were stored\",\n    )\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name or args.model,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile,\n    )\n    byzerllm.connect_cluster(address=args.ray_address)\n    llm = byzerllm.ByzerLLM()\n    if not server_args.served_model_name:\n        raise ValueError(\"served_model_name is required\")\n    llm.setup_default_model_name(server_args.served_model_name)\n    rag = SummaryRAG(llm=llm)    \n    llm_wrapper = LLWrapper(llm=llm, rag=rag)\n    serve(llm=llm_wrapper, args=server_args)\n\nif __name__ == \"__main__\":\n    main()\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/api_server.py\nimport os\nimport time\nimport uvicorn\nfrom fastapi import Request\nfrom fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import JSONResponse, StreamingResponse, Response\n\nfrom byzerllm.log import init_logger\nfrom byzerllm.utils import random_uuid\nfrom byzerllm.version import __version__ as version\nfrom byzerllm.utils.client import ByzerLLM, LLMRequest\nfrom byzerllm.utils.client.entrypoints.openai.serving_chat import OpenAIServingChat\nfrom byzerllm.utils.client.entrypoints.openai.serving_completion import OpenAIServingCompletion\nfrom byzerllm.utils.client.entrypoints.openai.protocol import (\n    ModelList,\n    ModelCard,\n    ModelPermission,\n    ChatCompletionRequest,\n    ErrorResponse,\n    CompletionRequest,\n    Embeddings,\n    EmbeddingsOutput,\n    EmbeddingsData,\n    EmbeddingsUsage,\n)\nfrom pydantic import BaseModel\nfrom typing import List\n\nlogger = init_logger(__name__)\n\nllm_client: ByzerLLM = None\nopenai_serving_chat: OpenAIServingChat = None\nopenai_serving_completion: OpenAIServingCompletion = None\n\nTIMEOUT_KEEP_ALIVE = 5  # seconds\n# timeout in 10 minutes. Streaming can take longer than 3 min\nTIMEOUT = float(os.environ.get(\"BYZERLLM_APISERVER_HTTP_TIMEOUT\", 600))\n\nrouter_app = FastAPI()\n\n\n@router_app.get(\"/health\")\nasync def health() -> Response:\n    \"\"\"Health check.\"\"\"\n    return Response(status_code=200)\n\n\n@router_app.get(\"/v1/models\")\nasync def show_available_models():\n    models = await openai_serving_chat.show_available_models()\n    return JSONResponse(content=models.model_dump())\n\n\n@router_app.get(\"/version\")\nasync def show_version():\n    return JSONResponse(content={\"version\": version})\n\n\n@router_app.get(\"/v1/models\", response_model=ModelList)\nasync def models() -> ModelList:\n    \"\"\"Show available models. Right now we only have one model.\"\"\"\n    model_cards = [\n        ModelCard(\n            id=\"\",\n            root=\"\",\n            permission=[ModelPermission()]\n        )\n    ]\n    return ModelList(data=model_cards)\n\n\n@router_app.post(\"/v1/completions\")\nasync def create_completion(\n        body: CompletionRequest,\n        request: Request\n):\n    generator = await openai_serving_completion.create_completion(body, request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            status_code=generator.code\n        )\n    if body.stream:\n        return StreamingResponse(\n            content=generator,\n            media_type=\"text/event-stream\"\n        )\n    else:\n        return JSONResponse(content=generator.model_dump())\n\n\n@router_app.post(\"/v1/chat/completions\")\nasync def create_chat_completion(\n        body: ChatCompletionRequest,\n        request: Request,\n):\n    \"\"\"Completion API similar to OpenAI's API.\n\n    See  https://platform.openai.com/docs/api-reference/chat/create\n    for the API specification. This API mimics the OpenAI ChatCompletion API.\n\n    NOTE: Currently we do not support the following features:\n        - function_call (Users should implement this by themselves)\n        - logit_bias (to be supported by vLLM engine)\n    \"\"\"\n    # async with async_timeout.timeout(TIMEOUT):\n\n    generator = await openai_serving_chat.create_chat_completion(body, request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            status_code=generator.code\n        )\n    if body.stream:\n        return StreamingResponse(\n            content=generator,\n            media_type=\"text/event-stream\"\n        )\n    else:\n        return JSONResponse(content=generator.model_dump())\n\n\n@router_app.post(\"/v1/embeddings\")\nasync def embed(body: Embeddings):\n    \"\"\"Given a prompt, the model will return one embedding.\n\n    Returns:\n        A response object with an embedding.\n    \"\"\"\n    embedding_id = f\"embed-{random_uuid()}\"\n\n    results_list = llm_client.emb(body.model, request=LLMRequest(instruction=body.input))\n    tokens = 0\n\n    return EmbeddingsOutput(\n        data=[\n            EmbeddingsData(\n                embedding=results.output,\n                index=i,\n                object=\"embedding\",\n            )\n            for i, results in enumerate(results_list)\n        ],\n        id=embedding_id,\n        object=\"list\",\n        created=int(time.time()),\n        model=body.model,\n        usage=EmbeddingsUsage(\n            prompt_tokens=tokens,\n            total_tokens=tokens,\n        ),\n    )\n\nclass ServerArgs(BaseModel):\n    host: str = None\n    port: int = 8000\n    uvicorn_log_level: str = \"info\"\n    allow_credentials: bool = False\n    allowed_origins: List[str] = [\"*\"]  \n    allowed_methods: List[str] = [\"*\"]\n    allowed_headers: List[str] = [\"*\"]\n    api_key: str = None\n    served_model_name: str = None\n    prompt_template: str = None\n    response_role: str = \"assistant\"\n    ssl_keyfile: str = None\n    ssl_certfile: str = None    \n\ndef serve(llm:ByzerLLM, args: ServerArgs):\n    \n    logger.info(f\"ByzerLLM API server version {version}\")\n    logger.info(f\"args: {args}\")\n\n    router_app.add_middleware(\n        CORSMiddleware,\n        allow_origins=args.allowed_origins,\n        allow_credentials=args.allow_credentials,\n        allow_methods=args.allowed_methods,\n        allow_headers=args.allowed_headers,\n    )\n    \n    if token := os.environ.get(\"BYZERLLM_API_KEY\") or args.api_key:\n\n        @router_app.middleware(\"http\")\n        async def authentication(request: Request, call_next):\n            if not request.url.path.startswith(\"/v1\"):\n                return await call_next(request)\n            if request.headers.get(\"Authorization\") != \"Bearer \" + token:\n                return JSONResponse(\n                    content={\"error\": \"Unauthorized\"},\n                    status_code=401\n                )\n            return await call_next(request)\n\n    # Register labels for metrics\n    # add_global_metrics_labels(model_name=engine_args.model)\n    global llm_client\n    llm_client = llm\n    \n    global openai_serving_chat\n    openai_serving_chat = OpenAIServingChat(\n        llm_client=llm_client,\n        response_role=args.response_role,\n        server_model_name=args.served_model_name,\n        prompt_template=args.prompt_template\n    )\n    global openai_serving_completion\n    openai_serving_completion = OpenAIServingCompletion(\n        llm_client=llm_client,\n        server_model_name=args.served_model_name,\n        prompt_template=args.prompt_template\n    )\n\n    uvicorn.run(\n        router_app,\n        host=args.host,\n        port=args.port,\n        log_level=args.uvicorn_log_level,\n        timeout_keep_alive=TIMEOUT_KEEP_ALIVE,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile\n    )\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/generator.py\nfrom typing import Optional, Dict, Any, List\nfrom openai import OpenAI\nimport json\nfrom .types import RAGConfig, ByzerSQLConfig, ModelConfig\nfrom byzerllm.utils.client import code_utils\nimport byzerllm\nimport requests\nfrom loguru import logger\nimport traceback\nfrom byzerllm.utils.nontext import TagExtractor\n\n\nclass SQLGenerator:\n\n    @byzerllm.prompt()\n    def gen_context(self, conversations: List[Dict[str, str]]) -> str:\n        \"\"\"\n        对话内容：\n        {% for msg in conversations %}\n        <{{ msg.role }}>: {{ msg.content }}\n        {% endfor %}\n\n        根据提供的对话内容，理解用户最后的问题，然后提取可能会用到的 Byzer-SQL 相关的知识。\n        \"\"\"\n\n    @byzerllm.prompt()\n    def gen_sql(\n        self,\n        conversations: List[Dict[str, str]],\n        context: str,\n        schema: str,\n        model: str = \"deepseek_chat\",\n    ) -> str:\n        \"\"\"\n        相关知识：\n        {{ context }}\n\n        Schema信息：\n        {{ schema }}\n\n        对话内容：\n        {% for msg in conversations %}\n        <{{ msg.role }}>: {{ msg.content }}\n        {% endfor %}\n\n        模型UDF函数：\n        {{ model }}\n\n        根据提供的相关知识以及对话内容，理解用户最后的问题，然后根据看到的schema 信息，生成 Byzer-SQL 语句。\n        Byzer-SQL 首先需要用 load 语法加载 schema 中的路径映射成表格，然后根据表格的信息生成对应的 Byzer-SQL 语句。\n\n        特别需要关注的是:\n        1. 用户的很多需求是要对表格的文本内容做自然语言处理，你需要使用 Byzer-SQL 中的大模型辅助函数来完成这个需求。\n        2. 使用 `llm_result` 函数处理 LLM 模型函数的响应,`llm_result` 函数得到的是一段文本，虽然我们要求模型函数只输出特定的文本，但考虑到大模型会有一些额外的字符输出，不会只是输出我们要求的文字，所以通过 `like` 来进行匹配。\n        3. 注意 Byzer-SQL 的语法，比如select 语句最后一定需要有 as <table_name>。\n        3. 输出的 Byzer-SQL 语句务必使用 <_byzer_sql_></_byzer_sql_> 标签包裹起来。\n        \"\"\"\n\n    @byzerllm.prompt()\n    def fetch_schema(self, context: str) -> str:\n        \"\"\"\n        下面是一些基础知识，包括名词术语等等：\n        {{ context }}\n\n        当理解对话内容的时候，需要参考上面的基础知识，然后更好的理解对应的表格schema信息是不是有用。\n        \"\"\"\n\n    def __init__(\n        self,\n        llm: byzerllm.ByzerLLM,\n        rag_config: RAGConfig,\n        byzer_sql_config: ByzerSQLConfig,\n        model_config: ModelConfig,\n    ) -> None:\n        self.llm = llm\n        self.schema_rag_client = OpenAI(\n            api_key=rag_config.schema_rag_api_key,\n            base_url=rag_config.schema_rag_base_url,\n        )\n        self.context_rag_client = OpenAI(\n            api_key=rag_config.context_rag_api_key,\n            base_url=rag_config.context_rag_base_url,\n        )\n        self.byzer_sql_url = byzer_sql_config.byzer_sql_url\n\n        self.llm_model = byzerllm.ByzerLLM()\n        self.llm_model.setup_default_model_name(model_config.llm_model)\n        self.gen_sql_llm_model = byzerllm.ByzerLLM()\n        self.gen_sql_llm_model.setup_default_model_name(model_config.gen_sql_llm_model)\n        self.model_config = model_config\n\n\n    def stream_chat_oai(\n        self,\n        conversations,\n        model: Optional[str] = None,\n        role_mapping=None,\n        delta_mode=False,\n        llm_config: Dict[str, Any] = {},\n    ):\n        try:\n            v, c = self._stream_chat_oai(\n                conversations=conversations,\n                model=model,\n                role_mapping=role_mapping,\n                delta_mode=delta_mode,\n                llm_config=llm_config,\n            )\n        except Exception as e:\n            error_msg = f\"Error in stream_chat_oai: {e}\\n{traceback.format_exc()}\"\n            logger.error(error_msg)\n            v, c = [str(e)], []\n\n        return v, c\n\n    def _stream_chat_oai(\n        self,\n        conversations,\n        model: Optional[str] = None,\n        role_mapping=None,\n        delta_mode=False,\n        llm_config: Dict[str, Any] = {},\n    ):\n        logger.info(f\"Starting stream_chat_oai with model: {model}\")\n        logger.debug(f\"Conversations: {conversations}\")\n        logger.debug(f\"LLM Config: {llm_config}\")\n\n        if not model:\n            model = self.llm_model.default_model_name\n        logger.info(f\"Using model: {model}\")\n\n        query = conversations[-1][\"content\"]\n\n        if (\n            \"使用四到五个字直接返回这句话的简要主题，不要解释、不要标点、不要语气词、不要多余文本，不要加粗，如果没有主题\"\n            in query\n            or \"简要总结一下对话内容，用作后续的上下文提示 prompt，控制在 200 字以内\"\n            in query\n        ):\n            chunks = self.llm.stream_chat_oai(\n                conversations=conversations,\n                model=model,\n                role_mapping=role_mapping,\n                llm_config=llm_config,\n                delta_mode=True,\n            )\n            return (chunk[0] for chunk in chunks), []\n\n        logger.info(\"Fetching context from context RAG client\")\n        context_query = self.gen_context.prompt(conversations=conversations)\n        context_response = self.context_rag_client.chat.completions.create(\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": json.dumps(\n                        {\"query\": context_query, \"only_contexts\": True},\n                        ensure_ascii=False,\n                    ),\n                }\n            ],\n            model=model,\n        )\n\n        context = context_response.choices[0].message.content\n        new_context = \"\"\n        for msg in context.split(\"\\n\"):\n            if msg.strip():\n                v = json.loads(msg)\n                new_context += v[\"source_code\"]\n        logger.info(f\"context {new_context[0:100]}...\")\n        logger.info(\"Context retrieved successfully\")\n\n        logger.info(\"Fetching schemas from schema RAG client\")\n        schema_query = self.fetch_schema.prompt(context=new_context)\n\n        response = self.schema_rag_client.chat.completions.create(\n            messages=conversations[0:-1]\n            + [\n                {\n                    \"role\": \"user\",\n                    \"content\": json.dumps(\n                        {\"query\": schema_query, \"only_contexts\": True},\n                        ensure_ascii=False,\n                    ),\n                }\n            ],\n            model=model,\n        )        \n\n        schema_str = response.choices[0].message.content\n        logger.debug(f\"Schema RAG response: {schema_str}\")\n        logger.info(\"Schema retrieved successfully\")\n\n        logger.info(\"Generating SQL string\")\n        sql_str = self.gen_sql.with_llm(self.gen_sql_llm_model).run(\n            conversations=conversations,\n            context=new_context,\n            schema=schema_str,\n            model=self.model_config.sql_func_llm_model,\n        )\n\n        logger.info(\"SQL string generated\")\n        logger.debug(f\"Generated SQL string: {sql_str}\")\n\n        # Extract SQL code from tags\n        tag_extractor = TagExtractor(sql_str)\n        extracted_tags = tag_extractor.extract()\n        sql = extracted_tags.content[0].content\n        logger.info(f\"Extracted SQL: {sql}\")\n\n        logger.info(f\"Sending SQL to Byzer SQL URL: {self.byzer_sql_url}\")\n        try:\n            response = requests.post(\n                self.byzer_sql_url,\n                data={\n                    \"sql\": sql,\n                    \"owner\": \"hello\",\n                    \"sessionPerUser\": True,\n                    \"sessionPerRequest\": True,\n                    \"includeSchema\": True,\n                },\n            )\n            response.raise_for_status()\n            result = response.content.decode(\"utf-8\")\n            logger.info(\"SQL execution result decoded\")\n            logger.debug(f\"SQL execution result: {result}\")\n            return (item for item in [result]), []\n        except requests.exceptions.RequestException as e:\n            logger.error(f\"Failed to send SQL to Byzer SQL URL: {e}\")\n            if hasattr(e.response, \"content\"):\n                error_content = e.response.content.decode(\"utf-8\")\n                logger.error(f\"Server response: {error_content}\")\n            else:\n                logger.error(\"No server response available\")\n            raise Exception(f\"Byzer SQL request failed: {str(e)}\") from e\n\n\n##File: /Users/allwefantasy/projects/super_analysis/setup.py\nfrom setuptools import setup, find_packages\nimport os\n\n# Read requirements from requirements.txt\nwith open('requirements.txt') as f:\n    requirements = f.read().splitlines()\n\n# Read version from version.py\nwith open(os.path.join('src', 'superanalysis', 'version.py'), 'r') as f:\n    exec(f.read())\n\nsetup(\n    name=\"super-analysis\",\n    version=__version__,\n    packages=find_packages(where=\"src\"),\n    package_dir={\"\": \"src\"},\n    install_requires=requirements,\n    entry_points={\n        \"console_scripts\": [\n            \"super-analysis.serve=superanalysis.serve.cli:main\",\n            \"super-analysis.convert=superanalysis.serve.convert:main\",\n            \"superanalysis.api=superanalysis.serve.rest:main\",\n        ],\n    },\n    author=\"allwefantasy\",\n    author_email=\"allwefantasy@gmail.com\",\n    description=\"A package for super analysis with ByzerLLM\",\n    long_description=open(\"README.md\").read(),\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/yourusername/super-analysis\",\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n    ],\n    python_requires='>=3.6',\n)\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/convert.py\nimport os\nimport argparse\nimport pandas as pd\nimport hashlib\nfrom typing import List, Dict\n\ndef get_csv_files(directory: str) -> List[str]:\n    \"\"\"Recursively get all CSV files in the given directory.\"\"\"\n    csv_files = []\n    for root, _, files in os.walk(directory):\n        for file in files:\n            if file.endswith('.csv'):\n                csv_files.append(os.path.join(root, file))\n    return csv_files\n\ndef extract_schema(df: pd.DataFrame) -> Dict[str, str]:\n    \"\"\"Extract schema from pandas DataFrame.\"\"\"\n    schema = {}\n    for column in df.columns:\n        dtype = str(df[column].dtype)\n        if dtype == 'object':\n            schema[column] = 'STRING'\n        elif dtype.startswith('int'):\n            schema[column] = 'INTEGER'\n        elif dtype.startswith('float'):\n            schema[column] = 'FLOAT'\n        elif dtype == 'bool':\n            schema[column] = 'BOOLEAN'\n        elif dtype.startswith('datetime'):\n            schema[column] = 'TIMESTAMP'\n        else:\n            schema[column] = 'STRING'  # Default to STRING for unknown types\n    return schema\n\ndef generate_schema_text(schema: Dict[str, str], csv_file: str, df: pd.DataFrame, include_rows_num: int) -> str:\n    \"\"\"Generate schema text from the schema dictionary and include the CSV file name and sample data.\"\"\"\n    schema_lines = [f\"CSV File: {csv_file}\\n\"]\n    schema_lines.extend([f\"{field}: {dtype}\" for field, dtype in schema.items()])\n    \n    if include_rows_num > 0:\n        schema_lines.append(f\"\\nSample Data (first {include_rows_num} rows):\")\n        sample_data = df.head(include_rows_num).to_string(index=False)\n        schema_lines.append(sample_data)\n    \n    return \"\\n\".join(schema_lines)\n\ndef save_schema(schema_text: str, csv_file: str, output_dir: str) -> None:\n    \"\"\"Save schema text to a file in the output directory.\"\"\"\n    # Generate MD5 hash of the schema text\n    file_name = hashlib.md5(schema_text.encode()).hexdigest() + '.md'\n    output_path = os.path.join(output_dir, file_name)\n    \n    with open(output_path, 'w') as f:\n        f.write(schema_text)\n    print(f\"Schema for {csv_file} saved to {output_path}\")\n\ndef convert(data_dir: str, doc_dir: str, include_rows_num: int) -> None:\n    \"\"\"Main conversion function.\"\"\"\n    csv_files = get_csv_files(data_dir)\n    \n    for csv_file in csv_files:\n        print(f\"Processing {csv_file}\")\n        df = pd.read_csv(csv_file)\n        schema = extract_schema(df)\n        schema_text = generate_schema_text(schema, csv_file, df, include_rows_num)\n        save_schema(schema_text, csv_file, doc_dir)\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Convert CSV files to schema documents\")\n    parser.add_argument(\"--data_dir\", required=True, help=\"Directory containing CSV files\")\n    parser.add_argument(\"--doc_dir\", required=True, help=\"Output directory for schema documents\")\n    parser.add_argument(\"--include-rows-num\", type=int, default=0, help=\"Number of sample rows to include in the schema\")\n    \n    args = parser.parse_args()\n    \n    if not os.path.exists(args.doc_dir):\n        os.makedirs(args.doc_dir)\n    \n    convert(args.data_dir, args.doc_dir, args.include_rows_num)\n\nif __name__ == \"__main__\":\n    main()\n\n##File: /Users/allwefantasy/projects/super_analysis/README.md\n# Super Analysis\n\nSuper Analysis 支持 NLP2SQL 以及 使用 SQL 对表中的文本字段做自然语言分析。\n\n## 安装\n\n你可以使用 pip 安装 Super Analysis：\n\n```bash\ngit clone https://github.com/allwefantasy/super_analysis\ncd super-analysis\npip install -e .\n```\n\n或者使用 Makefile 来管理项目：\n\n```bash\n# 清理项目\nmake clean\n\n# 构建项目\nmake build\n\n# 创建wheel包\nmake wheel\n\n# 安装项目\nmake install\n\n# 卸载项目\nmake uninstall\n\n# 以开发模式安装项目\nmake develop\n\n# 运行测试\nmake test\n```\n\n\n\n## 示例数据\n\n下载电影数据集： https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset/download?datasetVersionNumber=7\n\n\n## 启动分析服务\n\n整个 super-analysis 服务分为四个部分：\n\n1. byzer-sql 分布式执行引擎 \n2. schema 文档知识库\n3. context 文档知识库\n4. super-analysis 自身\n\n在启动 super-analysis 之前，你需要启动 schema 文档知识库和 context 文档知识库以及 byzer-sql 引擎。\n\n## 部署 Deepseek 模型代理\n\n在启动其他服务之前，我们需要先部署 Deepseek 模型。你可以使用以下命令来部署：\n\n```bash\nmake deploy_deepseek\n```\n\n或者直接运行以下命令：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--worker_concurrency 1000 \\\n--num_workers 1 \\\n--infer_params saas.base_url=\"https://api.deepseek.com/v1\" saas.api_key=${MODEL_DEEPSEEK_TOKEN} saas.model=deepseek-chat \\\n--model deepseek_chat\n```\n\n注意：请确保在运行此命令之前，已经设置了环境变量 `MODEL_DEEPSEEK_TOKEN`。\n\n## 部署 Byzer-SQL\n\n在你的电脑上参考 [这个文档](./docs/4.3.1%20安装与配置%20Byzer-SQL.pdf) 完成部署.\n\n> 启动的时候需要在安装有 super-analysis 的conda环境中启动。\n\n\n\n当 byzer-sql 部署完成后，注册账号为 `hello`, 然后进入到 byzer-sql 的控制台中执行以下脚本：\n\n```sql\n!byzerllm setup single;\n\nrun command as LLM.`` where \naction=\"infer\"\nand reconnect=\"true\"\nand pretrainedModelType=\"saas/openai\"\nand udfName=\"deepseek_chat\";\n```\n\n## 启动其他服务\n\n现在可以按如下方式做一些数据预处理或者启动相关服务：\n\n```bash\n## 将电影数据集的csv文件转换为schema文档\nsuper-analysis.convert --data_dir /Users/allwefantasy/data/movice --doc_dir /Users/allwefantasy/data/movice/schemas/\n\n# 启动schema文档知识库\nauto-coder doc serve \\\n--model deepseek_chat --index_filter_workers 100 \\\n--tokenizer_path /Users/allwefantasy/Downloads/tokenizer.json \\\n--doc_dir /Users/allwefantasy/data/movice/schemas/ \\\n--port 8001\n\n# 下载 Byzer-SQL 专门为大模型优化的文档\ngit clone https://github.com/allwefantasy/llm_friendly_packages\n\n# 启动文档知识库\nauto-coder doc serve \\\n--model deepseek_chat --index_filter_workers 100 \\\n--tokenizer_path /Users/allwefantasy/Downloads/tokenizer.json \\\n--doc_dir  /Users/allwefantasy/projects/llm_friendly_packages/github.com/allwefantasy \\\n--port 8002\n\n# 启动兼容 OpenAI Server 的分析服务\nsuper-analysis.serve --served-model-name deepseek_chat --port 8000 \\\n--schema-rag-base-url http://127.0.0.1:8001/v1 \\\n--context-rag-base-url http://127.0.0.1:8002/v1 \\\n--byzer-sql-url http://127.0.0.1:9003/run/script\n```\n\n现在你可以用 OpenAI SDK 去使用了。 具体测试和接口使用参考 [openai_local_api.ipynb](./notebooks/openai_local_api.ipynb)\n\n\n## 非 OpenAI 兼容接口\n\n除了兼容 OpenAI 的接口外，Super Analysis 还提供了一个简单的 REST API 接口，可以直接与模型进行交互。以下是如何使用这个接口：\n\n### 启动服务\n\n使用以下命令启动 REST API 服务：\n\n```bash\nsuperanalysis.api --schema-rag-base-url <schema_rag_url> \\\n                  --schema-rag-api-key <schema_rag_api_key> \\\n                  --context-rag-base-url <context_rag_url> \\\n                  --context-rag-api-key <context_rag_api_key> \\\n                  --byzer-sql-url <byzer_sql_url> \\\n                  --llm-model <llm_model_name> \\\n                  --gen-sql-llm-model <gen_sql_llm_model_name> \\\n                  --host <host> \\\n                  --port <port>\n```\n\n请替换尖括号中的值为实际的配置信息。\n\n### API 端点\n\n服务启动后，将提供以下 API 端点：\n\n#### POST /chat\n\n此端点用于与模型进行对话。\n\n请求体格式：\n\n```json\n{\n  \"conversations\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"你的问题或指令\"\n    }\n  ],\n  \"model\": \"可选的模型名称\"\n}\n```\n\n响应格式：\n\n```json\n{\n  \"response\": \"模型的回答\"\n}\n```\n\n### 示例使用\n\n以下是使用 Python 请求库与 API 交互的示例：\n\n```python\nimport requests\nimport json\n\nurl = \"http://localhost:8000/chat\"\npayload = {\n    \"conversations\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"你好，请介绍一下你自己。\"\n        }\n    ],\n    \"model\": \"your_model_name\"\n}\n\nresponse = requests.post(url, json=payload)\nresult = response.json()\n\nprint(result[\"response\"])\n```\n\n这个 API 相比兼容 OpenAI 接口，可以在请求中提供提供更多可选参数。\n\n## 使用方法\n\n安装完成后，你可以使用 `super-analysis.serve` 命令来启动 API 服务器：\n\n```bash\nsuper-analysis.serve --served-model-name your_model_name\n```\n\n### 命令行参数\n\n以下是可用的命令行参数：\n\n- `--host`: 服务器绑定的主机地址（默认：0.0.0.0）\n- `--port`: 服务器绑定的端口（默认：8000）\n- `--uvicorn-log-level`: Uvicorn 日志级别（默认：info）\n- `--allow-credentials`: 允许凭证（默认：False）\n- `--allowed-origins`: 允许的源（默认：[\"*\"]）\n- `--allowed-methods`: 允许的方法（默认：[\"*\"]）\n- `--allowed-headers`: 允许的头部（默认：[\"*\"]）\n- `--api-key`: 用于认证的 API 密钥\n- `--served-model-name`: 要服务的模型名称（必需）\n- `--prompt-template`: 提示模板\n- `--response-role`: 响应角色（默认：assistant）\n- `--ssl-keyfile`: SSL 密钥文件路径\n- `--ssl-certfile`: SSL 证书文件路径\n\n例如：\n\n```bash\nsuper-analysis.serve --served-model-name deepseek_chat --port 8000\n```\n\n\n## 抽取csv文件schema\n\nThe `super-analysis.convert` command allows you to convert CSV files to schema documents. This is useful for generating metadata about your CSV files that can be used for further analysis or documentation.\n\n#### Usage\n\n```\nsuper-analysis.convert --data_dir <path_to_csv_files> --doc_dir <output_directory> [--include-rows-num <number_of_sample_rows>]\n```\n\nArguments:\n- `--data_dir`: Directory containing CSV files (required)\n- `--doc_dir`: Output directory for schema documents (required)\n- `--include-rows-num`: Number of sample rows to include in the schema (optional, default is 0)\n\nThe utility will:\n1. Recursively find all CSV files in the specified directory\n2. Extract the schema from each CSV file\n3. Generate a schema text that includes:\n   - The CSV file name\n   - Field names and their data types\n   - Sample data (if `--include-rows-num` is specified)\n4. Save the schema text as a markdown file in the output directory\n\n## 联系方式\n\n如有任何问题或建议，请联系 allwefantasy@gmail.com。\n\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/rest.py\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nfrom typing import List, Optional\nimport uvicorn\nimport argparse\nfrom superanalysis.sql.generator import SQLGenerator\nfrom superanalysis.sql.types import RAGConfig, ByzerSQLConfig, ModelConfig\nimport byzerllm\n\napp = FastAPI()\n\nclass ChatMessage(BaseModel):\n    role: str\n    content: str\n\nclass ChatRequest(BaseModel):\n    conversations: List[ChatMessage]    \n    model_config: Optional[ModelConfig] = None\n\nclass ChatResponse(BaseModel):\n    response: str\n\n@app.post(\"/chat\", response_model=ChatResponse)\nasync def chat(request: ChatRequest):\n    try:\n        sql_generator = SQLGenerator(\n            llm=llm,\n            rag_config=rag_config,\n            byzer_sql_config=byzer_sql_config,\n            model_config=request.model_config\n        )\n        result, _ = sql_generator.stream_chat_oai(\n            conversations=[{\"role\": msg.role, \"content\": msg.content} for msg in request.conversations],\n            model=request.model_config.llm_model\n        )\n        return ChatResponse(response=\"\".join(result))\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"SuperAnalysis API Server\")\n    parser.add_argument(\"--host\", type=str, default=\"0.0.0.0\", help=\"Host to bind the server to\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"Port to bind the server to\")\n    parser.add_argument(\"--schema-rag-base-url\", type=str, required=True, help=\"Schema RAG base URL\")\n    parser.add_argument(\"--schema-rag-api-key\", type=str, required=True, help=\"Schema RAG API key\")\n    parser.add_argument(\"--context-rag-base-url\", type=str, required=True, help=\"Context RAG base URL\")\n    parser.add_argument(\"--context-rag-api-key\", type=str, required=True, help=\"Context RAG API key\")\n    parser.add_argument(\"--byzer-sql-url\", type=str, required=True, help=\"Byzer SQL URL\")\n    parser.add_argument(\"--data-dir\", type=str, default=\"\", help=\"Data directory\")\n    parser.add_argument(\"--owner\", type=str, default=\"hello\", help=\"Owner of the Byzer SQL session\")\n    parser.add_argument(\"--llm-model\", type=str, required=True, help=\"LLM model for general use\")\n    parser.add_argument(\"--gen-sql-llm-model\", type=str, required=True, help=\"LLM model for SQL generation\")\n    \n    args = parser.parse_args()\n\n    global rag_config\n\n    rag_config = RAGConfig(\n        schema_rag_base_url=args.schema_rag_base_url,\n        schema_rag_api_key=args.schema_rag_api_key,\n        context_rag_base_url=args.context_rag_base_url,\n        context_rag_api_key=args.context_rag_api_key\n    )\n    global byzer_sql_config\n    byzer_sql_config = ByzerSQLConfig(\n        byzer_sql_url=args.byzer_sql_url,\n        data_dir=args.data_dir,\n        owner=args.owner\n    )\n\n    global llm\n    llm = byzerllm.ByzerLLM()\n    llm.setup_default_model_name(args.llm_model)\n\n    uvicorn.run(app, host=args.host, port=args.port)\n\nif __name__ == \"__main__\":\n    main()\n\n##File: /Users/allwefantasy/projects/super_analysis/Makefile\n# Makefile for super-analysis project\n\n# Python interpreter\nPYTHON := python\n\n# Project name\nPROJECT_NAME := super-analysis\n\n# Directories\nSRC_DIR := src\nDIST_DIR := dist\nBUILD_DIR := build\n\n# Commands\nSETUP := $(PYTHON) setup.py\n\n.PHONY: all clean build wheel deploy_deepseek dist\n\nall: clean build wheel dist\n\ndeploy_deepseek:\n\tbyzerllm deploy --pretrained_model_type saas/openai \\\n\t--cpus_per_worker 0.001 \\\n\t--gpus_per_worker 0 \\\n\t--worker_concurrency 1000 \\\n\t--num_workers 1 \\\n\t--infer_params saas.base_url=\"https://api.deepseek.com/v1\" saas.api_key=${MODEL_DEEPSEEK_TOKEN} saas.model=deepseek-chat \\\n\t--model deepseek_chat\n\nclean:\n\t@echo \"Cleaning up...\"\n\trm -rf $(BUILD_DIR) $(DIST_DIR) *.egg-info\n\tfind . -type d -name \"__pycache__\" -exec rm -rf {} +\n\tfind . -type f -name \"*.pyc\" -delete\n\nbuild:\n\t@echo \"Building the project...\"\n\t$(SETUP) build\n\nwheel:\n\t@echo \"Creating wheel package...\"\n\t$(SETUP) bdist_wheel\n\ninstall:\n\t@echo \"Installing the package...\"\n\t$(PYTHON) -m pip install .\n\nuninstall:\n\t@echo \"Uninstalling the package...\"\n\t$(PYTHON) -m pip uninstall -y $(PROJECT_NAME)\n\ndevelop:\n\t@echo \"Installing the package in editable mode...\"\n\t$(PYTHON) -m pip install -e .\n\ntest:\n\t@echo \"Running tests...\"\n\t$(PYTHON) -m unittest discover tests\n\ndist: wheel\n\t@echo \"Creating distribution package...\"\n\t$(eval VERSION := $(shell python -c \"import sys; sys.path.append('src'); from superanalysis.version import __version__; print(__version__)\"))\n\trm -rf super_analysis-$(VERSION).tar.gz\n\tmkdir -p super_analysis-$(VERSION)\n\tcp -r $(DIST_DIR)/* super_analysis-$(VERSION)/\n\tcp \"/Users/allwefantasy/projects/super_analysis/docs/4.3.1 安装与配置 Byzer-SQL.pdf\" super_analysis-$(VERSION)/\n\tcp \"/Users/allwefantasy/projects/super_analysis/docs/deploy.md\" super_analysis-$(VERSION)/README.md\n\tcp \"/Users/allwefantasy/projects/super_analysis/notebooks/openai_local_api.ipynb\" super_analysis-$(VERSION)/\n\ttar -czf super_analysis-$(VERSION).tar.gz super_analysis-$(VERSION)\n\trm -rf super_analysis-$(VERSION)\n\t@echo \"Distribution package created: super_analysis-$(VERSION).tar.gz\"\n\n##File: /Users/allwefantasy/projects/super_analysis/docs/deploy.md\n\n# 🚀 Super Analysis 部署指南\n\n本指南将帮助你部署 Super Analysis 系统，包括安装必要组件、配置服务和启动系统。\n\n---\n\n## 📦 安装 Super Analysis\n\n\n```bash\npip install -U auto-coder\npip install super_analysis-0.1.0-py3-none-any.whl\n```\n\n\n---\n\n## 🤖 部署 Deepseek 模型代理\n\n在启动其他服务之前，我们需要先部署 Deepseek 模型：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--worker_concurrency 1000 \\\n--num_workers 1 \\\n--infer_params saas.base_url=\"https://api.deepseek.com/v1\" saas.api_key=${MODEL_DEEPSEEK_TOKEN} saas.model=deepseek-chat \\\n--model deepseek_chat\n```\n\n注意：确保已设置环境变量 `MODEL_DEEPSEEK_TOKEN`。\n\n---\n\n## 🛠️ 部署 Byzer-SQL\n\n参考 [安装与配置 Byzer-SQL 文档](./4.3.1%20安装与配置%20Byzer-SQL.pdf) 完成部署。\n\n> 启动时需要在安装有 super-analysis 的 conda 环境中启动。\n\n当 byzer-sql 部署完成后，注册账号为 `hello`，然后在 byzer-sql 控制台中执行：\n\n```sql\n!byzerllm setup single;\n\nrun command as LLM.`` where \naction=\"infer\"\nand reconnect=\"true\"\nand pretrainedModelType=\"saas/openai\"\nand udfName=\"deepseek_chat\";\n```\n\n---\n\n## 示例数据\n\n下载电影数据集： https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset/download?datasetVersionNumber=7\n\n---\n\n## 📊 数据预处理和服务启动\n\n1. 抽取电影数据集schema：\n\n```bash\nsuper-analysis.convert --data_dir /Users/allwefantasy/data/movice --doc_dir /Users/allwefantasy/data/movice/schemas/\n```\n\n你还可以添加 --include-rows-num 5 让系统在生成 schema 文档时同时提供一些示例数据。方便大模型更好的对这个表进行认知。\n\n\n2. 启动 schema 文档知识库：\n\n```bash\nauto-coder doc serve \\\n--model deepseek_chat --index_filter_workers 100 \\\n--tokenizer_path /Users/allwefantasy/Downloads/tokenizer.json \\\n--doc_dir /Users/allwefantasy/data/movice/schemas/ \\\n--port 8001\n```\n\n3. 下载 Byzer-SQL 文档并启动文档知识库：\n\n```bash\ngit clone https://github.com/allwefantasy/llm_friendly_packages\n\nauto-coder doc serve \\\n--model deepseek_chat --index_filter_workers 100 \\\n--tokenizer_path /Users/allwefantasy/Downloads/tokenizer.json \\\n--doc_dir  /Users/allwefantasy/projects/llm_friendly_packages/github.com/allwefantasy \\\n--port 8002\n```\n\n4. 启动兼容 OpenAI Server 的分析服务：\n\n```bash\nsuper-analysis.serve --served-model-name deepseek_chat --port 8000 \\\n--schema-rag-base-url http://127.0.0.1:8001/v1 \\\n--context-rag-base-url http://127.0.0.1:8002/v1 \\\n--byzer-sql-url http://127.0.0.1:9003/run/script\n```\n\n---\n\n现在，Super Analysis 系统已经完全部署并启动。你可以开始使用 OpenAI SDK 进行测试和接口调用。具体测试和接口使用方法请参考 [openai_local_api.ipynb](./openai_local_api.ipynb)。\n\n🎉 恭喜！你已经成功部署了 Super Analysis 系统。如有任何问题，请随时查阅文档或联系支持团队。\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/version.py\n__version__ = '0.1.0'\n\n##File: .auto-coder/libs/llm_friendly_packages/github.com/allwefantasy/byzer-llm/README.md\n# byzerllm 大模型编程快速指南\n\n本文示例 [Notebook](../../notebooks/003_byzerllm_大模型编程快速指南.ipynb)\n\n## 安装\n\n```bash\npip install -U byzerllm\nray start --head\n```\n## 启动一个模型代理\n\nbyzerllm 支持私有化模型或者SaaS模型的部署。\n\n这里以 deepseek 官方API 为例：\n\n```bash\neasy-byzerllm deploy deepseek-chat --token xxxxx --alias deepseek_chat\n```\n\n或者跬基流动API:\n\n```bash\neasy-byzerllm deploy alibaba/Qwen1.5-110B-Chat --token xxxxx --alias qwen110b_chat\n```\n\n将上面的 API KEY 替换成你们自己的。\n\n如果你想部署私有化模型或者对接 Ollama 等更多需求，参考 [002_使用byzerllm进行模型部署.md](./002_使用byzerllm进行模型部署.md) 或者 [README.md](../../README.md)。\n\n之后，你就可以在代码里使用  deepseek_chat 或者 qwen110b_chat  访问模型了。\n\n## hello world\n\n来和我们的大模型打个招呼:\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\n@byzerllm.prompt(llm=llm)\ndef hello(q:str) ->str:\n    '''\n    你好, {{ q }}\n    '''\n\ns = hello(\"你是谁\")    \nprint(s)\n\n## 输出:\n## '你好！我是一个人工智能助手，专门设计来回答问题、提供信息和帮助解决问题。如果你有任何疑问或需要帮助，请随时告诉我。'\n```\n\n恭喜，你和大模型成功打了招呼！\n\n可以看到，我们通过 `@byzerllm.prompt` 装饰器，将一个方法转换成了一个大模型的调用，然后这个方法的主题是一段文本，文本中\n使用了 jinja2 模板语法，来获得方法的参数。当正常调用该方法时，实际上就发起了和大模型的交互，并且返回了大模型的结果。\n\n在 byzerllm 中，我们把这种方法称之为 prompt 函数。\n\n## 查看发送给大模型的prompt\n\n很多情况你可能需要调试，查看自己的 prompt 渲染后到底是什么样子的，这个时候你可以通过如下方式\n获取渲染后的prompt:\n\n```python\nhello.prompt(\"你是谁\")\n## '你好, 你是谁'\n```            \n\n## 动态换一个模型\n\n前面的 hello 方法在初始化的时候，我们使用了默认的模型 deepseek_chat，如果我们想换一个模型，可以这样做：\n\n```python\nhello.with_llm(llm).run(\"你是谁\")\n## '你好！我是一个人工智能助手，专门设计来回答问题、提供信息和帮助解决问题。如果你有任何疑问或需要帮助，请随时告诉我。'\n```\n\n通过 with_llm 你可以设置一个新的 llm 对象，然后调用 run 方法，就可以使用新的模型了。\n\n## 超长文本生成\n\n我们知道，大模型一次生成的长度其实是有限的，如果你想生成超长文本，你可能需手动的不断获得\n生成结果，然后把他转化为输入，然后再次生成，这样的方式是比较麻烦的。\n\nbyzerllm 提供了更加易用的 API :\n\n```python\nimport byzerllm\nfrom byzerllm import ByzerLLM\n\nllm = ByzerLLM.from_default_model(\"deepseek_chat\")\n\n@byzerllm.prompt()\ndef tell_story() -> str:\n    \"\"\"\n    讲一个100字的故事。    \n    \"\"\"\n\n\ns = (\n    tell_story.with_llm(llm)\n    .with_response_markers()\n    .options({\"llm_config\": {\"max_length\": 10}})\n    .run()\n)\nprint(s)\n\n## 从前，森林里住着一只聪明的小狐狸。一天，它发现了一块闪闪发光的宝石。小狐狸决定用这块宝石帮助森林里的动物们。它用宝石的光芒指引迷路的小鹿找到了回家的路，用宝石的温暖治愈了受伤的小鸟。从此，小狐狸成了森林里的英雄，动物们都感激它的善良和智慧。\n```\n\n实际核心部分就是这一行：\n\n```python\ntell_story.with_llm(llm)\n    .with_response_markers()    \n    .run()\n```\n\n我们只需要调用 `with_response_markers` 方法，系统就会自动的帮我们生成超长文本。\n在上面的案例中，我们通过\n\n```python\n.options({\"llm_config\": {\"max_length\": 10}})\n```\n\n认为的限制大模型一次交互最多只能输出10个字符，但是系统依然自动完成了远超过10个字符的文本生成。\n\n## 对象输出\n\n前面我们的例子都是返回字符串，但是我们也可以返回对象，这样我们就可以更加灵活的处理返回结果。\n\n```python\nimport pydantic \n\nclass Story(pydantic.BaseModel):\n    '''\n    故事\n    '''\n\n    title: str = pydantic.Field(description=\"故事的标题\")\n    body: str = pydantic.Field(description=\"故事主体\")\n\n@byzerllm.prompt()\ndef tell_story()->Story:\n    '''\n    讲一个100字的故事。    \n    '''\n\ns = tell_story.with_llm(llm).run()\nprint(isinstance(s, Story))\nprint(s.title)\n\n## True\n## 勇敢的小鸟\n```\n\n可以看到，我们很轻松的将输出转化为格式化输出。\n\n## 自定义字段抽取\n\n前面的结构化输出，其实会消耗更多token,还有一种更加精准的结构化输出方式。\n比如让大模型生成一个正则表达式，但实际上大模型很难准确只输出一个正则表达式，这个时候我们可以通过自定义抽取函数来获取我们想要的结果。\n\n\n```python\nfrom loguru import logger\nimport re\n\n@byzerllm.prompt()\ndef generate_regex_pattern(desc: str) -> str:\n    \"\"\"\n    根据下面的描述生成一个正则表达式，要符合python re.compile 库的要求。\n\n    {{ desc }}\n\n    最后生成的正则表达式要在<REGEX></REGEX>标签对里。\n    \"\"\"    \n\ndef extract_regex_pattern(regex_block: str) -> str:    \n    pattern = re.search(r\"<REGEX>(.*)</REGEX>\", regex_block, re.DOTALL)\n    if pattern is None:\n        logger.warning(\"No regex pattern found in the generated block:\\n {regex_block}\")\n        raise None\n    return pattern.group(1)\n\npattern = \"匹配一个邮箱地址\"\nv = generate_regex_pattern.with_llm(llm).with_extractor(extract_regex_pattern).run(desc=pattern)\nprint(v)\n## ^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\n```\n\n在上面的例子里，我们根据一句话生成一个正则表达式。我们通过 `with_extractor` 方法，传入了一个自定义的抽取函数，这个函数会在大模型生成结果后，对结果进行处理，然后返回我们想要的结果。\n\n我们在 prompt 明确说了，生成的结果要放到 `<REGEX></REGEX>` 标签对里，然后我们通过 extract_regex_pattern 函数，从结果中提取出了我们想要的正则表达式。\n\n## 在实例方法中使用大模型\n\n```python\nimport byzerllm\ndata = {\n    'name': 'Jane Doe',\n    'task_count': 3,\n    'tasks': [\n        {'name': 'Submit report', 'due_date': '2024-03-10'},\n        {'name': 'Finish project', 'due_date': '2024-03-15'},\n        {'name': 'Reply to emails', 'due_date': '2024-03-08'}\n    ]\n}\n\n\nclass RAG():\n    def __init__(self):        \n        self.llm = byzerllm.ByzerLLM()\n        self.llm.setup_template(model=\"deepseek_chat\",template=\"auto\")\n        self.llm.setup_default_model_name(\"deepseek_chat\")        \n    \n    @byzerllm.prompt(lambda self:self.llm)\n    def generate_answer(self,name,task_count,tasks)->str:\n        '''\n        Hello {{ name }},\n\n        This is a reminder that you have {{ task_count }} pending tasks:\n        {% for task in tasks %}\n        - Task: {{ task.name }} | Due: {{ task.due_date }}\n        {% endfor %}\n\n        Best regards,\n        Your Reminder System\n        '''        \n\nt = RAG()\n\nresponse = t.generate_answer(**data)\nprint(response)\n\n## 输出:\n## Hello Jane Doe,\n##I hope this message finds you well. I wanted to remind you of your 3 pending tasks to ensure you stay on track:\n## 1. **Submit report** - This task is due on **2024-03-10**. Please ensure that you allocat\n```\n\n这里我们给了个比较复杂的例子，但我们可以看到，给一个实例prompt方法和普通prompt 方法差异不大。\n唯一的区别是如果你希望在定义的时候就指定大模型，使用一个lambda函数返回实例的 llm 对象即可。\n\n```python\n@byzerllm.prompt(lambda self:self.llm)\n```\n\n你也可以不返回，在调用的时候通过 `with_llm` 方法指定 llm 对象。\n\n此外，这个例子也展示了如何通过jinja2模板语法，来处理复杂的结构化数据。\n\n## 通过 Python 代码处理复杂入参\n\n上面的一个例子中，我们通过 jinja2 模板语法，来处理复杂的结构化数据，但是有时候我们可能需要更加复杂的处理，这个时候我们可以通过 Python 代码来处理。\n\n```python\nimport byzerllm\n\ndata = {\n    'name': 'Jane Doe',\n    'task_count': 3,\n    'tasks': [\n        {'name': 'Submit report', 'due_date': '2024-03-10'},\n        {'name': 'Finish project', 'due_date': '2024-03-15'},\n        {'name': 'Reply to emails', 'due_date': '2024-03-08'}\n    ]\n}\n\n\nclass RAG():\n    def __init__(self):        \n        self.llm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n    \n    @byzerllm.prompt()\n    def generate_answer(self,name,task_count,tasks)->str:\n        '''\n        Hello {{ name }},\n\n        This is a reminder that you have {{ task_count }} pending tasks:\n            \n        {{ tasks }}\n\n        Best regards,\n        Your Reminder System\n        '''\n        \n        tasks_str = \"\\n\".join([f\"- Task: {task['name']} | Due: { task['due_date'] }\" for task in tasks])\n        return {\"tasks\": tasks_str}\n\nt = RAG()\n\nresponse = t.generate_answer.with_llm(t.llm).run(**data)\nprint(response)\n\n## Just a gentle nudge to keep you on track with your pending tasks. Here's a quick recap:....\n```\n\n在这个例子里，我们直接把 tasks 在方法体里进行处理，然后作为一个字符串返回，最够构建一个字典，字典的key为 tasks,然后\n你就可以在 docstring 里使用 `{{ tasks }}` 来引用这个字符串。\n\n这样对于很复杂的入参，就不用谢繁琐的 jinja2 模板语法了。\n\n## 如何自动实现一个方法\n\n比如我定义一个签名，但是我不想自己实现里面的逻辑，让大模型来实现。这个在 byzerllm 中叫 function impl。我们来看看怎么\n实现:\n\n```python\nimport pydantic\nclass Time(pydantic.BaseModel):\n    time: str = pydantic.Field(...,description=\"时间，时间格式为 yyyy-MM-dd\")\n\n\n@llm.impl()\ndef calculate_current_time()->Time:\n    '''\n    计算当前时间\n    '''\n    pass \n\n\ncalculate_current_time()\n#output: Time(time='2024-06-14')\n```\n\n在这个例子里，我们定义了一个 calculate_current_time 方法，但是我们没有实现里面的逻辑，我们通过 `@llm.impl()` 装饰器，让大模型来实现这个方法。\n为了避免每次都要“生成”这个方法，导致无法适用，我们提供了缓存，用户可以按如下方式打印速度：\n\n```python\nstart = time.monotonic()\ncalculate_current_time()\nprint(f\"first time cost: {time.monotonic()-start}\")\n\nstart = time.monotonic()\ncalculate_current_time()\nprint(f\"second time cost: {time.monotonic()-start}\")\n\n# output:\n# first time cost: 6.067266260739416\n# second time cost: 4.347506910562515e-05\n```\n可以看到，第一次执行花费了6s,第二次几乎是瞬间完成的，这是因为第一次执行的时候，我们实际上是在生成这个方法，第二次执行的时候，我们是执行已经生成好的代码，所以速度会非常快。你可以显示的调用 `llm.clear_impl_cache()` 清理掉函数缓存。\n\n## Stream 模式\n\n前面的例子都是一次性生成结果，但是有时候我们可能需要一个流式的输出，这个时候我们可能需要用底层一点的API来完成了：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\nv = llm.stream_chat_oai(model=\"deepseek_chat\",conversations=[{\n    \"role\":\"user\",\n    \"content\":\"你好，你是谁\",\n}],delta_mode=True)\n\nfor t in v:\n    print(t,flush=True)  \n\n# 你好\n# ！\n# 我\n# 是一个\n# 人工智能\n# 助手\n# ，\n# 旨在\n# 提供\n# 信息\n# 、\n# 解答\n# 问题....\n```\n\n如果你不想要流式输出，但是想用底层一点的API，你可以使用 `llm.chat_oai` 方法：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\nv = llm.chat_oai(model=\"deepseek_chat\",conversations=[{\n    \"role\":\"user\",\n    \"content\":\"你好，你是谁\",\n}])\n\nprint(v[0].output)\n## 你好！我是一个人工智能助手，旨在提供信息、解答问题和帮助用户解决问题。如果你有任何问题或需要帮助，请随时告诉我。\n```\n\n## Function Calling \n\nbyzerllm 可以不依赖模型自身就能提供 function calling 支持，我们来看个例子：\n\n\n```python\nfrom typing import List,Dict,Any,Annotated\nimport pydantic \nimport datetime\nfrom dateutil.relativedelta import relativedelta\n\ndef compute_date_range(count:Annotated[int,\"时间跨度，数值类型\"],\n                       unit:Annotated[str,\"时间单位，字符串类型\",{\"enum\":[\"day\",\"week\",\"month\",\"year\"]}])->List[str]:\n    '''\n    计算日期范围\n\n    Args:\n        count: 时间跨度，数值类型\n        unit: 时间单位，字符串类型，可选值为 day,week,month,year\n    '''        \n    now = datetime.datetime.now()\n    now_str = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n    if unit == \"day\":\n        return [(now - relativedelta(days=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"week\":\n        return [(now - relativedelta(weeks=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"month\":\n        return [(now - relativedelta(months=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"year\":\n        return [(now - relativedelta(years=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    return [\"\",\"\"]\n\ndef compute_now()->str:\n    '''\n    计算当前时间\n    '''\n    return datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n```\n\n我们定义了两个方法，一个是计算日期范围，一个是计算当前时间。\n\n现在我么可以来测试下，系统如何根据自然语言决定调用哪个方法：\n\n```python\nt = llm.chat_oai([{\n    \"content\":'''计算当前时间''',\n    \"role\":\"user\"    \n}],tools=[compute_date_range,compute_now],execute_tool=True)\n\nt[0].values\n\n## output: ['2024-06-14 15:18:02']\n```\n\n我们可以看到，他正确的选择了 compute_now 方法。\n\n接着我们再试一个：\n\n```python\nt = llm.chat_oai([{\n    \"content\":'''最近三个月趋势''',\n    \"role\":\"user\"    \n}],tools=[compute_date_range,compute_now],execute_tool=True)\n\nt[0].values\n\n## output: [['2024-03-14 15:19:13', '2024-06-14 15:19:13']]\n```\n\n模型正确的选择了 compute_date_range 方法。\n\n## 多模态\n\nbyerllm 也能很好的支持多模态的交互，而且统一了多模态大模型的接口，比如你可以用一样的方式使用 openai 或者 claude 的图片转文字能力， 或者一致的方式使用火山，azuer, openai的语音合成接口。\n\n### 图生文\n\n```python\nimport byzerllm\nfrom byzerllm.types import ImagePath\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\n\n\n@byzerllm.prompt()\ndef what_in_image(image_path: ImagePath) -> str:\n    \"\"\"\n    {{ image_path }}\n    这个图片里有什么？\n    \"\"\"    \n\n\nv = what_in_image.with_llm(vl_llm).run(\n    ImagePath(value=\"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\")\n)\nv\n## OUTPUT: 这张图片展示了多只可爱的猫咪，采用了艺术风格的绘画。猫咪们有不同的颜色和花纹，背景是浅棕色，上面还点缀着一些红色的花朵。整体画面给人一种温馨和谐的感觉\n```\n\n可以看到，我们只需要把 prompt 函数的图片地址入参使用 byzerllm.types.ImagePath里进行包装，就可以直接在 prompt 函数体里\n带上图片。\n\n或者你可以这样：\n\n```python\nimport byzerllm\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\n\n\n@byzerllm.prompt()\ndef what_in_image(image_path: str) -> str:\n    \"\"\"\n    {{ image }}\n    这个图片里有什么？\n    \"\"\"\n    return {\"image\": byzerllm.Image.load_image_from_path(image_path)}\n\n\nv = what_in_image.with_llm(vl_llm).run(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv\n```\n\n通过 `image_path` 参数，然后通过 `byzerllm.Image.load_image_from_path` 方法，转化为一个图片对象 image，最后在 prompt 函数体里\n使用 `{{ image }}` 引用这个图片对象。\n\n另外我们也是可以支持配置多张图片的。\n\n另外，我们也可以使用基础的 `llm.chat_oai` 方法来实现：\n\n```python\nimport byzerllm\nimport json\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\nimage = byzerllm.Image.load_image_from_path(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv = vl_llm.chat_oai(\n    conversations=[\n        {\n            \"role\": \"user\",\n            \"content\": json.dumps(\n                [{\"image\": image, \"text\": \"这个图片里有什么？\"}], ensure_ascii=False\n            ),\n        }\n    ]\n)\nv[0].output\n```\n\n还可以这么写：\n\n```python\nimport byzerllm\nimport json\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\nimage = byzerllm.Image.load_image_from_path(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv = vl_llm.chat_oai(\n    conversations=[\n        {\n            \"role\": \"user\",\n            \"content\": json.dumps(\n                [\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\"url\": image, \"detail\": \"high\"},\n                    },\n                    {\"text\": \"这个图片里有什么？\", \"type\": \"text\"},\n                ],\n                ensure_ascii=False,\n            ),\n        }\n    ]\n)\nv[0].output\n```\n\n### 语音合成\n\n这里以 openai 的 tts 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=tts-1 \\\n--model openai_tts\n```\n\n此外，byzerllm 支持 azure,火山引擎等 tts 语音合成引擎。\n\n接着你可以这么用：\n\n```python\nimport byzerllm\nimport base64\nimport json\n\nllm = byzerllm.ByzerLLM.from_default_model(\"openai_tts\")\n\n\nt = llm.chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\": json.dumps({\n        \"input\":\"hello, open_tts\",\n        \"voice\": \"alloy\",\n        \"response_format\": \"mp3\"\n    },ensure_ascii=False)\n}])\n\nwith open(\"voice.mp3\",\"wb\") as f:\n    f.write(base64.b64decode(t[0].output))\n```\n\ntts 模型生成没有prompt函数可以用，你需要直接使用 chat_oai。\n\n\n### 语音识别\n\n这里以 openai 的 whisper-1 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--worker_concurrency 10 \\\n--infer_params saas.model=whisper-1 saas.api_key=${MODEL_OPENAI_TOKEN} \\\n--model speech_to_text\n```\n\n语音识别的使用方式和图生文类似，我们可以直接在 prompt 函数体里带上音频文件。\n\n```python\nimport byzerllm\nimport json\nimport base64\nfrom byzerllm.types import AudioPath\n\nllm = byzerllm.ByzerLLM.from_default_model(\"speech_to_text\")\n\naudio_file = \"/Users/allwefantasy/videos/output_audio.mp3\"\n\n@byzerllm.prompt(llm=llm)\ndef audio_to_text(audio_file: AudioPath):\n    \"\"\"\n    {{ audio_file }}\n    \"\"\"\n\nv = audio_to_text(AudioPath(value=audio_file))\njson.loads(v)\n```\n输出的数据格式略微复杂：\n\n```\n{'text': 'In the last chapter, you and I started to step through the internal workings of a transformer. This is one of the key pieces of technology inside large language models, and a lot of other tools in the modern wave of AI.',\n 'task': 'transcribe',\n 'language': 'english',\n 'duration': 10.0,\n 'segments': [{'id': 0,\n   'seek': 0,\n   'start': 0.0,\n   'end': 4.78000020980835,\n   'text': ' In the last chapter, you and I started to step through the internal workings of a transformer.',\n   'tokens': [50364,\n    682,\n    264,\n    1036,\n    7187,\n    11,\n    291,\n    293,\n    286,\n    1409,\n    281,\n    1823,\n    807,\n    264,\n    6920,\n    589,\n    1109,\n    295,\n    257,\n    31782,\n    13,\n    50586],\n   'temperature': 0.0,\n   'avg_logprob': -0.28872039914131165,\n   'compression_ratio': 1.4220778942108154,\n   'no_speech_prob': 0.016033057123422623},\n  ....\n  {'id': 2,\n   'seek': 0,\n   'start': 8.579999923706055,\n   'end': 9.979999542236328,\n   'text': ' and a lot of other tools in the modern wave of AI.',\n   'tokens': [50759,\n    293,\n    257,\n    688,\n    295,\n    661,\n    3873,\n    294,\n    264,\n    4363,\n    5772,\n    295,\n    7318,\n    13,\n    50867],\n   'temperature': 0.0,\n   'avg_logprob': -0.28872039914131165,\n   'compression_ratio': 1.4220778942108154,\n   'no_speech_prob': 0.016033057123422623}],\n 'words': [{'word': 'In', 'start': 0.0, 'end': 0.18000000715255737},\n  {'word': 'the', 'start': 0.18000000715255737, 'end': 0.23999999463558197},\n  {'word': 'last', 'start': 0.23999999463558197, 'end': 0.5400000214576721},\n  {'word': 'chapter', 'start': 0.5400000214576721, 'end': 0.800000011920929},\n  ....\n  {'word': 'AI', 'start': 9.920000076293945, 'end': 9.979999542236328}]}\n```\n\n会输出每一句话以及每一个字所在的起始时间和截止时间。你可以根据需要来使用。\n\n\n### 文生图\n\n文生图和语音合成类似，首先要启动合适的模型,以openai 的 dall-e-3 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=dall-e-3 \\\n--model openai_image_gen\n```\n\n启动模型后，只需要记住几个模板参数即可使用，这里直接使用 chat_oai 方法来使用：\n\n```python\n\nimport byzerllm\nimport json\nimport base64\n\nllm = byzerllm.ByzerLLM.from_default_model(\"openai_image_gen\")\nt = llm.chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\": json.dumps({\n        \"input\":\"a white siamese cat\",\n        \"size\": \"1024x1024\",\n        \"quality\": \"standard\"\n    },ensure_ascii=False)\n}])\n\nwith open(\"output1.jpg\",\"wb\") as f:\n    f.write(base64.b64decode(t[0].output))\n\n\nimport matplotlib.pyplot as plt\n\nimage_path = \"output1.jpg\"\nimage = plt.imread(image_path)\n\nplt.imshow(image)\nplt.axis('off')\nplt.show()\n```\n\n## Prompt 函数的流式输出\n\nbyzerllm 底层支持流式输出，非 prompt 函数的用法是这样的：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\n\nv = llm.stream_chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\":\"讲一个100字的故事\"\n}])\n\nfor s  in v:\n    print(s[0], end=\"\")\n```\n\n如果你像用 prompt 函数，可以这么用：\n\n\n```python\nimport byzerllm\nimport json\nimport base64\nfrom typing import Generator\n\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\n\n@byzerllm.prompt()\ndef tell_story() -> Generator[str, None, None]:\n    '''\n    给我讲一个一百多字的故事\n    '''\n\nv = tell_story.with_llm(llm).run()    \nfor i in v:\n    print(i, end=\"\")\n```\n\n可以看到，和普通的 prompt 函数的区别在于，返回值是一个生成器，然后你可以通过 for 循环来获取结果。\n\n## 向量化模型\n\nbyzerllm 支持向量化模型,你可以这样启动一个本地的模型：\n\n```bash\n!byzerllm deploy --pretrained_model_type custom/bge \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--worker_concurrency 10 \\\n--model_path /home/winubuntu/.auto-coder/storage/models/AI-ModelScope/bge-large-zh \\\n--infer_backend transformers \\\n--num_workers 1 \\\n--model emb\n```\n\n注意两个参数:\n\n1. --infer_backend transformers: 表示使用 transformers 作为推理后端。\n2. --model_path: 表示模型的路径。\n\n也可以启动一个 SaaS 的emb模型,比如 qwen 的 emb 模型：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/qianwen \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 2 \\\n--infer_params saas.api_key=${MODEL_QIANWEN_TOKEN}  saas.model=text-embedding-v2 \\\n--model qianwen_emb\n```\n\n或者 openai 的 emb 模型：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--worker_concurrency 10 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=text-embedding-3-small \\\n--model gpt_emb\n```\nSaaS 模型无需配置 `--infer_backend` 参数。\n\n无论是本地模型还是 SaaS 模型，我们都可以这样使用：\n\n```python\nimport byzerllm\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\nllm.setup_default_emb_model_name(\"emb\")\nllm.emb_query(\"你好\")\n```\n\n如果你配置 byzerllm 中的 Storage 使用，比如你这样启动了存储：\n\n```bash\nbyzerllm storage start --enable_emb\n```\n\n那么需要这么使用：\n\n```python\nfrom byzerllm.apps.byzer_storage.simple_api import ByzerStorage, DataType, FieldOption,SortOption\nstorage = ByzerStorage(\"byzerai_store\", \"memory\", \"memory\")\nstorage.emb(\"你好\")\n```\n\n## 一些辅助工具\n\n当调用 prompt 函数返回字符串的时候，如果想从里面抽取代码，可以使用如下方式：\n\n```python\nfrom byzerllm.utils.client import code_utils\ntext_with_markdown = '''\n```shell\nls -l\n```\n'''\ncode_blocks = code_utils.extract_code(text_with_markdown)\nfor code_block in code_blocks:\n    if code_block[0] == \"shell\":\n        print(code_block[1])\n##output: ls -l        \n```\n\nTagExtractor 工具用于对任意 `<_tag_></_tag_>` 标签对的抽取，下面是一个使用示例：\n\n```python\nfrom byzerllm.apps.utils import TagExtractor\n\nextractor = TagExtractor('''\n大家好\n<_image_>data:image/jpeg;base64,xxxxxx</_image_>\n                         大家好\n<_image_>data:image/jpeg;base64,xxxxxx2</_image_>\n''')\n\nv = extractor.extract()\nprint(v.content[0].content)\nprint(v.content[1].content)\n```\n输出为:\n\n```\ndata:image/jpeg;base64,xxxxxx\ndata:image/jpeg;base64,xxxxxx2\n```\n\n我们成功的将 <_image_></_image_> 标签对里的内容抽取出来了。\n\n\n## 注意事项\n\n1. prompt函数方法体返回只能是dict，实际的返回类型和方法签名可以不一样，但是方法体返回只能是dict。\n2. 大部分情况prompt函数体为空，如果一定要有方法体，可以返回一个空字典。\n3. 调用prompt方法的时候，如果在@byzerllm.prompt()里没有指定llm对象，那么需要在调用的时候通过with_llm方法指定llm对象。\n\n===========================\n\n# Byzer Storage: 最易用的AI存储引擎\n\nByzer Storage是一个为AI应用设计的高性能存储引擎,它提供了简单易用的API,支持向量搜索、全文检索以及结构化查询。本文将详细介绍Byzer Storage的使用方法和主要特性。\n\n## 0. 安装和启动\n\n```bash\npip install byzerllm\nbyzerllm storage start\n```\n\nThat's it! Byzer Storage已经安装并启动成功,现在我们可以开始使用它了。默认会启动一个 byzerai_store 的集群。\n\n注意，如果你是这样启动的：\n\n```bash\nbyzerllm storage start --enable_emb\n```\n\n那么会自动启动一个 emb 模型，名字就叫 emb， ByzerStorage 会自动使用该模型，无需做任何其他配置。\n\n## 1. 初始化\n\n创建一个 ByzerStorage 对象，链接 byzerai_store 集群，并且指定数据库和表名（可以不存在）。\n\n```python\nfrom byzerllm.apps.byzer_storage.simple_api import ByzerStorage, DataType, FieldOption, SortOption\n\nstorage = ByzerStorage(\"byzerai_store\", \"my_database1\", \"my_table4s\")\n```\n\n\n## 2. 创建库表（可选）\n\nByzer Storage使用Schema来定义数据结构。我们可以使用SchemaBuilder来构建Schema:\n\n```python\n_ = (\n    storage.schema_builder()\n    .add_field(\"_id\", DataType.STRING)\n    .add_field(\"name\", DataType.STRING)\n    .add_field(\"content\", DataType.STRING, [FieldOption.ANALYZE])\n    .add_field(\"raw_content\", DataType.STRING, [FieldOption.NO_INDEX])    \n    .add_array_field(\"summary\", DataType.FLOAT)    \n    .add_field(\"created_time\", DataType.LONG, [FieldOption.SORT])    \n    .execute()\n)\n```\n\n这个Schema定义了以下字段:\n- `_id`: 字符串类型的主键\n- `name`: 字符串类型,可用于过滤条件\n- `content`: 字符串类型,带有ANALYZE选项,用于全文搜索\n- `raw_content`: 字符串类型,带有NO_INDEX选项,不会被索引\n- `summary`: 浮点数组类型,用于存储向量\n- `created_time`: 长整型,带有SORT选项,可用于排序\n\n需要注意的是：\n\n1. 如果一个字段带有ANALYZE选项,则该字段会被分词,并且可以用于全文搜索，但是就无法返回原始的文本了。所以你需要添加一个新字段专门用来存储原文，比如在我们这个例子里，我们新增了 raw_content 字段，并且显示的指定了 NO_INDEX 选项，这样就不会被索引，也就不会被分词，可以后续被检索后用来获取原始文本。\n2. 对于需要作为向量检索和存储的字段，需要指定为数组类型，比如我们这个例子里的 summary 字段。\n3. 如果你需要拿到向量字段的原始文本，那么你也需要添加一个新字段专门用来存储原文，就像我们这个例子里的 raw_content 字段一样。\n\n\n## 3. 写入数据\n\n准备数据并使用WriteBuilder写入Storage:\n\n```python\ndata = [\n    {\"_id\": \"1\", \"name\": \"Hello\", \"content\": \"Hello, world!\", \"raw_content\": \"Hello, world!\", \"summary\": \"hello world\", \"created_time\": 1612137600},\n    {\"_id\": \"2\", \"name\": \"Byzer\", \"content\": \"Byzer, world!\", \"raw_content\": \"Byzer, world!\", \"summary\": \"byzer\", \"created_time\": 1612137601},\n    {\"_id\": \"3\", \"name\": \"AI\", \"content\": \"AI, world!\", \"raw_content\": \"AI, world!\", \"summary\": \"AI\", \"created_time\": 16121376002},\n    {\"_id\": \"4\", \"name\": \"ByzerAI\", \"content\": \"ByzerAI, world!\", \"raw_content\": \"ByzerAI, world!\", \"summary\": \"ByzerAi\", \"created_time\": 16121376003},\n]\n\nstorage.write_builder().add_items(data, vector_fields=[\"summary\"], search_fields=[\"content\"]).execute()\nstorage.commit()\n```\n\n这里我们使用`add_items`方法批量添加数据,并指定了`summary`为向量字段,`content`为搜索字段。最后调用`commit()`来确保数据被持久化。\n\n\n从上面我们要写入到 byzer storage 的数据我们可以看到如下几个特点：\n\n1. 需要包含我们之前定义的 Schema中罗列的所有的字段，同时需要指定哪些是向量字段，哪些是检索字段。\n2. 向量和检索不能是同一个字段。\n3. 对于向量，检索字段，我们给到 write_builder 的都是文本，ByzerStorage 会根据 Schema 的定义自动将其转换为向量，检索字段需要的格式。\n\n## 4. 查询数据\n\nByzer Storage支持多种查询方式,包括向量搜索、全文检索、过滤和排序。\n\n### 4.1 向量搜索 + 全文检索\n\n```python\nquery = storage.query_builder()\nquery.set_vector_query(\"ByzerAI\", fields=[\"summary\"])\nresults = query.set_search_query(\"Hello\", fields=[\"content\"]).execute()\nprint(results)\n```\n\n这个查询结合了向量搜索和全文检索,它会在`summary`字段中搜索与\"ByzerAI\"相似的向量,同时在`content`字段中搜索包含\"Hello\"的文档。\n\n### 4.2 过滤 + 向量搜索 + 全文检索\n\n```python\nquery = storage.query_builder()\nquery.and_filter().add_condition(\"name\", \"AI\").build()\nquery.set_vector_query(\"ByzerAI\", fields=\"summary\")\nresults = query.set_search_query(\"Hello\", fields=[\"content\"]).execute()\nprint(results)\n```\n\n这个查询首先过滤`name`字段等于\"AI\"的文档,然后在结果中进行向量搜索和全文检索。\n\n### 4.3 过滤 + 排序\n\n```python\nquery = storage.query_builder()\nquery.and_filter().add_condition(\"name\", \"AI\").build().sort(\"created_time\", SortOption.DESC)\nresults = query.execute()\nprint(results)\n```\n\n这个查询过滤`name`字段等于\"AI\"的文档,然后按`created_time`字段降序排序。\n\n## 5. 删除数据\n\n### 5.1 根据ID删除\n\n```python\nstorage.delete_by_ids([\"3\"])\n\nquery = storage.query_builder()\nquery.and_filter().add_condition(\"name\", \"AI\").build()\nresults = query.execute()\nprint(results)\n```\n\n这里我们删除了ID为\"3\"的文档,然后查询验证删除结果。\n\n### 5.2 删除整个表\n\n```python\nstorage.drop()\n```\n\n这个操作会删除整个表及其所有数据,请谨慎使用。\n\n## 结论\n\nByzer Storage提供了一套简洁而强大的API,能够轻松实现向量搜索、全文检索、结构化查询等功能。它的设计非常适合AI应用场景,可以有效地存储和检索各种类型的数据。通过本文的介绍,相信读者已经对Byzer Storage有了基本的了解,并能够开始在自己的项目中使用这个强大的存储引擎。\n\n\n     \n\n下面是用户的需求：\n\nmakefile 中的disk 最好打的包要放到 releases 目录下\n使用 SEARCH/REPLACE 格式输出\n"}, {"role": "assistant", "content": "好的，我理解您希望修改 Makefile，将生成的发布包放到 releases 目录下。我会使用 SEARCH/REPLACE 格式来提供修改建议。\n\n```makefile\n##File: /Users/allwefantasy/projects/super_analysis/Makefile\n<<<<<<< SEARCH\ndist: wheel\n\t@echo \"Creating distribution package...\"\n\t$(eval VERSION := $(shell python -c \"import sys; sys.path.append('src'); from superanalysis.version import __version__; print(__version__)\"))\n\trm -rf super_analysis-$(VERSION).tar.gz\n\tmkdir -p super_analysis-$(VERSION)\n\tcp -r $(DIST_DIR)/* super_analysis-$(VERSION)/\n\tcp \"/Users/allwefantasy/projects/super_analysis/docs/4.3.1 安装与配置 Byzer-SQL.pdf\" super_analysis-$(VERSION)/\n\tcp \"/Users/allwefantasy/projects/super_analysis/docs/deploy.md\" super_analysis-$(VERSION)/README.md\n\tcp \"/Users/allwefantasy/projects/super_analysis/notebooks/openai_local_api.ipynb\" super_analysis-$(VERSION)/\n\ttar -czf super_analysis-$(VERSION).tar.gz super_analysis-$(VERSION)\n\trm -rf super_analysis-$(VERSION)\n\t@echo \"Distribution package created: super_analysis-$(VERSION).tar.gz\"\n=======\ndist: wheel\n\t@echo \"Creating distribution package...\"\n\t$(eval VERSION := $(shell python -c \"import sys; sys.path.append('src'); from superanalysis.version import __version__; print(__version__)\"))\n\tmkdir -p releases\n\trm -rf releases/super_analysis-$(VERSION).tar.gz\n\tmkdir -p super_analysis-$(VERSION)\n\tcp -r $(DIST_DIR)/* super_analysis-$(VERSION)/\n\tcp \"/Users/allwefantasy/projects/super_analysis/docs/4.3.1 安装与配置 Byzer-SQL.pdf\" super_analysis-$(VERSION)/\n\tcp \"/Users/allwefantasy/projects/super_analysis/docs/deploy.md\" super_analysis-$(VERSION)/README.md\n\tcp \"/Users/allwefantasy/projects/super_analysis/notebooks/openai_local_api.ipynb\" super_analysis-$(VERSION)/\n\ttar -czf releases/super_analysis-$(VERSION).tar.gz super_analysis-$(VERSION)\n\trm -rf super_analysis-$(VERSION)\n\t@echo \"Distribution package created: releases/super_analysis-$(VERSION).tar.gz\"\n>>>>>>> REPLACE\n```\n\n这个修改主要做了以下几点改变：\n\n1. 添加了 `mkdir -p releases` 命令来确保 releases 目录存在。\n2. 将 `rm -rf` 和 `tar -czf` 命令中的文件路径改为 `releases/super_analysis-$(VERSION).tar.gz`。\n3. 更新了最后的 echo 消息，显示新的文件路径。\n\n这样修改后，生成的发布包将被放置在 releases 目录下。"}], "model": "human", "yaml_file": "actions/150_chat_action.yml"}
{"instruction": "@Makefile 添加一个新的命令 deploy , 参考 if [[ ${MODE} == \"release\" ]]; then\n git tag v${version}\n git push origin v${version}  # 请根据实际情况使用正确的远程仓库名\n echo \"Upload ${project} ${version}\"\n twine upload dist/*\nfi 这个的实现来实现 deploy 命令\n使用 SEARCH/REPLACE 格式输出", "conversations": [{"role": "user", "content": "\n如果你需要生成代码，对于每个需要更改的文件,你需要按 *SEARCH/REPLACE block* 的格式进行生成。\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The opening fence and code language, eg: ```python\n2. The file path alone on a line, starting with \"##File:\" and verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n*SEARCH/REPLACE* blocks will replace *all* matching occurrences.\nInclude enough lines to make the SEARCH blocks unique.\n\nInclude *ALL* the code being searched and replaced!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n下面我们来看一个例子：\n\n当前项目目录结构：\n1. 项目根目录： /tmp/projects/mathweb\n2. 项目子目录/文件列表(类似tree 命令输出)\nflask/\n    app.py\n    templates/\n        index.html\n    static/\n        style.css\n\n用户需求： Change get_factorial() to use math.factorial\n\n回答： To make this change we need to modify `/tmp/projects/mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n用户需求： Refactor hello() into its own file.\n\n回答：To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/hello.py\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/main.py\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n现在让我们开始一个新的任务:\n\n当前项目目录结构：\n1. 项目根目录： /Users/allwefantasy/projects/super_analysis\n2. 项目子目录/文件列表(类似tree 命令输出)：\nsetup.py\n.auto-coder/\n    libs/\n        llm_friendly_packages/\n            src/\n                example1/\n                    main.py\nsrc/\n    superanalysis/\n        version.py\n        __init__.py\n        serve/\n            rest.py\n            convert.py\n            __init__.py\n            llm_wrapper.py\n            api_server.py\n            cli.py\n        sql/\n            __init__.py\n            types.py\n            generator.py\n下面是一些文件路径以及每个文件对应的源码：\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/types.py\nfrom pydantic import BaseModel\n\nclass RAGConfig(BaseModel):\n    schema_rag_base_url: str = \"\"\n    schema_rag_api_key: str = \"xxxx\"\n\n    context_rag_base_url: str = \"\"\n    context_rag_api_key: str = \"xxxx\"\n\nclass ByzerSQLConfig(BaseModel):\n    byzer_sql_url: str = \"\"\n    data_dir: str = \"\"\n    owner: str = \"\"\n    result_synthesis: bool = False\n\nclass ModelConfig(BaseModel):\n    llm_model: str = \"\"\n    gen_sql_llm_model: str = \"\"\n    sql_func_llm_model: str = \"\"\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/cli.py\nimport argparse\nimport byzerllm\nfrom byzerllm.utils.client import ByzerLLM\nfrom superanalysis.serve.api_server import serve, ServerArgs\nfrom superanalysis.sql.types import RAGConfig, ByzerSQLConfig, ModelConfig\nfrom superanalysis.sql.generator import SQLGenerator\nfrom superanalysis.serve.llm_wrapper import LLWrapper\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"ByzerLLM API Server\")\n    parser.add_argument(\"--ray-address\", type=str, default=\"auto\", help=\"ray address\")\n    parser.add_argument(\n        \"--host\", type=str, default=\"0.0.0.0\", help=\"Host to bind the server to\"\n    )\n    parser.add_argument(\n        \"--port\", type=int, default=8000, help=\"Port to bind the server to\"\n    )\n    parser.add_argument(\n        \"--uvicorn-log-level\", type=str, default=\"info\", help=\"Uvicorn log level\"\n    )\n    parser.add_argument(\n        \"--allow-credentials\", action=\"store_true\", help=\"Allow credentials\"\n    )\n    parser.add_argument(\n        \"--allowed-origins\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed origins\"\n    )\n    parser.add_argument(\n        \"--allowed-methods\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed methods\"\n    )\n    parser.add_argument(\n        \"--allowed-headers\", type=str, nargs=\"+\", default=[\"*\"], help=\"Allowed headers\"\n    )\n    parser.add_argument(\n        \"--api-key\", type=str, default=\"\", help=\"API key for authentication\"\n    )\n    parser.add_argument(\n        \"--served-model-name\",\n        type=str,\n        required=True,\n        help=\"Name of the model to be served\",\n    )\n    parser.add_argument(\n        \"--prompt-template\", type=str, default=\"\", help=\"Prompt template\"\n    )\n    parser.add_argument(\n        \"--response-role\", type=str, default=\"assistant\", help=\"Response role\"\n    )\n    parser.add_argument(\"--ssl-keyfile\", type=str, default=\"\", help=\"SSL key file\")\n    parser.add_argument(\n        \"--ssl-certfile\", type=str, default=\"\", help=\"SSL certificate file\"\n    )\n\n    # Add new arguments for RAGConfig\n    parser.add_argument(\n        \"--schema-rag-base-url\", type=str, default=\"\", help=\"RAG base URL\"\n    )\n    parser.add_argument(\n        \"--schema-rag-api-key\", type=str, default=\"xxxx\", help=\"RAG API key\"\n    )\n\n    parser.add_argument(\n        \"--context-rag-base-url\", type=str, default=\"\", help=\"RAG base URL\"\n    )\n    parser.add_argument(\n        \"--context-rag-api-key\", type=str, default=\"xxxx\", help=\"RAG API key\"\n    )\n\n    # Add new argument for ByzerSQLConfig\n    parser.add_argument(\"--byzer-sql-url\", type=str, default=\"\", help=\"Byzer SQL URL\")\n    parser.add_argument(\"--data-dir\", type=str, default=\"\", help=\"Data directory\")\n    parser.add_argument(\n        \"--owner\", type=str, default=\"hello\", help=\"Owner of the Byzer SQL session\"\n    )\n    parser.add_argument(\n        \"--llm-model\", type=str, default=\"\", help=\"LLM model for general use\"\n    )\n    parser.add_argument(\n        \"--gen-sql-llm-model\", type=str, default=\"\", help=\"LLM model for SQL generation\"\n    )\n    parser.add_argument(\n        \"--sql-func-llm-model\", type=str, default=\"\", help=\"LLM model for SQL functions\"\n    )\n\n    parser.add_argument(\n        \"--result-synthesis\", action=\"store_true\", help=\"Enable result synthesis\"\n    )\n\n    args = parser.parse_args()\n\n    server_args = ServerArgs(\n        host=args.host,\n        port=args.port,\n        uvicorn_log_level=args.uvicorn_log_level,\n        allow_credentials=args.allow_credentials,\n        allowed_origins=args.allowed_origins,\n        allowed_methods=args.allowed_methods,\n        allowed_headers=args.allowed_headers,\n        api_key=args.api_key,\n        served_model_name=args.served_model_name,\n        prompt_template=args.prompt_template,\n        response_role=args.response_role,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile,\n    )\n\n    # Create RAGConfig instance\n    rag_config = RAGConfig(\n        schema_rag_base_url=args.schema_rag_base_url,\n        schema_rag_api_key=args.schema_rag_api_key,\n        context_rag_base_url=args.context_rag_base_url,\n        context_rag_api_key=args.context_rag_api_key,\n    )\n\n    # Create ByzerSQLConfig instance\n    byzer_sql_config = ByzerSQLConfig(\n        byzer_sql_url=args.byzer_sql_url,\n        data_dir=args.data_dir,\n        owner=args.owner,\n        result_synthesis=args.result_synthesis,\n    )\n\n    model_config = ModelConfig(\n        llm_model=args.llm_model or args.served_model_name,\n        gen_sql_llm_model=args.gen_sql_llm_model or args.served_model_name,\n        sql_func_llm_model=args.sql_func_llm_model or args.served_model_name,\n    )\n\n    byzerllm.connect_cluster(address=args.ray_address)\n    llm = ByzerLLM()\n    llm.setup_default_model_name(args.served_model_name)\n    llm = byzerllm.ByzerLLM()\n    if not server_args.served_model_name:\n        raise ValueError(\"served_model_name is required\")\n    llm.setup_default_model_name(server_args.served_model_name)\n\n    print(\"RAG Config: \", rag_config)\n    print(\"Model Config: \", model_config)\n    rag = SQLGenerator(\n        llm=llm,\n        rag_config=rag_config,\n        byzer_sql_config=byzer_sql_config,\n        model_config=model_config,\n    )\n    llm_wrapper = LLWrapper(llm=llm, entry=rag)\n    serve(llm_wrapper, server_args)\n\n\nif __name__ == \"__main__\":\n    main()\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/__init__.py\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/__init__.py\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/llm_wrapper.py\nfrom typing import Any, Dict, List, Optional, Union,Callable\nfrom byzerllm.utils.client.types import (    \n    LLMFunctionCallResponse,\n    LLMClassResponse,LLMResponse    \n)\nimport pydantic\nfrom byzerllm import ByzerLLM\nfrom byzerllm.utils.client import LLMResponse\nfrom byzerllm.utils.types import SingleOutputMeta\nfrom byzerllm.utils.langutil import asyncfy_with_semaphore\nfrom superanalysis.sql.generator import SQLGenerator\nfrom loguru import logger\n\nclass LLWrapper:\n\n    def __init__(self,llm:ByzerLLM,entry:SQLGenerator):\n        self.entry = entry\n        self.llm = llm\n        \n\n    def chat_oai(self,\n                 conversations,\n                 tools:List[Union[Callable,str]]=[], \n                 tool_choice:Optional[Union[Callable,str]]=None,\n                 execute_tool:bool=False,  \n                 impl_func:Optional[Callable]=None,\n                 execute_impl_func:bool=False,\n                 impl_func_params:Optional[Dict[str,Any]]=None,\n                 func_params:Optional[Dict[str,Any]]=None,\n                 response_class:Optional[Union[pydantic.BaseModel,str]] = None, \n                 response_after_chat:Optional[Union[pydantic.BaseModel,str]] = False,\n                 enable_default_sys_message:bool=True,                 \n                 model:Optional[str] = None,\n                 role_mapping=None,llm_config:Dict[str,Any]={}\n                 )->Union[List[LLMResponse],List[LLMFunctionCallResponse],List[LLMClassResponse]]:         \n        res,contexts = self.entry.stream_chat_oai(conversations)\n        s = \"\".join(res)        \n        return [LLMResponse(output=s,metadata={},input=\"\")]\n\n    def stream_chat_oai(self,conversations, \n                        model:Optional[str]=None, \n                        role_mapping=None,\n                        delta_mode=False,\n                        llm_config:Dict[str,Any]={}): \n        res,contexts = self.entry.stream_chat_oai(conversations)        \n        for t in res:                        \n            yield (t,SingleOutputMeta(0,0))\n\n    async def async_stream_chat_oai(self,conversations, \n                        model:Optional[str]=None, \n                        role_mapping=None,\n                        delta_mode=False,\n                        llm_config:Dict[str,Any]={}): \n        res,contexts = await asyncfy_with_semaphore(lambda: self.entry.stream_chat_oai(conversations))()\n        for t in res:                                    \n            yield (t,SingleOutputMeta(0,0))\n                         \n\n    def __getattr__(self, name):        \n        return getattr(self.llm, name)\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/api_server.py\nimport os\nimport time\nimport uvicorn\nfrom fastapi import Request\nfrom fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import JSONResponse, StreamingResponse, Response\n\nfrom byzerllm.log import init_logger\nfrom byzerllm.utils import random_uuid\nfrom byzerllm.version import __version__ as version\nfrom byzerllm.utils.client import ByzerLLM, LLMRequest\nfrom byzerllm.utils.client.entrypoints.openai.serving_chat import OpenAIServingChat\nfrom byzerllm.utils.client.entrypoints.openai.serving_completion import OpenAIServingCompletion\nfrom byzerllm.utils.client.entrypoints.openai.protocol import (\n    ModelList,\n    ModelCard,\n    ModelPermission,\n    ChatCompletionRequest,\n    ErrorResponse,\n    CompletionRequest,\n    Embeddings,\n    EmbeddingsOutput,\n    EmbeddingsData,\n    EmbeddingsUsage,\n)\nfrom pydantic import BaseModel\nfrom typing import List\n\nlogger = init_logger(__name__)\n\nllm_client: ByzerLLM = None\nopenai_serving_chat: OpenAIServingChat = None\nopenai_serving_completion: OpenAIServingCompletion = None\n\nTIMEOUT_KEEP_ALIVE = 5  # seconds\n# timeout in 10 minutes. Streaming can take longer than 3 min\nTIMEOUT = float(os.environ.get(\"BYZERLLM_APISERVER_HTTP_TIMEOUT\", 600))\n\nrouter_app = FastAPI()\n\n\n@router_app.get(\"/health\")\nasync def health() -> Response:\n    \"\"\"Health check.\"\"\"\n    return Response(status_code=200)\n\n\n@router_app.get(\"/v1/models\")\nasync def show_available_models():\n    models = await openai_serving_chat.show_available_models()\n    return JSONResponse(content=models.model_dump())\n\n\n@router_app.get(\"/version\")\nasync def show_version():\n    return JSONResponse(content={\"version\": version})\n\n\n@router_app.get(\"/v1/models\", response_model=ModelList)\nasync def models() -> ModelList:\n    \"\"\"Show available models. Right now we only have one model.\"\"\"\n    model_cards = [\n        ModelCard(\n            id=\"\",\n            root=\"\",\n            permission=[ModelPermission()]\n        )\n    ]\n    return ModelList(data=model_cards)\n\n\n@router_app.post(\"/v1/completions\")\nasync def create_completion(\n        body: CompletionRequest,\n        request: Request\n):\n    generator = await openai_serving_completion.create_completion(body, request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            status_code=generator.code\n        )\n    if body.stream:\n        return StreamingResponse(\n            content=generator,\n            media_type=\"text/event-stream\"\n        )\n    else:\n        return JSONResponse(content=generator.model_dump())\n\n\n@router_app.post(\"/v1/chat/completions\")\nasync def create_chat_completion(\n        body: ChatCompletionRequest,\n        request: Request,\n):\n    \"\"\"Completion API similar to OpenAI's API.\n\n    See  https://platform.openai.com/docs/api-reference/chat/create\n    for the API specification. This API mimics the OpenAI ChatCompletion API.\n\n    NOTE: Currently we do not support the following features:\n        - function_call (Users should implement this by themselves)\n        - logit_bias (to be supported by vLLM engine)\n    \"\"\"\n    # async with async_timeout.timeout(TIMEOUT):\n\n    generator = await openai_serving_chat.create_chat_completion(body, request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            status_code=generator.code\n        )\n    if body.stream:\n        return StreamingResponse(\n            content=generator,\n            media_type=\"text/event-stream\"\n        )\n    else:\n        return JSONResponse(content=generator.model_dump())\n\n\n@router_app.post(\"/v1/embeddings\")\nasync def embed(body: Embeddings):\n    \"\"\"Given a prompt, the model will return one embedding.\n\n    Returns:\n        A response object with an embedding.\n    \"\"\"\n    embedding_id = f\"embed-{random_uuid()}\"\n\n    results_list = llm_client.emb(body.model, request=LLMRequest(instruction=body.input))\n    tokens = 0\n\n    return EmbeddingsOutput(\n        data=[\n            EmbeddingsData(\n                embedding=results.output,\n                index=i,\n                object=\"embedding\",\n            )\n            for i, results in enumerate(results_list)\n        ],\n        id=embedding_id,\n        object=\"list\",\n        created=int(time.time()),\n        model=body.model,\n        usage=EmbeddingsUsage(\n            prompt_tokens=tokens,\n            total_tokens=tokens,\n        ),\n    )\n\nclass ServerArgs(BaseModel):\n    host: str = None\n    port: int = 8000\n    uvicorn_log_level: str = \"info\"\n    allow_credentials: bool = False\n    allowed_origins: List[str] = [\"*\"]  \n    allowed_methods: List[str] = [\"*\"]\n    allowed_headers: List[str] = [\"*\"]\n    api_key: str = None\n    served_model_name: str = None\n    prompt_template: str = None\n    response_role: str = \"assistant\"\n    ssl_keyfile: str = None\n    ssl_certfile: str = None    \n\ndef serve(llm:ByzerLLM, args: ServerArgs):\n    \n    logger.info(f\"ByzerLLM API server version {version}\")\n    logger.info(f\"args: {args}\")\n\n    router_app.add_middleware(\n        CORSMiddleware,\n        allow_origins=args.allowed_origins,\n        allow_credentials=args.allow_credentials,\n        allow_methods=args.allowed_methods,\n        allow_headers=args.allowed_headers,\n    )\n    \n    if token := os.environ.get(\"BYZERLLM_API_KEY\") or args.api_key:\n\n        @router_app.middleware(\"http\")\n        async def authentication(request: Request, call_next):\n            if not request.url.path.startswith(\"/v1\"):\n                return await call_next(request)\n            if request.headers.get(\"Authorization\") != \"Bearer \" + token:\n                return JSONResponse(\n                    content={\"error\": \"Unauthorized\"},\n                    status_code=401\n                )\n            return await call_next(request)\n\n    # Register labels for metrics\n    # add_global_metrics_labels(model_name=engine_args.model)\n    global llm_client\n    llm_client = llm\n    \n    global openai_serving_chat\n    openai_serving_chat = OpenAIServingChat(\n        llm_client=llm_client,\n        response_role=args.response_role,\n        server_model_name=args.served_model_name,\n        prompt_template=args.prompt_template\n    )\n    global openai_serving_completion\n    openai_serving_completion = OpenAIServingCompletion(\n        llm_client=llm_client,\n        server_model_name=args.served_model_name,\n        prompt_template=args.prompt_template\n    )\n\n    uvicorn.run(\n        router_app,\n        host=args.host,\n        port=args.port,\n        log_level=args.uvicorn_log_level,\n        timeout_keep_alive=TIMEOUT_KEEP_ALIVE,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile\n    )\n\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/sql/generator.py\nfrom typing import Optional, Dict, Any, List\nfrom openai import OpenAI\nimport json\nfrom .types import RAGConfig, ByzerSQLConfig, ModelConfig\nfrom byzerllm.utils.client import code_utils\nimport byzerllm\nimport requests\nfrom loguru import logger\nimport traceback\nfrom byzerllm.utils.nontext import TagExtractor\n\n\nclass SQLGenerator:\n\n    @byzerllm.prompt()\n    def gen_context(self, conversations: List[Dict[str, str]]) -> str:\n        \"\"\"\n        对话内容：\n        {% for msg in conversations %}\n        <{{ msg.role }}>: {{ msg.content }}\n        {% endfor %}\n\n        根据提供的对话内容，理解用户最后的问题，然后提取可能会用到的 Byzer-SQL 相关的知识。\n        \"\"\"\n\n    @byzerllm.prompt()\n    def gen_sql(\n        self,\n        conversations: List[Dict[str, str]],\n        context: str,\n        schema: str,\n        model: str = \"deepseek_chat\",\n    ) -> str:\n        \"\"\"\n        相关知识：\n        {{ context }}\n\n        Schema信息：\n        {{ schema }}\n\n        对话内容：\n        {% for msg in conversations %}\n        <{{ msg.role }}>: {{ msg.content }}\n        {% endfor %}\n\n        模型UDF函数：\n        {{ model }}\n\n        根据提供的相关知识以及对话内容，理解用户最后的问题，然后根据看到的schema 信息，生成 Byzer-SQL 语句。\n        Byzer-SQL 首先需要用 load 语法加载 schema 中的路径映射成表格，然后根据表格的信息生成对应的 Byzer-SQL 语句。\n\n        特别需要关注的是:\n        1. 用户的很多需求是要对表格的文本内容做自然语言处理，你需要使用 Byzer-SQL 中的大模型辅助函数来完成这个需求。\n        2. 使用 `llm_result` 函数处理 LLM 模型函数的响应,`llm_result` 函数得到的是一段文本，虽然我们要求模型函数只输出特定的文本，但考虑到大模型会有一些额外的字符输出，不会只是输出我们要求的文字，所以通过 `like` 来进行匹配。\n        3. 注意 Byzer-SQL 的语法，比如select 语句最后一定需要有 as <table_name>。\n        3. 输出的 Byzer-SQL 语句务必使用 <_byzer_sql_></_byzer_sql_> 标签包裹起来。\n        \"\"\"\n\n    @byzerllm.prompt()\n    def fetch_schema(self, context: str) -> str:\n        \"\"\"\n        下面是一些基础知识，包括名词术语等等：\n        {{ context }}\n\n        当理解对话内容的时候，需要参考上面的基础知识，然后更好的理解对应的表格schema信息是不是有用。\n        \"\"\"\n    \n    @byzerllm.prompt()\n    def result_synthesis(self, conversations: List[Dict[str, str]], result: str) -> str:\n        \"\"\"\n        对话内容：\n        {% for msg in conversations %}\n        <{{ msg.role }}>: {{ msg.content }}\n        {% endfor %}\n\n        系统返回的数据：\n        {{ result }}\n\n        根据提供的对话内容以及系统返回的数据，对数据使用自然语言进行表述，要友好、简洁、准确。\n        \"\"\"\n\n    def __init__(\n        self,\n        llm: byzerllm.ByzerLLM,\n        rag_config: RAGConfig,\n        byzer_sql_config: ByzerSQLConfig,\n        model_config: ModelConfig,\n    ) -> None:\n        self.llm = llm\n        self.schema_rag_client = OpenAI(\n            api_key=rag_config.schema_rag_api_key,\n            base_url=rag_config.schema_rag_base_url,\n        )\n        self.context_rag_client = OpenAI(\n            api_key=rag_config.context_rag_api_key,\n            base_url=rag_config.context_rag_base_url,\n        )\n        self.byzer_sql_url = byzer_sql_config.byzer_sql_url\n        self.byzer_sql_config = byzer_sql_config\n        self.llm_model = byzerllm.ByzerLLM()\n        self.llm_model.setup_default_model_name(model_config.llm_model)\n        self.gen_sql_llm_model = byzerllm.ByzerLLM()\n        self.gen_sql_llm_model.setup_default_model_name(model_config.gen_sql_llm_model)\n        self.model_config = model_config\n\n\n    def stream_chat_oai(\n        self,\n        conversations,\n        model: Optional[str] = None,\n        role_mapping=None,\n        delta_mode=False,\n        llm_config: Dict[str, Any] = {},\n    ):\n        try:\n            v, c = self._stream_chat_oai(\n                conversations=conversations,\n                model=model,\n                role_mapping=role_mapping,\n                delta_mode=delta_mode,\n                llm_config=llm_config,\n            )\n        except Exception as e:\n            error_msg = f\"Error in stream_chat_oai: {e}\\n{traceback.format_exc()}\"\n            logger.error(error_msg)\n            v, c = [str(e)], []\n\n        return v, c\n\n    def _stream_chat_oai(\n        self,\n        conversations,\n        model: Optional[str] = None,\n        role_mapping=None,\n        delta_mode=False,\n        llm_config: Dict[str, Any] = {},\n    ):\n        logger.info(f\"Starting stream_chat_oai with model: {model}\")\n        logger.debug(f\"Conversations: {conversations}\")\n        logger.debug(f\"LLM Config: {llm_config}\")\n\n        if not model:\n            model = self.llm_model.default_model_name\n        logger.info(f\"Using model: {model}\")\n\n        query = conversations[-1][\"content\"]\n\n        if (\n            \"使用四到五个字直接返回这句话的简要主题，不要解释、不要标点、不要语气词、不要多余文本，不要加粗，如果没有主题\"\n            in query\n            or \"简要总结一下对话内容，用作后续的上下文提示 prompt，控制在 200 字以内\"\n            in query\n        ):\n            chunks = self.llm.stream_chat_oai(\n                conversations=conversations,\n                model=model,\n                role_mapping=role_mapping,\n                llm_config=llm_config,\n                delta_mode=True,\n            )\n            return (chunk[0] for chunk in chunks), []\n\n        logger.info(\"Fetching context from context RAG client\")\n        context_query = self.gen_context.prompt(conversations=conversations)\n        context_response = self.context_rag_client.chat.completions.create(\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": json.dumps(\n                        {\"query\": context_query, \"only_contexts\": True},\n                        ensure_ascii=False,\n                    ),\n                }\n            ],\n            model=model,\n        )\n\n        context = context_response.choices[0].message.content\n        new_context = \"\"\n        for msg in context.split(\"\\n\"):\n            if msg.strip():\n                v = json.loads(msg)\n                new_context += v[\"source_code\"]\n        logger.info(f\"context {new_context[0:100]}...\")\n        logger.info(\"Context retrieved successfully\")\n\n        logger.info(\"Fetching schemas from schema RAG client\")\n        schema_query = self.fetch_schema.prompt(context=new_context)\n\n        response = self.schema_rag_client.chat.completions.create(\n            messages=conversations[0:-1]\n            + [\n                {\n                    \"role\": \"user\",\n                    \"content\": json.dumps(\n                        {\"query\": schema_query, \"only_contexts\": True},\n                        ensure_ascii=False,\n                    ),\n                }\n            ],\n            model=model,\n        )        \n\n        schema_str = response.choices[0].message.content\n        logger.debug(f\"Schema RAG response: {schema_str}\")\n        logger.info(\"Schema retrieved successfully\")\n\n        logger.info(\"Generating SQL string\")\n        sql_str = self.gen_sql.with_llm(self.gen_sql_llm_model).run(\n            conversations=conversations,\n            context=new_context,\n            schema=schema_str,\n            model=self.model_config.sql_func_llm_model,\n        )\n\n        logger.info(\"SQL string generated\")\n        logger.debug(f\"Generated SQL string: {sql_str}\")\n\n        # Extract SQL code from tags\n        tag_extractor = TagExtractor(sql_str)\n        extracted_tags = tag_extractor.extract()\n        sql = extracted_tags.content[0].content\n        logger.info(f\"Extracted SQL: {sql}\")\n\n        if not self.byzer_sql_url:\n            return (item for item in [sql]), []\n\n        logger.info(f\"Sending SQL to Byzer SQL URL: {self.byzer_sql_url}\")\n        try:\n            response = requests.post(\n                self.byzer_sql_url,\n                data={\n                    \"sql\": sql,\n                    \"owner\": self.byzer_sql_config.owner or \"hello\",\n                    \"sessionPerUser\": True,\n                    \"sessionPerRequest\": True,\n                    \"includeSchema\": True,\n                },\n            )\n            response.raise_for_status()\n            result = response.content.decode(\"utf-8\")\n            logger.info(\"SQL execution result decoded\")\n            logger.debug(f\"SQL execution result: {result}\")\n            if self.byzer_sql_config.result_synthesis:\n                synthesized_result = self.result_synthesis.with_llm(self.llm_model).run(\n                    conversations=conversations,\n                    result=result,\n                )\n                return (item for item in [synthesized_result]), []\n            else:\n                return (item for item in [result]), []\n        except requests.exceptions.RequestException as e:\n            logger.error(f\"Failed to send SQL to Byzer SQL URL: {e}\")\n            if hasattr(e.response, \"content\"):\n                error_content = e.response.content.decode(\"utf-8\")\n                logger.error(f\"Server response: {error_content}\")\n            else:\n                logger.error(\"No server response available\")\n            raise Exception(f\"Byzer SQL request failed: {str(e)}\") from e\n\n\n##File: /Users/allwefantasy/projects/super_analysis/setup.py\nfrom setuptools import setup, find_packages\nimport os\n\nfolder = os.path.dirname(__file__)\nversion_path = os.path.join(folder, \"src\",\"superanalysis\", \"version.py\")\n\n__version__ = None\nwith open(version_path) as f:\n    exec(f.read(), globals())\n\nreq_path = os.path.join(folder, \"requirements.txt\")\ninstall_requires = []\nif os.path.exists(req_path):\n    with open(req_path, 'r') as fp:\n        install_requires = [line.strip() for line in fp]\n\nreadme_path = os.path.join(folder, \"README.md\")\nreadme_contents = \"\"\nif os.path.exists(readme_path):\n    with open(readme_path, 'r') as fp:\n        readme_contents = fp.read().strip()\n\nsetup(\n    name=\"super-analysis\",\n    version=__version__,\n    package_dir={\"\": \"src\"},\n    packages=find_packages(\"src\"),    \n    package_data={        \n    },\n    install_requires=install_requires,\n    entry_points={\n        \"console_scripts\": [\n            \"super-analysis.serve=superanalysis.serve.cli:main\",\n            \"super-analysis.convert=superanalysis.serve.convert:main\",\n            \"superanalysis.api=superanalysis.serve.rest:main\",\n        ],\n    },\n    author=\"allwefantasy\",\n    author_email=\"allwefantasy@gmail.com\",\n    description=\"A package for super analysis with ByzerLLM\",\n    long_description=open(\"README.md\").read(),\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/yourusername/super-analysis\",\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n    ],\n    python_requires='>=3.6',\n)\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/convert.py\nimport os\nimport argparse\nimport pandas as pd\nimport hashlib\nfrom typing import List, Dict\n\ndef get_csv_files(directory: str) -> List[str]:\n    \"\"\"Recursively get all CSV files in the given directory.\"\"\"\n    csv_files = []\n    for root, _, files in os.walk(directory):\n        for file in files:\n            if file.endswith('.csv'):\n                csv_files.append(os.path.join(root, file))\n    return csv_files\n\ndef extract_schema(df: pd.DataFrame) -> Dict[str, str]:\n    \"\"\"Extract schema from pandas DataFrame.\"\"\"\n    schema = {}\n    for column in df.columns:\n        dtype = str(df[column].dtype)\n        if dtype == 'object':\n            schema[column] = 'STRING'\n        elif dtype.startswith('int'):\n            schema[column] = 'INTEGER'\n        elif dtype.startswith('float'):\n            schema[column] = 'FLOAT'\n        elif dtype == 'bool':\n            schema[column] = 'BOOLEAN'\n        elif dtype.startswith('datetime'):\n            schema[column] = 'TIMESTAMP'\n        else:\n            schema[column] = 'STRING'  # Default to STRING for unknown types\n    return schema\n\ndef generate_schema_text(schema: Dict[str, str], csv_file: str, df: pd.DataFrame, include_rows_num: int) -> str:\n    \"\"\"Generate schema text from the schema dictionary and include the CSV file name and sample data.\"\"\"\n    schema_lines = [f\"CSV File: {csv_file}\\n\"]\n    schema_lines.extend([f\"{field}: {dtype}\" for field, dtype in schema.items()])\n    \n    if include_rows_num > 0:\n        schema_lines.append(f\"\\nSample Data (first {include_rows_num} rows):\")\n        sample_data = df.head(include_rows_num).to_string(index=False)\n        schema_lines.append(sample_data)\n    \n    return \"\\n\".join(schema_lines)\n\ndef save_schema(schema_text: str, csv_file: str, output_dir: str) -> None:\n    \"\"\"Save schema text to a file in the output directory.\"\"\"\n    # Generate MD5 hash of the schema text\n    file_name = hashlib.md5(schema_text.encode()).hexdigest() + '.md'\n    output_path = os.path.join(output_dir, file_name)\n    \n    with open(output_path, 'w') as f:\n        f.write(schema_text)\n    print(f\"Schema for {csv_file} saved to {output_path}\")\n\ndef convert(data_dir: str, doc_dir: str, include_rows_num: int) -> None:\n    \"\"\"Main conversion function.\"\"\"\n    csv_files = get_csv_files(data_dir)\n    \n    for csv_file in csv_files:\n        print(f\"Processing {csv_file}\")\n        df = pd.read_csv(csv_file)\n        schema = extract_schema(df)\n        schema_text = generate_schema_text(schema, csv_file, df, include_rows_num)\n        save_schema(schema_text, csv_file, doc_dir)\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Convert CSV files to schema documents\")\n    parser.add_argument(\"--data_dir\", required=True, help=\"Directory containing CSV files\")\n    parser.add_argument(\"--doc_dir\", required=True, help=\"Output directory for schema documents\")\n    parser.add_argument(\"--include-rows-num\", type=int, default=0, help=\"Number of sample rows to include in the schema\")\n    \n    args = parser.parse_args()\n    \n    if not os.path.exists(args.doc_dir):\n        os.makedirs(args.doc_dir)\n    \n    convert(args.data_dir, args.doc_dir, args.include_rows_num)\n\nif __name__ == \"__main__\":\n    main()\n\n##File: /Users/allwefantasy/projects/super_analysis/README.md\n\n# 🚀 Super Analysis 部署指南\n\n本指南将帮助你部署 Super Analysis 系统，包括安装必要组件、配置服务和启动系统。\n\n---\n\n## 📦 安装 Super Analysis\n\n\n```bash\npip install -U auto-coder\npip install super_analysis-xxxx-py3-none-any.whl\n```\n\n注意替换下 xxxx 为版本号。\n\n---\n\n## 🤖 部署 Deepseek 模型代理\n\n在启动其他服务之前，我们需要先部署 Deepseek 模型：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--worker_concurrency 1000 \\\n--num_workers 1 \\\n--infer_params saas.base_url=\"https://api.deepseek.com/v1\" saas.api_key=${MODEL_DEEPSEEK_TOKEN} saas.model=deepseek-chat \\\n--model deepseek_chat\n```\n\n注意：确保已设置环境变量 `MODEL_DEEPSEEK_TOKEN`。\n\n---\n\n## 🛠️ 部署 Byzer-SQL\n\n参考 [安装与配置 Byzer-SQL 文档](./4.3.1%20安装与配置%20Byzer-SQL.pdf) 完成部署。\n根据 [Byzer-SQL 和大模型整合文档](./4.2.1.3%20Byzer-SQL%20和大模型的整合.pdf) 中安装插件，然后注册 `deepseek_chat` 函数。\n\n> 启动时需要在安装有 super-analysis 的 conda 环境中启动。\n\n当 byzer-sql 部署完成后，注册账号为 `hello`，然后在 byzer-sql 控制台中执行：\n\n```sql\n!byzerllm setup single;\n\nrun command as LLM.`` where \naction=\"infer\"\nand reconnect=\"true\"\nand pretrainedModelType=\"saas/openai\"\nand udfName=\"deepseek_chat\";\n```\n\n---\n\n## 示例数据\n\n下载电影数据集： https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset/download?datasetVersionNumber=7\n\n---\n\n> 下面的指令都是在命令行里操作哈\n\n## 📊 数据预处理和服务启动\n\n1. 抽取电影数据集schema：\n\n```bash\nsuper-analysis.convert --data_dir /Users/allwefantasy/data/movice --doc_dir /Users/allwefantasy/data/movice/schemas/\n```\n\n你还可以添加 --include-rows-num 5 让系统在生成 schema 文档时同时提供一些示例数据。方便大模型更好的对这个表进行认知。\n\n\n2. 启动 schema 文档知识库：\n\n```bash\nauto-coder.rag serve \\\n--model deepseek_chat --index_filter_workers 100 \\\n--tokenizer_path /Users/allwefantasy/Downloads/tokenizer.json \\\n--doc_dir /Users/allwefantasy/data/movice/schemas/ \\\n--port 8001\n```\n\n3. 下载 Byzer-SQL 文档并启动文档知识库：\n\n```bash\ngit clone https://github.com/allwefantasy/llm_friendly_packages\n\nauto-coder.rag serve \\\n--model deepseek_chat --index_filter_workers 100 \\\n--tokenizer_path /Users/allwefantasy/Downloads/tokenizer.json \\\n--doc_dir  /Users/allwefantasy/projects/llm_friendly_packages/github.com/allwefantasy \\\n--port 8002\n```\n\n4. 启动兼容 OpenAI Server 的分析服务：\n\n```bash\nsuper-analysis.serve --served-model-name deepseek_chat --port 8000 \\\n--schema-rag-base-url http://127.0.0.1:8001/v1 \\\n--context-rag-base-url http://127.0.0.1:8002/v1 \\\n--byzer-sql-url http://127.0.0.1:9003/run/script\n```\n\n你可以通过 `--sql-func-llm-model` 函数单独为 SQL 函数指定模型(比如配置一个速度极快的模型)。注意，同样的，你需要在 Byzer-SQL 中注册这个函数。\n\n---\n\n现在，Super Analysis 系统已经完全部署并启动。你可以开始使用 OpenAI SDK 进行测试和接口调用。具体测试和接口使用方法请参考 [openai_local_api.ipynb](./openai_local_api.ipynb)。\n\n🎉 恭喜！你已经成功部署了 Super Analysis 系统。如有任何问题，请随时查阅文档或联系支持团队。\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/serve/rest.py\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nfrom typing import List, Optional\nimport uvicorn\nimport argparse\nfrom superanalysis.sql.generator import SQLGenerator\nfrom superanalysis.sql.types import RAGConfig, ByzerSQLConfig, ModelConfig\nimport byzerllm\n\napp = FastAPI()\n\nclass ChatMessage(BaseModel):\n    role: str\n    content: str\n\nclass ChatRequest(BaseModel):\n    conversations: List[ChatMessage]    \n    model_config: Optional[ModelConfig] = None\n\nclass ChatResponse(BaseModel):\n    response: str\n\n@app.post(\"/chat\", response_model=ChatResponse)\nasync def chat(request: ChatRequest):\n    try:\n        sql_generator = SQLGenerator(\n            llm=llm,\n            rag_config=rag_config,\n            byzer_sql_config=byzer_sql_config,\n            model_config=request.model_config\n        )\n        result, _ = sql_generator.stream_chat_oai(\n            conversations=[{\"role\": msg.role, \"content\": msg.content} for msg in request.conversations],\n            model=request.model_config.llm_model\n        )\n        return ChatResponse(response=\"\".join(result))\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"SuperAnalysis API Server\")\n    parser.add_argument(\"--host\", type=str, default=\"0.0.0.0\", help=\"Host to bind the server to\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"Port to bind the server to\")\n    parser.add_argument(\"--schema-rag-base-url\", type=str, required=True, help=\"Schema RAG base URL\")\n    parser.add_argument(\"--schema-rag-api-key\", type=str, required=True, help=\"Schema RAG API key\")\n    parser.add_argument(\"--context-rag-base-url\", type=str, required=True, help=\"Context RAG base URL\")\n    parser.add_argument(\"--context-rag-api-key\", type=str, required=True, help=\"Context RAG API key\")\n    parser.add_argument(\"--byzer-sql-url\", type=str, required=True, help=\"Byzer SQL URL\")\n    parser.add_argument(\"--data-dir\", type=str, default=\"\", help=\"Data directory\")\n    parser.add_argument(\"--owner\", type=str, default=\"hello\", help=\"Owner of the Byzer SQL session\")\n    parser.add_argument(\"--llm-model\", type=str, required=True, help=\"LLM model for general use\")\n    parser.add_argument(\"--gen-sql-llm-model\", type=str, required=True, help=\"LLM model for SQL generation\")\n    \n    args = parser.parse_args()\n\n    global rag_config\n\n    rag_config = RAGConfig(\n        schema_rag_base_url=args.schema_rag_base_url,\n        schema_rag_api_key=args.schema_rag_api_key,\n        context_rag_base_url=args.context_rag_base_url,\n        context_rag_api_key=args.context_rag_api_key\n    )\n    global byzer_sql_config\n    byzer_sql_config = ByzerSQLConfig(\n        byzer_sql_url=args.byzer_sql_url,\n        data_dir=args.data_dir,\n        owner=args.owner\n    )\n\n    global llm\n    llm = byzerllm.ByzerLLM()\n    llm.setup_default_model_name(args.llm_model)\n\n    uvicorn.run(app, host=args.host, port=args.port)\n\nif __name__ == \"__main__\":\n    main()\n\n##File: /Users/allwefantasy/projects/super_analysis/Makefile\n# Makefile for super-analysis project\n\n# Python interpreter\nPYTHON := python\n\n# Project name\nPROJECT_NAME := super-analysis\n\n# Directories\nSRC_DIR := src\nDIST_DIR := dist\nBUILD_DIR := build\n\n# Commands\nSETUP := $(PYTHON) setup.py\n\n.PHONY: all clean build wheel deploy_deepseek dist\n\nall: clean build wheel dist\n\ndeploy_deepseek:\n\tbyzerllm deploy --pretrained_model_type saas/openai \\\n\t--cpus_per_worker 0.001 \\\n\t--gpus_per_worker 0 \\\n\t--worker_concurrency 1000 \\\n\t--num_workers 1 \\\n\t--infer_params saas.base_url=\"https://api.deepseek.com/v1\" saas.api_key=${MODEL_DEEPSEEK_TOKEN} saas.model=deepseek-chat \\\n\t--model deepseek_chat\n\nclean:\n\t@echo \"Cleaning up...\"\n\trm -rf $(BUILD_DIR) $(DIST_DIR) *.egg-info\n\tfind . -type d -name \"__pycache__\" -exec rm -rf {} +\n\tfind . -type f -name \"*.pyc\" -delete\n\nbuild:\n\t@echo \"Building the project...\"\n\t$(SETUP) build\n\nwheel:\n\t@echo \"Creating wheel package...\"\t\n\t$(SETUP) bdist_wheel\n\ninstall:\n\t@echo \"Installing the package...\"\n\t$(PYTHON) -m pip install .\n\nuninstall:\n\t@echo \"Uninstalling the package...\"\n\t$(PYTHON) -m pip uninstall -y $(PROJECT_NAME)\n\ndevelop:\n\t@echo \"Installing the package in editable mode...\"\n\t$(PYTHON) -m pip install -e .\n\ntest:\n\t@echo \"Running tests...\"\n\t$(PYTHON) -m unittest discover tests\n\ndist: wheel\n\t@echo \"Creating distribution package...\"\n\t$(eval VERSION := $(shell python -c \"import sys; sys.path.append('src'); from superanalysis.version import __version__; print(__version__)\"))\n\tmkdir -p releases\n\trm -rf releases/super_analysis-$(VERSION).tar.gz\n\tmkdir -p super_analysis-$(VERSION)\n\tcp -r $(DIST_DIR)/* super_analysis-$(VERSION)/\n\tcp \"./docs/4.3.1 安装与配置 Byzer-SQL.pdf\" super_analysis-$(VERSION)/\n\tcp \"./docs/4.2.1.3 Byzer-SQL 和大模型的整合.pdf\" super_analysis-$(VERSION)/\n\tcp \"./docs/deploy.md\" super_analysis-$(VERSION)/README.md\n\tcp \"./notebooks/openai_local_api.ipynb\" super_analysis-$(VERSION)/\n\ttar -czf releases/super_analysis-$(VERSION).tar.gz super_analysis-$(VERSION)\n\trm -rf super_analysis-$(VERSION)\n\t@echo \"Distribution package created: releases/super_analysis-$(VERSION).tar.gz\"\n\n##File: /Users/allwefantasy/projects/super_analysis/docs/deploy.md\n\n# 🚀 Super Analysis 部署指南\n\n本指南将帮助你部署 Super Analysis 系统，包括安装必要组件、配置服务和启动系统。\n\n---\n\n## 📦 安装 Super Analysis\n\n\n```bash\npip install -U auto-coder\npip install super_analysis-xxxx-py3-none-any.whl\n```\n\n注意替换下 xxxx 为版本号。\n\n---\n\n## 🤖 部署 Deepseek 模型代理\n\n在启动其他服务之前，我们需要先部署 Deepseek 模型：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--worker_concurrency 1000 \\\n--num_workers 1 \\\n--infer_params saas.base_url=\"https://api.deepseek.com/v1\" saas.api_key=${MODEL_DEEPSEEK_TOKEN} saas.model=deepseek-chat \\\n--model deepseek_chat\n```\n\n注意：确保已设置环境变量 `MODEL_DEEPSEEK_TOKEN`。\n\n---\n\n## 🛠️ 部署 Byzer-SQL\n\n参考 [安装与配置 Byzer-SQL 文档](./4.3.1%20安装与配置%20Byzer-SQL.pdf) 完成部署。\n根据 [Byzer-SQL 和大模型整合文档](./4.2.1.3%20Byzer-SQL%20和大模型的整合.pdf) 中安装插件，然后注册 `deepseek_chat` 函数。\n\n> 启动时需要在安装有 super-analysis 的 conda 环境中启动。\n\n当 byzer-sql 部署完成后，注册账号为 `hello`，然后在 byzer-sql 控制台中执行：\n\n```sql\n!byzerllm setup single;\n\nrun command as LLM.`` where \naction=\"infer\"\nand reconnect=\"true\"\nand pretrainedModelType=\"saas/openai\"\nand udfName=\"deepseek_chat\";\n```\n\n---\n\n## 示例数据\n\n下载电影数据集： https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset/download?datasetVersionNumber=7\n\n---\n\n> 下面的指令都是在命令行里操作哈\n\n## 📊 数据预处理和服务启动\n\n1. 抽取电影数据集schema：\n\n```bash\nsuper-analysis.convert --data_dir /Users/allwefantasy/data/movice --doc_dir /Users/allwefantasy/data/movice/schemas/\n```\n\n你还可以添加 --include-rows-num 5 让系统在生成 schema 文档时同时提供一些示例数据。方便大模型更好的对这个表进行认知。\n\n\n2. 启动 schema 文档知识库：\n\n```bash\nauto-coder.rag serve \\\n--model deepseek_chat --index_filter_workers 100 \\\n--tokenizer_path /Users/allwefantasy/Downloads/tokenizer.json \\\n--doc_dir /Users/allwefantasy/data/movice/schemas/ \\\n--port 8001\n```\n\n3. 下载 Byzer-SQL 文档并启动文档知识库：\n\n```bash\ngit clone https://github.com/allwefantasy/llm_friendly_packages\n\nauto-coder.rag serve \\\n--model deepseek_chat --index_filter_workers 100 \\\n--tokenizer_path /Users/allwefantasy/Downloads/tokenizer.json \\\n--doc_dir  /Users/allwefantasy/projects/llm_friendly_packages/github.com/allwefantasy \\\n--port 8002\n```\n\n4. 启动兼容 OpenAI Server 的分析服务：\n\n```bash\nsuper-analysis.serve --served-model-name deepseek_chat --port 8000 \\\n--schema-rag-base-url http://127.0.0.1:8001/v1 \\\n--context-rag-base-url http://127.0.0.1:8002/v1 \\\n--byzer-sql-url http://127.0.0.1:9003/run/script\n```\n\n你可以通过 `--sql-func-llm-model` 函数单独为 SQL 函数指定模型(比如配置一个速度极快的模型)。注意，同样的，你需要在 Byzer-SQL 中注册这个函数。\n\n---\n\n现在，Super Analysis 系统已经完全部署并启动。你可以开始使用 OpenAI SDK 进行测试和接口调用。具体测试和接口使用方法请参考 [openai_local_api.ipynb](./openai_local_api.ipynb)。\n\n🎉 恭喜！你已经成功部署了 Super Analysis 系统。如有任何问题，请随时查阅文档或联系支持团队。\n\n##File: /Users/allwefantasy/projects/super_analysis/src/superanalysis/version.py\n__version__ = '0.1.4'\n\n##File: .auto-coder/libs/llm_friendly_packages/github.com/allwefantasy/byzer-llm/README.md\n# byzerllm 大模型编程快速指南\n\n本文示例 [Notebook](../../notebooks/003_byzerllm_大模型编程快速指南.ipynb)\n\n## 安装\n\n```bash\npip install -U byzerllm\nray start --head\n```\n## 启动一个模型代理\n\nbyzerllm 支持私有化模型或者SaaS模型的部署。\n\n这里以 deepseek 官方API 为例：\n\n```bash\neasy-byzerllm deploy deepseek-chat --token xxxxx --alias deepseek_chat\n```\n\n或者跬基流动API:\n\n```bash\neasy-byzerllm deploy alibaba/Qwen1.5-110B-Chat --token xxxxx --alias qwen110b_chat\n```\n\n将上面的 API KEY 替换成你们自己的。\n\n如果你想部署私有化模型或者对接 Ollama 等更多需求，参考 [002_使用byzerllm进行模型部署.md](./002_使用byzerllm进行模型部署.md) 或者 [README.md](../../README.md)。\n\n之后，你就可以在代码里使用  deepseek_chat 或者 qwen110b_chat  访问模型了。\n\n## hello world\n\n来和我们的大模型打个招呼:\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\n@byzerllm.prompt(llm=llm)\ndef hello(q:str) ->str:\n    '''\n    你好, {{ q }}\n    '''\n\ns = hello(\"你是谁\")    \nprint(s)\n\n## 输出:\n## '你好！我是一个人工智能助手，专门设计来回答问题、提供信息和帮助解决问题。如果你有任何疑问或需要帮助，请随时告诉我。'\n```\n\n恭喜，你和大模型成功打了招呼！\n\n可以看到，我们通过 `@byzerllm.prompt` 装饰器，将一个方法转换成了一个大模型的调用，然后这个方法的主题是一段文本，文本中\n使用了 jinja2 模板语法，来获得方法的参数。当正常调用该方法时，实际上就发起了和大模型的交互，并且返回了大模型的结果。\n\n在 byzerllm 中，我们把这种方法称之为 prompt 函数。\n\n## 查看发送给大模型的prompt\n\n很多情况你可能需要调试，查看自己的 prompt 渲染后到底是什么样子的，这个时候你可以通过如下方式\n获取渲染后的prompt:\n\n```python\nhello.prompt(\"你是谁\")\n## '你好, 你是谁'\n```            \n\n## 动态换一个模型\n\n前面的 hello 方法在初始化的时候，我们使用了默认的模型 deepseek_chat，如果我们想换一个模型，可以这样做：\n\n```python\nhello.with_llm(llm).run(\"你是谁\")\n## '你好！我是一个人工智能助手，专门设计来回答问题、提供信息和帮助解决问题。如果你有任何疑问或需要帮助，请随时告诉我。'\n```\n\n通过 with_llm 你可以设置一个新的 llm 对象，然后调用 run 方法，就可以使用新的模型了。\n\n## 超长文本生成\n\n我们知道，大模型一次生成的长度其实是有限的，如果你想生成超长文本，你可能需手动的不断获得\n生成结果，然后把他转化为输入，然后再次生成，这样的方式是比较麻烦的。\n\nbyzerllm 提供了更加易用的 API :\n\n```python\nimport byzerllm\nfrom byzerllm import ByzerLLM\n\nllm = ByzerLLM.from_default_model(\"deepseek_chat\")\n\n@byzerllm.prompt()\ndef tell_story() -> str:\n    \"\"\"\n    讲一个100字的故事。    \n    \"\"\"\n\n\ns = (\n    tell_story.with_llm(llm)\n    .with_response_markers()\n    .options({\"llm_config\": {\"max_length\": 10}})\n    .run()\n)\nprint(s)\n\n## 从前，森林里住着一只聪明的小狐狸。一天，它发现了一块闪闪发光的宝石。小狐狸决定用这块宝石帮助森林里的动物们。它用宝石的光芒指引迷路的小鹿找到了回家的路，用宝石的温暖治愈了受伤的小鸟。从此，小狐狸成了森林里的英雄，动物们都感激它的善良和智慧。\n```\n\n实际核心部分就是这一行：\n\n```python\ntell_story.with_llm(llm)\n    .with_response_markers()    \n    .run()\n```\n\n我们只需要调用 `with_response_markers` 方法，系统就会自动的帮我们生成超长文本。\n在上面的案例中，我们通过\n\n```python\n.options({\"llm_config\": {\"max_length\": 10}})\n```\n\n认为的限制大模型一次交互最多只能输出10个字符，但是系统依然自动完成了远超过10个字符的文本生成。\n\n## 对象输出\n\n前面我们的例子都是返回字符串，但是我们也可以返回对象，这样我们就可以更加灵活的处理返回结果。\n\n```python\nimport pydantic \n\nclass Story(pydantic.BaseModel):\n    '''\n    故事\n    '''\n\n    title: str = pydantic.Field(description=\"故事的标题\")\n    body: str = pydantic.Field(description=\"故事主体\")\n\n@byzerllm.prompt()\ndef tell_story()->Story:\n    '''\n    讲一个100字的故事。    \n    '''\n\ns = tell_story.with_llm(llm).run()\nprint(isinstance(s, Story))\nprint(s.title)\n\n## True\n## 勇敢的小鸟\n```\n\n可以看到，我们很轻松的将输出转化为格式化输出。\n\n## 自定义字段抽取\n\n前面的结构化输出，其实会消耗更多token,还有一种更加精准的结构化输出方式。\n比如让大模型生成一个正则表达式，但实际上大模型很难准确只输出一个正则表达式，这个时候我们可以通过自定义抽取函数来获取我们想要的结果。\n\n\n```python\nfrom loguru import logger\nimport re\n\n@byzerllm.prompt()\ndef generate_regex_pattern(desc: str) -> str:\n    \"\"\"\n    根据下面的描述生成一个正则表达式，要符合python re.compile 库的要求。\n\n    {{ desc }}\n\n    最后生成的正则表达式要在<REGEX></REGEX>标签对里。\n    \"\"\"    \n\ndef extract_regex_pattern(regex_block: str) -> str:    \n    pattern = re.search(r\"<REGEX>(.*)</REGEX>\", regex_block, re.DOTALL)\n    if pattern is None:\n        logger.warning(\"No regex pattern found in the generated block:\\n {regex_block}\")\n        raise None\n    return pattern.group(1)\n\npattern = \"匹配一个邮箱地址\"\nv = generate_regex_pattern.with_llm(llm).with_extractor(extract_regex_pattern).run(desc=pattern)\nprint(v)\n## ^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\n```\n\n在上面的例子里，我们根据一句话生成一个正则表达式。我们通过 `with_extractor` 方法，传入了一个自定义的抽取函数，这个函数会在大模型生成结果后，对结果进行处理，然后返回我们想要的结果。\n\n我们在 prompt 明确说了，生成的结果要放到 `<REGEX></REGEX>` 标签对里，然后我们通过 extract_regex_pattern 函数，从结果中提取出了我们想要的正则表达式。\n\n## 在实例方法中使用大模型\n\n```python\nimport byzerllm\ndata = {\n    'name': 'Jane Doe',\n    'task_count': 3,\n    'tasks': [\n        {'name': 'Submit report', 'due_date': '2024-03-10'},\n        {'name': 'Finish project', 'due_date': '2024-03-15'},\n        {'name': 'Reply to emails', 'due_date': '2024-03-08'}\n    ]\n}\n\n\nclass RAG():\n    def __init__(self):        \n        self.llm = byzerllm.ByzerLLM()\n        self.llm.setup_template(model=\"deepseek_chat\",template=\"auto\")\n        self.llm.setup_default_model_name(\"deepseek_chat\")        \n    \n    @byzerllm.prompt(lambda self:self.llm)\n    def generate_answer(self,name,task_count,tasks)->str:\n        '''\n        Hello {{ name }},\n\n        This is a reminder that you have {{ task_count }} pending tasks:\n        {% for task in tasks %}\n        - Task: {{ task.name }} | Due: {{ task.due_date }}\n        {% endfor %}\n\n        Best regards,\n        Your Reminder System\n        '''        \n\nt = RAG()\n\nresponse = t.generate_answer(**data)\nprint(response)\n\n## 输出:\n## Hello Jane Doe,\n##I hope this message finds you well. I wanted to remind you of your 3 pending tasks to ensure you stay on track:\n## 1. **Submit report** - This task is due on **2024-03-10**. Please ensure that you allocat\n```\n\n这里我们给了个比较复杂的例子，但我们可以看到，给一个实例prompt方法和普通prompt 方法差异不大。\n唯一的区别是如果你希望在定义的时候就指定大模型，使用一个lambda函数返回实例的 llm 对象即可。\n\n```python\n@byzerllm.prompt(lambda self:self.llm)\n```\n\n你也可以不返回，在调用的时候通过 `with_llm` 方法指定 llm 对象。\n\n此外，这个例子也展示了如何通过jinja2模板语法，来处理复杂的结构化数据。\n\n## 通过 Python 代码处理复杂入参\n\n上面的一个例子中，我们通过 jinja2 模板语法，来处理复杂的结构化数据，但是有时候我们可能需要更加复杂的处理，这个时候我们可以通过 Python 代码来处理。\n\n```python\nimport byzerllm\n\ndata = {\n    'name': 'Jane Doe',\n    'task_count': 3,\n    'tasks': [\n        {'name': 'Submit report', 'due_date': '2024-03-10'},\n        {'name': 'Finish project', 'due_date': '2024-03-15'},\n        {'name': 'Reply to emails', 'due_date': '2024-03-08'}\n    ]\n}\n\n\nclass RAG():\n    def __init__(self):        \n        self.llm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n    \n    @byzerllm.prompt()\n    def generate_answer(self,name,task_count,tasks)->str:\n        '''\n        Hello {{ name }},\n\n        This is a reminder that you have {{ task_count }} pending tasks:\n            \n        {{ tasks }}\n\n        Best regards,\n        Your Reminder System\n        '''\n        \n        tasks_str = \"\\n\".join([f\"- Task: {task['name']} | Due: { task['due_date'] }\" for task in tasks])\n        return {\"tasks\": tasks_str}\n\nt = RAG()\n\nresponse = t.generate_answer.with_llm(t.llm).run(**data)\nprint(response)\n\n## Just a gentle nudge to keep you on track with your pending tasks. Here's a quick recap:....\n```\n\n在这个例子里，我们直接把 tasks 在方法体里进行处理，然后作为一个字符串返回，最够构建一个字典，字典的key为 tasks,然后\n你就可以在 docstring 里使用 `{{ tasks }}` 来引用这个字符串。\n\n这样对于很复杂的入参，就不用谢繁琐的 jinja2 模板语法了。\n\n## 如何自动实现一个方法\n\n比如我定义一个签名，但是我不想自己实现里面的逻辑，让大模型来实现。这个在 byzerllm 中叫 function impl。我们来看看怎么\n实现:\n\n```python\nimport pydantic\nclass Time(pydantic.BaseModel):\n    time: str = pydantic.Field(...,description=\"时间，时间格式为 yyyy-MM-dd\")\n\n\n@llm.impl()\ndef calculate_current_time()->Time:\n    '''\n    计算当前时间\n    '''\n    pass \n\n\ncalculate_current_time()\n#output: Time(time='2024-06-14')\n```\n\n在这个例子里，我们定义了一个 calculate_current_time 方法，但是我们没有实现里面的逻辑，我们通过 `@llm.impl()` 装饰器，让大模型来实现这个方法。\n为了避免每次都要“生成”这个方法，导致无法适用，我们提供了缓存，用户可以按如下方式打印速度：\n\n```python\nstart = time.monotonic()\ncalculate_current_time()\nprint(f\"first time cost: {time.monotonic()-start}\")\n\nstart = time.monotonic()\ncalculate_current_time()\nprint(f\"second time cost: {time.monotonic()-start}\")\n\n# output:\n# first time cost: 6.067266260739416\n# second time cost: 4.347506910562515e-05\n```\n可以看到，第一次执行花费了6s,第二次几乎是瞬间完成的，这是因为第一次执行的时候，我们实际上是在生成这个方法，第二次执行的时候，我们是执行已经生成好的代码，所以速度会非常快。你可以显示的调用 `llm.clear_impl_cache()` 清理掉函数缓存。\n\n## Stream 模式\n\n前面的例子都是一次性生成结果，但是有时候我们可能需要一个流式的输出，这个时候我们可能需要用底层一点的API来完成了：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\nv = llm.stream_chat_oai(model=\"deepseek_chat\",conversations=[{\n    \"role\":\"user\",\n    \"content\":\"你好，你是谁\",\n}],delta_mode=True)\n\nfor t in v:\n    print(t,flush=True)  \n\n# 你好\n# ！\n# 我\n# 是一个\n# 人工智能\n# 助手\n# ，\n# 旨在\n# 提供\n# 信息\n# 、\n# 解答\n# 问题....\n```\n\n如果你不想要流式输出，但是想用底层一点的API，你可以使用 `llm.chat_oai` 方法：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\nv = llm.chat_oai(model=\"deepseek_chat\",conversations=[{\n    \"role\":\"user\",\n    \"content\":\"你好，你是谁\",\n}])\n\nprint(v[0].output)\n## 你好！我是一个人工智能助手，旨在提供信息、解答问题和帮助用户解决问题。如果你有任何问题或需要帮助，请随时告诉我。\n```\n\n## Function Calling \n\nbyzerllm 可以不依赖模型自身就能提供 function calling 支持，我们来看个例子：\n\n\n```python\nfrom typing import List,Dict,Any,Annotated\nimport pydantic \nimport datetime\nfrom dateutil.relativedelta import relativedelta\n\ndef compute_date_range(count:Annotated[int,\"时间跨度，数值类型\"],\n                       unit:Annotated[str,\"时间单位，字符串类型\",{\"enum\":[\"day\",\"week\",\"month\",\"year\"]}])->List[str]:\n    '''\n    计算日期范围\n\n    Args:\n        count: 时间跨度，数值类型\n        unit: 时间单位，字符串类型，可选值为 day,week,month,year\n    '''        \n    now = datetime.datetime.now()\n    now_str = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n    if unit == \"day\":\n        return [(now - relativedelta(days=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"week\":\n        return [(now - relativedelta(weeks=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"month\":\n        return [(now - relativedelta(months=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"year\":\n        return [(now - relativedelta(years=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    return [\"\",\"\"]\n\ndef compute_now()->str:\n    '''\n    计算当前时间\n    '''\n    return datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n```\n\n我们定义了两个方法，一个是计算日期范围，一个是计算当前时间。\n\n现在我么可以来测试下，系统如何根据自然语言决定调用哪个方法：\n\n```python\nt = llm.chat_oai([{\n    \"content\":'''计算当前时间''',\n    \"role\":\"user\"    \n}],tools=[compute_date_range,compute_now],execute_tool=True)\n\nt[0].values\n\n## output: ['2024-06-14 15:18:02']\n```\n\n我们可以看到，他正确的选择了 compute_now 方法。\n\n接着我们再试一个：\n\n```python\nt = llm.chat_oai([{\n    \"content\":'''最近三个月趋势''',\n    \"role\":\"user\"    \n}],tools=[compute_date_range,compute_now],execute_tool=True)\n\nt[0].values\n\n## output: [['2024-03-14 15:19:13', '2024-06-14 15:19:13']]\n```\n\n模型正确的选择了 compute_date_range 方法。\n\n## 多模态\n\nbyerllm 也能很好的支持多模态的交互，而且统一了多模态大模型的接口，比如你可以用一样的方式使用 openai 或者 claude 的图片转文字能力， 或者一致的方式使用火山，azuer, openai的语音合成接口。\n\n### 图生文\n\n```python\nimport byzerllm\nfrom byzerllm.types import ImagePath\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\n\n\n@byzerllm.prompt()\ndef what_in_image(image_path: ImagePath) -> str:\n    \"\"\"\n    {{ image_path }}\n    这个图片里有什么？\n    \"\"\"    \n\n\nv = what_in_image.with_llm(vl_llm).run(\n    ImagePath(value=\"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\")\n)\nv\n## OUTPUT: 这张图片展示了多只可爱的猫咪，采用了艺术风格的绘画。猫咪们有不同的颜色和花纹，背景是浅棕色，上面还点缀着一些红色的花朵。整体画面给人一种温馨和谐的感觉\n```\n\n可以看到，我们只需要把 prompt 函数的图片地址入参使用 byzerllm.types.ImagePath里进行包装，就可以直接在 prompt 函数体里\n带上图片。\n\n或者你可以这样：\n\n```python\nimport byzerllm\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\n\n\n@byzerllm.prompt()\ndef what_in_image(image_path: str) -> str:\n    \"\"\"\n    {{ image }}\n    这个图片里有什么？\n    \"\"\"\n    return {\"image\": byzerllm.Image.load_image_from_path(image_path)}\n\n\nv = what_in_image.with_llm(vl_llm).run(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv\n```\n\n通过 `image_path` 参数，然后通过 `byzerllm.Image.load_image_from_path` 方法，转化为一个图片对象 image，最后在 prompt 函数体里\n使用 `{{ image }}` 引用这个图片对象。\n\n另外我们也是可以支持配置多张图片的。\n\n另外，我们也可以使用基础的 `llm.chat_oai` 方法来实现：\n\n```python\nimport byzerllm\nimport json\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\nimage = byzerllm.Image.load_image_from_path(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv = vl_llm.chat_oai(\n    conversations=[\n        {\n            \"role\": \"user\",\n            \"content\": json.dumps(\n                [{\"image\": image, \"text\": \"这个图片里有什么？\"}], ensure_ascii=False\n            ),\n        }\n    ]\n)\nv[0].output\n```\n\n还可以这么写：\n\n```python\nimport byzerllm\nimport json\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\nimage = byzerllm.Image.load_image_from_path(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv = vl_llm.chat_oai(\n    conversations=[\n        {\n            \"role\": \"user\",\n            \"content\": json.dumps(\n                [\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\"url\": image, \"detail\": \"high\"},\n                    },\n                    {\"text\": \"这个图片里有什么？\", \"type\": \"text\"},\n                ],\n                ensure_ascii=False,\n            ),\n        }\n    ]\n)\nv[0].output\n```\n\n### 语音合成\n\n这里以 openai 的 tts 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=tts-1 \\\n--model openai_tts\n```\n\n此外，byzerllm 支持 azure,火山引擎等 tts 语音合成引擎。\n\n接着你可以这么用：\n\n```python\nimport byzerllm\nimport base64\nimport json\n\nllm = byzerllm.ByzerLLM.from_default_model(\"openai_tts\")\n\n\nt = llm.chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\": json.dumps({\n        \"input\":\"hello, open_tts\",\n        \"voice\": \"alloy\",\n        \"response_format\": \"mp3\"\n    },ensure_ascii=False)\n}])\n\nwith open(\"voice.mp3\",\"wb\") as f:\n    f.write(base64.b64decode(t[0].output))\n```\n\ntts 模型生成没有prompt函数可以用，你需要直接使用 chat_oai。\n\n\n### 语音识别\n\n这里以 openai 的 whisper-1 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--worker_concurrency 10 \\\n--infer_params saas.model=whisper-1 saas.api_key=${MODEL_OPENAI_TOKEN} \\\n--model speech_to_text\n```\n\n语音识别的使用方式和图生文类似，我们可以直接在 prompt 函数体里带上音频文件。\n\n```python\nimport byzerllm\nimport json\nimport base64\nfrom byzerllm.types import AudioPath\n\nllm = byzerllm.ByzerLLM.from_default_model(\"speech_to_text\")\n\naudio_file = \"/Users/allwefantasy/videos/output_audio.mp3\"\n\n@byzerllm.prompt(llm=llm)\ndef audio_to_text(audio_file: AudioPath):\n    \"\"\"\n    {{ audio_file }}\n    \"\"\"\n\nv = audio_to_text(AudioPath(value=audio_file))\njson.loads(v)\n```\n输出的数据格式略微复杂：\n\n```\n{'text': 'In the last chapter, you and I started to step through the internal workings of a transformer. This is one of the key pieces of technology inside large language models, and a lot of other tools in the modern wave of AI.',\n 'task': 'transcribe',\n 'language': 'english',\n 'duration': 10.0,\n 'segments': [{'id': 0,\n   'seek': 0,\n   'start': 0.0,\n   'end': 4.78000020980835,\n   'text': ' In the last chapter, you and I started to step through the internal workings of a transformer.',\n   'tokens': [50364,\n    682,\n    264,\n    1036,\n    7187,\n    11,\n    291,\n    293,\n    286,\n    1409,\n    281,\n    1823,\n    807,\n    264,\n    6920,\n    589,\n    1109,\n    295,\n    257,\n    31782,\n    13,\n    50586],\n   'temperature': 0.0,\n   'avg_logprob': -0.28872039914131165,\n   'compression_ratio': 1.4220778942108154,\n   'no_speech_prob': 0.016033057123422623},\n  ....\n  {'id': 2,\n   'seek': 0,\n   'start': 8.579999923706055,\n   'end': 9.979999542236328,\n   'text': ' and a lot of other tools in the modern wave of AI.',\n   'tokens': [50759,\n    293,\n    257,\n    688,\n    295,\n    661,\n    3873,\n    294,\n    264,\n    4363,\n    5772,\n    295,\n    7318,\n    13,\n    50867],\n   'temperature': 0.0,\n   'avg_logprob': -0.28872039914131165,\n   'compression_ratio': 1.4220778942108154,\n   'no_speech_prob': 0.016033057123422623}],\n 'words': [{'word': 'In', 'start': 0.0, 'end': 0.18000000715255737},\n  {'word': 'the', 'start': 0.18000000715255737, 'end': 0.23999999463558197},\n  {'word': 'last', 'start': 0.23999999463558197, 'end': 0.5400000214576721},\n  {'word': 'chapter', 'start': 0.5400000214576721, 'end': 0.800000011920929},\n  ....\n  {'word': 'AI', 'start': 9.920000076293945, 'end': 9.979999542236328}]}\n```\n\n会输出每一句话以及每一个字所在的起始时间和截止时间。你可以根据需要来使用。\n\n\n### 文生图\n\n文生图和语音合成类似，首先要启动合适的模型,以openai 的 dall-e-3 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=dall-e-3 \\\n--model openai_image_gen\n```\n\n启动模型后，只需要记住几个模板参数即可使用，这里直接使用 chat_oai 方法来使用：\n\n```python\n\nimport byzerllm\nimport json\nimport base64\n\nllm = byzerllm.ByzerLLM.from_default_model(\"openai_image_gen\")\nt = llm.chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\": json.dumps({\n        \"input\":\"a white siamese cat\",\n        \"size\": \"1024x1024\",\n        \"quality\": \"standard\"\n    },ensure_ascii=False)\n}])\n\nwith open(\"output1.jpg\",\"wb\") as f:\n    f.write(base64.b64decode(t[0].output))\n\n\nimport matplotlib.pyplot as plt\n\nimage_path = \"output1.jpg\"\nimage = plt.imread(image_path)\n\nplt.imshow(image)\nplt.axis('off')\nplt.show()\n```\n\n## Prompt 函数的流式输出\n\nbyzerllm 底层支持流式输出，非 prompt 函数的用法是这样的：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\n\nv = llm.stream_chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\":\"讲一个100字的故事\"\n}])\n\nfor s  in v:\n    print(s[0], end=\"\")\n```\n\n如果你像用 prompt 函数，可以这么用：\n\n\n```python\nimport byzerllm\nimport json\nimport base64\nfrom typing import Generator\n\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\n\n@byzerllm.prompt()\ndef tell_story() -> Generator[str, None, None]:\n    '''\n    给我讲一个一百多字的故事\n    '''\n\nv = tell_story.with_llm(llm).run()    \nfor i in v:\n    print(i, end=\"\")\n```\n\n可以看到，和普通的 prompt 函数的区别在于，返回值是一个生成器，然后你可以通过 for 循环来获取结果。\n\n## 向量化模型\n\nbyzerllm 支持向量化模型,你可以这样启动一个本地的模型：\n\n```bash\n!byzerllm deploy --pretrained_model_type custom/bge \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--worker_concurrency 10 \\\n--model_path /home/winubuntu/.auto-coder/storage/models/AI-ModelScope/bge-large-zh \\\n--infer_backend transformers \\\n--num_workers 1 \\\n--model emb\n```\n\n注意两个参数:\n\n1. --infer_backend transformers: 表示使用 transformers 作为推理后端。\n2. --model_path: 表示模型的路径。\n\n也可以启动一个 SaaS 的emb模型,比如 qwen 的 emb 模型：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/qianwen \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 2 \\\n--infer_params saas.api_key=${MODEL_QIANWEN_TOKEN}  saas.model=text-embedding-v2 \\\n--model qianwen_emb\n```\n\n或者 openai 的 emb 模型：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--worker_concurrency 10 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=text-embedding-3-small \\\n--model gpt_emb\n```\nSaaS 模型无需配置 `--infer_backend` 参数。\n\n无论是本地模型还是 SaaS 模型，我们都可以这样使用：\n\n```python\nimport byzerllm\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\nllm.setup_default_emb_model_name(\"emb\")\nllm.emb_query(\"你好\")\n```\n\n如果你配置 byzerllm 中的 Storage 使用，比如你这样启动了存储：\n\n```bash\nbyzerllm storage start --enable_emb\n```\n\n那么需要这么使用：\n\n```python\nfrom byzerllm.apps.byzer_storage.simple_api import ByzerStorage, DataType, FieldOption,SortOption\nstorage = ByzerStorage(\"byzerai_store\", \"memory\", \"memory\")\nstorage.emb(\"你好\")\n```\n\n## 一些辅助工具\n\n当调用 prompt 函数返回字符串的时候，如果想从里面抽取代码，可以使用如下方式：\n\n```python\nfrom byzerllm.utils.client import code_utils\ntext_with_markdown = '''\n```shell\nls -l\n```\n'''\ncode_blocks = code_utils.extract_code(text_with_markdown)\nfor code_block in code_blocks:\n    if code_block[0] == \"shell\":\n        print(code_block[1])\n##output: ls -l        \n```\n\nTagExtractor 工具用于对任意 `<_tag_></_tag_>` 标签对的抽取，下面是一个使用示例：\n\n```python\nfrom byzerllm.apps.utils import TagExtractor\n\nextractor = TagExtractor('''\n大家好\n<_image_>data:image/jpeg;base64,xxxxxx</_image_>\n                         大家好\n<_image_>data:image/jpeg;base64,xxxxxx2</_image_>\n''')\n\nv = extractor.extract()\nprint(v.content[0].content)\nprint(v.content[1].content)\n```\n输出为:\n\n```\ndata:image/jpeg;base64,xxxxxx\ndata:image/jpeg;base64,xxxxxx2\n```\n\n我们成功的将 <_image_></_image_> 标签对里的内容抽取出来了。\n\n\n## 注意事项\n\n1. prompt函数方法体返回只能是dict，实际的返回类型和方法签名可以不一样，但是方法体返回只能是dict。\n2. 大部分情况prompt函数体为空，如果一定要有方法体，可以返回一个空字典。\n3. 调用prompt方法的时候，如果在@byzerllm.prompt()里没有指定llm对象，那么需要在调用的时候通过with_llm方法指定llm对象。\n\n===========================\n\n# Byzer Storage: 最易用的AI存储引擎\n\nByzer Storage是一个为AI应用设计的高性能存储引擎,它提供了简单易用的API,支持向量搜索、全文检索以及结构化查询。本文将详细介绍Byzer Storage的使用方法和主要特性。\n\n## 0. 安装和启动\n\n```bash\npip install byzerllm\nbyzerllm storage start\n```\n\nThat's it! Byzer Storage已经安装并启动成功,现在我们可以开始使用它了。默认会启动一个 byzerai_store 的集群。\n\n注意，如果你是这样启动的：\n\n```bash\nbyzerllm storage start --enable_emb\n```\n\n那么会自动启动一个 emb 模型，名字就叫 emb， ByzerStorage 会自动使用该模型，无需做任何其他配置。\n\n## 1. 初始化\n\n创建一个 ByzerStorage 对象，链接 byzerai_store 集群，并且指定数据库和表名（可以不存在）。\n\n```python\nfrom byzerllm.apps.byzer_storage.simple_api import ByzerStorage, DataType, FieldOption, SortOption\n\nstorage = ByzerStorage(\"byzerai_store\", \"my_database1\", \"my_table4s\")\n```\n\n\n## 2. 创建库表（可选）\n\nByzer Storage使用Schema来定义数据结构。我们可以使用SchemaBuilder来构建Schema:\n\n```python\n_ = (\n    storage.schema_builder()\n    .add_field(\"_id\", DataType.STRING)\n    .add_field(\"name\", DataType.STRING)\n    .add_field(\"content\", DataType.STRING, [FieldOption.ANALYZE])\n    .add_field(\"raw_content\", DataType.STRING, [FieldOption.NO_INDEX])    \n    .add_array_field(\"summary\", DataType.FLOAT)    \n    .add_field(\"created_time\", DataType.LONG, [FieldOption.SORT])    \n    .execute()\n)\n```\n\n这个Schema定义了以下字段:\n- `_id`: 字符串类型的主键\n- `name`: 字符串类型,可用于过滤条件\n- `content`: 字符串类型,带有ANALYZE选项,用于全文搜索\n- `raw_content`: 字符串类型,带有NO_INDEX选项,不会被索引\n- `summary`: 浮点数组类型,用于存储向量\n- `created_time`: 长整型,带有SORT选项,可用于排序\n\n需要注意的是：\n\n1. 如果一个字段带有ANALYZE选项,则该字段会被分词,并且可以用于全文搜索，但是就无法返回原始的文本了。所以你需要添加一个新字段专门用来存储原文，比如在我们这个例子里，我们新增了 raw_content 字段，并且显示的指定了 NO_INDEX 选项，这样就不会被索引，也就不会被分词，可以后续被检索后用来获取原始文本。\n2. 对于需要作为向量检索和存储的字段，需要指定为数组类型，比如我们这个例子里的 summary 字段。\n3. 如果你需要拿到向量字段的原始文本，那么你也需要添加一个新字段专门用来存储原文，就像我们这个例子里的 raw_content 字段一样。\n\n\n## 3. 写入数据\n\n准备数据并使用WriteBuilder写入Storage:\n\n```python\ndata = [\n    {\"_id\": \"1\", \"name\": \"Hello\", \"content\": \"Hello, world!\", \"raw_content\": \"Hello, world!\", \"summary\": \"hello world\", \"created_time\": 1612137600},\n    {\"_id\": \"2\", \"name\": \"Byzer\", \"content\": \"Byzer, world!\", \"raw_content\": \"Byzer, world!\", \"summary\": \"byzer\", \"created_time\": 1612137601},\n    {\"_id\": \"3\", \"name\": \"AI\", \"content\": \"AI, world!\", \"raw_content\": \"AI, world!\", \"summary\": \"AI\", \"created_time\": 16121376002},\n    {\"_id\": \"4\", \"name\": \"ByzerAI\", \"content\": \"ByzerAI, world!\", \"raw_content\": \"ByzerAI, world!\", \"summary\": \"ByzerAi\", \"created_time\": 16121376003},\n]\n\nstorage.write_builder().add_items(data, vector_fields=[\"summary\"], search_fields=[\"content\"]).execute()\nstorage.commit()\n```\n\n这里我们使用`add_items`方法批量添加数据,并指定了`summary`为向量字段,`content`为搜索字段。最后调用`commit()`来确保数据被持久化。\n\n\n从上面我们要写入到 byzer storage 的数据我们可以看到如下几个特点：\n\n1. 需要包含我们之前定义的 Schema中罗列的所有的字段，同时需要指定哪些是向量字段，哪些是检索字段。\n2. 向量和检索不能是同一个字段。\n3. 对于向量，检索字段，我们给到 write_builder 的都是文本，ByzerStorage 会根据 Schema 的定义自动将其转换为向量，检索字段需要的格式。\n\n## 4. 查询数据\n\nByzer Storage支持多种查询方式,包括向量搜索、全文检索、过滤和排序。\n\n### 4.1 向量搜索 + 全文检索\n\n```python\nquery = storage.query_builder()\nquery.set_vector_query(\"ByzerAI\", fields=[\"summary\"])\nresults = query.set_search_query(\"Hello\", fields=[\"content\"]).execute()\nprint(results)\n```\n\n这个查询结合了向量搜索和全文检索,它会在`summary`字段中搜索与\"ByzerAI\"相似的向量,同时在`content`字段中搜索包含\"Hello\"的文档。\n\n### 4.2 过滤 + 向量搜索 + 全文检索\n\n```python\nquery = storage.query_builder()\nquery.and_filter().add_condition(\"name\", \"AI\").build()\nquery.set_vector_query(\"ByzerAI\", fields=\"summary\")\nresults = query.set_search_query(\"Hello\", fields=[\"content\"]).execute()\nprint(results)\n```\n\n这个查询首先过滤`name`字段等于\"AI\"的文档,然后在结果中进行向量搜索和全文检索。\n\n### 4.3 过滤 + 排序\n\n```python\nquery = storage.query_builder()\nquery.and_filter().add_condition(\"name\", \"AI\").build().sort(\"created_time\", SortOption.DESC)\nresults = query.execute()\nprint(results)\n```\n\n这个查询过滤`name`字段等于\"AI\"的文档,然后按`created_time`字段降序排序。\n\n## 5. 删除数据\n\n### 5.1 根据ID删除\n\n```python\nstorage.delete_by_ids([\"3\"])\n\nquery = storage.query_builder()\nquery.and_filter().add_condition(\"name\", \"AI\").build()\nresults = query.execute()\nprint(results)\n```\n\n这里我们删除了ID为\"3\"的文档,然后查询验证删除结果。\n\n### 5.2 删除整个表\n\n```python\nstorage.drop()\n```\n\n这个操作会删除整个表及其所有数据,请谨慎使用。\n\n## 结论\n\nByzer Storage提供了一套简洁而强大的API,能够轻松实现向量搜索、全文检索、结构化查询等功能。它的设计非常适合AI应用场景,可以有效地存储和检索各种类型的数据。通过本文的介绍,相信读者已经对Byzer Storage有了基本的了解,并能够开始在自己的项目中使用这个强大的存储引擎。\n\n\n\n下面是对上面文件按分组给到的一些描述，当用户的需求正好匹配描述的时候，参考描述来做修改：\n\n     \n\n下面是用户的需求：\n\n@Makefile 添加一个新的命令 deploy , 参考 if [[ ${MODE} == \"release\" ]]; then\n git tag v${version}\n git push origin v${version}  # 请根据实际情况使用正确的远程仓库名\n echo \"Upload ${project} ${version}\"\n twine upload dist/*\nfi 这个的实现来实现 deploy 命令\n使用 SEARCH/REPLACE 格式输出\n"}, {"role": "assistant", "content": "好的,我可以帮你在Makefile中添加一个新的deploy命令。这里是使用SEARCH/REPLACE格式的修改:\n\n```python\n##File: /Users/allwefantasy/projects/super_analysis/Makefile\n<<<<<<< SEARCH\ndist: wheel\n\t@echo \"Creating distribution package...\"\n\t$(eval VERSION := $(shell python -c \"import sys; sys.path.append('src'); from superanalysis.version import __version__; print(__version__)\"))\n\tmkdir -p releases\n\trm -rf releases/super_analysis-$(VERSION).tar.gz\n\tmkdir -p super_analysis-$(VERSION)\n\tcp -r $(DIST_DIR)/* super_analysis-$(VERSION)/\n\tcp \"./docs/4.3.1 安装与配置 Byzer-SQL.pdf\" super_analysis-$(VERSION)/\n\tcp \"./docs/4.2.1.3 Byzer-SQL 和大模型的整合.pdf\" super_analysis-$(VERSION)/\n\tcp \"./docs/deploy.md\" super_analysis-$(VERSION)/README.md\n\tcp \"./notebooks/openai_local_api.ipynb\" super_analysis-$(VERSION)/\n\ttar -czf releases/super_analysis-$(VERSION).tar.gz super_analysis-$(VERSION)\n\trm -rf super_analysis-$(VERSION)\n\t@echo \"Distribution package created: releases/super_analysis-$(VERSION).tar.gz\"\n=======\ndist: wheel\n\t@echo \"Creating distribution package...\"\n\t$(eval VERSION := $(shell python -c \"import sys; sys.path.append('src'); from superanalysis.version import __version__; print(__version__)\"))\n\tmkdir -p releases\n\trm -rf releases/super_analysis-$(VERSION).tar.gz\n\tmkdir -p super_analysis-$(VERSION)\n\tcp -r $(DIST_DIR)/* super_analysis-$(VERSION)/\n\tcp \"./docs/4.3.1 安装与配置 Byzer-SQL.pdf\" super_analysis-$(VERSION)/\n\tcp \"./docs/4.2.1.3 Byzer-SQL 和大模型的整合.pdf\" super_analysis-$(VERSION)/\n\tcp \"./docs/deploy.md\" super_analysis-$(VERSION)/README.md\n\tcp \"./notebooks/openai_local_api.ipynb\" super_analysis-$(VERSION)/\n\ttar -czf releases/super_analysis-$(VERSION).tar.gz super_analysis-$(VERSION)\n\trm -rf super_analysis-$(VERSION)\n\t@echo \"Distribution package created: releases/super_analysis-$(VERSION).tar.gz\"\n\ndeploy: dist\n\t@echo \"Deploying package...\"\n\t$(eval VERSION := $(shell python -c \"import sys; sys.path.append('src'); from superanalysis.version import __version__; print(__version__)\"))\n\t@if [ \"$(MODE)\" = \"release\" ]; then \\\n\t\tgit tag v$(VERSION); \\\n\t\tgit push origin v$(VERSION); \\\n\t\techo \"Uploading super_analysis $(VERSION)\"; \\\n\t\ttwine upload dist/*; \\\n\telse \\\n\t\techo \"Skipping release deployment. Use MODE=release to deploy.\"; \\\n\tfi\n>>>>>>> REPLACE\n```\n\n这个修改添加了一个新的`deploy`目标,它依赖于`dist`目标。deploy命令会执行以下操作:\n\n1. 首先运行`dist`目标来创建分发包。\n2. 然后检查`MODE`环境变量是否设置为\"release\"。\n3. 如果是release模式:\n   - 创建一个新的git tag\n   - 推送这个tag到远程仓库\n   - 使用twine上传分发包到PyPI\n4. 如果不是release模式,则跳过发布步骤并显示一条消息。\n\n你可以通过运行`make deploy MODE=release`来执行完整的发布流程,或者只运行`make deploy`来创建分发包但不执行发布步骤。\n\n请确保在使用这个命令之前已经安装了`twine`,并且已经正确配置了你的PyPI凭证。"}], "model": "human", "yaml_file": "actions/153_chat_action.yml"}
