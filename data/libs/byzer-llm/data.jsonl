{"instruction": "把 stage == \"pt\" 和 stage == \"sft\" 抽取出来,放到同目录下的 train_sft.py, train_pt.py 中,同时新建一个 trani_dpo.py 注意,保留使用prompt 函数", "conversations": [{"role": "user", "content": "\n如果你需要生成代码，对于每个需要更改的文件,你需要按 *SEARCH/REPLACE block* 的格式进行生成。\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The opening fence and code language, eg: ```python\n2. The file path alone on a line, starting with \"##File:\" and verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n*SEARCH/REPLACE* blocks will replace *all* matching occurrences.\nInclude enough lines to make the SEARCH blocks unique.\n\nInclude *ALL* the code being searched and replaced!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n下面我们来看一个例子：\n\n当前项目目录结构：\n1. 项目根目录： /tmp/projects/mathweb\n2. 项目子目录/文件列表(类似tree 命令输出)\nflask/\n    app.py\n    templates/\n        index.html\n    static/\n        style.css\n\n用户需求： Change get_factorial() to use math.factorial\n\n回答： To make this change we need to modify `/tmp/projects/mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n用户需求： Refactor hello() into its own file.\n\n回答：To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/hello.py\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/main.py\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n现在让我们开始一个新的任务:\n\n当前项目目录结构：\n1. 项目根目录： /Users/allwefantasy/projects/byzer-llm\n2. 项目子目录/文件列表(类似tree 命令输出)：\nsetup.py\ntests/\n    test_emb_rerank.py\n    test_deepseek_code.py\n    test_prompt.py\n    test_model_meta.py\n    __init__.py\n    test_transformers_deploy.py\n    test_data_analysis.py\n    test_yi.py\n    test_deploy.py\n    functions_suite.py\n    test_template.py\n    auto/\n        test_backend_llama_cpp.py\n        __init__.py\nsetup-machine/\n    convert_safetensor.py\nnotebooks/\n    test.py\nsrc/\n    byzerllm/\n        lang.py\n        store.py\n        version.py\n        log.py\n        __init__.py\n        types.py\n        easy_byzerllm_command.py\n        byzerllm_command.py\n        command_args.py\n        bark/\n            generation.py\n            __init__.py\n            model.py\n            api.py\n            model_fine.py\n            bark_voice.py\n        moss/\n            finetune_moss.py\n            __init__.py\n            moss_inference.py\n            models/\n                configuration_moss.py\n                quantization.py\n                custom_autotune.py\n                __init__.py\n                modeling_moss.py\n                tokenization_moss.py\n        llama2/\n            __init__.py\n        records/\n            __init__.py\n        visualglm/\n            __init__.py\n        auto/\n            __init__.py\n            backend_llama_cpp.py\n            backend_vllm.py\n            backend_ds.py\n        alpha_moss/\n            __init__.py\n        processor/\n            __init__.py\n        qwen/\n            __init__.py\n        utils/\n            json_repaire.py\n            object_store_ref_util.py\n            __init__.py\n            types.py\n            tokenizer.py\n            openai_utils.py\n            ray_utils.py\n            text_generator.py\n            emb.py\n            connect_ray.py\n            testing.py\n            langutil.py\n            nontext.py\n            fulltune/\n                deepspeed_trainner.py\n                __init__.py\n                launch.py\n                trainner.py\n                base_model/\n                    __init__.py\n                    configuration_baichuan.py\n                    modeling_baichuan.py\n                pretrain/\n                    __init__.py\n                    convert_to_transformers.py\n            metrics/\n                __init__.py\n            config/\n                __init__.py\n            retrieval/\n                rest.py\n                __init__.py\n                udf.py\n            sft/\n                argument.py\n                collator.py\n                qlora.py\n                __init__.py\n                model.py\n                dataset.py\n                loss.py\n                merge_lora.py\n                trainer.py\n            client/\n                message_utils.py\n                math_utils.py\n                byzerllm_client.py\n                parallel_utils.py\n                __init__.py\n                types.py\n                code_utils.py\n                img_utils.py\n                entrypoints/\n                    __init__.py\n                    openai/\n                        protocol.py\n                        __init__.py\n                        serve.py\n                        serving_chat.py\n                        api_server.py\n                        serving_engine.py\n                        serving_completion.py\n                        tool.py\n        chatglm2/\n            __init__.py\n        baichuan/\n            __init__.py\n        m3e/\n            __init__.py\n        stable_diffusion/\n            config.py\n            __init__.py\n            logger.py\n            shared.py\n            model.py\n            utils.py\n            images.py\n            diffusion/\n                __init__.py\n                embeddings.py\n                utils.py\n                networks/\n                    lyco.py\n                    __init__.py\n                upscalers/\n                    __init__.py\n                    multidiffusion.py\n                    samplers.py\n                piplines/\n                    __init__.py\n                    lpw.py\n                    diffusers.py\n            lib/\n                __init__.py\n                diffusers/\n                    __init__.py\n                    scheduler.py\n            api/\n                __init__.py\n                plugin.py\n                models/\n                    diffusion.py\n                    __init__.py\n                diffusion/\n                    __init__.py\n                    pipelines/\n                        __init__.py\n                        diffusers.py\n                events/\n                    generation.py\n                    __init__.py\n        bge/\n            __init__.py\n        bge_rerank/\n            __init__.py\n        apps/\n            client.py\n            qa_strategy.py\n            __init__.py\n            builder.py\n            vector_db.py\n            utils.py\n            qa.py\n            byzer_storage/\n                memory_model_based.py\n                command.py\n                env.py\n                __init__.py\n                simple_api.py\n            byzer_sql/\n                __init__.py\n            llama_index/\n                byzerai_vectordb.py\n                byzerai.py\n                byzerai_embedding.py\n                __init__.py\n                collection_manager.py\n                byzerai_index_store.py\n                byzerai_docstore.py\n                simple_retrieval.py\n                byzerai_kvstore.py\n            agent/\n                conversable_agent.py\n                registry.py\n                groupchat.py\n                user_proxy_agent.py\n                __init__.py\n                agent.py\n                extensions/\n                    simple_retrieval_client.py\n                    planner.py\n                    python_codesandbox_agent.py\n                    preview_file_agent.py\n                    llama_index_retrieval_agent.py\n                    visualization_agent.py\n                    assistant_agent.py\n                    load_data_agent.py\n                    retrieval_agent.py\n                    output_agent.py\n                    __init__.py\n                    byzer_engine_agent.py\n                    subquestion_agent.py\n                    sql_reviewer_agent.py\n                    spark_sql_agent.py\n                    rhetorical_agent.py\n                    data_analysis.py\n                    data_analysis_pipeline_agent.py\n                    common_agent.py\n                    query_rewrite/\n                        time.py\n                        rhetorical.py\n                        __init__.py\n                        condition.py\n                        context.py\n                store/\n                    memory_store.py\n                    __init__.py\n                    stores.py\n        saas/\n            __init__.py\n            chatglm/\n                __init__.py\n            sparkdesk/\n                __init__.py\n            gemini/\n                __init__.py\n            azure_openai/\n                __init__.py\n            azure/\n                __init__.py\n            qianfan/\n                __init__.py\n            claude/\n                __init__.py\n            qianwen/\n                __init__.py\n            volcengine/\n                __init__.py\n            minimax/\n                __init__.py\n            baichuan/\n                __init__.py\n            qianwen_vl/\n                __init__.py\n            official_openai/\n                __init__.py\n            zhipu/\n                __init__.py\n            aws_bedrock/\n                __init__.py\n            openai/\n                __init__.py\n        falcon/\n            __init__.py\n        chatglm6b/\n            trainer_seq2seq.py\n            arguments.py\n            __init__.py\n            finetune.py\n            tunning/\n                __init__.py\n                infer.py\n                finetune.py\n                utils/\n                    config.py\n                    __init__.py\n                    other.py\n                    common.py\n                    seq2seq.py\n        starcode/\n            __init__.py\n        qwen_vl_chat/\n            __init__.py\n        zephyr/\n            __init__.py\n        dolly/\n            generate.py\n            __init__.py\n            consts.py\n            dolly_inference.py\n            trainer.py\n        llama/\n            __init__.py\n        whisper/\n            whisper_inference.py\n            __init__.py\n下面是一些文件路径以及每个文件对应的源码：\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/apps/byzer_storage/memory_model_based.py\nimport asyncio\nfrom typing import List, Dict, Any\n\nimport ray.experimental\nfrom byzerllm.apps.byzer_storage.simple_api import ByzerStorage\nimport time\nimport json\nimport os\nimport concurrent.futures\nimport io\nimport sys\nfrom contextlib import redirect_stdout, redirect_stderr\nimport queue\nimport threading\nimport ray\nimport byzerllm\nfrom typing import List, Union, Dict\nfrom pydantic import BaseModel\nfrom byzerllm.apps.utils import TagExtractor, Tag\nimport importlib\nfrom loguru import logger\n\n\nclass QAPair(BaseModel):\n    question: str\n    answer: str\n\n\ndef read_alpaca_zh():\n    with importlib.resources.path(\n        \"byzerllm.apps.byzer_storage\", \"alpaca_zh.json\"\n    ) as json_path:\n        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n            return json.load(f)\n\n\nclass MemoryManager:\n    _queue = asyncio.Queue()\n    _is_processing = False\n\n    def __init__(\n        self,\n        storage: ByzerStorage,\n        base_dir: str,\n        model_name: str = \"deepseek_chat\",\n        remote: bool = True,\n    ):\n        self.storage = storage\n        home = os.path.expanduser(\"~\")\n        self.base_dir = base_dir or os.path.join(home, \".auto-coder\")\n        self.thread_pool = concurrent.futures.ThreadPoolExecutor(max_workers=5)\n        self.log_file = None\n        self.remote = remote\n        self.model_name = model_name\n\n    @classmethod\n    async def add_to_queue(cls, name: str, memories: List[str]):\n        await cls._queue.put((name, memories))\n        if not cls._is_processing:\n            asyncio.create_task(cls.process_queue())\n\n    @classmethod\n    async def process_queue(cls):\n        cls._is_processing = True\n        while not cls._queue.empty():\n            name, memories = await cls._queue.get()\n            instance = cls.get_instance()\n            await instance.memorize(name, memories)\n            cls._queue.task_done()\n        cls._is_processing = False\n\n    async def memorize(\n        self, name: str, memories: List[str], options: Dict[str, Any] = {}\n    ):\n        try:\n            loop = asyncio.get_running_loop()\n            await loop.run_in_executor(\n                self.thread_pool, self._memorize_with_logs, name, memories, options\n            )\n        finally:\n            if self.remote:\n                ray.actor.exit_actor()\n\n        print(f\"Memorization for {name} completed.\")\n\n    def _memorize_with_logs(\n        self, name: str, memories: List[str], options: Dict[str, Any] = {}\n    ):\n\n        if self.remote:\n            self._memorize(name, memories, options=options)\n            return\n\n        logs_dir = os.path.join(self.base_dir, \"storage\", \"logs\", \"memorize\")\n        os.makedirs(logs_dir, exist_ok=True)\n        log_file = os.path.join(logs_dir, f\"{name}.log\")\n\n        log_queue = queue.Queue()\n        stop_event = threading.Event()\n\n        # Start the log writer thread\n        log_writer_thread = threading.Thread(\n            target=self._log_writer, args=(log_queue, log_file, stop_event)\n        )\n        self.log_file = log_file\n        log_writer_thread.start()\n\n        class QueueStream:\n            def __init__(self, queue):\n                self.queue = queue\n\n            def write(self, msg):\n                self.queue.put(msg)\n\n            def flush(self):\n                pass\n\n        queue_stream = QueueStream(log_queue)\n\n        with redirect_stdout(queue_stream), redirect_stderr(queue_stream):\n            self._memorize(name, memories)\n\n        # Signal the log writer to stop and wait for it to finish\n        stop_event.set()\n        log_writer_thread.join()\n\n    def _log_writer(\n        self, log_queue: queue.Queue, log_file: str, stop_event: threading.Event\n    ):\n        with open(log_file, \"w\") as f:\n            while not stop_event.is_set() or not log_queue.empty():\n                try:\n                    msg = log_queue.get(timeout=0.1)\n                    f.write(msg)\n                    f.flush()\n                except queue.Empty:\n                    continue\n\n    @byzerllm.prompt()\n    def _convert_pretrain_text_to_instruction(self, text: str) -> str:\n        \"\"\"\n        根据提供的信息，生成多个相关的问题，这些问题的回答，最终要能覆盖里面所有的信息,生成的问题越多越好。\n\n        下面是一些信息：\n\n        {{text}}\n\n        现在请开始生成问题，每个问题使用<_question_>标签包裹，每个回答使用<_answer_>标签包裹，最后\n        每个一组问题和回答使用<_group_>标签包裹。\n        \"\"\"\n\n    @byzerllm.prompt()\n    def _format(self, text: str) -> str:\n        \"\"\"\n        下面是一些问答信息：\n\n        {{text}}\n\n\n        请将每个问题使用<_question_>标签包裹，每个回答使用<_answer_>标签包裹，最后\n        每个一组问题和回答使用<_group_>标签包裹。\n        \"\"\"\n\n    def to_qa_pairs(self, text: str, llm) -> List[QAPair]:\n        print(f\"Generating QA pairs for {text}\", flush=True)\n        v = self._convert_pretrain_text_to_instruction.with_llm(llm).run(text)\n        # format_v = self._format.with_llm(llm).run(v)\n        root_tag = TagExtractor(v).extract()\n        qa_pairs = []\n        # _group_\n        for item in root_tag.content:\n            qas = item.content\n            if len(qas) == 2:\n                if (\n                    qas[0].start_tag == \"<_question_>\"\n                    and qas[0].end_tag == \"</_question_>\"\n                    and qas[1].start_tag == \"<_answer_>\"\n                    and qas[1].end_tag == \"</_answer_>\"\n                ):\n                    qa_pairs.append(\n                        QAPair(question=qas[0].content, answer=qas[1].content)\n                    )\n\n        print(f\"Generated {len(qa_pairs)} QA pairs.\", flush=True)\n        logger.info(f\"Generated {len(qa_pairs)} QA pairs.\")\n        return qa_pairs\n\n    def _memorize(\n        self,\n        name: str,\n        memories: List[Union[str, Dict[str, Any]]],\n        options: Dict[str, Any] = {},\n    ):\n        data = []\n        stage = options.get(\"stage\", \"pt\")\n        min_samples = options.pop(\"min_samples\", 1000)\n        base_model_dir = os.path.join(self.base_dir, \"storage\", \"models\")\n        llama_model = os.path.join(\n            base_model_dir, \"meta-llama\", \"Meta-Llama-3-8B-Instruct-GPTQ\"\n        )\n\n        loras_dir = os.path.join(self.base_dir, \"storage\", \"loras\")\n        dataset_dir = os.path.join(self.base_dir, \"storage\", \"datasets\", name)\n\n        os.makedirs(loras_dir, exist_ok=True)\n        os.makedirs(dataset_dir, exist_ok=True)\n\n        if stage == \"pt\":\n            for memory in memories:\n                item = {\n                    \"text\": memory,\n                }\n                data.append(item)\n        elif stage == \"sft\":\n            if memories:\n                with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:\n                    futures = []\n                    llm = byzerllm.ByzerLLM().from_default_model(self.model_name)\n                    for memory in memories:\n                        futures.append(executor.submit(self.to_qa_pairs, memory, llm))\n\n                    for future in concurrent.futures.as_completed(futures):\n                        qa_pairs = future.result()\n                        for qa_pair in qa_pairs:\n                            item = {\n                                \"instruction\": qa_pair.question,\n                                \"input\": \"\",\n                                \"output\": qa_pair.answer,\n                            }\n                            data.append(item)\n            else:\n                v = read_alpaca_zh()\n                data.extend(v)\n\n        if len(data) < min_samples:\n            data = data * (min_samples // len(data) + 1)\n        \n        with open(f\"{dataset_dir}/data.json\", \"w\", encoding=\"utf-8\") as f:\n            json.dump(data, f, indent=2, ensure_ascii=False)\n        \n        with open(f\"{dataset_dir}/dataset_info.json\", \"w\", encoding=\"utf-8\") as f:\n            r = {\"data\": {\"file_name\": \"data.json\"}}\n            if stage == \"pt\":\n                r[\"data\"][\"columns\"] = {\"prompt\": \"text\"}\n            f.write(\n                json.dumps(\n                    r,\n                    indent=2,\n                    ensure_ascii=False,\n                )\n            )\n\n        args = dict(\n            stage=\"pt\",\n            do_train=True,\n            model_name_or_path=llama_model,\n            dataset=\"data\",\n            dataset_dir=dataset_dir,\n            cutoff_len=1024,\n            max_samples=1000000,\n            overwrite_cache=True,\n            preprocessing_num_workers=1,\n            template=\"llama3\",\n            finetuning_type=\"lora\",\n            lora_target=\"all\",\n            output_dir=f\"{loras_dir}/{name}\",\n            overwrite_output_dir=True,\n            per_device_train_batch_size=2,\n            gradient_accumulation_steps=4,\n            lr_scheduler_type=\"cosine\",\n            logging_steps=10,\n            warmup_ratio=0.1,\n            save_steps=1000,\n            plot_loss=True,\n            learning_rate=5e-5,\n            num_train_epochs=1000.0,\n            max_grad_norm=1.0,\n            quantization_bit=4,\n            loraplus_lr_ratio=16.0,\n            fp16=True,\n            ddp_timeout=180000000,\n        )\n        os.environ[\"WANDB_DISABLED\"] = \"true\"\n        from llamafactory.train import tuner\n\n        tuner.run_exp({**args, **options})\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/docs/zh/003_byzerllm_大模型编程快速指南.md\n##  byzerllm 大模型编程快速指南\n\n本文示例 [Notebook](../../notebooks/003_byzerllm_大模型编程快速指南.ipynb)\n\n## 安装\n\n```bash\npip install -U byzerllm\nray start --head\n```\n## 启动一个模型代理\n\nbyzerllm 支持私有化模型或者SaaS模型的部署。\n\n这里以 deepseek 官方API 为例：\n\n```bash\neasy-byzerllm deploy deepseek-chat --token xxxxx --alias deepseek_chat\n```\n\n或者跬基流动API:\n\n```bash\neasy-byzerllm deploy alibaba/Qwen1.5-110B-Chat --token xxxxx --alias qwen110b_chat\n```\n\n将上面的 API KEY 替换成你们自己的。\n\n如果你想部署私有化模型或者对接 Ollama 等更多需求，参考 [002_使用byzerllm进行模型部署.md](./002_使用byzerllm进行模型部署.md) 或者 [README.md](../../README.md)。\n\n之后，你就可以在代码里使用  deepseek_chat 或者 qwen110b_chat  访问模型了。\n\n## hello world\n\n来和我们的大模型打个招呼:\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\n@byzerllm.prompt(llm=llm)\ndef hello(q:str) ->str:\n    '''\n    你好, {{ q }}\n    '''\n\ns = hello(\"你是谁\")    \nprint(s)\n\n## 输出:\n## '你好！我是一个人工智能助手，专门设计来回答问题、提供信息和帮助解决问题。如果你有任何疑问或需要帮助，请随时告诉我。'\n```\n\n恭喜，你和大模型成功打了招呼！\n\n可以看到，我们通过 `@byzerllm.prompt` 装饰器，将一个方法转换成了一个大模型的调用，然后这个方法的主题是一段文本，文本中\n使用了 jinja2 模板语法，来获得方法的参数。当正常调用该方法时，实际上就发起了和大模型的交互，并且返回了大模型的结果。\n\n在 byzerllm 中，我们把这种方法称之为 prompt 函数。\n\n## 查看发送给大模型的prompt\n\n很多情况你可能需要调试，查看自己的 prompt 渲染后到底是什么样子的，这个时候你可以通过如下方式\n获取渲染后的prompt:\n\n```python\nhello.prompt(\"你是谁\")\n## '你好, 你是谁'\n```            \n\n## 动态换一个模型\n\n前面的 hello 方法在初始化的时候，我们使用了默认的模型 deepseek_chat，如果我们想换一个模型，可以这样做：\n\n```python\nhello.with_llm(llm).run(\"你是谁\")\n## '你好！我是一个人工智能助手，专门设计来回答问题、提供信息和帮助解决问题。如果你有任何疑问或需要帮助，请随时告诉我。'\n```\n\n通过 with_llm 你可以设置一个新的 llm 对象，然后调用 run 方法，就可以使用新的模型了。\n\n## 超长文本生成\n\n我们知道，大模型一次生成的长度其实是有限的，如果你想生成超长文本，你可能需手动的不断获得\n生成结果，然后把他转化为输入，然后再次生成，这样的方式是比较麻烦的。\n\nbyzerllm 提供了更加易用的 API :\n\n```python\nimport byzerllm\nfrom byzerllm import ByzerLLM\n\nllm = ByzerLLM.from_default_model(\"deepseek_chat\")\n\n@byzerllm.prompt()\ndef tell_story() -> str:\n    \"\"\"\n    讲一个100字的故事。    \n    \"\"\"\n\n\ns = (\n    tell_story.with_llm(llm)\n    .with_response_markers()\n    .options({\"llm_config\": {\"max_length\": 10}})\n    .run()\n)\nprint(s)\n\n## 从前，森林里住着一只聪明的小狐狸。一天，它发现了一块闪闪发光的宝石。小狐狸决定用这块宝石帮助森林里的动物们。它用宝石的光芒指引迷路的小鹿找到了回家的路，用宝石的温暖治愈了受伤的小鸟。从此，小狐狸成了森林里的英雄，动物们都感激它的善良和智慧。\n```\n\n实际核心部分就是这一行：\n\n```python\ntell_story.with_llm(llm)\n    .with_response_markers()    \n    .run()\n```\n\n我们只需要调用 `with_response_markers` 方法，系统就会自动的帮我们生成超长文本。\n在上面的案例中，我们通过\n\n```python\n.options({\"llm_config\": {\"max_length\": 10}})\n```\n\n认为的限制大模型一次交互最多只能输出10个字符，但是系统依然自动完成了远超过10个字符的文本生成。\n\n## 对象输出\n\n前面我们的例子都是返回字符串，但是我们也可以返回对象，这样我们就可以更加灵活的处理返回结果。\n\n```python\nimport pydantic \n\nclass Story(pydantic.BaseModel):\n    '''\n    故事\n    '''\n\n    title: str = pydantic.Field(description=\"故事的标题\")\n    body: str = pydantic.Field(description=\"故事主体\")\n\n@byzerllm.prompt()\ndef tell_story()->Story:\n    '''\n    讲一个100字的故事。    \n    '''\n\ns = tell_story.with_llm(llm).run()\nprint(isinstance(s, Story))\nprint(s.title)\n\n## True\n## 勇敢的小鸟\n```\n\n可以看到，我们很轻松的将输出转化为格式化输出。\n\n## 自定义字段抽取\n\n前面的结构化输出，其实会消耗更多token,还有一种更加精准的结构化输出方式。\n比如让大模型生成一个正则表达式，但实际上大模型很难准确只输出一个正则表达式，这个时候我们可以通过自定义抽取函数来获取我们想要的结果。\n\n\n```python\nfrom loguru import logger\nimport re\n\n@byzerllm.prompt()\ndef generate_regex_pattern(desc: str) -> str:\n    \"\"\"\n    根据下面的描述生成一个正则表达式，要符合python re.compile 库的要求。\n\n    {{ desc }}\n\n    最后生成的正则表达式要在<REGEX></REGEX>标签对里。\n    \"\"\"    \n\ndef extract_regex_pattern(regex_block: str) -> str:    \n    pattern = re.search(r\"<REGEX>(.*)</REGEX>\", regex_block, re.DOTALL)\n    if pattern is None:\n        logger.warning(\"No regex pattern found in the generated block:\\n {regex_block}\")\n        raise None\n    return pattern.group(1)\n\npattern = \"匹配一个邮箱地址\"\nv = generate_regex_pattern.with_llm(llm).with_extractor(extract_regex_pattern).run(desc=pattern)\nprint(v)\n## ^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\n```\n\n在上面的例子里，我们根据一句话生成一个正则表达式。我们通过 `with_extractor` 方法，传入了一个自定义的抽取函数，这个函数会在大模型生成结果后，对结果进行处理，然后返回我们想要的结果。\n\n我们在 prompt 明确说了，生成的结果要放到 `<REGEX></REGEX>` 标签对里，然后我们通过 extract_regex_pattern 函数，从结果中提取出了我们想要的正则表达式。\n\n## 在实例方法中使用大模型\n\n```python\nimport byzerllm\ndata = {\n    'name': 'Jane Doe',\n    'task_count': 3,\n    'tasks': [\n        {'name': 'Submit report', 'due_date': '2024-03-10'},\n        {'name': 'Finish project', 'due_date': '2024-03-15'},\n        {'name': 'Reply to emails', 'due_date': '2024-03-08'}\n    ]\n}\n\n\nclass RAG():\n    def __init__(self):        \n        self.llm = byzerllm.ByzerLLM()\n        self.llm.setup_template(model=\"deepseek_chat\",template=\"auto\")\n        self.llm.setup_default_model_name(\"deepseek_chat\")        \n    \n    @byzerllm.prompt(lambda self:self.llm)\n    def generate_answer(self,name,task_count,tasks)->str:\n        '''\n        Hello {{ name }},\n\n        This is a reminder that you have {{ task_count }} pending tasks:\n        {% for task in tasks %}\n        - Task: {{ task.name }} | Due: {{ task.due_date }}\n        {% endfor %}\n\n        Best regards,\n        Your Reminder System\n        '''        \n\nt = RAG()\n\nresponse = t.generate_answer(**data)\nprint(response)\n\n## 输出:\n## Hello Jane Doe,\n##I hope this message finds you well. I wanted to remind you of your 3 pending tasks to ensure you stay on track:\n## 1. **Submit report** - This task is due on **2024-03-10**. Please ensure that you allocat\n```\n\n这里我们给了个比较复杂的例子，但我们可以看到，给一个实例prompt方法和普通prompt 方法差异不大。\n唯一的区别是如果你希望在定义的时候就指定大模型，使用一个lambda函数返回实例的 llm 对象即可。\n\n```python\n@byzerllm.prompt(lambda self:self.llm)\n```\n\n你也可以不返回，在调用的时候通过 `with_llm` 方法指定 llm 对象。\n\n此外，这个例子也展示了如何通过jinja2模板语法，来处理复杂的结构化数据。\n\n## 通过 Python 代码处理复杂入参\n\n上面的一个例子中，我们通过 jinja2 模板语法，来处理复杂的结构化数据，但是有时候我们可能需要更加复杂的处理，这个时候我们可以通过 Python 代码来处理。\n\n```python\nimport byzerllm\n\ndata = {\n    'name': 'Jane Doe',\n    'task_count': 3,\n    'tasks': [\n        {'name': 'Submit report', 'due_date': '2024-03-10'},\n        {'name': 'Finish project', 'due_date': '2024-03-15'},\n        {'name': 'Reply to emails', 'due_date': '2024-03-08'}\n    ]\n}\n\n\nclass RAG():\n    def __init__(self):        \n        self.llm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n    \n    @byzerllm.prompt()\n    def generate_answer(self,name,task_count,tasks)->str:\n        '''\n        Hello {{ name }},\n\n        This is a reminder that you have {{ task_count }} pending tasks:\n            \n        {{ tasks }}\n\n        Best regards,\n        Your Reminder System\n        '''\n        \n        tasks_str = \"\\n\".join([f\"- Task: {task['name']} | Due: { task['due_date'] }\" for task in tasks])\n        return {\"tasks\": tasks_str}\n\nt = RAG()\n\nresponse = t.generate_answer.with_llm(t.llm).run(**data)\nprint(response)\n\n## Just a gentle nudge to keep you on track with your pending tasks. Here's a quick recap:....\n```\n\n在这个例子里，我们直接把 tasks 在方法体里进行处理，然后作为一个字符串返回，最够构建一个字典，字典的key为 tasks,然后\n你就可以在 docstring 里使用 `{{ tasks }}` 来引用这个字符串。\n\n这样对于很复杂的入参，就不用谢繁琐的 jinja2 模板语法了。\n\n## 如何自动实现一个方法\n\n比如我定义一个签名，但是我不想自己实现里面的逻辑，让大模型来实现。这个在 byzerllm 中叫 function impl。我们来看看怎么\n实现:\n\n```python\nimport pydantic\nclass Time(pydantic.BaseModel):\n    time: str = pydantic.Field(...,description=\"时间，时间格式为 yyyy-MM-dd\")\n\n\n@llm.impl()\ndef calculate_current_time()->Time:\n    '''\n    计算当前时间\n    '''\n    pass \n\n\ncalculate_current_time()\n#output: Time(time='2024-06-14')\n```\n\n在这个例子里，我们定义了一个 calculate_current_time 方法，但是我们没有实现里面的逻辑，我们通过 `@llm.impl()` 装饰器，让大模型来实现这个方法。\n为了避免每次都要“生成”这个方法，导致无法适用，我们提供了缓存，用户可以按如下方式打印速度：\n\n```python\nstart = time.monotonic()\ncalculate_current_time()\nprint(f\"first time cost: {time.monotonic()-start}\")\n\nstart = time.monotonic()\ncalculate_current_time()\nprint(f\"second time cost: {time.monotonic()-start}\")\n\n# output:\n# first time cost: 6.067266260739416\n# second time cost: 4.347506910562515e-05\n```\n可以看到，第一次执行花费了6s,第二次几乎是瞬间完成的，这是因为第一次执行的时候，我们实际上是在生成这个方法，第二次执行的时候，我们是执行已经生成好的代码，所以速度会非常快。你可以显示的调用 `llm.clear_impl_cache()` 清理掉函数缓存。\n\n## Stream 模式\n\n前面的例子都是一次性生成结果，但是有时候我们可能需要一个流式的输出，这个时候我们可能需要用底层一点的API来完成了：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\nv = llm.stream_chat_oai(model=\"deepseek_chat\",conversations=[{\n    \"role\":\"user\",\n    \"content\":\"你好，你是谁\",\n}],delta_mode=True)\n\nfor t in v:\n    print(t,flush=True)  \n\n# 你好\n# ！\n# 我\n# 是一个\n# 人工智能\n# 助手\n# ，\n# 旨在\n# 提供\n# 信息\n# 、\n# 解答\n# 问题....\n```\n\n如果你不想要流式输出，但是想用底层一点的API，你可以使用 `llm.chat_oai` 方法：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\nv = llm.chat_oai(model=\"deepseek_chat\",conversations=[{\n    \"role\":\"user\",\n    \"content\":\"你好，你是谁\",\n}])\n\nprint(v[0].output)\n## 你好！我是一个人工智能助手，旨在提供信息、解答问题和帮助用户解决问题。如果你有任何问题或需要帮助，请随时告诉我。\n```\n\n## Function Calling \n\nbyzerllm 可以不依赖模型自身就能提供 function calling 支持，我们来看个例子：\n\n\n```python\nfrom typing import List,Dict,Any,Annotated\nimport pydantic \nimport datetime\nfrom dateutil.relativedelta import relativedelta\n\ndef compute_date_range(count:Annotated[int,\"时间跨度，数值类型\"],\n                       unit:Annotated[str,\"时间单位，字符串类型\",{\"enum\":[\"day\",\"week\",\"month\",\"year\"]}])->List[str]:\n    '''\n    计算日期范围\n\n    Args:\n        count: 时间跨度，数值类型\n        unit: 时间单位，字符串类型，可选值为 day,week,month,year\n    '''        \n    now = datetime.datetime.now()\n    now_str = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n    if unit == \"day\":\n        return [(now - relativedelta(days=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"week\":\n        return [(now - relativedelta(weeks=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"month\":\n        return [(now - relativedelta(months=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"year\":\n        return [(now - relativedelta(years=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    return [\"\",\"\"]\n\ndef compute_now()->str:\n    '''\n    计算当前时间\n    '''\n    return datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n```\n\n我们定义了两个方法，一个是计算日期范围，一个是计算当前时间。\n\n现在我么可以来测试下，系统如何根据自然语言决定调用哪个方法：\n\n```python\nt = llm.chat_oai([{\n    \"content\":'''计算当前时间''',\n    \"role\":\"user\"    \n}],tools=[compute_date_range,compute_now],execute_tool=True)\n\nt[0].values\n\n## output: ['2024-06-14 15:18:02']\n```\n\n我们可以看到，他正确的选择了 compute_now 方法。\n\n接着我们再试一个：\n\n```python\nt = llm.chat_oai([{\n    \"content\":'''最近三个月趋势''',\n    \"role\":\"user\"    \n}],tools=[compute_date_range,compute_now],execute_tool=True)\n\nt[0].values\n\n## output: [['2024-03-14 15:19:13', '2024-06-14 15:19:13']]\n```\n\n模型正确的选择了 compute_date_range 方法。\n\n## 多模态\n\nbyerllm 也能很好的支持多模态的交互，而且统一了多模态大模型的接口，比如你可以用一样的方式使用 openai 或者 claude 的图片转文字能力， 或者一致的方式使用火山，azuer, openai的语音合成接口。\n\n### 图生文\n\n```python\nimport byzerllm\nfrom byzerllm.types import ImagePath\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\n\n\n@byzerllm.prompt()\ndef what_in_image(image_path: ImagePath) -> str:\n    \"\"\"\n    {{ image_path }}\n    这个图片里有什么？\n    \"\"\"    \n\n\nv = what_in_image.with_llm(vl_llm).run(\n    ImagePath(value=\"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\")\n)\nv\n## OUTPUT: 这张图片展示了多只可爱的猫咪，采用了艺术风格的绘画。猫咪们有不同的颜色和花纹，背景是浅棕色，上面还点缀着一些红色的花朵。整体画面给人一种温馨和谐的感觉\n```\n\n可以看到，我们只需要把 prompt 函数的图片地址入参使用 byzerllm.types.ImagePath里进行包装，就可以直接在 prompt 函数体里\n带上图片。\n\n或者你可以这样：\n\n```python\nimport byzerllm\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\n\n\n@byzerllm.prompt()\ndef what_in_image(image_path: str) -> str:\n    \"\"\"\n    {{ image }}\n    这个图片里有什么？\n    \"\"\"\n    return {\"image\": byzerllm.Image.load_image_from_path(image_path)}\n\n\nv = what_in_image.with_llm(vl_llm).run(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv\n```\n\n通过 `image_path` 参数，然后通过 `byzerllm.Image.load_image_from_path` 方法，转化为一个图片对象 image，最后在 prompt 函数体里\n使用 `{{ image }}` 引用这个图片对象。\n\n另外我们也是可以支持配置多张图片的。\n\n另外，我们也可以使用基础的 `llm.chat_oai` 方法来实现：\n\n```python\nimport byzerllm\nimport json\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\nimage = byzerllm.Image.load_image_from_path(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv = vl_llm.chat_oai(\n    conversations=[\n        {\n            \"role\": \"user\",\n            \"content\": json.dumps(\n                [{\"image\": image, \"text\": \"这个图片里有什么？\"}], ensure_ascii=False\n            ),\n        }\n    ]\n)\nv[0].output\n```\n\n还可以这么写：\n\n```python\nimport byzerllm\nimport json\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\nimage = byzerllm.Image.load_image_from_path(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv = vl_llm.chat_oai(\n    conversations=[\n        {\n            \"role\": \"user\",\n            \"content\": json.dumps(\n                [\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\"url\": image, \"detail\": \"high\"},\n                    },\n                    {\"text\": \"这个图片里有什么？\", \"type\": \"text\"},\n                ],\n                ensure_ascii=False,\n            ),\n        }\n    ]\n)\nv[0].output\n```\n\n### 语音合成\n\n这里以 openai 的 tts 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=tts-1 \\\n--model openai_tts\n```\n\n此外，byzerllm 支持 azure,火山引擎等 tts 语音合成引擎。\n\n接着你可以这么用：\n\n```python\nimport byzerllm\nimport base64\nimport json\n\nllm = byzerllm.ByzerLLM.from_default_model(\"openai_tts\")\n\n\nt = llm.chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\": json.dumps({\n        \"input\":\"hello, open_tts\",\n        \"voice\": \"alloy\",\n        \"response_format\": \"mp3\"\n    },ensure_ascii=False)\n}])\n\nwith open(\"voice.mp3\",\"wb\") as f:\n    f.write(base64.b64decode(t[0].output))\n```\n\ntts 模型生成没有prompt函数可以用，你需要直接使用 chat_oai。\n\n\n### 语音识别\n\n这里以 openai 的 whisper-1 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--worker_concurrency 10 \\\n--infer_params saas.model=whisper-1 saas.api_key=${MODEL_OPENAI_TOKEN} \\\n--model speech_to_text\n```\n\n语音识别的使用方式和图生文类似，我们可以直接在 prompt 函数体里带上音频文件。\n\n```python\nimport byzerllm\nimport json\nimport base64\nfrom byzerllm.types import AudioPath\n\nllm = byzerllm.ByzerLLM.from_default_model(\"speech_to_text\")\n\naudio_file = \"/Users/allwefantasy/videos/output_audio.mp3\"\n\n@byzerllm.prompt(llm=llm)\ndef audio_to_text(audio_file: AudioPath):\n    \"\"\"\n    {{ audio_file }}\n    \"\"\"\n\nv = audio_to_text(AudioPath(value=audio_file))\njson.loads(v)\n```\n输出的数据格式略微复杂：\n\n```\n{'text': 'In the last chapter, you and I started to step through the internal workings of a transformer. This is one of the key pieces of technology inside large language models, and a lot of other tools in the modern wave of AI.',\n 'task': 'transcribe',\n 'language': 'english',\n 'duration': 10.0,\n 'segments': [{'id': 0,\n   'seek': 0,\n   'start': 0.0,\n   'end': 4.78000020980835,\n   'text': ' In the last chapter, you and I started to step through the internal workings of a transformer.',\n   'tokens': [50364,\n    682,\n    264,\n    1036,\n    7187,\n    11,\n    291,\n    293,\n    286,\n    1409,\n    281,\n    1823,\n    807,\n    264,\n    6920,\n    589,\n    1109,\n    295,\n    257,\n    31782,\n    13,\n    50586],\n   'temperature': 0.0,\n   'avg_logprob': -0.28872039914131165,\n   'compression_ratio': 1.4220778942108154,\n   'no_speech_prob': 0.016033057123422623},\n  ....\n  {'id': 2,\n   'seek': 0,\n   'start': 8.579999923706055,\n   'end': 9.979999542236328,\n   'text': ' and a lot of other tools in the modern wave of AI.',\n   'tokens': [50759,\n    293,\n    257,\n    688,\n    295,\n    661,\n    3873,\n    294,\n    264,\n    4363,\n    5772,\n    295,\n    7318,\n    13,\n    50867],\n   'temperature': 0.0,\n   'avg_logprob': -0.28872039914131165,\n   'compression_ratio': 1.4220778942108154,\n   'no_speech_prob': 0.016033057123422623}],\n 'words': [{'word': 'In', 'start': 0.0, 'end': 0.18000000715255737},\n  {'word': 'the', 'start': 0.18000000715255737, 'end': 0.23999999463558197},\n  {'word': 'last', 'start': 0.23999999463558197, 'end': 0.5400000214576721},\n  {'word': 'chapter', 'start': 0.5400000214576721, 'end': 0.800000011920929},\n  ....\n  {'word': 'AI', 'start': 9.920000076293945, 'end': 9.979999542236328}]}\n```\n\n会输出每一句话以及每一个字所在的起始时间和截止时间。你可以根据需要来使用。\n\n### 文生图\n\n文生图和语音合成类似，首先要启动合适的模型,以openai 的 dall-e-3 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=dall-e-3 \\\n--model openai_image_gen\n```\n\n启动模型后，只需要记住几个模板参数即可使用，这里直接使用 chat_oai 方法来使用：\n\n```python\n\nimport byzerllm\nimport json\nimport base64\n\nllm = byzerllm.ByzerLLM.from_default_model(\"openai_image_gen\")\nt = llm.chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\": json.dumps({\n        \"input\":\"a white siamese cat\",\n        \"size\": \"1024x1024\",\n        \"quality\": \"standard\"\n    },ensure_ascii=False)\n}])\n\nwith open(\"output1.jpg\",\"wb\") as f:\n    f.write(base64.b64decode(t[0].output))\n\n\nimport matplotlib.pyplot as plt\n\nimage_path = \"output1.jpg\"\nimage = plt.imread(image_path)\n\nplt.imshow(image)\nplt.axis('off')\nplt.show()\n```\n\n## Prompt 函数的流式输出\n\nbyzerllm 底层支持流式输出，非 prompt 函数的用法是这样的：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\n\nv = llm.stream_chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\":\"讲一个100字的故事\"\n}])\n\nfor s  in v:\n    print(s[0], end=\"\")\n```\n\n如果你像用 prompt 函数，可以这么用：\n\n\n```python\nimport byzerllm\nimport json\nimport base64\nfrom typing import Generator\n\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\n\n@byzerllm.prompt()\ndef tell_story() -> Generator[str, None, None]:\n    '''\n    给我讲一个一百多字的故事\n    '''\n\nv = tell_story.with_llm(llm).run()    \nfor i in v:\n    print(i, end=\"\")\n```\n\n可以看到，和普通的 prompt 函数的区别在于，返回值是一个生成器，然后你可以通过 for 循环来获取结果。\n\n## 注意事项\n\n1. prompt函数方法体返回只能是dict，实际的返回类型和方法签名可以不一样，但是方法体返回只能是dict。\n2. 大部分情况prompt函数体为空，如果一定要有方法体，可以返回一个空字典。\n3. 调用prompt方法的时候，如果在@byzerllm.prompt()里没有指定llm对象，那么需要在调用的时候通过with_llm方法指定llm对象。\n\n\n\n\n\n\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/apps/byzer_storage/memory_model_based.py\nimport asyncio\nfrom typing import List, Dict, Any\n\nimport ray.experimental\nfrom byzerllm.apps.byzer_storage.simple_api import ByzerStorage\nimport time\nimport json\nimport os\nimport concurrent.futures\nimport io\nimport sys\nfrom contextlib import redirect_stdout, redirect_stderr\nimport queue\nimport threading\nimport ray\nimport byzerllm\nfrom typing import List, Union, Dict\nfrom pydantic import BaseModel\nfrom byzerllm.apps.utils import TagExtractor, Tag\nimport importlib\nfrom loguru import logger\n\n\nclass QAPair(BaseModel):\n    question: str\n    answer: str\n\n\ndef read_alpaca_zh():\n    with importlib.resources.path(\n        \"byzerllm.apps.byzer_storage\", \"alpaca_zh.json\"\n    ) as json_path:\n        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n            return json.load(f)\n\n\nclass MemoryManager:\n    _queue = asyncio.Queue()\n    _is_processing = False\n\n    def __init__(\n        self,\n        storage: ByzerStorage,\n        base_dir: str,\n        model_name: str = \"deepseek_chat\",\n        remote: bool = True,\n    ):\n        self.storage = storage\n        home = os.path.expanduser(\"~\")\n        self.base_dir = base_dir or os.path.join(home, \".auto-coder\")\n        self.thread_pool = concurrent.futures.ThreadPoolExecutor(max_workers=5)\n        self.log_file = None\n        self.remote = remote\n        self.model_name = model_name\n\n    @classmethod\n    async def add_to_queue(cls, name: str, memories: List[str]):\n        await cls._queue.put((name, memories))\n        if not cls._is_processing:\n            asyncio.create_task(cls.process_queue())\n\n    @classmethod\n    async def process_queue(cls):\n        cls._is_processing = True\n        while not cls._queue.empty():\n            name, memories = await cls._queue.get()\n            instance = cls.get_instance()\n            await instance.memorize(name, memories)\n            cls._queue.task_done()\n        cls._is_processing = False\n\n    async def memorize(\n        self, name: str, memories: List[str], options: Dict[str, Any] = {}\n    ):\n        try:\n            loop = asyncio.get_running_loop()\n            await loop.run_in_executor(\n                self.thread_pool, self._memorize_with_logs, name, memories, options\n            )\n        finally:\n            if self.remote:\n                ray.actor.exit_actor()\n\n        print(f\"Memorization for {name} completed.\")\n\n    def _memorize_with_logs(\n        self, name: str, memories: List[str], options: Dict[str, Any] = {}\n    ):\n\n        if self.remote:\n            self._memorize(name, memories, options=options)\n            return\n\n        logs_dir = os.path.join(self.base_dir, \"storage\", \"logs\", \"memorize\")\n        os.makedirs(logs_dir, exist_ok=True)\n        log_file = os.path.join(logs_dir, f\"{name}.log\")\n\n        log_queue = queue.Queue()\n        stop_event = threading.Event()\n\n        # Start the log writer thread\n        log_writer_thread = threading.Thread(\n            target=self._log_writer, args=(log_queue, log_file, stop_event)\n        )\n        self.log_file = log_file\n        log_writer_thread.start()\n\n        class QueueStream:\n            def __init__(self, queue):\n                self.queue = queue\n\n            def write(self, msg):\n                self.queue.put(msg)\n\n            def flush(self):\n                pass\n\n        queue_stream = QueueStream(log_queue)\n\n        with redirect_stdout(queue_stream), redirect_stderr(queue_stream):\n            self._memorize(name, memories)\n\n        # Signal the log writer to stop and wait for it to finish\n        stop_event.set()\n        log_writer_thread.join()\n\n    def _log_writer(\n        self, log_queue: queue.Queue, log_file: str, stop_event: threading.Event\n    ):\n        with open(log_file, \"w\") as f:\n            while not stop_event.is_set() or not log_queue.empty():\n                try:\n                    msg = log_queue.get(timeout=0.1)\n                    f.write(msg)\n                    f.flush()\n                except queue.Empty:\n                    continue\n\n    @byzerllm.prompt()\n    def _convert_pretrain_text_to_instruction(self, text: str) -> str:\n        \"\"\"\n        根据提供的信息，生成多个相关的问题，这些问题的回答，最终要能覆盖里面所有的信息,生成的问题越多越好。\n\n        下面是一些信息：\n\n        {{text}}\n\n        现在请开始生成问题，每个问题使用<_question_>标签包裹，每个回答使用<_answer_>标签包裹，最后\n        每个一组问题和回答使用<_group_>标签包裹。\n        \"\"\"\n\n    @byzerllm.prompt()\n    def _format(self, text: str) -> str:\n        \"\"\"\n        下面是一些问答信息：\n\n        {{text}}\n\n\n        请将每个问题使用<_question_>标签包裹，每个回答使用<_answer_>标签包裹，最后\n        每个一组问题和回答使用<_group_>标签包裹。\n        \"\"\"\n\n    def to_qa_pairs(self, text: str, llm) -> List[QAPair]:\n        print(f\"Generating QA pairs for {text}\", flush=True)\n        v = self._convert_pretrain_text_to_instruction.with_llm(llm).run(text)\n        # format_v = self._format.with_llm(llm).run(v)\n        root_tag = TagExtractor(v).extract()\n        qa_pairs = []\n        # _group_\n        for item in root_tag.content:\n            qas = item.content\n            if len(qas) == 2:\n                if (\n                    qas[0].start_tag == \"<_question_>\"\n                    and qas[0].end_tag == \"</_question_>\"\n                    and qas[1].start_tag == \"<_answer_>\"\n                    and qas[1].end_tag == \"</_answer_>\"\n                ):\n                    qa_pairs.append(\n                        QAPair(question=qas[0].content, answer=qas[1].content)\n                    )\n\n        print(f\"Generated {len(qa_pairs)} QA pairs.\", flush=True)\n        logger.info(f\"Generated {len(qa_pairs)} QA pairs.\")\n        return qa_pairs\n\n    def _memorize(\n        self,\n        name: str,\n        memories: List[Union[str, Dict[str, Any]]],\n        options: Dict[str, Any] = {},\n    ):\n        data = []\n        stage = options.get(\"stage\", \"pt\")\n        min_samples = options.pop(\"min_samples\", 1000)\n        base_model_dir = os.path.join(self.base_dir, \"storage\", \"models\")\n        llama_model = os.path.join(\n            base_model_dir, \"meta-llama\", \"Meta-Llama-3-8B-Instruct-GPTQ\"\n        )\n\n        loras_dir = os.path.join(self.base_dir, \"storage\", \"loras\")\n        dataset_dir = os.path.join(self.base_dir, \"storage\", \"datasets\", name)\n\n        os.makedirs(loras_dir, exist_ok=True)\n        os.makedirs(dataset_dir, exist_ok=True)\n\n        if stage == \"pt\":\n            for memory in memories:\n                item = {\n                    \"text\": memory,\n                }\n                data.append(item)\n        elif stage == \"sft\":\n            if memories:\n                with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:\n                    futures = []\n                    llm = byzerllm.ByzerLLM().from_default_model(self.model_name)\n                    for memory in memories:\n                        futures.append(executor.submit(self.to_qa_pairs, memory, llm))\n\n                    for future in concurrent.futures.as_completed(futures):\n                        qa_pairs = future.result()\n                        for qa_pair in qa_pairs:\n                            item = {\n                                \"instruction\": qa_pair.question,\n                                \"input\": \"\",\n                                \"output\": qa_pair.answer,\n                            }\n                            data.append(item)\n            else:\n                v = read_alpaca_zh()\n                data.extend(v)\n\n        if len(data) < min_samples:\n            data = data * (min_samples // len(data) + 1)\n        \n        with open(f\"{dataset_dir}/data.json\", \"w\", encoding=\"utf-8\") as f:\n            json.dump(data, f, indent=2, ensure_ascii=False)\n        \n        with open(f\"{dataset_dir}/dataset_info.json\", \"w\", encoding=\"utf-8\") as f:\n            r = {\"data\": {\"file_name\": \"data.json\"}}\n            if stage == \"pt\":\n                r[\"data\"][\"columns\"] = {\"prompt\": \"text\"}\n            f.write(\n                json.dumps(\n                    r,\n                    indent=2,\n                    ensure_ascii=False,\n                )\n            )\n\n        args = dict(\n            stage=\"pt\",\n            do_train=True,\n            model_name_or_path=llama_model,\n            dataset=\"data\",\n            dataset_dir=dataset_dir,\n            cutoff_len=1024,\n            max_samples=1000000,\n            overwrite_cache=True,\n            preprocessing_num_workers=1,\n            template=\"llama3\",\n            finetuning_type=\"lora\",\n            lora_target=\"all\",\n            output_dir=f\"{loras_dir}/{name}\",\n            overwrite_output_dir=True,\n            per_device_train_batch_size=2,\n            gradient_accumulation_steps=4,\n            lr_scheduler_type=\"cosine\",\n            logging_steps=10,\n            warmup_ratio=0.1,\n            save_steps=1000,\n            plot_loss=True,\n            learning_rate=5e-5,\n            num_train_epochs=1000.0,\n            max_grad_norm=1.0,\n            quantization_bit=4,\n            loraplus_lr_ratio=16.0,\n            fp16=True,\n            ddp_timeout=180000000,\n        )\n        os.environ[\"WANDB_DISABLED\"] = \"true\"\n        from llamafactory.train import tuner\n\n        tuner.run_exp({**args, **options})\n\n\n     \n\n下面是用户的需求：\n\n把 stage == \"pt\" 和 stage == \"sft\" 抽取出来,放到同目录下的 train_sft.py, train_pt.py 中,同时新建一个 trani_dpo.py 注意,保留使用prompt 函数\n"}, {"role": "assistant", "content": "好的，我会按照您的要求进行代码重构。我们将创建三个新文件：`train_sft.py`、`train_pt.py` 和 `train_dpo.py`，并将相应的逻辑从 `memory_model_based.py` 中抽取出来。以下是需要进行的更改：\n\n1. 在 `memory_model_based.py` 中：\n\n```python\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/apps/byzer_storage/memory_model_based.py\n<<<<<<< SEARCH\nif stage == \"pt\":\n    for memory in memories:\n        item = {\n            \"text\": memory,\n        }\n        data.append(item)\nelif stage == \"sft\":\n    if memories:\n        with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:\n            futures = []\n            llm = byzerllm.ByzerLLM().from_default_model(self.model_name)\n            for memory in memories:\n                futures.append(executor.submit(self.to_qa_pairs, memory, llm))\n\n            for future in concurrent.futures.as_completed(futures):\n                qa_pairs = future.result()\n                for qa_pair in qa_pairs:\n                    item = {\n                        \"instruction\": qa_pair.question,\n                        \"input\": \"\",\n                        \"output\": qa_pair.answer,\n                    }\n                    data.append(item)\n    else:\n        v = read_alpaca_zh()\n        data.extend(v)\n\nif len(data) < min_samples:\n    data = data * (min_samples // len(data) + 1)\n\nwith open(f\"{dataset_dir}/data.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(data, f, indent=2, ensure_ascii=False)\n\nwith open(f\"{dataset_dir}/dataset_info.json\", \"w\", encoding=\"utf-8\") as f:\n    r = {\"data\": {\"file_name\": \"data.json\"}}\n    if stage == \"pt\":\n        r[\"data\"][\"columns\"] = {\"prompt\": \"text\"}\n    f.write(\n        json.dumps(\n            r,\n            indent=2,\n            ensure_ascii=False,\n        )\n    )\n\nargs = dict(\n    stage=\"pt\",\n    do_train=True,\n    model_name_or_path=llama_model,\n    dataset=\"data\",\n    dataset_dir=dataset_dir,\n    cutoff_len=1024,\n    max_samples=1000000,\n    overwrite_cache=True,\n    preprocessing_num_workers=1,\n    template=\"llama3\",\n    finetuning_type=\"lora\",\n    lora_target=\"all\",\n    output_dir=f\"{loras_dir}/{name}\",\n    overwrite_output_dir=True,\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=4,\n    lr_scheduler_type=\"cosine\",\n    logging_steps=10,\n    warmup_ratio=0.1,\n    save_steps=1000,\n    plot_loss=True,\n    learning_rate=5e-5,\n    num_train_epochs=1000.0,\n    max_grad_norm=1.0,\n    quantization_bit=4,\n    loraplus_lr_ratio=16.0,\n    fp16=True,\n    ddp_timeout=180000000,\n)\nos.environ[\"WANDB_DISABLED\"] = \"true\"\nfrom llamafactory.train import tuner\n\ntuner.run_exp({**args, **options})\n=======\nif stage == \"pt\":\n    from .train_pt import train_pt\n    train_pt(self, name, memories, options, dataset_dir, loras_dir, llama_model)\nelif stage == \"sft\":\n    from .train_sft import train_sft\n    train_sft(self, name, memories, options, dataset_dir, loras_dir, llama_model)\nelif stage == \"dpo\":\n    from .train_dpo import train_dpo\n    train_dpo(self, name, memories, options, dataset_dir, loras_dir, llama_model)\nelse:\n    raise ValueError(f\"Unsupported stage: {stage}\")\n>>>>>>> REPLACE\n```\n\n2. 创建 `train_pt.py`：\n\n```python\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/apps/byzer_storage/train_pt.py\n<<<<<<< SEARCH\n=======\nimport json\nimport os\nfrom typing import List, Dict, Any\nimport byzerllm\n\ndef train_pt(self, name: str, memories: List[str], options: Dict[str, Any], dataset_dir: str, loras_dir: str, llama_model: str):\n    data = []\n    min_samples = options.pop(\"min_samples\", 1000)\n\n    for memory in memories:\n        item = {\n            \"text\": memory,\n        }\n        data.append(item)\n\n    if len(data) < min_samples:\n        data = data * (min_samples // len(data) + 1)\n\n    with open(f\"{dataset_dir}/data.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(data, f, indent=2, ensure_ascii=False)\n\n    with open(f\"{dataset_dir}/dataset_info.json\", \"w\", encoding=\"utf-8\") as f:\n        r = {\"data\": {\"file_name\": \"data.json\", \"columns\": {\"prompt\": \"text\"}}}\n        f.write(json.dumps(r, indent=2, ensure_ascii=False))\n\n    args = dict(\n        stage=\"pt\",\n        do_train=True,\n        model_name_or_path=llama_model,\n        dataset=\"data\",\n        dataset_dir=dataset_dir,\n        cutoff_len=1024,\n        max_samples=1000000,\n        overwrite_cache=True,\n        preprocessing_num_workers=1,\n        template=\"llama3\",\n        finetuning_type=\"lora\",\n        lora_target=\"all\",\n        output_dir=f\"{loras_dir}/{name}\",\n        overwrite_output_dir=True,\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=4,\n        lr_scheduler_type=\"cosine\",\n        logging_steps=10,\n        warmup_ratio=0.1,\n        save_steps=1000,\n        plot_loss=True,\n        learning_rate=5e-5,\n        num_train_epochs=1000.0,\n        max_grad_norm=1.0,\n        quantization_bit=4,\n        loraplus_lr_ratio=16.0,\n        fp16=True,\n        ddp_timeout=180000000,\n    )\n    os.environ[\"WANDB_DISABLED\"] = \"true\"\n    from llamafactory.train import tuner\n\n    tuner.run_exp({**args, **options})\n>>>>>>> REPLACE\n```\n\n3. 创建 `train_sft.py`：\n\n```python\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/apps/byzer_storage/train_sft.py\n<<<<<<< SEARCH\n=======\nimport json\nimport os\nimport concurrent.futures\nfrom typing import List, Dict, Any\nimport byzerllm\nfrom .memory_model_based import read_alpaca_zh\n\ndef train_sft(self, name: str, memories: List[str], options: Dict[str, Any], dataset_dir: str, loras_dir: str, llama_model: str):\n    data = []\n    min_samples = options.pop(\"min_samples\", 1000)\n\n    if memories:\n        with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:\n            futures = []\n            llm = byzerllm.ByzerLLM().from_default_model(self.model_name)\n            for memory in memories:\n                futures.append(executor.submit(self.to_qa_pairs, memory, llm))\n\n            for future in concurrent.futures.as_completed(futures):\n                qa_pairs = future.result()\n                for qa_pair in qa_pairs:\n                    item = {\n                        \"instruction\": qa_pair.question,\n                        \"input\": \"\",\n                        \"output\": qa_pair.answer,\n                    }\n                    data.append(item)\n    else:\n        v = read_alpaca_zh()\n        data.extend(v)\n\n    if len(data) < min_samples:\n        data = data * (min_samples // len(data) + 1)\n\n    with open(f\"{dataset_dir}/data.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(data, f, indent=2, ensure_ascii=False)\n\n    with open(f\"{dataset_dir}/dataset_info.json\", \"w\", encoding=\"utf-8\") as f:\n        r = {\"data\": {\"file_name\": \"data.json\"}}\n        f.write(json.dumps(r, indent=2, ensure_ascii=False))\n\n    args = dict(\n        stage=\"sft\",\n        do_train=True,\n        model_name_or_path=llama_model,\n        dataset=\"data\",\n        dataset_dir=dataset_dir,\n        cutoff_len=1024,\n        max_samples=1000000,\n        overwrite_cache=True,\n        preprocessing_num_workers=1,\n        template=\"llama3\",\n        finetuning_type=\"lora\",\n        lora_target=\"all\",\n        output_dir=f\"{loras_dir}/{name}\",\n        overwrite_output_dir=True,\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=4,\n        lr_scheduler_type=\"cosine\",\n        logging_steps=10,\n        warmup_ratio=0.1,\n        save_steps=1000,\n        plot_loss=True,\n        learning_rate=5e-5,\n        num_train_epochs=1000.0,\n        max_grad_norm=1.0,\n        quantization_bit=4,\n        loraplus_lr_ratio=16.0,\n        fp16=True,\n        ddp_timeout=180000000,\n    )\n    os.environ[\"WANDB_DISABLED\"] = \"true\"\n    from llamafactory.train import tuner\n\n    tuner.run_exp({**args, **options})\n>>>>>>> REPLACE\n```\n\n4. 创建 `train_dpo.py`：\n\n```python\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/apps/byzer_storage/train_dpo.py\n<<<<<<< SEARCH\n=======\nimport json\nimport os\nfrom typing import List, Dict, Any\nimport byzerllm\n\n@byzerllm.prompt()\ndef generate_dpo_data(memory: str) -> str:\n    \"\"\"\n    根据以下内容，生成一个DPO（Direct Preference Optimization）训练数据样本。\n    每个样本应包含一个提示（prompt），一个首选回答（preferred_output）和一个拒绝回答（rejected_output）。\n    首选回答应该是高质量、有帮助的回答，而拒绝回答应该是质量较差或不太有帮助的回答。\n\n    内容：\n    {{memory}}\n\n    请生成DPO训练数据，并以JSON格式返回，格式如下：\n    {\n        \"prompt\": \"用户提问或指令\",\n        \"preferred_output\": \"高质量、有帮助的回答\",\n        \"rejected_output\": \"质量较差或不太有帮助的回答\"\n    }\n    \"\"\"\n\ndef train_dpo(self, name: str, memories: List[str], options: Dict[str, Any], dataset_dir: str, loras_dir: str, llama_model: str):\n    data = []\n    min_samples = options.pop(\"min_samples\", 1000)\n\n    llm = byzerllm.ByzerLLM().from_default_model(self.model_name)\n    for memory in memories:\n        dpo_sample = generate_dpo_data.with_llm(llm).run(memory)\n        data.append(json.loads(dpo_sample))\n\n    if len(data) < min_samples:\n        data = data * (min_samples // len(data) + 1)\n\n    with open(f\"{dataset_dir}/data.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(data, f, indent=2, ensure_ascii=False)\n\n    with open(f\"{dataset_dir}/dataset_info.json\", \"w\", encoding=\"utf-8\") as f:\n        r = {\"data\": {\"file_name\": \"data.json\"}}\n        f.write(json.dumps(r, indent=2, ensure_ascii=False))\n\n    args = dict(\n        stage=\"dpo\",\n        do_train=True,\n        model_name_or_path=llama_model,\n        dataset=\"data\",\n        dataset_dir=dataset_dir,\n        cutoff_len=1024,\n        max_samples=1000000,\n        overwrite_cache=True,\n        preprocessing_num_workers=1,\n        template=\"llama3\",\n        finetuning_type=\"lora\",\n        lora_target=\"all\",\n        output_dir=f\"{loras_dir}/{name}\",\n        overwrite_output_dir=True,\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=4,\n        lr_scheduler_type=\"cosine\",\n        logging_steps=10,\n        warmup_ratio=0.1,\n        save_steps=1000,\n        plot_loss=True,\n        learning_rate=5e-5,\n        num_train_epochs=1000.0,\n        max_grad_norm=1.0,\n        quantization_bit=4,\n        loraplus_lr_ratio=16.0,\n        fp16=True,\n        ddp_timeout=180000000,\n    )\n    os.environ[\"WANDB_DISABLED\"] = \"true\"\n    from llamafactory.train import tuner\n\n    tuner.run_exp({**args, **options})\n>>>>>>> REPLACE\n```\n\n这些更改将把 PT、SFT 和 DPO 的训练逻辑分别放到各自的文件中，并在 `memory_model_based.py` 中根据 `stage` 参数调用相应的训练函数。我们还为 DPO 训练添加了一个新的 prompt 函数来生成 DPO 训练数据。\n\n请注意，您可能需要根据实际需求调整 `train_dpo.py` 中的 DPO 训练逻辑，因为我只是提供了一个基本的框架。\n\n这样的重构可以使代码更加模块化，便于维护和扩展。同时，我们保留了使用 prompt 函数的方式，特别是在 DPO 训练数据生成中。"}], "model": "human", "yaml_file": "actions/006_chat_action.yml"}
{"instruction": "把 stage == \"pt\" 和 stage == \"sft\" 抽取出来,放到同目录下的 train_sft.py, train_pt.py 中,同时新建一个 trani_dpo.py 注意,保留使用prompt 函数", "conversations": [{"role": "user", "content": "\n如果你需要生成代码，对于每个需要更改的文件,你需要按 *SEARCH/REPLACE block* 的格式进行生成。\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The opening fence and code language, eg: ```python\n2. The file path alone on a line, starting with \"##File:\" and verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n*SEARCH/REPLACE* blocks will replace *all* matching occurrences.\nInclude enough lines to make the SEARCH blocks unique.\n\nInclude *ALL* the code being searched and replaced!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n下面我们来看一个例子：\n\n当前项目目录结构：\n1. 项目根目录： /tmp/projects/mathweb\n2. 项目子目录/文件列表(类似tree 命令输出)\nflask/\n    app.py\n    templates/\n        index.html\n    static/\n        style.css\n\n用户需求： Change get_factorial() to use math.factorial\n\n回答： To make this change we need to modify `/tmp/projects/mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n用户需求： Refactor hello() into its own file.\n\n回答：To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/hello.py\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/main.py\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n现在让我们开始一个新的任务:\n\n当前项目目录结构：\n1. 项目根目录： /Users/allwefantasy/projects/byzer-llm\n2. 项目子目录/文件列表(类似tree 命令输出)：\nsetup.py\ntests/\n    test_emb_rerank.py\n    test_deepseek_code.py\n    test_prompt.py\n    test_model_meta.py\n    __init__.py\n    test_transformers_deploy.py\n    test_data_analysis.py\n    test_yi.py\n    test_deploy.py\n    functions_suite.py\n    test_template.py\n    auto/\n        test_backend_llama_cpp.py\n        __init__.py\nsetup-machine/\n    convert_safetensor.py\nnotebooks/\n    test.py\nsrc/\n    byzerllm/\n        lang.py\n        store.py\n        version.py\n        log.py\n        __init__.py\n        types.py\n        easy_byzerllm_command.py\n        byzerllm_command.py\n        command_args.py\n        bark/\n            generation.py\n            __init__.py\n            model.py\n            api.py\n            model_fine.py\n            bark_voice.py\n        moss/\n            finetune_moss.py\n            __init__.py\n            moss_inference.py\n            models/\n                configuration_moss.py\n                quantization.py\n                custom_autotune.py\n                __init__.py\n                modeling_moss.py\n                tokenization_moss.py\n        llama2/\n            __init__.py\n        records/\n            __init__.py\n        visualglm/\n            __init__.py\n        auto/\n            __init__.py\n            backend_llama_cpp.py\n            backend_vllm.py\n            backend_ds.py\n        alpha_moss/\n            __init__.py\n        processor/\n            __init__.py\n        qwen/\n            __init__.py\n        utils/\n            json_repaire.py\n            object_store_ref_util.py\n            __init__.py\n            types.py\n            tokenizer.py\n            openai_utils.py\n            ray_utils.py\n            text_generator.py\n            emb.py\n            connect_ray.py\n            testing.py\n            langutil.py\n            nontext.py\n            fulltune/\n                deepspeed_trainner.py\n                __init__.py\n                launch.py\n                trainner.py\n                base_model/\n                    __init__.py\n                    configuration_baichuan.py\n                    modeling_baichuan.py\n                pretrain/\n                    __init__.py\n                    convert_to_transformers.py\n            metrics/\n                __init__.py\n            config/\n                __init__.py\n            retrieval/\n                rest.py\n                __init__.py\n                udf.py\n            sft/\n                argument.py\n                collator.py\n                qlora.py\n                __init__.py\n                model.py\n                dataset.py\n                loss.py\n                merge_lora.py\n                trainer.py\n            client/\n                message_utils.py\n                math_utils.py\n                byzerllm_client.py\n                parallel_utils.py\n                __init__.py\n                types.py\n                code_utils.py\n                img_utils.py\n                entrypoints/\n                    __init__.py\n                    openai/\n                        protocol.py\n                        __init__.py\n                        serve.py\n                        serving_chat.py\n                        api_server.py\n                        serving_engine.py\n                        serving_completion.py\n                        tool.py\n        chatglm2/\n            __init__.py\n        baichuan/\n            __init__.py\n        m3e/\n            __init__.py\n        stable_diffusion/\n            config.py\n            __init__.py\n            logger.py\n            shared.py\n            model.py\n            utils.py\n            images.py\n            diffusion/\n                __init__.py\n                embeddings.py\n                utils.py\n                networks/\n                    lyco.py\n                    __init__.py\n                upscalers/\n                    __init__.py\n                    multidiffusion.py\n                    samplers.py\n                piplines/\n                    __init__.py\n                    lpw.py\n                    diffusers.py\n            lib/\n                __init__.py\n                diffusers/\n                    __init__.py\n                    scheduler.py\n            api/\n                __init__.py\n                plugin.py\n                models/\n                    diffusion.py\n                    __init__.py\n                diffusion/\n                    __init__.py\n                    pipelines/\n                        __init__.py\n                        diffusers.py\n                events/\n                    generation.py\n                    __init__.py\n        bge/\n            __init__.py\n        bge_rerank/\n            __init__.py\n        apps/\n            client.py\n            qa_strategy.py\n            __init__.py\n            builder.py\n            vector_db.py\n            utils.py\n            qa.py\n            byzer_storage/\n                memory_model_based.py\n                command.py\n                env.py\n                __init__.py\n                simple_api.py\n            byzer_sql/\n                __init__.py\n            llama_index/\n                byzerai_vectordb.py\n                byzerai.py\n                byzerai_embedding.py\n                __init__.py\n                collection_manager.py\n                byzerai_index_store.py\n                byzerai_docstore.py\n                simple_retrieval.py\n                byzerai_kvstore.py\n            agent/\n                conversable_agent.py\n                registry.py\n                groupchat.py\n                user_proxy_agent.py\n                __init__.py\n                agent.py\n                extensions/\n                    simple_retrieval_client.py\n                    planner.py\n                    python_codesandbox_agent.py\n                    preview_file_agent.py\n                    llama_index_retrieval_agent.py\n                    visualization_agent.py\n                    assistant_agent.py\n                    load_data_agent.py\n                    retrieval_agent.py\n                    output_agent.py\n                    __init__.py\n                    byzer_engine_agent.py\n                    subquestion_agent.py\n                    sql_reviewer_agent.py\n                    spark_sql_agent.py\n                    rhetorical_agent.py\n                    data_analysis.py\n                    data_analysis_pipeline_agent.py\n                    common_agent.py\n                    query_rewrite/\n                        time.py\n                        rhetorical.py\n                        __init__.py\n                        condition.py\n                        context.py\n                store/\n                    memory_store.py\n                    __init__.py\n                    stores.py\n        saas/\n            __init__.py\n            chatglm/\n                __init__.py\n            sparkdesk/\n                __init__.py\n            gemini/\n                __init__.py\n            azure_openai/\n                __init__.py\n            azure/\n                __init__.py\n            qianfan/\n                __init__.py\n            claude/\n                __init__.py\n            qianwen/\n                __init__.py\n            volcengine/\n                __init__.py\n            minimax/\n                __init__.py\n            baichuan/\n                __init__.py\n            qianwen_vl/\n                __init__.py\n            official_openai/\n                __init__.py\n            zhipu/\n                __init__.py\n            aws_bedrock/\n                __init__.py\n            openai/\n                __init__.py\n        falcon/\n            __init__.py\n        chatglm6b/\n            trainer_seq2seq.py\n            arguments.py\n            __init__.py\n            finetune.py\n            tunning/\n                __init__.py\n                infer.py\n                finetune.py\n                utils/\n                    config.py\n                    __init__.py\n                    other.py\n                    common.py\n                    seq2seq.py\n        starcode/\n            __init__.py\n        qwen_vl_chat/\n            __init__.py\n        zephyr/\n            __init__.py\n        dolly/\n            generate.py\n            __init__.py\n            consts.py\n            dolly_inference.py\n            trainer.py\n        llama/\n            __init__.py\n        whisper/\n            whisper_inference.py\n            __init__.py\n下面是一些文件路径以及每个文件对应的源码：\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/apps/byzer_storage/memory_model_based.py\nimport asyncio\nfrom typing import List, Dict, Any\n\nimport ray.experimental\nfrom byzerllm.apps.byzer_storage.simple_api import ByzerStorage\nimport time\nimport json\nimport os\nimport concurrent.futures\nimport io\nimport sys\nfrom contextlib import redirect_stdout, redirect_stderr\nimport queue\nimport threading\nimport ray\nimport byzerllm\nfrom typing import List, Union, Dict\nfrom pydantic import BaseModel\nfrom byzerllm.apps.utils import TagExtractor, Tag\nimport importlib\nfrom loguru import logger\n\n\nclass QAPair(BaseModel):\n    question: str\n    answer: str\n\n\ndef read_alpaca_zh():\n    with importlib.resources.path(\n        \"byzerllm.apps.byzer_storage\", \"alpaca_zh.json\"\n    ) as json_path:\n        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n            return json.load(f)\n\n\nclass MemoryManager:\n    _queue = asyncio.Queue()\n    _is_processing = False\n\n    def __init__(\n        self,\n        storage: ByzerStorage,\n        base_dir: str,\n        model_name: str = \"deepseek_chat\",\n        remote: bool = True,\n    ):\n        self.storage = storage\n        home = os.path.expanduser(\"~\")\n        self.base_dir = base_dir or os.path.join(home, \".auto-coder\")\n        self.thread_pool = concurrent.futures.ThreadPoolExecutor(max_workers=5)\n        self.log_file = None\n        self.remote = remote\n        self.model_name = model_name\n\n    @classmethod\n    async def add_to_queue(cls, name: str, memories: List[str]):\n        await cls._queue.put((name, memories))\n        if not cls._is_processing:\n            asyncio.create_task(cls.process_queue())\n\n    @classmethod\n    async def process_queue(cls):\n        cls._is_processing = True\n        while not cls._queue.empty():\n            name, memories = await cls._queue.get()\n            instance = cls.get_instance()\n            await instance.memorize(name, memories)\n            cls._queue.task_done()\n        cls._is_processing = False\n\n    async def memorize(\n        self, name: str, memories: List[str], options: Dict[str, Any] = {}\n    ):\n        try:\n            loop = asyncio.get_running_loop()\n            await loop.run_in_executor(\n                self.thread_pool, self._memorize_with_logs, name, memories, options\n            )\n        finally:\n            if self.remote:\n                ray.actor.exit_actor()\n\n        print(f\"Memorization for {name} completed.\")\n\n    def _memorize_with_logs(\n        self, name: str, memories: List[str], options: Dict[str, Any] = {}\n    ):\n\n        if self.remote:\n            self._memorize(name, memories, options=options)\n            return\n\n        logs_dir = os.path.join(self.base_dir, \"storage\", \"logs\", \"memorize\")\n        os.makedirs(logs_dir, exist_ok=True)\n        log_file = os.path.join(logs_dir, f\"{name}.log\")\n\n        log_queue = queue.Queue()\n        stop_event = threading.Event()\n\n        # Start the log writer thread\n        log_writer_thread = threading.Thread(\n            target=self._log_writer, args=(log_queue, log_file, stop_event)\n        )\n        self.log_file = log_file\n        log_writer_thread.start()\n\n        class QueueStream:\n            def __init__(self, queue):\n                self.queue = queue\n\n            def write(self, msg):\n                self.queue.put(msg)\n\n            def flush(self):\n                pass\n\n        queue_stream = QueueStream(log_queue)\n\n        with redirect_stdout(queue_stream), redirect_stderr(queue_stream):\n            self._memorize(name, memories)\n\n        # Signal the log writer to stop and wait for it to finish\n        stop_event.set()\n        log_writer_thread.join()\n\n    def _log_writer(\n        self, log_queue: queue.Queue, log_file: str, stop_event: threading.Event\n    ):\n        with open(log_file, \"w\") as f:\n            while not stop_event.is_set() or not log_queue.empty():\n                try:\n                    msg = log_queue.get(timeout=0.1)\n                    f.write(msg)\n                    f.flush()\n                except queue.Empty:\n                    continue\n\n    @byzerllm.prompt()\n    def _convert_pretrain_text_to_instruction(self, text: str) -> str:\n        \"\"\"\n        根据提供的信息，生成多个相关的问题，这些问题的回答，最终要能覆盖里面所有的信息,生成的问题越多越好。\n\n        下面是一些信息：\n\n        {{text}}\n\n        现在请开始生成问题，每个问题使用<_question_>标签包裹，每个回答使用<_answer_>标签包裹，最后\n        每个一组问题和回答使用<_group_>标签包裹。\n        \"\"\"\n\n    @byzerllm.prompt()\n    def _format(self, text: str) -> str:\n        \"\"\"\n        下面是一些问答信息：\n\n        {{text}}\n\n\n        请将每个问题使用<_question_>标签包裹，每个回答使用<_answer_>标签包裹，最后\n        每个一组问题和回答使用<_group_>标签包裹。\n        \"\"\"\n\n    def to_qa_pairs(self, text: str, llm) -> List[QAPair]:\n        print(f\"Generating QA pairs for {text}\", flush=True)\n        v = self._convert_pretrain_text_to_instruction.with_llm(llm).run(text)\n        # format_v = self._format.with_llm(llm).run(v)\n        root_tag = TagExtractor(v).extract()\n        qa_pairs = []\n        # _group_\n        for item in root_tag.content:\n            qas = item.content\n            if len(qas) == 2:\n                if (\n                    qas[0].start_tag == \"<_question_>\"\n                    and qas[0].end_tag == \"</_question_>\"\n                    and qas[1].start_tag == \"<_answer_>\"\n                    and qas[1].end_tag == \"</_answer_>\"\n                ):\n                    qa_pairs.append(\n                        QAPair(question=qas[0].content, answer=qas[1].content)\n                    )\n\n        print(f\"Generated {len(qa_pairs)} QA pairs.\", flush=True)\n        logger.info(f\"Generated {len(qa_pairs)} QA pairs.\")\n        return qa_pairs\n\n    def _memorize(\n        self,\n        name: str,\n        memories: List[Union[str, Dict[str, Any]]],\n        options: Dict[str, Any] = {},\n    ):\n        data = []\n        stage = options.get(\"stage\", \"pt\")\n        min_samples = options.pop(\"min_samples\", 1000)\n        base_model_dir = os.path.join(self.base_dir, \"storage\", \"models\")\n        llama_model = os.path.join(\n            base_model_dir, \"meta-llama\", \"Meta-Llama-3-8B-Instruct-GPTQ\"\n        )\n\n        loras_dir = os.path.join(self.base_dir, \"storage\", \"loras\")\n        dataset_dir = os.path.join(self.base_dir, \"storage\", \"datasets\", name)\n\n        os.makedirs(loras_dir, exist_ok=True)\n        os.makedirs(dataset_dir, exist_ok=True)\n\n        if stage == \"pt\":\n            for memory in memories:\n                item = {\n                    \"text\": memory,\n                }\n                data.append(item)\n        elif stage == \"sft\":\n            if memories:\n                with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:\n                    futures = []\n                    llm = byzerllm.ByzerLLM().from_default_model(self.model_name)\n                    for memory in memories:\n                        futures.append(executor.submit(self.to_qa_pairs, memory, llm))\n\n                    for future in concurrent.futures.as_completed(futures):\n                        qa_pairs = future.result()\n                        for qa_pair in qa_pairs:\n                            item = {\n                                \"instruction\": qa_pair.question,\n                                \"input\": \"\",\n                                \"output\": qa_pair.answer,\n                            }\n                            data.append(item)\n            else:\n                v = read_alpaca_zh()\n                data.extend(v)\n\n        if len(data) < min_samples:\n            data = data * (min_samples // len(data) + 1)\n        \n        with open(f\"{dataset_dir}/data.json\", \"w\", encoding=\"utf-8\") as f:\n            json.dump(data, f, indent=2, ensure_ascii=False)\n        \n        with open(f\"{dataset_dir}/dataset_info.json\", \"w\", encoding=\"utf-8\") as f:\n            r = {\"data\": {\"file_name\": \"data.json\"}}\n            if stage == \"pt\":\n                r[\"data\"][\"columns\"] = {\"prompt\": \"text\"}\n            f.write(\n                json.dumps(\n                    r,\n                    indent=2,\n                    ensure_ascii=False,\n                )\n            )\n\n        args = dict(\n            stage=\"pt\",\n            do_train=True,\n            model_name_or_path=llama_model,\n            dataset=\"data\",\n            dataset_dir=dataset_dir,\n            cutoff_len=1024,\n            max_samples=1000000,\n            overwrite_cache=True,\n            preprocessing_num_workers=1,\n            template=\"llama3\",\n            finetuning_type=\"lora\",\n            lora_target=\"all\",\n            output_dir=f\"{loras_dir}/{name}\",\n            overwrite_output_dir=True,\n            per_device_train_batch_size=2,\n            gradient_accumulation_steps=4,\n            lr_scheduler_type=\"cosine\",\n            logging_steps=10,\n            warmup_ratio=0.1,\n            save_steps=1000,\n            plot_loss=True,\n            learning_rate=5e-5,\n            num_train_epochs=1000.0,\n            max_grad_norm=1.0,\n            quantization_bit=4,\n            loraplus_lr_ratio=16.0,\n            fp16=True,\n            ddp_timeout=180000000,\n        )\n        os.environ[\"WANDB_DISABLED\"] = \"true\"\n        from llamafactory.train import tuner\n\n        tuner.run_exp({**args, **options})\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/docs/zh/003_byzerllm_大模型编程快速指南.md\n##  byzerllm 大模型编程快速指南\n\n本文示例 [Notebook](../../notebooks/003_byzerllm_大模型编程快速指南.ipynb)\n\n## 安装\n\n```bash\npip install -U byzerllm\nray start --head\n```\n## 启动一个模型代理\n\nbyzerllm 支持私有化模型或者SaaS模型的部署。\n\n这里以 deepseek 官方API 为例：\n\n```bash\neasy-byzerllm deploy deepseek-chat --token xxxxx --alias deepseek_chat\n```\n\n或者跬基流动API:\n\n```bash\neasy-byzerllm deploy alibaba/Qwen1.5-110B-Chat --token xxxxx --alias qwen110b_chat\n```\n\n将上面的 API KEY 替换成你们自己的。\n\n如果你想部署私有化模型或者对接 Ollama 等更多需求，参考 [002_使用byzerllm进行模型部署.md](./002_使用byzerllm进行模型部署.md) 或者 [README.md](../../README.md)。\n\n之后，你就可以在代码里使用  deepseek_chat 或者 qwen110b_chat  访问模型了。\n\n## hello world\n\n来和我们的大模型打个招呼:\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\n@byzerllm.prompt(llm=llm)\ndef hello(q:str) ->str:\n    '''\n    你好, {{ q }}\n    '''\n\ns = hello(\"你是谁\")    \nprint(s)\n\n## 输出:\n## '你好！我是一个人工智能助手，专门设计来回答问题、提供信息和帮助解决问题。如果你有任何疑问或需要帮助，请随时告诉我。'\n```\n\n恭喜，你和大模型成功打了招呼！\n\n可以看到，我们通过 `@byzerllm.prompt` 装饰器，将一个方法转换成了一个大模型的调用，然后这个方法的主题是一段文本，文本中\n使用了 jinja2 模板语法，来获得方法的参数。当正常调用该方法时，实际上就发起了和大模型的交互，并且返回了大模型的结果。\n\n在 byzerllm 中，我们把这种方法称之为 prompt 函数。\n\n## 查看发送给大模型的prompt\n\n很多情况你可能需要调试，查看自己的 prompt 渲染后到底是什么样子的，这个时候你可以通过如下方式\n获取渲染后的prompt:\n\n```python\nhello.prompt(\"你是谁\")\n## '你好, 你是谁'\n```            \n\n## 动态换一个模型\n\n前面的 hello 方法在初始化的时候，我们使用了默认的模型 deepseek_chat，如果我们想换一个模型，可以这样做：\n\n```python\nhello.with_llm(llm).run(\"你是谁\")\n## '你好！我是一个人工智能助手，专门设计来回答问题、提供信息和帮助解决问题。如果你有任何疑问或需要帮助，请随时告诉我。'\n```\n\n通过 with_llm 你可以设置一个新的 llm 对象，然后调用 run 方法，就可以使用新的模型了。\n\n## 超长文本生成\n\n我们知道，大模型一次生成的长度其实是有限的，如果你想生成超长文本，你可能需手动的不断获得\n生成结果，然后把他转化为输入，然后再次生成，这样的方式是比较麻烦的。\n\nbyzerllm 提供了更加易用的 API :\n\n```python\nimport byzerllm\nfrom byzerllm import ByzerLLM\n\nllm = ByzerLLM.from_default_model(\"deepseek_chat\")\n\n@byzerllm.prompt()\ndef tell_story() -> str:\n    \"\"\"\n    讲一个100字的故事。    \n    \"\"\"\n\n\ns = (\n    tell_story.with_llm(llm)\n    .with_response_markers()\n    .options({\"llm_config\": {\"max_length\": 10}})\n    .run()\n)\nprint(s)\n\n## 从前，森林里住着一只聪明的小狐狸。一天，它发现了一块闪闪发光的宝石。小狐狸决定用这块宝石帮助森林里的动物们。它用宝石的光芒指引迷路的小鹿找到了回家的路，用宝石的温暖治愈了受伤的小鸟。从此，小狐狸成了森林里的英雄，动物们都感激它的善良和智慧。\n```\n\n实际核心部分就是这一行：\n\n```python\ntell_story.with_llm(llm)\n    .with_response_markers()    \n    .run()\n```\n\n我们只需要调用 `with_response_markers` 方法，系统就会自动的帮我们生成超长文本。\n在上面的案例中，我们通过\n\n```python\n.options({\"llm_config\": {\"max_length\": 10}})\n```\n\n认为的限制大模型一次交互最多只能输出10个字符，但是系统依然自动完成了远超过10个字符的文本生成。\n\n## 对象输出\n\n前面我们的例子都是返回字符串，但是我们也可以返回对象，这样我们就可以更加灵活的处理返回结果。\n\n```python\nimport pydantic \n\nclass Story(pydantic.BaseModel):\n    '''\n    故事\n    '''\n\n    title: str = pydantic.Field(description=\"故事的标题\")\n    body: str = pydantic.Field(description=\"故事主体\")\n\n@byzerllm.prompt()\ndef tell_story()->Story:\n    '''\n    讲一个100字的故事。    \n    '''\n\ns = tell_story.with_llm(llm).run()\nprint(isinstance(s, Story))\nprint(s.title)\n\n## True\n## 勇敢的小鸟\n```\n\n可以看到，我们很轻松的将输出转化为格式化输出。\n\n## 自定义字段抽取\n\n前面的结构化输出，其实会消耗更多token,还有一种更加精准的结构化输出方式。\n比如让大模型生成一个正则表达式，但实际上大模型很难准确只输出一个正则表达式，这个时候我们可以通过自定义抽取函数来获取我们想要的结果。\n\n\n```python\nfrom loguru import logger\nimport re\n\n@byzerllm.prompt()\ndef generate_regex_pattern(desc: str) -> str:\n    \"\"\"\n    根据下面的描述生成一个正则表达式，要符合python re.compile 库的要求。\n\n    {{ desc }}\n\n    最后生成的正则表达式要在<REGEX></REGEX>标签对里。\n    \"\"\"    \n\ndef extract_regex_pattern(regex_block: str) -> str:    \n    pattern = re.search(r\"<REGEX>(.*)</REGEX>\", regex_block, re.DOTALL)\n    if pattern is None:\n        logger.warning(\"No regex pattern found in the generated block:\\n {regex_block}\")\n        raise None\n    return pattern.group(1)\n\npattern = \"匹配一个邮箱地址\"\nv = generate_regex_pattern.with_llm(llm).with_extractor(extract_regex_pattern).run(desc=pattern)\nprint(v)\n## ^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\n```\n\n在上面的例子里，我们根据一句话生成一个正则表达式。我们通过 `with_extractor` 方法，传入了一个自定义的抽取函数，这个函数会在大模型生成结果后，对结果进行处理，然后返回我们想要的结果。\n\n我们在 prompt 明确说了，生成的结果要放到 `<REGEX></REGEX>` 标签对里，然后我们通过 extract_regex_pattern 函数，从结果中提取出了我们想要的正则表达式。\n\n## 在实例方法中使用大模型\n\n```python\nimport byzerllm\ndata = {\n    'name': 'Jane Doe',\n    'task_count': 3,\n    'tasks': [\n        {'name': 'Submit report', 'due_date': '2024-03-10'},\n        {'name': 'Finish project', 'due_date': '2024-03-15'},\n        {'name': 'Reply to emails', 'due_date': '2024-03-08'}\n    ]\n}\n\n\nclass RAG():\n    def __init__(self):        \n        self.llm = byzerllm.ByzerLLM()\n        self.llm.setup_template(model=\"deepseek_chat\",template=\"auto\")\n        self.llm.setup_default_model_name(\"deepseek_chat\")        \n    \n    @byzerllm.prompt(lambda self:self.llm)\n    def generate_answer(self,name,task_count,tasks)->str:\n        '''\n        Hello {{ name }},\n\n        This is a reminder that you have {{ task_count }} pending tasks:\n        {% for task in tasks %}\n        - Task: {{ task.name }} | Due: {{ task.due_date }}\n        {% endfor %}\n\n        Best regards,\n        Your Reminder System\n        '''        \n\nt = RAG()\n\nresponse = t.generate_answer(**data)\nprint(response)\n\n## 输出:\n## Hello Jane Doe,\n##I hope this message finds you well. I wanted to remind you of your 3 pending tasks to ensure you stay on track:\n## 1. **Submit report** - This task is due on **2024-03-10**. Please ensure that you allocat\n```\n\n这里我们给了个比较复杂的例子，但我们可以看到，给一个实例prompt方法和普通prompt 方法差异不大。\n唯一的区别是如果你希望在定义的时候就指定大模型，使用一个lambda函数返回实例的 llm 对象即可。\n\n```python\n@byzerllm.prompt(lambda self:self.llm)\n```\n\n你也可以不返回，在调用的时候通过 `with_llm` 方法指定 llm 对象。\n\n此外，这个例子也展示了如何通过jinja2模板语法，来处理复杂的结构化数据。\n\n## 通过 Python 代码处理复杂入参\n\n上面的一个例子中，我们通过 jinja2 模板语法，来处理复杂的结构化数据，但是有时候我们可能需要更加复杂的处理，这个时候我们可以通过 Python 代码来处理。\n\n```python\nimport byzerllm\n\ndata = {\n    'name': 'Jane Doe',\n    'task_count': 3,\n    'tasks': [\n        {'name': 'Submit report', 'due_date': '2024-03-10'},\n        {'name': 'Finish project', 'due_date': '2024-03-15'},\n        {'name': 'Reply to emails', 'due_date': '2024-03-08'}\n    ]\n}\n\n\nclass RAG():\n    def __init__(self):        \n        self.llm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n    \n    @byzerllm.prompt()\n    def generate_answer(self,name,task_count,tasks)->str:\n        '''\n        Hello {{ name }},\n\n        This is a reminder that you have {{ task_count }} pending tasks:\n            \n        {{ tasks }}\n\n        Best regards,\n        Your Reminder System\n        '''\n        \n        tasks_str = \"\\n\".join([f\"- Task: {task['name']} | Due: { task['due_date'] }\" for task in tasks])\n        return {\"tasks\": tasks_str}\n\nt = RAG()\n\nresponse = t.generate_answer.with_llm(t.llm).run(**data)\nprint(response)\n\n## Just a gentle nudge to keep you on track with your pending tasks. Here's a quick recap:....\n```\n\n在这个例子里，我们直接把 tasks 在方法体里进行处理，然后作为一个字符串返回，最够构建一个字典，字典的key为 tasks,然后\n你就可以在 docstring 里使用 `{{ tasks }}` 来引用这个字符串。\n\n这样对于很复杂的入参，就不用谢繁琐的 jinja2 模板语法了。\n\n## 如何自动实现一个方法\n\n比如我定义一个签名，但是我不想自己实现里面的逻辑，让大模型来实现。这个在 byzerllm 中叫 function impl。我们来看看怎么\n实现:\n\n```python\nimport pydantic\nclass Time(pydantic.BaseModel):\n    time: str = pydantic.Field(...,description=\"时间，时间格式为 yyyy-MM-dd\")\n\n\n@llm.impl()\ndef calculate_current_time()->Time:\n    '''\n    计算当前时间\n    '''\n    pass \n\n\ncalculate_current_time()\n#output: Time(time='2024-06-14')\n```\n\n在这个例子里，我们定义了一个 calculate_current_time 方法，但是我们没有实现里面的逻辑，我们通过 `@llm.impl()` 装饰器，让大模型来实现这个方法。\n为了避免每次都要“生成”这个方法，导致无法适用，我们提供了缓存，用户可以按如下方式打印速度：\n\n```python\nstart = time.monotonic()\ncalculate_current_time()\nprint(f\"first time cost: {time.monotonic()-start}\")\n\nstart = time.monotonic()\ncalculate_current_time()\nprint(f\"second time cost: {time.monotonic()-start}\")\n\n# output:\n# first time cost: 6.067266260739416\n# second time cost: 4.347506910562515e-05\n```\n可以看到，第一次执行花费了6s,第二次几乎是瞬间完成的，这是因为第一次执行的时候，我们实际上是在生成这个方法，第二次执行的时候，我们是执行已经生成好的代码，所以速度会非常快。你可以显示的调用 `llm.clear_impl_cache()` 清理掉函数缓存。\n\n## Stream 模式\n\n前面的例子都是一次性生成结果，但是有时候我们可能需要一个流式的输出，这个时候我们可能需要用底层一点的API来完成了：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\nv = llm.stream_chat_oai(model=\"deepseek_chat\",conversations=[{\n    \"role\":\"user\",\n    \"content\":\"你好，你是谁\",\n}],delta_mode=True)\n\nfor t in v:\n    print(t,flush=True)  \n\n# 你好\n# ！\n# 我\n# 是一个\n# 人工智能\n# 助手\n# ，\n# 旨在\n# 提供\n# 信息\n# 、\n# 解答\n# 问题....\n```\n\n如果你不想要流式输出，但是想用底层一点的API，你可以使用 `llm.chat_oai` 方法：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\nv = llm.chat_oai(model=\"deepseek_chat\",conversations=[{\n    \"role\":\"user\",\n    \"content\":\"你好，你是谁\",\n}])\n\nprint(v[0].output)\n## 你好！我是一个人工智能助手，旨在提供信息、解答问题和帮助用户解决问题。如果你有任何问题或需要帮助，请随时告诉我。\n```\n\n## Function Calling \n\nbyzerllm 可以不依赖模型自身就能提供 function calling 支持，我们来看个例子：\n\n\n```python\nfrom typing import List,Dict,Any,Annotated\nimport pydantic \nimport datetime\nfrom dateutil.relativedelta import relativedelta\n\ndef compute_date_range(count:Annotated[int,\"时间跨度，数值类型\"],\n                       unit:Annotated[str,\"时间单位，字符串类型\",{\"enum\":[\"day\",\"week\",\"month\",\"year\"]}])->List[str]:\n    '''\n    计算日期范围\n\n    Args:\n        count: 时间跨度，数值类型\n        unit: 时间单位，字符串类型，可选值为 day,week,month,year\n    '''        \n    now = datetime.datetime.now()\n    now_str = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n    if unit == \"day\":\n        return [(now - relativedelta(days=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"week\":\n        return [(now - relativedelta(weeks=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"month\":\n        return [(now - relativedelta(months=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"year\":\n        return [(now - relativedelta(years=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    return [\"\",\"\"]\n\ndef compute_now()->str:\n    '''\n    计算当前时间\n    '''\n    return datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n```\n\n我们定义了两个方法，一个是计算日期范围，一个是计算当前时间。\n\n现在我么可以来测试下，系统如何根据自然语言决定调用哪个方法：\n\n```python\nt = llm.chat_oai([{\n    \"content\":'''计算当前时间''',\n    \"role\":\"user\"    \n}],tools=[compute_date_range,compute_now],execute_tool=True)\n\nt[0].values\n\n## output: ['2024-06-14 15:18:02']\n```\n\n我们可以看到，他正确的选择了 compute_now 方法。\n\n接着我们再试一个：\n\n```python\nt = llm.chat_oai([{\n    \"content\":'''最近三个月趋势''',\n    \"role\":\"user\"    \n}],tools=[compute_date_range,compute_now],execute_tool=True)\n\nt[0].values\n\n## output: [['2024-03-14 15:19:13', '2024-06-14 15:19:13']]\n```\n\n模型正确的选择了 compute_date_range 方法。\n\n## 多模态\n\nbyerllm 也能很好的支持多模态的交互，而且统一了多模态大模型的接口，比如你可以用一样的方式使用 openai 或者 claude 的图片转文字能力， 或者一致的方式使用火山，azuer, openai的语音合成接口。\n\n### 图生文\n\n```python\nimport byzerllm\nfrom byzerllm.types import ImagePath\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\n\n\n@byzerllm.prompt()\ndef what_in_image(image_path: ImagePath) -> str:\n    \"\"\"\n    {{ image_path }}\n    这个图片里有什么？\n    \"\"\"    \n\n\nv = what_in_image.with_llm(vl_llm).run(\n    ImagePath(value=\"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\")\n)\nv\n## OUTPUT: 这张图片展示了多只可爱的猫咪，采用了艺术风格的绘画。猫咪们有不同的颜色和花纹，背景是浅棕色，上面还点缀着一些红色的花朵。整体画面给人一种温馨和谐的感觉\n```\n\n可以看到，我们只需要把 prompt 函数的图片地址入参使用 byzerllm.types.ImagePath里进行包装，就可以直接在 prompt 函数体里\n带上图片。\n\n或者你可以这样：\n\n```python\nimport byzerllm\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\n\n\n@byzerllm.prompt()\ndef what_in_image(image_path: str) -> str:\n    \"\"\"\n    {{ image }}\n    这个图片里有什么？\n    \"\"\"\n    return {\"image\": byzerllm.Image.load_image_from_path(image_path)}\n\n\nv = what_in_image.with_llm(vl_llm).run(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv\n```\n\n通过 `image_path` 参数，然后通过 `byzerllm.Image.load_image_from_path` 方法，转化为一个图片对象 image，最后在 prompt 函数体里\n使用 `{{ image }}` 引用这个图片对象。\n\n另外我们也是可以支持配置多张图片的。\n\n另外，我们也可以使用基础的 `llm.chat_oai` 方法来实现：\n\n```python\nimport byzerllm\nimport json\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\nimage = byzerllm.Image.load_image_from_path(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv = vl_llm.chat_oai(\n    conversations=[\n        {\n            \"role\": \"user\",\n            \"content\": json.dumps(\n                [{\"image\": image, \"text\": \"这个图片里有什么？\"}], ensure_ascii=False\n            ),\n        }\n    ]\n)\nv[0].output\n```\n\n还可以这么写：\n\n```python\nimport byzerllm\nimport json\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\nimage = byzerllm.Image.load_image_from_path(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv = vl_llm.chat_oai(\n    conversations=[\n        {\n            \"role\": \"user\",\n            \"content\": json.dumps(\n                [\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\"url\": image, \"detail\": \"high\"},\n                    },\n                    {\"text\": \"这个图片里有什么？\", \"type\": \"text\"},\n                ],\n                ensure_ascii=False,\n            ),\n        }\n    ]\n)\nv[0].output\n```\n\n### 语音合成\n\n这里以 openai 的 tts 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=tts-1 \\\n--model openai_tts\n```\n\n此外，byzerllm 支持 azure,火山引擎等 tts 语音合成引擎。\n\n接着你可以这么用：\n\n```python\nimport byzerllm\nimport base64\nimport json\n\nllm = byzerllm.ByzerLLM.from_default_model(\"openai_tts\")\n\n\nt = llm.chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\": json.dumps({\n        \"input\":\"hello, open_tts\",\n        \"voice\": \"alloy\",\n        \"response_format\": \"mp3\"\n    },ensure_ascii=False)\n}])\n\nwith open(\"voice.mp3\",\"wb\") as f:\n    f.write(base64.b64decode(t[0].output))\n```\n\ntts 模型生成没有prompt函数可以用，你需要直接使用 chat_oai。\n\n\n### 语音识别\n\n这里以 openai 的 whisper-1 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--worker_concurrency 10 \\\n--infer_params saas.model=whisper-1 saas.api_key=${MODEL_OPENAI_TOKEN} \\\n--model speech_to_text\n```\n\n语音识别的使用方式和图生文类似，我们可以直接在 prompt 函数体里带上音频文件。\n\n```python\nimport byzerllm\nimport json\nimport base64\nfrom byzerllm.types import AudioPath\n\nllm = byzerllm.ByzerLLM.from_default_model(\"speech_to_text\")\n\naudio_file = \"/Users/allwefantasy/videos/output_audio.mp3\"\n\n@byzerllm.prompt(llm=llm)\ndef audio_to_text(audio_file: AudioPath):\n    \"\"\"\n    {{ audio_file }}\n    \"\"\"\n\nv = audio_to_text(AudioPath(value=audio_file))\njson.loads(v)\n```\n输出的数据格式略微复杂：\n\n```\n{'text': 'In the last chapter, you and I started to step through the internal workings of a transformer. This is one of the key pieces of technology inside large language models, and a lot of other tools in the modern wave of AI.',\n 'task': 'transcribe',\n 'language': 'english',\n 'duration': 10.0,\n 'segments': [{'id': 0,\n   'seek': 0,\n   'start': 0.0,\n   'end': 4.78000020980835,\n   'text': ' In the last chapter, you and I started to step through the internal workings of a transformer.',\n   'tokens': [50364,\n    682,\n    264,\n    1036,\n    7187,\n    11,\n    291,\n    293,\n    286,\n    1409,\n    281,\n    1823,\n    807,\n    264,\n    6920,\n    589,\n    1109,\n    295,\n    257,\n    31782,\n    13,\n    50586],\n   'temperature': 0.0,\n   'avg_logprob': -0.28872039914131165,\n   'compression_ratio': 1.4220778942108154,\n   'no_speech_prob': 0.016033057123422623},\n  ....\n  {'id': 2,\n   'seek': 0,\n   'start': 8.579999923706055,\n   'end': 9.979999542236328,\n   'text': ' and a lot of other tools in the modern wave of AI.',\n   'tokens': [50759,\n    293,\n    257,\n    688,\n    295,\n    661,\n    3873,\n    294,\n    264,\n    4363,\n    5772,\n    295,\n    7318,\n    13,\n    50867],\n   'temperature': 0.0,\n   'avg_logprob': -0.28872039914131165,\n   'compression_ratio': 1.4220778942108154,\n   'no_speech_prob': 0.016033057123422623}],\n 'words': [{'word': 'In', 'start': 0.0, 'end': 0.18000000715255737},\n  {'word': 'the', 'start': 0.18000000715255737, 'end': 0.23999999463558197},\n  {'word': 'last', 'start': 0.23999999463558197, 'end': 0.5400000214576721},\n  {'word': 'chapter', 'start': 0.5400000214576721, 'end': 0.800000011920929},\n  ....\n  {'word': 'AI', 'start': 9.920000076293945, 'end': 9.979999542236328}]}\n```\n\n会输出每一句话以及每一个字所在的起始时间和截止时间。你可以根据需要来使用。\n\n### 文生图\n\n文生图和语音合成类似，首先要启动合适的模型,以openai 的 dall-e-3 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=dall-e-3 \\\n--model openai_image_gen\n```\n\n启动模型后，只需要记住几个模板参数即可使用，这里直接使用 chat_oai 方法来使用：\n\n```python\n\nimport byzerllm\nimport json\nimport base64\n\nllm = byzerllm.ByzerLLM.from_default_model(\"openai_image_gen\")\nt = llm.chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\": json.dumps({\n        \"input\":\"a white siamese cat\",\n        \"size\": \"1024x1024\",\n        \"quality\": \"standard\"\n    },ensure_ascii=False)\n}])\n\nwith open(\"output1.jpg\",\"wb\") as f:\n    f.write(base64.b64decode(t[0].output))\n\n\nimport matplotlib.pyplot as plt\n\nimage_path = \"output1.jpg\"\nimage = plt.imread(image_path)\n\nplt.imshow(image)\nplt.axis('off')\nplt.show()\n```\n\n## Prompt 函数的流式输出\n\nbyzerllm 底层支持流式输出，非 prompt 函数的用法是这样的：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\n\nv = llm.stream_chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\":\"讲一个100字的故事\"\n}])\n\nfor s  in v:\n    print(s[0], end=\"\")\n```\n\n如果你像用 prompt 函数，可以这么用：\n\n\n```python\nimport byzerllm\nimport json\nimport base64\nfrom typing import Generator\n\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\n\n@byzerllm.prompt()\ndef tell_story() -> Generator[str, None, None]:\n    '''\n    给我讲一个一百多字的故事\n    '''\n\nv = tell_story.with_llm(llm).run()    \nfor i in v:\n    print(i, end=\"\")\n```\n\n可以看到，和普通的 prompt 函数的区别在于，返回值是一个生成器，然后你可以通过 for 循环来获取结果。\n\n## 注意事项\n\n1. prompt函数方法体返回只能是dict，实际的返回类型和方法签名可以不一样，但是方法体返回只能是dict。\n2. 大部分情况prompt函数体为空，如果一定要有方法体，可以返回一个空字典。\n3. 调用prompt方法的时候，如果在@byzerllm.prompt()里没有指定llm对象，那么需要在调用的时候通过with_llm方法指定llm对象。\n\n\n\n\n\n\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/apps/byzer_storage/memory_model_based.py\nimport asyncio\nfrom typing import List, Dict, Any\n\nimport ray.experimental\nfrom byzerllm.apps.byzer_storage.simple_api import ByzerStorage\nimport time\nimport json\nimport os\nimport concurrent.futures\nimport io\nimport sys\nfrom contextlib import redirect_stdout, redirect_stderr\nimport queue\nimport threading\nimport ray\nimport byzerllm\nfrom typing import List, Union, Dict\nfrom pydantic import BaseModel\nfrom byzerllm.apps.utils import TagExtractor, Tag\nimport importlib\nfrom loguru import logger\n\n\nclass QAPair(BaseModel):\n    question: str\n    answer: str\n\n\ndef read_alpaca_zh():\n    with importlib.resources.path(\n        \"byzerllm.apps.byzer_storage\", \"alpaca_zh.json\"\n    ) as json_path:\n        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n            return json.load(f)\n\n\nclass MemoryManager:\n    _queue = asyncio.Queue()\n    _is_processing = False\n\n    def __init__(\n        self,\n        storage: ByzerStorage,\n        base_dir: str,\n        model_name: str = \"deepseek_chat\",\n        remote: bool = True,\n    ):\n        self.storage = storage\n        home = os.path.expanduser(\"~\")\n        self.base_dir = base_dir or os.path.join(home, \".auto-coder\")\n        self.thread_pool = concurrent.futures.ThreadPoolExecutor(max_workers=5)\n        self.log_file = None\n        self.remote = remote\n        self.model_name = model_name\n\n    @classmethod\n    async def add_to_queue(cls, name: str, memories: List[str]):\n        await cls._queue.put((name, memories))\n        if not cls._is_processing:\n            asyncio.create_task(cls.process_queue())\n\n    @classmethod\n    async def process_queue(cls):\n        cls._is_processing = True\n        while not cls._queue.empty():\n            name, memories = await cls._queue.get()\n            instance = cls.get_instance()\n            await instance.memorize(name, memories)\n            cls._queue.task_done()\n        cls._is_processing = False\n\n    async def memorize(\n        self, name: str, memories: List[str], options: Dict[str, Any] = {}\n    ):\n        try:\n            loop = asyncio.get_running_loop()\n            await loop.run_in_executor(\n                self.thread_pool, self._memorize_with_logs, name, memories, options\n            )\n        finally:\n            if self.remote:\n                ray.actor.exit_actor()\n\n        print(f\"Memorization for {name} completed.\")\n\n    def _memorize_with_logs(\n        self, name: str, memories: List[str], options: Dict[str, Any] = {}\n    ):\n\n        if self.remote:\n            self._memorize(name, memories, options=options)\n            return\n\n        logs_dir = os.path.join(self.base_dir, \"storage\", \"logs\", \"memorize\")\n        os.makedirs(logs_dir, exist_ok=True)\n        log_file = os.path.join(logs_dir, f\"{name}.log\")\n\n        log_queue = queue.Queue()\n        stop_event = threading.Event()\n\n        # Start the log writer thread\n        log_writer_thread = threading.Thread(\n            target=self._log_writer, args=(log_queue, log_file, stop_event)\n        )\n        self.log_file = log_file\n        log_writer_thread.start()\n\n        class QueueStream:\n            def __init__(self, queue):\n                self.queue = queue\n\n            def write(self, msg):\n                self.queue.put(msg)\n\n            def flush(self):\n                pass\n\n        queue_stream = QueueStream(log_queue)\n\n        with redirect_stdout(queue_stream), redirect_stderr(queue_stream):\n            self._memorize(name, memories)\n\n        # Signal the log writer to stop and wait for it to finish\n        stop_event.set()\n        log_writer_thread.join()\n\n    def _log_writer(\n        self, log_queue: queue.Queue, log_file: str, stop_event: threading.Event\n    ):\n        with open(log_file, \"w\") as f:\n            while not stop_event.is_set() or not log_queue.empty():\n                try:\n                    msg = log_queue.get(timeout=0.1)\n                    f.write(msg)\n                    f.flush()\n                except queue.Empty:\n                    continue\n\n    @byzerllm.prompt()\n    def _convert_pretrain_text_to_instruction(self, text: str) -> str:\n        \"\"\"\n        根据提供的信息，生成多个相关的问题，这些问题的回答，最终要能覆盖里面所有的信息,生成的问题越多越好。\n\n        下面是一些信息：\n\n        {{text}}\n\n        现在请开始生成问题，每个问题使用<_question_>标签包裹，每个回答使用<_answer_>标签包裹，最后\n        每个一组问题和回答使用<_group_>标签包裹。\n        \"\"\"\n\n    @byzerllm.prompt()\n    def _format(self, text: str) -> str:\n        \"\"\"\n        下面是一些问答信息：\n\n        {{text}}\n\n\n        请将每个问题使用<_question_>标签包裹，每个回答使用<_answer_>标签包裹，最后\n        每个一组问题和回答使用<_group_>标签包裹。\n        \"\"\"\n\n    def to_qa_pairs(self, text: str, llm) -> List[QAPair]:\n        print(f\"Generating QA pairs for {text}\", flush=True)\n        v = self._convert_pretrain_text_to_instruction.with_llm(llm).run(text)\n        # format_v = self._format.with_llm(llm).run(v)\n        root_tag = TagExtractor(v).extract()\n        qa_pairs = []\n        # _group_\n        for item in root_tag.content:\n            qas = item.content\n            if len(qas) == 2:\n                if (\n                    qas[0].start_tag == \"<_question_>\"\n                    and qas[0].end_tag == \"</_question_>\"\n                    and qas[1].start_tag == \"<_answer_>\"\n                    and qas[1].end_tag == \"</_answer_>\"\n                ):\n                    qa_pairs.append(\n                        QAPair(question=qas[0].content, answer=qas[1].content)\n                    )\n\n        print(f\"Generated {len(qa_pairs)} QA pairs.\", flush=True)\n        logger.info(f\"Generated {len(qa_pairs)} QA pairs.\")\n        return qa_pairs\n\n    def _memorize(\n        self,\n        name: str,\n        memories: List[Union[str, Dict[str, Any]]],\n        options: Dict[str, Any] = {},\n    ):\n        data = []\n        stage = options.get(\"stage\", \"pt\")\n        min_samples = options.pop(\"min_samples\", 1000)\n        base_model_dir = os.path.join(self.base_dir, \"storage\", \"models\")\n        llama_model = os.path.join(\n            base_model_dir, \"meta-llama\", \"Meta-Llama-3-8B-Instruct-GPTQ\"\n        )\n\n        loras_dir = os.path.join(self.base_dir, \"storage\", \"loras\")\n        dataset_dir = os.path.join(self.base_dir, \"storage\", \"datasets\", name)\n\n        os.makedirs(loras_dir, exist_ok=True)\n        os.makedirs(dataset_dir, exist_ok=True)\n\n        if stage == \"pt\":\n            for memory in memories:\n                item = {\n                    \"text\": memory,\n                }\n                data.append(item)\n        elif stage == \"sft\":\n            if memories:\n                with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:\n                    futures = []\n                    llm = byzerllm.ByzerLLM().from_default_model(self.model_name)\n                    for memory in memories:\n                        futures.append(executor.submit(self.to_qa_pairs, memory, llm))\n\n                    for future in concurrent.futures.as_completed(futures):\n                        qa_pairs = future.result()\n                        for qa_pair in qa_pairs:\n                            item = {\n                                \"instruction\": qa_pair.question,\n                                \"input\": \"\",\n                                \"output\": qa_pair.answer,\n                            }\n                            data.append(item)\n            else:\n                v = read_alpaca_zh()\n                data.extend(v)\n\n        if len(data) < min_samples:\n            data = data * (min_samples // len(data) + 1)\n        \n        with open(f\"{dataset_dir}/data.json\", \"w\", encoding=\"utf-8\") as f:\n            json.dump(data, f, indent=2, ensure_ascii=False)\n        \n        with open(f\"{dataset_dir}/dataset_info.json\", \"w\", encoding=\"utf-8\") as f:\n            r = {\"data\": {\"file_name\": \"data.json\"}}\n            if stage == \"pt\":\n                r[\"data\"][\"columns\"] = {\"prompt\": \"text\"}\n            f.write(\n                json.dumps(\n                    r,\n                    indent=2,\n                    ensure_ascii=False,\n                )\n            )\n\n        args = dict(\n            stage=\"pt\",\n            do_train=True,\n            model_name_or_path=llama_model,\n            dataset=\"data\",\n            dataset_dir=dataset_dir,\n            cutoff_len=1024,\n            max_samples=1000000,\n            overwrite_cache=True,\n            preprocessing_num_workers=1,\n            template=\"llama3\",\n            finetuning_type=\"lora\",\n            lora_target=\"all\",\n            output_dir=f\"{loras_dir}/{name}\",\n            overwrite_output_dir=True,\n            per_device_train_batch_size=2,\n            gradient_accumulation_steps=4,\n            lr_scheduler_type=\"cosine\",\n            logging_steps=10,\n            warmup_ratio=0.1,\n            save_steps=1000,\n            plot_loss=True,\n            learning_rate=5e-5,\n            num_train_epochs=1000.0,\n            max_grad_norm=1.0,\n            quantization_bit=4,\n            loraplus_lr_ratio=16.0,\n            fp16=True,\n            ddp_timeout=180000000,\n        )\n        os.environ[\"WANDB_DISABLED\"] = \"true\"\n        from llamafactory.train import tuner\n\n        tuner.run_exp({**args, **options})\n\n\n     \n\n下面是用户的需求：\n\n把 stage == \"pt\" 和 stage == \"sft\" 抽取出来,放到同目录下的 train_sft.py, train_pt.py 中,同时新建一个 trani_dpo.py 注意,保留使用prompt 函数\n"}, {"role": "assistant", "content": "好的，我会按照您的要求进行代码重构。我们将创建三个新文件：`train_sft.py`、`train_pt.py` 和 `train_dpo.py`，并将相应的逻辑从 `memory_model_based.py` 中抽取出来。以下是需要进行的更改：\n\n1. 在 `memory_model_based.py` 中：\n\n```python\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/apps/byzer_storage/memory_model_based.py\n<<<<<<< SEARCH\nif stage == \"pt\":\n    for memory in memories:\n        item = {\n            \"text\": memory,\n        }\n        data.append(item)\nelif stage == \"sft\":\n    if memories:\n        with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:\n            futures = []\n            llm = byzerllm.ByzerLLM().from_default_model(self.model_name)\n            for memory in memories:\n                futures.append(executor.submit(self.to_qa_pairs, memory, llm))\n\n            for future in concurrent.futures.as_completed(futures):\n                qa_pairs = future.result()\n                for qa_pair in qa_pairs:\n                    item = {\n                        \"instruction\": qa_pair.question,\n                        \"input\": \"\",\n                        \"output\": qa_pair.answer,\n                    }\n                    data.append(item)\n    else:\n        v = read_alpaca_zh()\n        data.extend(v)\n\nif len(data) < min_samples:\n    data = data * (min_samples // len(data) + 1)\n\nwith open(f\"{dataset_dir}/data.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(data, f, indent=2, ensure_ascii=False)\n\nwith open(f\"{dataset_dir}/dataset_info.json\", \"w\", encoding=\"utf-8\") as f:\n    r = {\"data\": {\"file_name\": \"data.json\"}}\n    if stage == \"pt\":\n        r[\"data\"][\"columns\"] = {\"prompt\": \"text\"}\n    f.write(\n        json.dumps(\n            r,\n            indent=2,\n            ensure_ascii=False,\n        )\n    )\n\nargs = dict(\n    stage=\"pt\",\n    do_train=True,\n    model_name_or_path=llama_model,\n    dataset=\"data\",\n    dataset_dir=dataset_dir,\n    cutoff_len=1024,\n    max_samples=1000000,\n    overwrite_cache=True,\n    preprocessing_num_workers=1,\n    template=\"llama3\",\n    finetuning_type=\"lora\",\n    lora_target=\"all\",\n    output_dir=f\"{loras_dir}/{name}\",\n    overwrite_output_dir=True,\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=4,\n    lr_scheduler_type=\"cosine\",\n    logging_steps=10,\n    warmup_ratio=0.1,\n    save_steps=1000,\n    plot_loss=True,\n    learning_rate=5e-5,\n    num_train_epochs=1000.0,\n    max_grad_norm=1.0,\n    quantization_bit=4,\n    loraplus_lr_ratio=16.0,\n    fp16=True,\n    ddp_timeout=180000000,\n)\nos.environ[\"WANDB_DISABLED\"] = \"true\"\nfrom llamafactory.train import tuner\n\ntuner.run_exp({**args, **options})\n=======\nif stage == \"pt\":\n    from .train_pt import train_pt\n    train_pt(self, name, memories, options, dataset_dir, loras_dir, llama_model)\nelif stage == \"sft\":\n    from .train_sft import train_sft\n    train_sft(self, name, memories, options, dataset_dir, loras_dir, llama_model)\nelif stage == \"dpo\":\n    from .train_dpo import train_dpo\n    train_dpo(self, name, memories, options, dataset_dir, loras_dir, llama_model)\nelse:\n    raise ValueError(f\"Unsupported stage: {stage}\")\n>>>>>>> REPLACE\n```\n\n2. 创建 `train_pt.py`：\n\n```python\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/apps/byzer_storage/train_pt.py\n<<<<<<< SEARCH\n=======\nimport json\nimport os\nfrom typing import List, Dict, Any\nimport byzerllm\n\ndef train_pt(self, name: str, memories: List[str], options: Dict[str, Any], dataset_dir: str, loras_dir: str, llama_model: str):\n    data = []\n    min_samples = options.pop(\"min_samples\", 1000)\n\n    for memory in memories:\n        item = {\n            \"text\": memory,\n        }\n        data.append(item)\n\n    if len(data) < min_samples:\n        data = data * (min_samples // len(data) + 1)\n\n    with open(f\"{dataset_dir}/data.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(data, f, indent=2, ensure_ascii=False)\n\n    with open(f\"{dataset_dir}/dataset_info.json\", \"w\", encoding=\"utf-8\") as f:\n        r = {\"data\": {\"file_name\": \"data.json\", \"columns\": {\"prompt\": \"text\"}}}\n        f.write(json.dumps(r, indent=2, ensure_ascii=False))\n\n    args = dict(\n        stage=\"pt\",\n        do_train=True,\n        model_name_or_path=llama_model,\n        dataset=\"data\",\n        dataset_dir=dataset_dir,\n        cutoff_len=1024,\n        max_samples=1000000,\n        overwrite_cache=True,\n        preprocessing_num_workers=1,\n        template=\"llama3\",\n        finetuning_type=\"lora\",\n        lora_target=\"all\",\n        output_dir=f\"{loras_dir}/{name}\",\n        overwrite_output_dir=True,\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=4,\n        lr_scheduler_type=\"cosine\",\n        logging_steps=10,\n        warmup_ratio=0.1,\n        save_steps=1000,\n        plot_loss=True,\n        learning_rate=5e-5,\n        num_train_epochs=1000.0,\n        max_grad_norm=1.0,\n        quantization_bit=4,\n        loraplus_lr_ratio=16.0,\n        fp16=True,\n        ddp_timeout=180000000,\n    )\n    os.environ[\"WANDB_DISABLED\"] = \"true\"\n    from llamafactory.train import tuner\n\n    tuner.run_exp({**args, **options})\n>>>>>>> REPLACE\n```\n\n3. 创建 `train_sft.py`：\n\n```python\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/apps/byzer_storage/train_sft.py\n<<<<<<< SEARCH\n=======\nimport json\nimport os\nimport concurrent.futures\nfrom typing import List, Dict, Any\nimport byzerllm\nfrom .memory_model_based import read_alpaca_zh\n\ndef train_sft(self, name: str, memories: List[str], options: Dict[str, Any], dataset_dir: str, loras_dir: str, llama_model: str):\n    data = []\n    min_samples = options.pop(\"min_samples\", 1000)\n\n    if memories:\n        with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:\n            futures = []\n            llm = byzerllm.ByzerLLM().from_default_model(self.model_name)\n            for memory in memories:\n                futures.append(executor.submit(self.to_qa_pairs, memory, llm))\n\n            for future in concurrent.futures.as_completed(futures):\n                qa_pairs = future.result()\n                for qa_pair in qa_pairs:\n                    item = {\n                        \"instruction\": qa_pair.question,\n                        \"input\": \"\",\n                        \"output\": qa_pair.answer,\n                    }\n                    data.append(item)\n    else:\n        v = read_alpaca_zh()\n        data.extend(v)\n\n    if len(data) < min_samples:\n        data = data * (min_samples // len(data) + 1)\n\n    with open(f\"{dataset_dir}/data.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(data, f, indent=2, ensure_ascii=False)\n\n    with open(f\"{dataset_dir}/dataset_info.json\", \"w\", encoding=\"utf-8\") as f:\n        r = {\"data\": {\"file_name\": \"data.json\"}}\n        f.write(json.dumps(r, indent=2, ensure_ascii=False))\n\n    args = dict(\n        stage=\"sft\",\n        do_train=True,\n        model_name_or_path=llama_model,\n        dataset=\"data\",\n        dataset_dir=dataset_dir,\n        cutoff_len=1024,\n        max_samples=1000000,\n        overwrite_cache=True,\n        preprocessing_num_workers=1,\n        template=\"llama3\",\n        finetuning_type=\"lora\",\n        lora_target=\"all\",\n        output_dir=f\"{loras_dir}/{name}\",\n        overwrite_output_dir=True,\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=4,\n        lr_scheduler_type=\"cosine\",\n        logging_steps=10,\n        warmup_ratio=0.1,\n        save_steps=1000,\n        plot_loss=True,\n        learning_rate=5e-5,\n        num_train_epochs=1000.0,\n        max_grad_norm=1.0,\n        quantization_bit=4,\n        loraplus_lr_ratio=16.0,\n        fp16=True,\n        ddp_timeout=180000000,\n    )\n    os.environ[\"WANDB_DISABLED\"] = \"true\"\n    from llamafactory.train import tuner\n\n    tuner.run_exp({**args, **options})\n>>>>>>> REPLACE\n```\n\n4. 创建 `train_dpo.py`：\n\n```python\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/apps/byzer_storage/train_dpo.py\n<<<<<<< SEARCH\n=======\nimport json\nimport os\nfrom typing import List, Dict, Any\nimport byzerllm\n\n@byzerllm.prompt()\ndef generate_dpo_data(memory: str) -> str:\n    \"\"\"\n    根据以下内容，生成一个DPO（Direct Preference Optimization）训练数据样本。\n    每个样本应包含一个提示（prompt），一个首选回答（preferred_output）和一个拒绝回答（rejected_output）。\n    首选回答应该是高质量、有帮助的回答，而拒绝回答应该是质量较差或不太有帮助的回答。\n\n    内容：\n    {{memory}}\n\n    请生成DPO训练数据，并以JSON格式返回，格式如下：\n    {\n        \"prompt\": \"用户提问或指令\",\n        \"preferred_output\": \"高质量、有帮助的回答\",\n        \"rejected_output\": \"质量较差或不太有帮助的回答\"\n    }\n    \"\"\"\n\ndef train_dpo(self, name: str, memories: List[str], options: Dict[str, Any], dataset_dir: str, loras_dir: str, llama_model: str):\n    data = []\n    min_samples = options.pop(\"min_samples\", 1000)\n\n    llm = byzerllm.ByzerLLM().from_default_model(self.model_name)\n    for memory in memories:\n        dpo_sample = generate_dpo_data.with_llm(llm).run(memory)\n        data.append(json.loads(dpo_sample))\n\n    if len(data) < min_samples:\n        data = data * (min_samples // len(data) + 1)\n\n    with open(f\"{dataset_dir}/data.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(data, f, indent=2, ensure_ascii=False)\n\n    with open(f\"{dataset_dir}/dataset_info.json\", \"w\", encoding=\"utf-8\") as f:\n        r = {\"data\": {\"file_name\": \"data.json\"}}\n        f.write(json.dumps(r, indent=2, ensure_ascii=False))\n\n    args = dict(\n        stage=\"dpo\",\n        do_train=True,\n        model_name_or_path=llama_model,\n        dataset=\"data\",\n        dataset_dir=dataset_dir,\n        cutoff_len=1024,\n        max_samples=1000000,\n        overwrite_cache=True,\n        preprocessing_num_workers=1,\n        template=\"llama3\",\n        finetuning_type=\"lora\",\n        lora_target=\"all\",\n        output_dir=f\"{loras_dir}/{name}\",\n        overwrite_output_dir=True,\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=4,\n        lr_scheduler_type=\"cosine\",\n        logging_steps=10,\n        warmup_ratio=0.1,\n        save_steps=1000,\n        plot_loss=True,\n        learning_rate=5e-5,\n        num_train_epochs=1000.0,\n        max_grad_norm=1.0,\n        quantization_bit=4,\n        loraplus_lr_ratio=16.0,\n        fp16=True,\n        ddp_timeout=180000000,\n    )\n    os.environ[\"WANDB_DISABLED\"] = \"true\"\n    from llamafactory.train import tuner\n\n    tuner.run_exp({**args, **options})\n>>>>>>> REPLACE\n```\n\n这些更改将把 PT、SFT 和 DPO 的训练逻辑分别放到各自的文件中，并在 `memory_model_based.py` 中根据 `stage` 参数调用相应的训练函数。我们还为 DPO 训练添加了一个新的 prompt 函数来生成 DPO 训练数据。\n\n请注意，您可能需要根据实际需求调整 `train_dpo.py` 中的 DPO 训练逻辑，因为我只是提供了一个基本的框架。\n\n这样的重构可以使代码更加模块化，便于维护和扩展。同时，我们保留了使用 prompt 函数的方式，特别是在 DPO 训练数据生成中。"}], "model": "human", "yaml_file": "actions/007_chat_action.yml"}
{"instruction": "train_sft 中的to_qa_pairs 需要打印更详细的日志,方便定位问题", "conversations": [{"role": "user", "content": "\n如果你需要生成代码，对于每个需要更改的文件,你需要按 *SEARCH/REPLACE block* 的格式进行生成。\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The opening fence and code language, eg: ```python\n2. The file path alone on a line, starting with \"##File:\" and verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n*SEARCH/REPLACE* blocks will replace *all* matching occurrences.\nInclude enough lines to make the SEARCH blocks unique.\n\nInclude *ALL* the code being searched and replaced!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n下面我们来看一个例子：\n\n当前项目目录结构：\n1. 项目根目录： /tmp/projects/mathweb\n2. 项目子目录/文件列表(类似tree 命令输出)\nflask/\n    app.py\n    templates/\n        index.html\n    static/\n        style.css\n\n用户需求： Change get_factorial() to use math.factorial\n\n回答： To make this change we need to modify `/tmp/projects/mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n用户需求： Refactor hello() into its own file.\n\n回答：To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/hello.py\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/main.py\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n现在让我们开始一个新的任务:\n\n当前项目目录结构：\n1. 项目根目录： /Users/allwefantasy/projects/byzer-llm\n2. 项目子目录/文件列表(类似tree 命令输出)：\nsetup.py\ntests/\n    test_emb_rerank.py\n    test_deepseek_code.py\n    test_prompt.py\n    test_model_meta.py\n    __init__.py\n    test_transformers_deploy.py\n    test_data_analysis.py\n    test_yi.py\n    test_deploy.py\n    functions_suite.py\n    test_template.py\n    auto/\n        test_backend_llama_cpp.py\n        __init__.py\nsetup-machine/\n    convert_safetensor.py\nnotebooks/\n    test.py\nsrc/\n    byzerllm/\n        lang.py\n        store.py\n        version.py\n        log.py\n        __init__.py\n        types.py\n        easy_byzerllm_command.py\n        byzerllm_command.py\n        command_args.py\n        bark/\n            generation.py\n            __init__.py\n            model.py\n            api.py\n            model_fine.py\n            bark_voice.py\n        moss/\n            finetune_moss.py\n            __init__.py\n            moss_inference.py\n            models/\n                configuration_moss.py\n                quantization.py\n                custom_autotune.py\n                __init__.py\n                modeling_moss.py\n                tokenization_moss.py\n        llama2/\n            __init__.py\n        records/\n            __init__.py\n        visualglm/\n            __init__.py\n        auto/\n            __init__.py\n            backend_llama_cpp.py\n            backend_vllm.py\n            backend_ds.py\n        alpha_moss/\n            __init__.py\n        processor/\n            __init__.py\n        qwen/\n            __init__.py\n        utils/\n            json_repaire.py\n            object_store_ref_util.py\n            __init__.py\n            types.py\n            tokenizer.py\n            openai_utils.py\n            ray_utils.py\n            text_generator.py\n            emb.py\n            connect_ray.py\n            testing.py\n            langutil.py\n            nontext.py\n            fulltune/\n                deepspeed_trainner.py\n                __init__.py\n                launch.py\n                trainner.py\n                base_model/\n                    __init__.py\n                    configuration_baichuan.py\n                    modeling_baichuan.py\n                pretrain/\n                    __init__.py\n                    convert_to_transformers.py\n            metrics/\n                __init__.py\n            config/\n                __init__.py\n            retrieval/\n                rest.py\n                __init__.py\n                udf.py\n            sft/\n                argument.py\n                collator.py\n                qlora.py\n                __init__.py\n                model.py\n                dataset.py\n                loss.py\n                merge_lora.py\n                trainer.py\n            client/\n                message_utils.py\n                math_utils.py\n                byzerllm_client.py\n                parallel_utils.py\n                __init__.py\n                types.py\n                code_utils.py\n                img_utils.py\n                entrypoints/\n                    __init__.py\n                    openai/\n                        protocol.py\n                        __init__.py\n                        serve.py\n                        serving_chat.py\n                        api_server.py\n                        serving_engine.py\n                        serving_completion.py\n                        tool.py\n        chatglm2/\n            __init__.py\n        baichuan/\n            __init__.py\n        m3e/\n            __init__.py\n        stable_diffusion/\n            config.py\n            __init__.py\n            logger.py\n            shared.py\n            model.py\n            utils.py\n            images.py\n            diffusion/\n                __init__.py\n                embeddings.py\n                utils.py\n                networks/\n                    lyco.py\n                    __init__.py\n                upscalers/\n                    __init__.py\n                    multidiffusion.py\n                    samplers.py\n                piplines/\n                    __init__.py\n                    lpw.py\n                    diffusers.py\n            lib/\n                __init__.py\n                diffusers/\n                    __init__.py\n                    scheduler.py\n            api/\n                __init__.py\n                plugin.py\n                models/\n                    diffusion.py\n                    __init__.py\n                diffusion/\n                    __init__.py\n                    pipelines/\n                        __init__.py\n                        diffusers.py\n                events/\n                    generation.py\n                    __init__.py\n        bge/\n            __init__.py\n        bge_rerank/\n            __init__.py\n        apps/\n            client.py\n            qa_strategy.py\n            __init__.py\n            builder.py\n            vector_db.py\n            utils.py\n            qa.py\n            byzer_storage/\n                memory_model_based.py\n                command.py\n                env.py\n                train_pt.py\n                train_sft.py\n                __init__.py\n                train_dpo.py\n                simple_api.py\n            byzer_sql/\n                __init__.py\n            llama_index/\n                byzerai_vectordb.py\n                byzerai.py\n                byzerai_embedding.py\n                __init__.py\n                collection_manager.py\n                byzerai_index_store.py\n                byzerai_docstore.py\n                simple_retrieval.py\n                byzerai_kvstore.py\n            agent/\n                conversable_agent.py\n                registry.py\n                groupchat.py\n                user_proxy_agent.py\n                __init__.py\n                agent.py\n                extensions/\n                    simple_retrieval_client.py\n                    planner.py\n                    python_codesandbox_agent.py\n                    preview_file_agent.py\n                    llama_index_retrieval_agent.py\n                    visualization_agent.py\n                    assistant_agent.py\n                    load_data_agent.py\n                    retrieval_agent.py\n                    output_agent.py\n                    __init__.py\n                    byzer_engine_agent.py\n                    subquestion_agent.py\n                    sql_reviewer_agent.py\n                    spark_sql_agent.py\n                    rhetorical_agent.py\n                    data_analysis.py\n                    data_analysis_pipeline_agent.py\n                    common_agent.py\n                    query_rewrite/\n                        time.py\n                        rhetorical.py\n                        __init__.py\n                        condition.py\n                        context.py\n                store/\n                    memory_store.py\n                    __init__.py\n                    stores.py\n        saas/\n            __init__.py\n            chatglm/\n                __init__.py\n            sparkdesk/\n                __init__.py\n            gemini/\n                __init__.py\n            azure_openai/\n                __init__.py\n            azure/\n                __init__.py\n            qianfan/\n                __init__.py\n            claude/\n                __init__.py\n            qianwen/\n                __init__.py\n            volcengine/\n                __init__.py\n            minimax/\n                __init__.py\n            baichuan/\n                __init__.py\n            qianwen_vl/\n                __init__.py\n            official_openai/\n                __init__.py\n            zhipu/\n                __init__.py\n            aws_bedrock/\n                __init__.py\n            openai/\n                __init__.py\n        falcon/\n            __init__.py\n        chatglm6b/\n            trainer_seq2seq.py\n            arguments.py\n            __init__.py\n            finetune.py\n            tunning/\n                __init__.py\n                infer.py\n                finetune.py\n                utils/\n                    config.py\n                    __init__.py\n                    other.py\n                    common.py\n                    seq2seq.py\n        starcode/\n            __init__.py\n        qwen_vl_chat/\n            __init__.py\n        zephyr/\n            __init__.py\n        dolly/\n            generate.py\n            __init__.py\n            consts.py\n            dolly_inference.py\n            trainer.py\n        llama/\n            __init__.py\n        whisper/\n            whisper_inference.py\n            __init__.py\n下面是一些文件路径以及每个文件对应的源码：\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/apps/byzer_storage/memory_model_based.py\nimport asyncio\nfrom typing import List, Dict, Any\n\nimport ray.experimental\nfrom byzerllm.apps.byzer_storage.simple_api import ByzerStorage\nimport time\nimport json\nimport os\nimport concurrent.futures\nimport io\nimport sys\nfrom contextlib import redirect_stdout, redirect_stderr\nimport queue\nimport threading\nimport ray\nimport byzerllm\nfrom typing import List, Union, Dict\nfrom pydantic import BaseModel\nimport importlib\nfrom loguru import logger\n\n\ndef read_alpaca_zh():\n    with importlib.resources.path(\n        \"byzerllm.apps.byzer_storage\", \"alpaca_zh.json\"\n    ) as json_path:\n        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n            return json.load(f)\n\n\nclass MemoryManager:\n    _queue = asyncio.Queue()\n    _is_processing = False\n\n    def __init__(\n        self,\n        storage: ByzerStorage,\n        base_dir: str,\n        model_name: str = \"deepseek_chat\",\n        remote: bool = True,\n    ):\n        self.storage = storage\n        home = os.path.expanduser(\"~\")\n        self.base_dir = base_dir or os.path.join(home, \".auto-coder\")\n        self.thread_pool = concurrent.futures.ThreadPoolExecutor(max_workers=5)\n        self.log_file = None\n        self.remote = remote\n        self.model_name = model_name\n\n    @classmethod\n    async def add_to_queue(cls, name: str, memories: List[str]):\n        await cls._queue.put((name, memories))\n        if not cls._is_processing:\n            asyncio.create_task(cls.process_queue())\n\n    @classmethod\n    async def process_queue(cls):\n        cls._is_processing = True\n        while not cls._queue.empty():\n            name, memories = await cls._queue.get()\n            instance = cls.get_instance()\n            await instance.memorize(name, memories)\n            cls._queue.task_done()\n        cls._is_processing = False\n\n    async def memorize(\n        self, name: str, memories: List[str], options: Dict[str, Any] = {}\n    ):\n        try:\n            loop = asyncio.get_running_loop()\n            await loop.run_in_executor(\n                self.thread_pool, self._memorize_with_logs, name, memories, options\n            )\n        finally:\n            if self.remote:\n                ray.actor.exit_actor()\n\n        print(f\"Memorization for {name} completed.\")\n\n    def _memorize_with_logs(\n        self, name: str, memories: List[str], options: Dict[str, Any] = {}\n    ):\n\n        if self.remote:\n            self._memorize(name, memories, options=options)\n            return\n\n        logs_dir = os.path.join(self.base_dir, \"storage\", \"logs\", \"memorize\")\n        os.makedirs(logs_dir, exist_ok=True)\n        log_file = os.path.join(logs_dir, f\"{name}.log\")\n\n        log_queue = queue.Queue()\n        stop_event = threading.Event()\n\n        # Start the log writer thread\n        log_writer_thread = threading.Thread(\n            target=self._log_writer, args=(log_queue, log_file, stop_event)\n        )\n        self.log_file = log_file\n        log_writer_thread.start()\n\n        class QueueStream:\n            def __init__(self, queue):\n                self.queue = queue\n\n            def write(self, msg):\n                self.queue.put(msg)\n\n            def flush(self):\n                pass\n\n        queue_stream = QueueStream(log_queue)\n\n        with redirect_stdout(queue_stream), redirect_stderr(queue_stream):\n            self._memorize(name, memories)\n\n        # Signal the log writer to stop and wait for it to finish\n        stop_event.set()\n        log_writer_thread.join()\n\n    def _log_writer(\n        self, log_queue: queue.Queue, log_file: str, stop_event: threading.Event\n    ):\n        with open(log_file, \"w\") as f:\n            while not stop_event.is_set() or not log_queue.empty():\n                try:\n                    msg = log_queue.get(timeout=0.1)\n                    f.write(msg)\n                    f.flush()\n                except queue.Empty:\n                    continue    \n\n    def _memorize(\n        self,\n        name: str,\n        memories: List[Union[str, Dict[str, Any]]],\n        options: Dict[str, Any] = {},\n    ):\n        data = []\n        stage = options.get(\"stage\", \"pt\")\n        min_samples = options.pop(\"min_samples\", 1000)\n        base_model_dir = os.path.join(self.base_dir, \"storage\", \"models\")\n        llama_model = os.path.join(\n            base_model_dir, \"meta-llama\", \"Meta-Llama-3-8B-Instruct-GPTQ\"\n        )\n\n        loras_dir = os.path.join(self.base_dir, \"storage\", \"loras\")\n        dataset_dir = os.path.join(self.base_dir, \"storage\", \"datasets\", name)\n\n        os.makedirs(loras_dir, exist_ok=True)\n        os.makedirs(dataset_dir, exist_ok=True)\n\n        if stage == \"pt\":\n            from .train_pt import train_pt\n            train_pt(self, name, memories, options, dataset_dir, loras_dir, llama_model)\n        elif stage == \"sft\":\n            from .train_sft import train_sft\n            train_sft(self, name, memories, options, dataset_dir, loras_dir, llama_model)\n        elif stage == \"dpo\":\n            from .train_dpo import train_dpo\n            train_dpo(self, name, memories, options, dataset_dir, loras_dir, llama_model)\n        else:\n            raise ValueError(f\"Unsupported stage: {stage}\")\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/docs/zh/003_byzerllm_大模型编程快速指南.md\n##  byzerllm 大模型编程快速指南\n\n本文示例 [Notebook](../../notebooks/003_byzerllm_大模型编程快速指南.ipynb)\n\n## 安装\n\n```bash\npip install -U byzerllm\nray start --head\n```\n## 启动一个模型代理\n\nbyzerllm 支持私有化模型或者SaaS模型的部署。\n\n这里以 deepseek 官方API 为例：\n\n```bash\neasy-byzerllm deploy deepseek-chat --token xxxxx --alias deepseek_chat\n```\n\n或者跬基流动API:\n\n```bash\neasy-byzerllm deploy alibaba/Qwen1.5-110B-Chat --token xxxxx --alias qwen110b_chat\n```\n\n将上面的 API KEY 替换成你们自己的。\n\n如果你想部署私有化模型或者对接 Ollama 等更多需求，参考 [002_使用byzerllm进行模型部署.md](./002_使用byzerllm进行模型部署.md) 或者 [README.md](../../README.md)。\n\n之后，你就可以在代码里使用  deepseek_chat 或者 qwen110b_chat  访问模型了。\n\n## hello world\n\n来和我们的大模型打个招呼:\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\n@byzerllm.prompt(llm=llm)\ndef hello(q:str) ->str:\n    '''\n    你好, {{ q }}\n    '''\n\ns = hello(\"你是谁\")    \nprint(s)\n\n## 输出:\n## '你好！我是一个人工智能助手，专门设计来回答问题、提供信息和帮助解决问题。如果你有任何疑问或需要帮助，请随时告诉我。'\n```\n\n恭喜，你和大模型成功打了招呼！\n\n可以看到，我们通过 `@byzerllm.prompt` 装饰器，将一个方法转换成了一个大模型的调用，然后这个方法的主题是一段文本，文本中\n使用了 jinja2 模板语法，来获得方法的参数。当正常调用该方法时，实际上就发起了和大模型的交互，并且返回了大模型的结果。\n\n在 byzerllm 中，我们把这种方法称之为 prompt 函数。\n\n## 查看发送给大模型的prompt\n\n很多情况你可能需要调试，查看自己的 prompt 渲染后到底是什么样子的，这个时候你可以通过如下方式\n获取渲染后的prompt:\n\n```python\nhello.prompt(\"你是谁\")\n## '你好, 你是谁'\n```            \n\n## 动态换一个模型\n\n前面的 hello 方法在初始化的时候，我们使用了默认的模型 deepseek_chat，如果我们想换一个模型，可以这样做：\n\n```python\nhello.with_llm(llm).run(\"你是谁\")\n## '你好！我是一个人工智能助手，专门设计来回答问题、提供信息和帮助解决问题。如果你有任何疑问或需要帮助，请随时告诉我。'\n```\n\n通过 with_llm 你可以设置一个新的 llm 对象，然后调用 run 方法，就可以使用新的模型了。\n\n## 超长文本生成\n\n我们知道，大模型一次生成的长度其实是有限的，如果你想生成超长文本，你可能需手动的不断获得\n生成结果，然后把他转化为输入，然后再次生成，这样的方式是比较麻烦的。\n\nbyzerllm 提供了更加易用的 API :\n\n```python\nimport byzerllm\nfrom byzerllm import ByzerLLM\n\nllm = ByzerLLM.from_default_model(\"deepseek_chat\")\n\n@byzerllm.prompt()\ndef tell_story() -> str:\n    \"\"\"\n    讲一个100字的故事。    \n    \"\"\"\n\n\ns = (\n    tell_story.with_llm(llm)\n    .with_response_markers()\n    .options({\"llm_config\": {\"max_length\": 10}})\n    .run()\n)\nprint(s)\n\n## 从前，森林里住着一只聪明的小狐狸。一天，它发现了一块闪闪发光的宝石。小狐狸决定用这块宝石帮助森林里的动物们。它用宝石的光芒指引迷路的小鹿找到了回家的路，用宝石的温暖治愈了受伤的小鸟。从此，小狐狸成了森林里的英雄，动物们都感激它的善良和智慧。\n```\n\n实际核心部分就是这一行：\n\n```python\ntell_story.with_llm(llm)\n    .with_response_markers()    \n    .run()\n```\n\n我们只需要调用 `with_response_markers` 方法，系统就会自动的帮我们生成超长文本。\n在上面的案例中，我们通过\n\n```python\n.options({\"llm_config\": {\"max_length\": 10}})\n```\n\n认为的限制大模型一次交互最多只能输出10个字符，但是系统依然自动完成了远超过10个字符的文本生成。\n\n## 对象输出\n\n前面我们的例子都是返回字符串，但是我们也可以返回对象，这样我们就可以更加灵活的处理返回结果。\n\n```python\nimport pydantic \n\nclass Story(pydantic.BaseModel):\n    '''\n    故事\n    '''\n\n    title: str = pydantic.Field(description=\"故事的标题\")\n    body: str = pydantic.Field(description=\"故事主体\")\n\n@byzerllm.prompt()\ndef tell_story()->Story:\n    '''\n    讲一个100字的故事。    \n    '''\n\ns = tell_story.with_llm(llm).run()\nprint(isinstance(s, Story))\nprint(s.title)\n\n## True\n## 勇敢的小鸟\n```\n\n可以看到，我们很轻松的将输出转化为格式化输出。\n\n## 自定义字段抽取\n\n前面的结构化输出，其实会消耗更多token,还有一种更加精准的结构化输出方式。\n比如让大模型生成一个正则表达式，但实际上大模型很难准确只输出一个正则表达式，这个时候我们可以通过自定义抽取函数来获取我们想要的结果。\n\n\n```python\nfrom loguru import logger\nimport re\n\n@byzerllm.prompt()\ndef generate_regex_pattern(desc: str) -> str:\n    \"\"\"\n    根据下面的描述生成一个正则表达式，要符合python re.compile 库的要求。\n\n    {{ desc }}\n\n    最后生成的正则表达式要在<REGEX></REGEX>标签对里。\n    \"\"\"    \n\ndef extract_regex_pattern(regex_block: str) -> str:    \n    pattern = re.search(r\"<REGEX>(.*)</REGEX>\", regex_block, re.DOTALL)\n    if pattern is None:\n        logger.warning(\"No regex pattern found in the generated block:\\n {regex_block}\")\n        raise None\n    return pattern.group(1)\n\npattern = \"匹配一个邮箱地址\"\nv = generate_regex_pattern.with_llm(llm).with_extractor(extract_regex_pattern).run(desc=pattern)\nprint(v)\n## ^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\n```\n\n在上面的例子里，我们根据一句话生成一个正则表达式。我们通过 `with_extractor` 方法，传入了一个自定义的抽取函数，这个函数会在大模型生成结果后，对结果进行处理，然后返回我们想要的结果。\n\n我们在 prompt 明确说了，生成的结果要放到 `<REGEX></REGEX>` 标签对里，然后我们通过 extract_regex_pattern 函数，从结果中提取出了我们想要的正则表达式。\n\n## 在实例方法中使用大模型\n\n```python\nimport byzerllm\ndata = {\n    'name': 'Jane Doe',\n    'task_count': 3,\n    'tasks': [\n        {'name': 'Submit report', 'due_date': '2024-03-10'},\n        {'name': 'Finish project', 'due_date': '2024-03-15'},\n        {'name': 'Reply to emails', 'due_date': '2024-03-08'}\n    ]\n}\n\n\nclass RAG():\n    def __init__(self):        \n        self.llm = byzerllm.ByzerLLM()\n        self.llm.setup_template(model=\"deepseek_chat\",template=\"auto\")\n        self.llm.setup_default_model_name(\"deepseek_chat\")        \n    \n    @byzerllm.prompt(lambda self:self.llm)\n    def generate_answer(self,name,task_count,tasks)->str:\n        '''\n        Hello {{ name }},\n\n        This is a reminder that you have {{ task_count }} pending tasks:\n        {% for task in tasks %}\n        - Task: {{ task.name }} | Due: {{ task.due_date }}\n        {% endfor %}\n\n        Best regards,\n        Your Reminder System\n        '''        \n\nt = RAG()\n\nresponse = t.generate_answer(**data)\nprint(response)\n\n## 输出:\n## Hello Jane Doe,\n##I hope this message finds you well. I wanted to remind you of your 3 pending tasks to ensure you stay on track:\n## 1. **Submit report** - This task is due on **2024-03-10**. Please ensure that you allocat\n```\n\n这里我们给了个比较复杂的例子，但我们可以看到，给一个实例prompt方法和普通prompt 方法差异不大。\n唯一的区别是如果你希望在定义的时候就指定大模型，使用一个lambda函数返回实例的 llm 对象即可。\n\n```python\n@byzerllm.prompt(lambda self:self.llm)\n```\n\n你也可以不返回，在调用的时候通过 `with_llm` 方法指定 llm 对象。\n\n此外，这个例子也展示了如何通过jinja2模板语法，来处理复杂的结构化数据。\n\n## 通过 Python 代码处理复杂入参\n\n上面的一个例子中，我们通过 jinja2 模板语法，来处理复杂的结构化数据，但是有时候我们可能需要更加复杂的处理，这个时候我们可以通过 Python 代码来处理。\n\n```python\nimport byzerllm\n\ndata = {\n    'name': 'Jane Doe',\n    'task_count': 3,\n    'tasks': [\n        {'name': 'Submit report', 'due_date': '2024-03-10'},\n        {'name': 'Finish project', 'due_date': '2024-03-15'},\n        {'name': 'Reply to emails', 'due_date': '2024-03-08'}\n    ]\n}\n\n\nclass RAG():\n    def __init__(self):        \n        self.llm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n    \n    @byzerllm.prompt()\n    def generate_answer(self,name,task_count,tasks)->str:\n        '''\n        Hello {{ name }},\n\n        This is a reminder that you have {{ task_count }} pending tasks:\n            \n        {{ tasks }}\n\n        Best regards,\n        Your Reminder System\n        '''\n        \n        tasks_str = \"\\n\".join([f\"- Task: {task['name']} | Due: { task['due_date'] }\" for task in tasks])\n        return {\"tasks\": tasks_str}\n\nt = RAG()\n\nresponse = t.generate_answer.with_llm(t.llm).run(**data)\nprint(response)\n\n## Just a gentle nudge to keep you on track with your pending tasks. Here's a quick recap:....\n```\n\n在这个例子里，我们直接把 tasks 在方法体里进行处理，然后作为一个字符串返回，最够构建一个字典，字典的key为 tasks,然后\n你就可以在 docstring 里使用 `{{ tasks }}` 来引用这个字符串。\n\n这样对于很复杂的入参，就不用谢繁琐的 jinja2 模板语法了。\n\n## 如何自动实现一个方法\n\n比如我定义一个签名，但是我不想自己实现里面的逻辑，让大模型来实现。这个在 byzerllm 中叫 function impl。我们来看看怎么\n实现:\n\n```python\nimport pydantic\nclass Time(pydantic.BaseModel):\n    time: str = pydantic.Field(...,description=\"时间，时间格式为 yyyy-MM-dd\")\n\n\n@llm.impl()\ndef calculate_current_time()->Time:\n    '''\n    计算当前时间\n    '''\n    pass \n\n\ncalculate_current_time()\n#output: Time(time='2024-06-14')\n```\n\n在这个例子里，我们定义了一个 calculate_current_time 方法，但是我们没有实现里面的逻辑，我们通过 `@llm.impl()` 装饰器，让大模型来实现这个方法。\n为了避免每次都要“生成”这个方法，导致无法适用，我们提供了缓存，用户可以按如下方式打印速度：\n\n```python\nstart = time.monotonic()\ncalculate_current_time()\nprint(f\"first time cost: {time.monotonic()-start}\")\n\nstart = time.monotonic()\ncalculate_current_time()\nprint(f\"second time cost: {time.monotonic()-start}\")\n\n# output:\n# first time cost: 6.067266260739416\n# second time cost: 4.347506910562515e-05\n```\n可以看到，第一次执行花费了6s,第二次几乎是瞬间完成的，这是因为第一次执行的时候，我们实际上是在生成这个方法，第二次执行的时候，我们是执行已经生成好的代码，所以速度会非常快。你可以显示的调用 `llm.clear_impl_cache()` 清理掉函数缓存。\n\n## Stream 模式\n\n前面的例子都是一次性生成结果，但是有时候我们可能需要一个流式的输出，这个时候我们可能需要用底层一点的API来完成了：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\nv = llm.stream_chat_oai(model=\"deepseek_chat\",conversations=[{\n    \"role\":\"user\",\n    \"content\":\"你好，你是谁\",\n}],delta_mode=True)\n\nfor t in v:\n    print(t,flush=True)  \n\n# 你好\n# ！\n# 我\n# 是一个\n# 人工智能\n# 助手\n# ，\n# 旨在\n# 提供\n# 信息\n# 、\n# 解答\n# 问题....\n```\n\n如果你不想要流式输出，但是想用底层一点的API，你可以使用 `llm.chat_oai` 方法：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\nv = llm.chat_oai(model=\"deepseek_chat\",conversations=[{\n    \"role\":\"user\",\n    \"content\":\"你好，你是谁\",\n}])\n\nprint(v[0].output)\n## 你好！我是一个人工智能助手，旨在提供信息、解答问题和帮助用户解决问题。如果你有任何问题或需要帮助，请随时告诉我。\n```\n\n## Function Calling \n\nbyzerllm 可以不依赖模型自身就能提供 function calling 支持，我们来看个例子：\n\n\n```python\nfrom typing import List,Dict,Any,Annotated\nimport pydantic \nimport datetime\nfrom dateutil.relativedelta import relativedelta\n\ndef compute_date_range(count:Annotated[int,\"时间跨度，数值类型\"],\n                       unit:Annotated[str,\"时间单位，字符串类型\",{\"enum\":[\"day\",\"week\",\"month\",\"year\"]}])->List[str]:\n    '''\n    计算日期范围\n\n    Args:\n        count: 时间跨度，数值类型\n        unit: 时间单位，字符串类型，可选值为 day,week,month,year\n    '''        \n    now = datetime.datetime.now()\n    now_str = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n    if unit == \"day\":\n        return [(now - relativedelta(days=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"week\":\n        return [(now - relativedelta(weeks=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"month\":\n        return [(now - relativedelta(months=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"year\":\n        return [(now - relativedelta(years=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    return [\"\",\"\"]\n\ndef compute_now()->str:\n    '''\n    计算当前时间\n    '''\n    return datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n```\n\n我们定义了两个方法，一个是计算日期范围，一个是计算当前时间。\n\n现在我么可以来测试下，系统如何根据自然语言决定调用哪个方法：\n\n```python\nt = llm.chat_oai([{\n    \"content\":'''计算当前时间''',\n    \"role\":\"user\"    \n}],tools=[compute_date_range,compute_now],execute_tool=True)\n\nt[0].values\n\n## output: ['2024-06-14 15:18:02']\n```\n\n我们可以看到，他正确的选择了 compute_now 方法。\n\n接着我们再试一个：\n\n```python\nt = llm.chat_oai([{\n    \"content\":'''最近三个月趋势''',\n    \"role\":\"user\"    \n}],tools=[compute_date_range,compute_now],execute_tool=True)\n\nt[0].values\n\n## output: [['2024-03-14 15:19:13', '2024-06-14 15:19:13']]\n```\n\n模型正确的选择了 compute_date_range 方法。\n\n## 多模态\n\nbyerllm 也能很好的支持多模态的交互，而且统一了多模态大模型的接口，比如你可以用一样的方式使用 openai 或者 claude 的图片转文字能力， 或者一致的方式使用火山，azuer, openai的语音合成接口。\n\n### 图生文\n\n```python\nimport byzerllm\nfrom byzerllm.types import ImagePath\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\n\n\n@byzerllm.prompt()\ndef what_in_image(image_path: ImagePath) -> str:\n    \"\"\"\n    {{ image_path }}\n    这个图片里有什么？\n    \"\"\"    \n\n\nv = what_in_image.with_llm(vl_llm).run(\n    ImagePath(value=\"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\")\n)\nv\n## OUTPUT: 这张图片展示了多只可爱的猫咪，采用了艺术风格的绘画。猫咪们有不同的颜色和花纹，背景是浅棕色，上面还点缀着一些红色的花朵。整体画面给人一种温馨和谐的感觉\n```\n\n可以看到，我们只需要把 prompt 函数的图片地址入参使用 byzerllm.types.ImagePath里进行包装，就可以直接在 prompt 函数体里\n带上图片。\n\n或者你可以这样：\n\n```python\nimport byzerllm\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\n\n\n@byzerllm.prompt()\ndef what_in_image(image_path: str) -> str:\n    \"\"\"\n    {{ image }}\n    这个图片里有什么？\n    \"\"\"\n    return {\"image\": byzerllm.Image.load_image_from_path(image_path)}\n\n\nv = what_in_image.with_llm(vl_llm).run(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv\n```\n\n通过 `image_path` 参数，然后通过 `byzerllm.Image.load_image_from_path` 方法，转化为一个图片对象 image，最后在 prompt 函数体里\n使用 `{{ image }}` 引用这个图片对象。\n\n另外我们也是可以支持配置多张图片的。\n\n另外，我们也可以使用基础的 `llm.chat_oai` 方法来实现：\n\n```python\nimport byzerllm\nimport json\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\nimage = byzerllm.Image.load_image_from_path(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv = vl_llm.chat_oai(\n    conversations=[\n        {\n            \"role\": \"user\",\n            \"content\": json.dumps(\n                [{\"image\": image, \"text\": \"这个图片里有什么？\"}], ensure_ascii=False\n            ),\n        }\n    ]\n)\nv[0].output\n```\n\n还可以这么写：\n\n```python\nimport byzerllm\nimport json\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\nimage = byzerllm.Image.load_image_from_path(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv = vl_llm.chat_oai(\n    conversations=[\n        {\n            \"role\": \"user\",\n            \"content\": json.dumps(\n                [\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\"url\": image, \"detail\": \"high\"},\n                    },\n                    {\"text\": \"这个图片里有什么？\", \"type\": \"text\"},\n                ],\n                ensure_ascii=False,\n            ),\n        }\n    ]\n)\nv[0].output\n```\n\n### 语音合成\n\n这里以 openai 的 tts 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=tts-1 \\\n--model openai_tts\n```\n\n此外，byzerllm 支持 azure,火山引擎等 tts 语音合成引擎。\n\n接着你可以这么用：\n\n```python\nimport byzerllm\nimport base64\nimport json\n\nllm = byzerllm.ByzerLLM.from_default_model(\"openai_tts\")\n\n\nt = llm.chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\": json.dumps({\n        \"input\":\"hello, open_tts\",\n        \"voice\": \"alloy\",\n        \"response_format\": \"mp3\"\n    },ensure_ascii=False)\n}])\n\nwith open(\"voice.mp3\",\"wb\") as f:\n    f.write(base64.b64decode(t[0].output))\n```\n\ntts 模型生成没有prompt函数可以用，你需要直接使用 chat_oai。\n\n\n### 语音识别\n\n这里以 openai 的 whisper-1 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--worker_concurrency 10 \\\n--infer_params saas.model=whisper-1 saas.api_key=${MODEL_OPENAI_TOKEN} \\\n--model speech_to_text\n```\n\n语音识别的使用方式和图生文类似，我们可以直接在 prompt 函数体里带上音频文件。\n\n```python\nimport byzerllm\nimport json\nimport base64\nfrom byzerllm.types import AudioPath\n\nllm = byzerllm.ByzerLLM.from_default_model(\"speech_to_text\")\n\naudio_file = \"/Users/allwefantasy/videos/output_audio.mp3\"\n\n@byzerllm.prompt(llm=llm)\ndef audio_to_text(audio_file: AudioPath):\n    \"\"\"\n    {{ audio_file }}\n    \"\"\"\n\nv = audio_to_text(AudioPath(value=audio_file))\njson.loads(v)\n```\n输出的数据格式略微复杂：\n\n```\n{'text': 'In the last chapter, you and I started to step through the internal workings of a transformer. This is one of the key pieces of technology inside large language models, and a lot of other tools in the modern wave of AI.',\n 'task': 'transcribe',\n 'language': 'english',\n 'duration': 10.0,\n 'segments': [{'id': 0,\n   'seek': 0,\n   'start': 0.0,\n   'end': 4.78000020980835,\n   'text': ' In the last chapter, you and I started to step through the internal workings of a transformer.',\n   'tokens': [50364,\n    682,\n    264,\n    1036,\n    7187,\n    11,\n    291,\n    293,\n    286,\n    1409,\n    281,\n    1823,\n    807,\n    264,\n    6920,\n    589,\n    1109,\n    295,\n    257,\n    31782,\n    13,\n    50586],\n   'temperature': 0.0,\n   'avg_logprob': -0.28872039914131165,\n   'compression_ratio': 1.4220778942108154,\n   'no_speech_prob': 0.016033057123422623},\n  ....\n  {'id': 2,\n   'seek': 0,\n   'start': 8.579999923706055,\n   'end': 9.979999542236328,\n   'text': ' and a lot of other tools in the modern wave of AI.',\n   'tokens': [50759,\n    293,\n    257,\n    688,\n    295,\n    661,\n    3873,\n    294,\n    264,\n    4363,\n    5772,\n    295,\n    7318,\n    13,\n    50867],\n   'temperature': 0.0,\n   'avg_logprob': -0.28872039914131165,\n   'compression_ratio': 1.4220778942108154,\n   'no_speech_prob': 0.016033057123422623}],\n 'words': [{'word': 'In', 'start': 0.0, 'end': 0.18000000715255737},\n  {'word': 'the', 'start': 0.18000000715255737, 'end': 0.23999999463558197},\n  {'word': 'last', 'start': 0.23999999463558197, 'end': 0.5400000214576721},\n  {'word': 'chapter', 'start': 0.5400000214576721, 'end': 0.800000011920929},\n  ....\n  {'word': 'AI', 'start': 9.920000076293945, 'end': 9.979999542236328}]}\n```\n\n会输出每一句话以及每一个字所在的起始时间和截止时间。你可以根据需要来使用。\n\n### 文生图\n\n文生图和语音合成类似，首先要启动合适的模型,以openai 的 dall-e-3 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=dall-e-3 \\\n--model openai_image_gen\n```\n\n启动模型后，只需要记住几个模板参数即可使用，这里直接使用 chat_oai 方法来使用：\n\n```python\n\nimport byzerllm\nimport json\nimport base64\n\nllm = byzerllm.ByzerLLM.from_default_model(\"openai_image_gen\")\nt = llm.chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\": json.dumps({\n        \"input\":\"a white siamese cat\",\n        \"size\": \"1024x1024\",\n        \"quality\": \"standard\"\n    },ensure_ascii=False)\n}])\n\nwith open(\"output1.jpg\",\"wb\") as f:\n    f.write(base64.b64decode(t[0].output))\n\n\nimport matplotlib.pyplot as plt\n\nimage_path = \"output1.jpg\"\nimage = plt.imread(image_path)\n\nplt.imshow(image)\nplt.axis('off')\nplt.show()\n```\n\n## Prompt 函数的流式输出\n\nbyzerllm 底层支持流式输出，非 prompt 函数的用法是这样的：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\n\nv = llm.stream_chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\":\"讲一个100字的故事\"\n}])\n\nfor s  in v:\n    print(s[0], end=\"\")\n```\n\n如果你像用 prompt 函数，可以这么用：\n\n\n```python\nimport byzerllm\nimport json\nimport base64\nfrom typing import Generator\n\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\n\n@byzerllm.prompt()\ndef tell_story() -> Generator[str, None, None]:\n    '''\n    给我讲一个一百多字的故事\n    '''\n\nv = tell_story.with_llm(llm).run()    \nfor i in v:\n    print(i, end=\"\")\n```\n\n可以看到，和普通的 prompt 函数的区别在于，返回值是一个生成器，然后你可以通过 for 循环来获取结果。\n\n## 注意事项\n\n1. prompt函数方法体返回只能是dict，实际的返回类型和方法签名可以不一样，但是方法体返回只能是dict。\n2. 大部分情况prompt函数体为空，如果一定要有方法体，可以返回一个空字典。\n3. 调用prompt方法的时候，如果在@byzerllm.prompt()里没有指定llm对象，那么需要在调用的时候通过with_llm方法指定llm对象。\n\n\n\n\n\n\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/apps/byzer_storage/train_pt.py\nimport json\nimport os\nfrom typing import List, Dict, Any\nimport byzerllm\n\ndef train_pt(\n    self,\n    name: str,\n    memories: List[str],\n    options: Dict[str, Any],\n    dataset_dir: str,\n    loras_dir: str,\n    llama_model: str,\n):\n    data = []\n    min_samples = options.pop(\"min_samples\", 1000)\n\n    for memory in memories:\n        item = {\n            \"text\": memory,\n        }\n        data.append(item)\n\n    if len(data) < min_samples:\n        data = data * (min_samples // len(data) + 1)\n\n    with open(f\"{dataset_dir}/data.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(data, f, indent=2, ensure_ascii=False)\n\n    with open(f\"{dataset_dir}/dataset_info.json\", \"w\", encoding=\"utf-8\") as f:\n        r = {\"data\": {\"file_name\": \"data.json\", \"columns\": {\"prompt\": \"text\"}}}\n        f.write(json.dumps(r, indent=2, ensure_ascii=False))\n\n    args = dict(\n        stage=\"pt\",\n        do_train=True,\n        model_name_or_path=llama_model,\n        dataset=\"data\",\n        dataset_dir=dataset_dir,\n        cutoff_len=1024,\n        max_samples=1000000,\n        overwrite_cache=True,\n        preprocessing_num_workers=1,\n        template=\"llama3\",\n        finetuning_type=\"lora\",\n        lora_target=\"all\",\n        output_dir=f\"{loras_dir}/{name}\",\n        overwrite_output_dir=True,\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=4,\n        lr_scheduler_type=\"cosine\",\n        logging_steps=10,\n        warmup_ratio=0.1,\n        save_steps=1000,\n        plot_loss=True,\n        learning_rate=5e-5,\n        num_train_epochs=1000.0,\n        max_grad_norm=1.0,\n        quantization_bit=4,\n        loraplus_lr_ratio=16.0,\n        fp16=True,\n        ddp_timeout=180000000,\n    )\n    os.environ[\"WANDB_DISABLED\"] = \"true\"\n    from llamafactory.train import tuner\n\n    tuner.run_exp({**args, **options})\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/apps/byzer_storage/train_sft.py\nimport json\nimport os\nimport concurrent.futures\nfrom typing import List, Dict, Any\nimport byzerllm\nfrom .memory_model_based import read_alpaca_zh\nfrom loguru import logger\nfrom pydantic import BaseModel\nfrom byzerllm.apps.utils import TagExtractor, Tag\n\n\nclass QAPair(BaseModel):\n    question: str\n    answer: str\n\n\n@byzerllm.prompt()\ndef _convert_pretrain_text_to_instruction(text: str, num: int = 10) -> str:\n    \"\"\"\n    根据提供的信息，生成多个相关的问题，这些问题的回答，最终要能覆盖里面所有的信息,生成 {{num}} 个。\n\n    下面是一些信息：\n\n    {{text}}\n\n    现在请开始生成问题，每个问题使用<_question_>标签包裹，每个回答使用<_answer_>标签包裹，最后\n    每个一组问题和回答使用<_group_>标签包裹。\n    \"\"\"\n\n\n@byzerllm.prompt()\ndef get_more(text: str, questions_and_answers: str, num: int = 10) -> str:\n    \"\"\"\n    下面是一些问答信息：\n\n    {{text}}\n\n    下面是已有的问题和回答：\n\n    {{questions_and_answers}}\n\n    下面是已经生成的问题和回答，请继续生成{{num}}个问题和回答，不要和前面的重复，请将每个问题使用<_question_>标签包裹，每个回答使用<_answer_>标签包裹，最后\n    每个一组问题和回答使用<_group_>标签包裹。\n    \"\"\"\n\n\ndef to_qa_pairs(text: str, llm, num: int = 30) -> List[QAPair]:\n    print(f\"Generating QA pairs for {text}\", flush=True)\n    v = _convert_pretrain_text_to_instruction.with_llm(llm).run(text)\n\n    max_turns = int(num / 10)\n    if max_turns < 1:\n        max_turns = 1\n\n    while max_turns > 0:\n        t = get_more.with_llm(llm).run(text, v)\n        v += t\n        max_turns -= 1\n\n    # format_v = self._format.with_llm(llm).run(v)\n    root_tag = TagExtractor(v).extract()\n    qa_pairs = []\n    # _group_\n    for item in root_tag.content:\n        qas = item.content\n        if len(qas) == 2:\n            if (\n                qas[0].start_tag == \"<_question_>\"\n                and qas[0].end_tag == \"</_question_>\"\n                and qas[1].start_tag == \"<_answer_>\"\n                and qas[1].end_tag == \"</_answer_>\"\n            ):\n                qa_pairs.append(QAPair(question=qas[0].content, answer=qas[1].content))\n\n    print(f\"Generated {len(qa_pairs)} QA pairs.\", flush=True)\n    logger.info(f\"Generated {len(qa_pairs)} QA pairs.\")\n    return qa_pairs\n\n\ndef train_sft(    \n    name: str,\n    memories: List[str],\n    options: Dict[str, Any],\n    dataset_dir: str,\n    loras_dir: str,\n    model_name: str,\n):\n    data = []\n    min_samples = options.pop(\"min_samples\", 1000)\n\n    if memories:\n        with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:\n            futures = []\n            llm = byzerllm.ByzerLLM().from_default_model(model_name)\n            for memory in memories:\n                futures.append(executor.submit(to_qa_pairs, memory, llm))\n\n            for future in concurrent.futures.as_completed(futures):\n                qa_pairs = future.result()\n                for qa_pair in qa_pairs:\n                    item = {\n                        \"instruction\": qa_pair.question,\n                        \"input\": \"\",\n                        \"output\": qa_pair.answer,\n                    }\n                    data.append(item)\n    else:\n        v = read_alpaca_zh()\n        data.extend(v)\n\n    if len(data) < min_samples:\n        data = data * (min_samples // len(data) + 1)\n\n    with open(f\"{dataset_dir}/data.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(data, f, indent=2, ensure_ascii=False)\n\n    with open(f\"{dataset_dir}/dataset_info.json\", \"w\", encoding=\"utf-8\") as f:\n        r = {\"data\": {\"file_name\": \"data.json\"}}\n        f.write(json.dumps(r, indent=2, ensure_ascii=False))\n\n    args = dict(\n        stage=\"sft\",\n        do_train=True,\n        model_name_or_path=llama_model,\n        dataset=\"data\",\n        dataset_dir=dataset_dir,\n        cutoff_len=1024,\n        max_samples=1000000,\n        overwrite_cache=True,\n        preprocessing_num_workers=1,\n        template=\"llama3\",\n        finetuning_type=\"lora\",\n        lora_target=\"all\",\n        output_dir=f\"{loras_dir}/{name}\",\n        overwrite_output_dir=True,\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=4,\n        lr_scheduler_type=\"cosine\",\n        logging_steps=10,\n        warmup_ratio=0.1,\n        save_steps=1000,\n        plot_loss=True,\n        learning_rate=5e-5,\n        num_train_epochs=1000.0,\n        max_grad_norm=1.0,\n        quantization_bit=4,\n        loraplus_lr_ratio=16.0,\n        fp16=True,\n        ddp_timeout=180000000,\n    )\n    os.environ[\"WANDB_DISABLED\"] = \"true\"\n    from llamafactory.train import tuner\n\n    tuner.run_exp({**args, **options})\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/apps/byzer_storage/train_dpo.py\nimport json\nimport os\nfrom typing import List, Dict, Any\nimport byzerllm\n\n\n@byzerllm.prompt()\ndef generate_dpo_data(memory: str) -> str:\n    \"\"\"\n    根据以下内容，生成一个DPO（Direct Preference Optimization）训练数据样本。\n    每个样本应包含一个提示（prompt），一个首选回答（preferred_output）和一个拒绝回答（rejected_output）。\n    首选回答应该是高质量、有帮助的回答，而拒绝回答应该是质量较差或不太有帮助的回答。\n\n    内容：\n    {{memory}}\n\n    请生成DPO训练数据，并以JSON格式返回，格式如下：\n    {\n        \"prompt\": \"用户提问或指令\",\n        \"preferred_output\": \"高质量、有帮助的回答\",\n        \"rejected_output\": \"质量较差或不太有帮助的回答\"\n    }\n    \"\"\"\n\n\ndef train_dpo(\n    self,\n    name: str,\n    memories: List[str],\n    options: Dict[str, Any],\n    dataset_dir: str,\n    loras_dir: str,\n    llama_model: str,\n):\n    data = []\n    min_samples = options.pop(\"min_samples\", 1000)\n\n    llm = byzerllm.ByzerLLM().from_default_model(self.model_name)\n    for memory in memories:\n        dpo_sample = generate_dpo_data.with_llm(llm).run(memory)\n        data.append(json.loads(dpo_sample))\n\n    if len(data) < min_samples:\n        data = data * (min_samples // len(data) + 1)\n\n    with open(f\"{dataset_dir}/data.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(data, f, indent=2, ensure_ascii=False)\n\n    with open(f\"{dataset_dir}/dataset_info.json\", \"w\", encoding=\"utf-8\") as f:\n        r = {\"data\": {\"file_name\": \"data.json\"}}\n        f.write(json.dumps(r, indent=2, ensure_ascii=False))\n\n    args = dict(\n        stage=\"dpo\",\n        do_train=True,\n        model_name_or_path=llama_model,\n        dataset=\"data\",\n        dataset_dir=dataset_dir,\n        cutoff_len=1024,\n        max_samples=1000000,\n        overwrite_cache=True,\n        preprocessing_num_workers=1,\n        template=\"llama3\",\n        finetuning_type=\"lora\",\n        lora_target=\"all\",\n        output_dir=f\"{loras_dir}/{name}\",\n        overwrite_output_dir=True,\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=4,\n        lr_scheduler_type=\"cosine\",\n        logging_steps=10,\n        warmup_ratio=0.1,\n        save_steps=1000,\n        plot_loss=True,\n        learning_rate=5e-5,\n        num_train_epochs=1000.0,\n        max_grad_norm=1.0,\n        quantization_bit=4,\n        loraplus_lr_ratio=16.0,\n        fp16=True,\n        ddp_timeout=180000000,\n    )\n    os.environ[\"WANDB_DISABLED\"] = \"true\"\n    from llamafactory.train import tuner\n\n    tuner.run_exp({**args, **options})\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/apps/byzer_storage/memory_model_based.py\nimport asyncio\nfrom typing import List, Dict, Any\n\nimport ray.experimental\nfrom byzerllm.apps.byzer_storage.simple_api import ByzerStorage\nimport time\nimport json\nimport os\nimport concurrent.futures\nimport io\nimport sys\nfrom contextlib import redirect_stdout, redirect_stderr\nimport queue\nimport threading\nimport ray\nimport byzerllm\nfrom typing import List, Union, Dict\nfrom pydantic import BaseModel\nimport importlib\nfrom loguru import logger\n\n\ndef read_alpaca_zh():\n    with importlib.resources.path(\n        \"byzerllm.apps.byzer_storage\", \"alpaca_zh.json\"\n    ) as json_path:\n        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n            return json.load(f)\n\n\nclass MemoryManager:\n    _queue = asyncio.Queue()\n    _is_processing = False\n\n    def __init__(\n        self,\n        storage: ByzerStorage,\n        base_dir: str,\n        model_name: str = \"deepseek_chat\",\n        remote: bool = True,\n    ):\n        self.storage = storage\n        home = os.path.expanduser(\"~\")\n        self.base_dir = base_dir or os.path.join(home, \".auto-coder\")\n        self.thread_pool = concurrent.futures.ThreadPoolExecutor(max_workers=5)\n        self.log_file = None\n        self.remote = remote\n        self.model_name = model_name\n\n    @classmethod\n    async def add_to_queue(cls, name: str, memories: List[str]):\n        await cls._queue.put((name, memories))\n        if not cls._is_processing:\n            asyncio.create_task(cls.process_queue())\n\n    @classmethod\n    async def process_queue(cls):\n        cls._is_processing = True\n        while not cls._queue.empty():\n            name, memories = await cls._queue.get()\n            instance = cls.get_instance()\n            await instance.memorize(name, memories)\n            cls._queue.task_done()\n        cls._is_processing = False\n\n    async def memorize(\n        self, name: str, memories: List[str], options: Dict[str, Any] = {}\n    ):\n        try:\n            loop = asyncio.get_running_loop()\n            await loop.run_in_executor(\n                self.thread_pool, self._memorize_with_logs, name, memories, options\n            )\n        finally:\n            if self.remote:\n                ray.actor.exit_actor()\n\n        print(f\"Memorization for {name} completed.\")\n\n    def _memorize_with_logs(\n        self, name: str, memories: List[str], options: Dict[str, Any] = {}\n    ):\n\n        if self.remote:\n            self._memorize(name, memories, options=options)\n            return\n\n        logs_dir = os.path.join(self.base_dir, \"storage\", \"logs\", \"memorize\")\n        os.makedirs(logs_dir, exist_ok=True)\n        log_file = os.path.join(logs_dir, f\"{name}.log\")\n\n        log_queue = queue.Queue()\n        stop_event = threading.Event()\n\n        # Start the log writer thread\n        log_writer_thread = threading.Thread(\n            target=self._log_writer, args=(log_queue, log_file, stop_event)\n        )\n        self.log_file = log_file\n        log_writer_thread.start()\n\n        class QueueStream:\n            def __init__(self, queue):\n                self.queue = queue\n\n            def write(self, msg):\n                self.queue.put(msg)\n\n            def flush(self):\n                pass\n\n        queue_stream = QueueStream(log_queue)\n\n        with redirect_stdout(queue_stream), redirect_stderr(queue_stream):\n            self._memorize(name, memories)\n\n        # Signal the log writer to stop and wait for it to finish\n        stop_event.set()\n        log_writer_thread.join()\n\n    def _log_writer(\n        self, log_queue: queue.Queue, log_file: str, stop_event: threading.Event\n    ):\n        with open(log_file, \"w\") as f:\n            while not stop_event.is_set() or not log_queue.empty():\n                try:\n                    msg = log_queue.get(timeout=0.1)\n                    f.write(msg)\n                    f.flush()\n                except queue.Empty:\n                    continue    \n\n    def _memorize(\n        self,\n        name: str,\n        memories: List[Union[str, Dict[str, Any]]],\n        options: Dict[str, Any] = {},\n    ):\n        data = []\n        stage = options.get(\"stage\", \"pt\")\n        min_samples = options.pop(\"min_samples\", 1000)\n        base_model_dir = os.path.join(self.base_dir, \"storage\", \"models\")\n        llama_model = os.path.join(\n            base_model_dir, \"meta-llama\", \"Meta-Llama-3-8B-Instruct-GPTQ\"\n        )\n\n        loras_dir = os.path.join(self.base_dir, \"storage\", \"loras\")\n        dataset_dir = os.path.join(self.base_dir, \"storage\", \"datasets\", name)\n\n        os.makedirs(loras_dir, exist_ok=True)\n        os.makedirs(dataset_dir, exist_ok=True)\n\n        if stage == \"pt\":\n            from .train_pt import train_pt\n            train_pt(self, name, memories, options, dataset_dir, loras_dir, llama_model)\n        elif stage == \"sft\":\n            from .train_sft import train_sft\n            train_sft(self, name, memories, options, dataset_dir, loras_dir, llama_model)\n        elif stage == \"dpo\":\n            from .train_dpo import train_dpo\n            train_dpo(self, name, memories, options, dataset_dir, loras_dir, llama_model)\n        else:\n            raise ValueError(f\"Unsupported stage: {stage}\")\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/apps/byzer_storage/train_pt.py\nimport json\nimport os\nfrom typing import List, Dict, Any\nimport byzerllm\n\ndef train_pt(\n    self,\n    name: str,\n    memories: List[str],\n    options: Dict[str, Any],\n    dataset_dir: str,\n    loras_dir: str,\n    llama_model: str,\n):\n    data = []\n    min_samples = options.pop(\"min_samples\", 1000)\n\n    for memory in memories:\n        item = {\n            \"text\": memory,\n        }\n        data.append(item)\n\n    if len(data) < min_samples:\n        data = data * (min_samples // len(data) + 1)\n\n    with open(f\"{dataset_dir}/data.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(data, f, indent=2, ensure_ascii=False)\n\n    with open(f\"{dataset_dir}/dataset_info.json\", \"w\", encoding=\"utf-8\") as f:\n        r = {\"data\": {\"file_name\": \"data.json\", \"columns\": {\"prompt\": \"text\"}}}\n        f.write(json.dumps(r, indent=2, ensure_ascii=False))\n\n    args = dict(\n        stage=\"pt\",\n        do_train=True,\n        model_name_or_path=llama_model,\n        dataset=\"data\",\n        dataset_dir=dataset_dir,\n        cutoff_len=1024,\n        max_samples=1000000,\n        overwrite_cache=True,\n        preprocessing_num_workers=1,\n        template=\"llama3\",\n        finetuning_type=\"lora\",\n        lora_target=\"all\",\n        output_dir=f\"{loras_dir}/{name}\",\n        overwrite_output_dir=True,\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=4,\n        lr_scheduler_type=\"cosine\",\n        logging_steps=10,\n        warmup_ratio=0.1,\n        save_steps=1000,\n        plot_loss=True,\n        learning_rate=5e-5,\n        num_train_epochs=1000.0,\n        max_grad_norm=1.0,\n        quantization_bit=4,\n        loraplus_lr_ratio=16.0,\n        fp16=True,\n        ddp_timeout=180000000,\n    )\n    os.environ[\"WANDB_DISABLED\"] = \"true\"\n    from llamafactory.train import tuner\n\n    tuner.run_exp({**args, **options})\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/apps/byzer_storage/train_sft.py\nimport json\nimport os\nimport concurrent.futures\nfrom typing import List, Dict, Any\nimport byzerllm\nfrom .memory_model_based import read_alpaca_zh\nfrom loguru import logger\nfrom pydantic import BaseModel\nfrom byzerllm.apps.utils import TagExtractor, Tag\n\n\nclass QAPair(BaseModel):\n    question: str\n    answer: str\n\n\n@byzerllm.prompt()\ndef _convert_pretrain_text_to_instruction(text: str, num: int = 10) -> str:\n    \"\"\"\n    根据提供的信息，生成多个相关的问题，这些问题的回答，最终要能覆盖里面所有的信息,生成 {{num}} 个。\n\n    下面是一些信息：\n\n    {{text}}\n\n    现在请开始生成问题，每个问题使用<_question_>标签包裹，每个回答使用<_answer_>标签包裹，最后\n    每个一组问题和回答使用<_group_>标签包裹。\n    \"\"\"\n\n\n@byzerllm.prompt()\ndef get_more(text: str, questions_and_answers: str, num: int = 10) -> str:\n    \"\"\"\n    下面是一些问答信息：\n\n    {{text}}\n\n    下面是已有的问题和回答：\n\n    {{questions_and_answers}}\n\n    下面是已经生成的问题和回答，请继续生成{{num}}个问题和回答，不要和前面的重复，请将每个问题使用<_question_>标签包裹，每个回答使用<_answer_>标签包裹，最后\n    每个一组问题和回答使用<_group_>标签包裹。\n    \"\"\"\n\n\ndef to_qa_pairs(text: str, llm, num: int = 30) -> List[QAPair]:\n    print(f\"Generating QA pairs for {text}\", flush=True)\n    v = _convert_pretrain_text_to_instruction.with_llm(llm).run(text)\n\n    max_turns = int(num / 10)\n    if max_turns < 1:\n        max_turns = 1\n\n    while max_turns > 0:\n        t = get_more.with_llm(llm).run(text, v)\n        v += t\n        max_turns -= 1\n\n    # format_v = self._format.with_llm(llm).run(v)\n    root_tag = TagExtractor(v).extract()\n    qa_pairs = []\n    # _group_\n    for item in root_tag.content:\n        qas = item.content\n        if len(qas) == 2:\n            if (\n                qas[0].start_tag == \"<_question_>\"\n                and qas[0].end_tag == \"</_question_>\"\n                and qas[1].start_tag == \"<_answer_>\"\n                and qas[1].end_tag == \"</_answer_>\"\n            ):\n                qa_pairs.append(QAPair(question=qas[0].content, answer=qas[1].content))\n\n    print(f\"Generated {len(qa_pairs)} QA pairs.\", flush=True)\n    logger.info(f\"Generated {len(qa_pairs)} QA pairs.\")\n    return qa_pairs\n\n\ndef train_sft(    \n    name: str,\n    memories: List[str],\n    options: Dict[str, Any],\n    dataset_dir: str,\n    loras_dir: str,\n    model_name: str,\n):\n    data = []\n    min_samples = options.pop(\"min_samples\", 1000)\n\n    if memories:\n        with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:\n            futures = []\n            llm = byzerllm.ByzerLLM().from_default_model(model_name)\n            for memory in memories:\n                futures.append(executor.submit(to_qa_pairs, memory, llm))\n\n            for future in concurrent.futures.as_completed(futures):\n                qa_pairs = future.result()\n                for qa_pair in qa_pairs:\n                    item = {\n                        \"instruction\": qa_pair.question,\n                        \"input\": \"\",\n                        \"output\": qa_pair.answer,\n                    }\n                    data.append(item)\n    else:\n        v = read_alpaca_zh()\n        data.extend(v)\n\n    if len(data) < min_samples:\n        data = data * (min_samples // len(data) + 1)\n\n    with open(f\"{dataset_dir}/data.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(data, f, indent=2, ensure_ascii=False)\n\n    with open(f\"{dataset_dir}/dataset_info.json\", \"w\", encoding=\"utf-8\") as f:\n        r = {\"data\": {\"file_name\": \"data.json\"}}\n        f.write(json.dumps(r, indent=2, ensure_ascii=False))\n\n    args = dict(\n        stage=\"sft\",\n        do_train=True,\n        model_name_or_path=llama_model,\n        dataset=\"data\",\n        dataset_dir=dataset_dir,\n        cutoff_len=1024,\n        max_samples=1000000,\n        overwrite_cache=True,\n        preprocessing_num_workers=1,\n        template=\"llama3\",\n        finetuning_type=\"lora\",\n        lora_target=\"all\",\n        output_dir=f\"{loras_dir}/{name}\",\n        overwrite_output_dir=True,\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=4,\n        lr_scheduler_type=\"cosine\",\n        logging_steps=10,\n        warmup_ratio=0.1,\n        save_steps=1000,\n        plot_loss=True,\n        learning_rate=5e-5,\n        num_train_epochs=1000.0,\n        max_grad_norm=1.0,\n        quantization_bit=4,\n        loraplus_lr_ratio=16.0,\n        fp16=True,\n        ddp_timeout=180000000,\n    )\n    os.environ[\"WANDB_DISABLED\"] = \"true\"\n    from llamafactory.train import tuner\n\n    tuner.run_exp({**args, **options})\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/apps/byzer_storage/train_dpo.py\nimport json\nimport os\nfrom typing import List, Dict, Any\nimport byzerllm\n\n\n@byzerllm.prompt()\ndef generate_dpo_data(memory: str) -> str:\n    \"\"\"\n    根据以下内容，生成一个DPO（Direct Preference Optimization）训练数据样本。\n    每个样本应包含一个提示（prompt），一个首选回答（preferred_output）和一个拒绝回答（rejected_output）。\n    首选回答应该是高质量、有帮助的回答，而拒绝回答应该是质量较差或不太有帮助的回答。\n\n    内容：\n    {{memory}}\n\n    请生成DPO训练数据，并以JSON格式返回，格式如下：\n    {\n        \"prompt\": \"用户提问或指令\",\n        \"preferred_output\": \"高质量、有帮助的回答\",\n        \"rejected_output\": \"质量较差或不太有帮助的回答\"\n    }\n    \"\"\"\n\n\ndef train_dpo(\n    self,\n    name: str,\n    memories: List[str],\n    options: Dict[str, Any],\n    dataset_dir: str,\n    loras_dir: str,\n    llama_model: str,\n):\n    data = []\n    min_samples = options.pop(\"min_samples\", 1000)\n\n    llm = byzerllm.ByzerLLM().from_default_model(self.model_name)\n    for memory in memories:\n        dpo_sample = generate_dpo_data.with_llm(llm).run(memory)\n        data.append(json.loads(dpo_sample))\n\n    if len(data) < min_samples:\n        data = data * (min_samples // len(data) + 1)\n\n    with open(f\"{dataset_dir}/data.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(data, f, indent=2, ensure_ascii=False)\n\n    with open(f\"{dataset_dir}/dataset_info.json\", \"w\", encoding=\"utf-8\") as f:\n        r = {\"data\": {\"file_name\": \"data.json\"}}\n        f.write(json.dumps(r, indent=2, ensure_ascii=False))\n\n    args = dict(\n        stage=\"dpo\",\n        do_train=True,\n        model_name_or_path=llama_model,\n        dataset=\"data\",\n        dataset_dir=dataset_dir,\n        cutoff_len=1024,\n        max_samples=1000000,\n        overwrite_cache=True,\n        preprocessing_num_workers=1,\n        template=\"llama3\",\n        finetuning_type=\"lora\",\n        lora_target=\"all\",\n        output_dir=f\"{loras_dir}/{name}\",\n        overwrite_output_dir=True,\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=4,\n        lr_scheduler_type=\"cosine\",\n        logging_steps=10,\n        warmup_ratio=0.1,\n        save_steps=1000,\n        plot_loss=True,\n        learning_rate=5e-5,\n        num_train_epochs=1000.0,\n        max_grad_norm=1.0,\n        quantization_bit=4,\n        loraplus_lr_ratio=16.0,\n        fp16=True,\n        ddp_timeout=180000000,\n    )\n    os.environ[\"WANDB_DISABLED\"] = \"true\"\n    from llamafactory.train import tuner\n\n    tuner.run_exp({**args, **options})\n\n\n     \n\n下面是用户的需求：\n\ntrain_sft 中的to_qa_pairs 需要打印更详细的日志,方便定位问题\n"}, {"role": "assistant", "content": "为了在 `train_sft` 函数中的 `to_qa_pairs` 函数中添加更详细的日志，我们需要修改 `/Users/allwefantasy/projects/byzer-llm/src/byzerllm/apps/byzer_storage/train_sft.py` 文件。以下是修改后的代码：\n\n```python\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/apps/byzer_storage/train_sft.py\n<<<<<<< SEARCH\ndef to_qa_pairs(text: str, llm, num: int = 30) -> List[QAPair]:\n    print(f\"Generating QA pairs for {text}\", flush=True)\n    v = _convert_pretrain_text_to_instruction.with_llm(llm).run(text)\n\n    max_turns = int(num / 10)\n    if max_turns < 1:\n        max_turns = 1\n\n    while max_turns > 0:\n        t = get_more.with_llm(llm).run(text, v)\n        v += t\n        max_turns -= 1\n\n    # format_v = self._format.with_llm(llm).run(v)\n    root_tag = TagExtractor(v).extract()\n    qa_pairs = []\n    # _group_\n    for item in root_tag.content:\n        qas = item.content\n        if len(qas) == 2:\n            if (\n                qas[0].start_tag == \"<_question_>\"\n                and qas[0].end_tag == \"</_question_>\"\n                and qas[1].start_tag == \"<_answer_>\"\n                and qas[1].end_tag == \"</_answer_>\"\n            ):\n                qa_pairs.append(QAPair(question=qas[0].content, answer=qas[1].content))\n\n    print(f\"Generated {len(qa_pairs)} QA pairs.\", flush=True)\n    logger.info(f\"Generated {len(qa_pairs)} QA pairs.\")\n    return qa_pairs\n=======\ndef to_qa_pairs(text: str, llm, num: int = 30) -> List[QAPair]:\n    logger.info(f\"Starting to generate QA pairs for text: {text[:100]}...\")\n    print(f\"Starting to generate QA pairs for text: {text[:100]}...\", flush=True)\n\n    logger.info(\"Generating initial QA pairs...\")\n    v = _convert_pretrain_text_to_instruction.with_llm(llm).run(text)\n    logger.info(f\"Initial QA pairs generated. Length: {len(v)}\")\n\n    max_turns = int(num / 10)\n    if max_turns < 1:\n        max_turns = 1\n    logger.info(f\"Will generate additional QA pairs for {max_turns} turns\")\n\n    turn = 0\n    while max_turns > 0:\n        turn += 1\n        logger.info(f\"Generating additional QA pairs (turn {turn}/{max_turns})...\")\n        t = get_more.with_llm(llm).run(text, v)\n        v += t\n        logger.info(f\"Additional QA pairs generated. New total length: {len(v)}\")\n        max_turns -= 1\n\n    logger.info(\"Extracting QA pairs from generated text...\")\n    root_tag = TagExtractor(v).extract()\n    qa_pairs = []\n    for i, item in enumerate(root_tag.content):\n        qas = item.content\n        if len(qas) == 2:\n            if (\n                qas[0].start_tag == \"<_question_>\"\n                and qas[0].end_tag == \"</_question_>\"\n                and qas[1].start_tag == \"<_answer_>\"\n                and qas[1].end_tag == \"</_answer_>\"\n            ):\n                qa_pairs.append(QAPair(question=qas[0].content, answer=qas[1].content))\n                logger.debug(f\"Extracted QA pair {i+1}: Q: {qas[0].content[:50]}... A: {qas[1].content[:50]}...\")\n            else:\n                logger.warning(f\"Skipping item {i+1} due to incorrect tags: {qas[0].start_tag}, {qas[0].end_tag}, {qas[1].start_tag}, {qas[1].end_tag}\")\n        else:\n            logger.warning(f\"Skipping item {i+1} due to incorrect number of elements: {len(qas)}\")\n\n    logger.info(f\"Extracted {len(qa_pairs)} valid QA pairs.\")\n    print(f\"Extracted {len(qa_pairs)} valid QA pairs.\", flush=True)\n    return qa_pairs\n>>>>>>> REPLACE\n```\n\n这些修改增加了更详细的日志记录，包括：\n\n1. 记录开始生成QA对的时间和输入文本的前100个字符。\n2. 记录初始QA对生成的结果长度。\n3. 记录将要生成额外QA对的轮次数。\n4. 在每一轮生成额外QA对时记录进度。\n5. 记录从生成的文本中提取QA对的过程，包括成功提取的对数和跳过的项目（因为标签不正确或元素数量不正确）。\n6. 使用logger.debug来记录每个提取的QA对的前50个字符，这可以帮助调试但不会在正常运行时产生过多输出。\n7. 最后记录成功提取的有效QA对的总数。\n\n这些额外的日志将帮助你更好地理解QA对生成的过程，并在出现问题时更容易定位问题所在。你可以根据需要调整日志级别（例如，将一些logger.info改为logger.debug），以控制输出的详细程度。"}], "model": "human", "yaml_file": "actions/008_chat_action.yml"}
{"instruction": "to_qa_pairs 方法要支持缓存,text 参数做md5,该md5值作为缓存key, 产生的数据保存成json格式,放到当前用户 .byzerllm/cache 目录下", "conversations": [{"role": "user", "content": "\n如果你需要生成代码，对于每个需要更改的文件,你需要按 *SEARCH/REPLACE block* 的格式进行生成。\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The opening fence and code language, eg: ```python\n2. The file path alone on a line, starting with \"##File:\" and verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n*SEARCH/REPLACE* blocks will replace *all* matching occurrences.\nInclude enough lines to make the SEARCH blocks unique.\n\nInclude *ALL* the code being searched and replaced!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n下面我们来看一个例子：\n\n当前项目目录结构：\n1. 项目根目录： /tmp/projects/mathweb\n2. 项目子目录/文件列表(类似tree 命令输出)\nflask/\n    app.py\n    templates/\n        index.html\n    static/\n        style.css\n\n用户需求： Change get_factorial() to use math.factorial\n\n回答： To make this change we need to modify `/tmp/projects/mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n用户需求： Refactor hello() into its own file.\n\n回答：To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/hello.py\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/main.py\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n现在让我们开始一个新的任务:\n\n当前项目目录结构：\n1. 项目根目录： /Users/allwefantasy/projects/byzer-llm\n2. 项目子目录/文件列表(类似tree 命令输出)：\nsetup.py\ntests/\n    test_emb_rerank.py\n    test_deepseek_code.py\n    test_prompt.py\n    test_model_meta.py\n    __init__.py\n    test_transformers_deploy.py\n    test_data_analysis.py\n    test_yi.py\n    test_deploy.py\n    functions_suite.py\n    test_template.py\n    auto/\n        test_backend_llama_cpp.py\n        __init__.py\nsetup-machine/\n    convert_safetensor.py\nnotebooks/\n    test.py\nsrc/\n    byzerllm/\n        lang.py\n        store.py\n        version.py\n        log.py\n        __init__.py\n        types.py\n        easy_byzerllm_command.py\n        byzerllm_command.py\n        command_args.py\n        bark/\n            generation.py\n            __init__.py\n            model.py\n            api.py\n            model_fine.py\n            bark_voice.py\n        moss/\n            finetune_moss.py\n            __init__.py\n            moss_inference.py\n            models/\n                configuration_moss.py\n                quantization.py\n                custom_autotune.py\n                __init__.py\n                modeling_moss.py\n                tokenization_moss.py\n        llama2/\n            __init__.py\n        records/\n            __init__.py\n        visualglm/\n            __init__.py\n        auto/\n            __init__.py\n            backend_llama_cpp.py\n            backend_vllm.py\n            backend_ds.py\n        alpha_moss/\n            __init__.py\n        processor/\n            __init__.py\n        qwen/\n            __init__.py\n        utils/\n            json_repaire.py\n            object_store_ref_util.py\n            __init__.py\n            types.py\n            tokenizer.py\n            openai_utils.py\n            ray_utils.py\n            text_generator.py\n            emb.py\n            connect_ray.py\n            testing.py\n            langutil.py\n            nontext.py\n            fulltune/\n                deepspeed_trainner.py\n                __init__.py\n                launch.py\n                trainner.py\n                base_model/\n                    __init__.py\n                    configuration_baichuan.py\n                    modeling_baichuan.py\n                pretrain/\n                    __init__.py\n                    convert_to_transformers.py\n            metrics/\n                __init__.py\n            config/\n                __init__.py\n            retrieval/\n                rest.py\n                __init__.py\n                udf.py\n            sft/\n                argument.py\n                collator.py\n                qlora.py\n                __init__.py\n                model.py\n                dataset.py\n                loss.py\n                merge_lora.py\n                trainer.py\n            client/\n                message_utils.py\n                math_utils.py\n                byzerllm_client.py\n                parallel_utils.py\n                __init__.py\n                types.py\n                code_utils.py\n                img_utils.py\n                entrypoints/\n                    __init__.py\n                    openai/\n                        protocol.py\n                        __init__.py\n                        serve.py\n                        serving_chat.py\n                        api_server.py\n                        serving_engine.py\n                        serving_completion.py\n                        tool.py\n        chatglm2/\n            __init__.py\n        baichuan/\n            __init__.py\n        m3e/\n            __init__.py\n        stable_diffusion/\n            config.py\n            __init__.py\n            logger.py\n            shared.py\n            model.py\n            utils.py\n            images.py\n            diffusion/\n                __init__.py\n                embeddings.py\n                utils.py\n                networks/\n                    lyco.py\n                    __init__.py\n                upscalers/\n                    __init__.py\n                    multidiffusion.py\n                    samplers.py\n                piplines/\n                    __init__.py\n                    lpw.py\n                    diffusers.py\n            lib/\n                __init__.py\n                diffusers/\n                    __init__.py\n                    scheduler.py\n            api/\n                __init__.py\n                plugin.py\n                models/\n                    diffusion.py\n                    __init__.py\n                diffusion/\n                    __init__.py\n                    pipelines/\n                        __init__.py\n                        diffusers.py\n                events/\n                    generation.py\n                    __init__.py\n        bge/\n            __init__.py\n        bge_rerank/\n            __init__.py\n        apps/\n            client.py\n            qa_strategy.py\n            __init__.py\n            builder.py\n            vector_db.py\n            utils.py\n            qa.py\n            byzer_storage/\n                memory_model_based.py\n                command.py\n                env.py\n                train_pt.py\n                train_sft.py\n                __init__.py\n                train_dpo.py\n                simple_api.py\n                generate_sft_data.py\n            byzer_sql/\n                __init__.py\n            llama_index/\n                byzerai_vectordb.py\n                byzerai.py\n                byzerai_embedding.py\n                __init__.py\n                collection_manager.py\n                byzerai_index_store.py\n                byzerai_docstore.py\n                simple_retrieval.py\n                byzerai_kvstore.py\n            agent/\n                conversable_agent.py\n                registry.py\n                groupchat.py\n                user_proxy_agent.py\n                __init__.py\n                agent.py\n                extensions/\n                    simple_retrieval_client.py\n                    planner.py\n                    python_codesandbox_agent.py\n                    preview_file_agent.py\n                    llama_index_retrieval_agent.py\n                    visualization_agent.py\n                    assistant_agent.py\n                    load_data_agent.py\n                    retrieval_agent.py\n                    output_agent.py\n                    __init__.py\n                    byzer_engine_agent.py\n                    subquestion_agent.py\n                    sql_reviewer_agent.py\n                    spark_sql_agent.py\n                    rhetorical_agent.py\n                    data_analysis.py\n                    data_analysis_pipeline_agent.py\n                    common_agent.py\n                    query_rewrite/\n                        time.py\n                        rhetorical.py\n                        __init__.py\n                        condition.py\n                        context.py\n                store/\n                    memory_store.py\n                    __init__.py\n                    stores.py\n        saas/\n            __init__.py\n            chatglm/\n                __init__.py\n            sparkdesk/\n                __init__.py\n            gemini/\n                __init__.py\n            azure_openai/\n                __init__.py\n            azure/\n                __init__.py\n            qianfan/\n                __init__.py\n            claude/\n                __init__.py\n            qianwen/\n                __init__.py\n            volcengine/\n                __init__.py\n            minimax/\n                __init__.py\n            baichuan/\n                __init__.py\n            qianwen_vl/\n                __init__.py\n            official_openai/\n                __init__.py\n            zhipu/\n                __init__.py\n            aws_bedrock/\n                __init__.py\n            openai/\n                __init__.py\n        falcon/\n            __init__.py\n        chatglm6b/\n            trainer_seq2seq.py\n            arguments.py\n            __init__.py\n            finetune.py\n            tunning/\n                __init__.py\n                infer.py\n                finetune.py\n                utils/\n                    config.py\n                    __init__.py\n                    other.py\n                    common.py\n                    seq2seq.py\n        starcode/\n            __init__.py\n        qwen_vl_chat/\n            __init__.py\n        zephyr/\n            __init__.py\n        dolly/\n            generate.py\n            __init__.py\n            consts.py\n            dolly_inference.py\n            trainer.py\n        llama/\n            __init__.py\n        whisper/\n            whisper_inference.py\n            __init__.py\n下面是一些文件路径以及每个文件对应的源码：\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/apps/byzer_storage/memory_model_based.py\nimport asyncio\nfrom typing import List, Dict, Any\n\nimport ray.experimental\nfrom byzerllm.apps.byzer_storage.simple_api import ByzerStorage\nimport time\nimport json\nimport os\nimport concurrent.futures\nimport io\nimport sys\nfrom contextlib import redirect_stdout, redirect_stderr\nimport queue\nimport threading\nimport ray\nimport byzerllm\nfrom typing import List, Union, Dict\nfrom pydantic import BaseModel\nimport importlib\nfrom loguru import logger\n\nclass MemoryManager:\n    _queue = asyncio.Queue()\n    _is_processing = False\n\n    def __init__(\n        self,\n        storage: ByzerStorage,\n        base_dir: str,\n        model_name: str = \"deepseek_chat\",\n        remote: bool = True,\n    ):\n        self.storage = storage\n        home = os.path.expanduser(\"~\")\n        self.base_dir = base_dir or os.path.join(home, \".auto-coder\")\n        self.thread_pool = concurrent.futures.ThreadPoolExecutor(max_workers=5)\n        self.log_file = None\n        self.remote = remote\n        self.model_name = model_name\n\n    @classmethod\n    async def add_to_queue(cls, name: str, memories: List[str]):\n        await cls._queue.put((name, memories))\n        if not cls._is_processing:\n            asyncio.create_task(cls.process_queue())\n\n    @classmethod\n    async def process_queue(cls):\n        cls._is_processing = True\n        while not cls._queue.empty():\n            name, memories = await cls._queue.get()\n            instance = cls.get_instance()\n            await instance.memorize(name, memories)\n            cls._queue.task_done()\n        cls._is_processing = False\n\n    async def memorize(\n        self, name: str, memories: List[str], options: Dict[str, Any] = {}\n    ):\n        try:\n            loop = asyncio.get_running_loop()\n            await loop.run_in_executor(\n                self.thread_pool, self._memorize_with_logs, name, memories, options\n            )\n        finally:\n            if self.remote:\n                ray.actor.exit_actor()\n\n        print(f\"Memorization for {name} completed.\")\n\n    def _memorize_with_logs(\n        self, name: str, memories: List[str], options: Dict[str, Any] = {}\n    ):\n\n        if self.remote:\n            self._memorize(name, memories, options=options)\n            return\n\n        logs_dir = os.path.join(self.base_dir, \"storage\", \"logs\", \"memorize\")\n        os.makedirs(logs_dir, exist_ok=True)\n        log_file = os.path.join(logs_dir, f\"{name}.log\")\n\n        log_queue = queue.Queue()\n        stop_event = threading.Event()\n\n        # Start the log writer thread\n        log_writer_thread = threading.Thread(\n            target=self._log_writer, args=(log_queue, log_file, stop_event)\n        )\n        self.log_file = log_file\n        log_writer_thread.start()\n\n        class QueueStream:\n            def __init__(self, queue):\n                self.queue = queue\n\n            def write(self, msg):\n                self.queue.put(msg)\n\n            def flush(self):\n                pass\n\n        queue_stream = QueueStream(log_queue)\n\n        with redirect_stdout(queue_stream), redirect_stderr(queue_stream):\n            self._memorize(name, memories)\n\n        # Signal the log writer to stop and wait for it to finish\n        stop_event.set()\n        log_writer_thread.join()\n\n    def _log_writer(\n        self, log_queue: queue.Queue, log_file: str, stop_event: threading.Event\n    ):\n        with open(log_file, \"w\") as f:\n            while not stop_event.is_set() or not log_queue.empty():\n                try:\n                    msg = log_queue.get(timeout=0.1)\n                    f.write(msg)\n                    f.flush()\n                except queue.Empty:\n                    continue    \n\n    def _memorize(\n        self,\n        name: str,\n        memories: List[Union[str, Dict[str, Any]]],\n        options: Dict[str, Any] = {},\n    ):\n        data = []\n        stage = options.get(\"stage\", \"pt\")\n        min_samples = options.pop(\"min_samples\", 1000)\n        base_model_dir = os.path.join(self.base_dir, \"storage\", \"models\")\n        llama_model = os.path.join(\n            base_model_dir, \"meta-llama\", \"Meta-Llama-3-8B-Instruct-GPTQ\"\n        )\n\n        loras_dir = os.path.join(self.base_dir, \"storage\", \"loras\")\n        dataset_dir = os.path.join(self.base_dir, \"storage\", \"datasets\", name)\n\n        os.makedirs(loras_dir, exist_ok=True)\n        os.makedirs(dataset_dir, exist_ok=True)\n\n        if stage == \"pt\":\n            from .train_pt import train_pt\n            train_pt(self, name, memories, options, dataset_dir, loras_dir, llama_model)\n        elif stage == \"sft\":\n            from .train_sft import train_sft\n            train_sft(self, name, memories, options, dataset_dir, loras_dir, llama_model)\n        elif stage == \"dpo\":\n            from .train_dpo import train_dpo\n            train_dpo(self, name, memories, options, dataset_dir, loras_dir, llama_model)\n        else:\n            raise ValueError(f\"Unsupported stage: {stage}\")\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/docs/zh/003_byzerllm_大模型编程快速指南.md\n##  byzerllm 大模型编程快速指南\n\n本文示例 [Notebook](../../notebooks/003_byzerllm_大模型编程快速指南.ipynb)\n\n## 安装\n\n```bash\npip install -U byzerllm\nray start --head\n```\n## 启动一个模型代理\n\nbyzerllm 支持私有化模型或者SaaS模型的部署。\n\n这里以 deepseek 官方API 为例：\n\n```bash\neasy-byzerllm deploy deepseek-chat --token xxxxx --alias deepseek_chat\n```\n\n或者跬基流动API:\n\n```bash\neasy-byzerllm deploy alibaba/Qwen1.5-110B-Chat --token xxxxx --alias qwen110b_chat\n```\n\n将上面的 API KEY 替换成你们自己的。\n\n如果你想部署私有化模型或者对接 Ollama 等更多需求，参考 [002_使用byzerllm进行模型部署.md](./002_使用byzerllm进行模型部署.md) 或者 [README.md](../../README.md)。\n\n之后，你就可以在代码里使用  deepseek_chat 或者 qwen110b_chat  访问模型了。\n\n## hello world\n\n来和我们的大模型打个招呼:\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\n@byzerllm.prompt(llm=llm)\ndef hello(q:str) ->str:\n    '''\n    你好, {{ q }}\n    '''\n\ns = hello(\"你是谁\")    \nprint(s)\n\n## 输出:\n## '你好！我是一个人工智能助手，专门设计来回答问题、提供信息和帮助解决问题。如果你有任何疑问或需要帮助，请随时告诉我。'\n```\n\n恭喜，你和大模型成功打了招呼！\n\n可以看到，我们通过 `@byzerllm.prompt` 装饰器，将一个方法转换成了一个大模型的调用，然后这个方法的主题是一段文本，文本中\n使用了 jinja2 模板语法，来获得方法的参数。当正常调用该方法时，实际上就发起了和大模型的交互，并且返回了大模型的结果。\n\n在 byzerllm 中，我们把这种方法称之为 prompt 函数。\n\n## 查看发送给大模型的prompt\n\n很多情况你可能需要调试，查看自己的 prompt 渲染后到底是什么样子的，这个时候你可以通过如下方式\n获取渲染后的prompt:\n\n```python\nhello.prompt(\"你是谁\")\n## '你好, 你是谁'\n```            \n\n## 动态换一个模型\n\n前面的 hello 方法在初始化的时候，我们使用了默认的模型 deepseek_chat，如果我们想换一个模型，可以这样做：\n\n```python\nhello.with_llm(llm).run(\"你是谁\")\n## '你好！我是一个人工智能助手，专门设计来回答问题、提供信息和帮助解决问题。如果你有任何疑问或需要帮助，请随时告诉我。'\n```\n\n通过 with_llm 你可以设置一个新的 llm 对象，然后调用 run 方法，就可以使用新的模型了。\n\n## 超长文本生成\n\n我们知道，大模型一次生成的长度其实是有限的，如果你想生成超长文本，你可能需手动的不断获得\n生成结果，然后把他转化为输入，然后再次生成，这样的方式是比较麻烦的。\n\nbyzerllm 提供了更加易用的 API :\n\n```python\nimport byzerllm\nfrom byzerllm import ByzerLLM\n\nllm = ByzerLLM.from_default_model(\"deepseek_chat\")\n\n@byzerllm.prompt()\ndef tell_story() -> str:\n    \"\"\"\n    讲一个100字的故事。    \n    \"\"\"\n\n\ns = (\n    tell_story.with_llm(llm)\n    .with_response_markers()\n    .options({\"llm_config\": {\"max_length\": 10}})\n    .run()\n)\nprint(s)\n\n## 从前，森林里住着一只聪明的小狐狸。一天，它发现了一块闪闪发光的宝石。小狐狸决定用这块宝石帮助森林里的动物们。它用宝石的光芒指引迷路的小鹿找到了回家的路，用宝石的温暖治愈了受伤的小鸟。从此，小狐狸成了森林里的英雄，动物们都感激它的善良和智慧。\n```\n\n实际核心部分就是这一行：\n\n```python\ntell_story.with_llm(llm)\n    .with_response_markers()    \n    .run()\n```\n\n我们只需要调用 `with_response_markers` 方法，系统就会自动的帮我们生成超长文本。\n在上面的案例中，我们通过\n\n```python\n.options({\"llm_config\": {\"max_length\": 10}})\n```\n\n认为的限制大模型一次交互最多只能输出10个字符，但是系统依然自动完成了远超过10个字符的文本生成。\n\n## 对象输出\n\n前面我们的例子都是返回字符串，但是我们也可以返回对象，这样我们就可以更加灵活的处理返回结果。\n\n```python\nimport pydantic \n\nclass Story(pydantic.BaseModel):\n    '''\n    故事\n    '''\n\n    title: str = pydantic.Field(description=\"故事的标题\")\n    body: str = pydantic.Field(description=\"故事主体\")\n\n@byzerllm.prompt()\ndef tell_story()->Story:\n    '''\n    讲一个100字的故事。    \n    '''\n\ns = tell_story.with_llm(llm).run()\nprint(isinstance(s, Story))\nprint(s.title)\n\n## True\n## 勇敢的小鸟\n```\n\n可以看到，我们很轻松的将输出转化为格式化输出。\n\n## 自定义字段抽取\n\n前面的结构化输出，其实会消耗更多token,还有一种更加精准的结构化输出方式。\n比如让大模型生成一个正则表达式，但实际上大模型很难准确只输出一个正则表达式，这个时候我们可以通过自定义抽取函数来获取我们想要的结果。\n\n\n```python\nfrom loguru import logger\nimport re\n\n@byzerllm.prompt()\ndef generate_regex_pattern(desc: str) -> str:\n    \"\"\"\n    根据下面的描述生成一个正则表达式，要符合python re.compile 库的要求。\n\n    {{ desc }}\n\n    最后生成的正则表达式要在<REGEX></REGEX>标签对里。\n    \"\"\"    \n\ndef extract_regex_pattern(regex_block: str) -> str:    \n    pattern = re.search(r\"<REGEX>(.*)</REGEX>\", regex_block, re.DOTALL)\n    if pattern is None:\n        logger.warning(\"No regex pattern found in the generated block:\\n {regex_block}\")\n        raise None\n    return pattern.group(1)\n\npattern = \"匹配一个邮箱地址\"\nv = generate_regex_pattern.with_llm(llm).with_extractor(extract_regex_pattern).run(desc=pattern)\nprint(v)\n## ^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\n```\n\n在上面的例子里，我们根据一句话生成一个正则表达式。我们通过 `with_extractor` 方法，传入了一个自定义的抽取函数，这个函数会在大模型生成结果后，对结果进行处理，然后返回我们想要的结果。\n\n我们在 prompt 明确说了，生成的结果要放到 `<REGEX></REGEX>` 标签对里，然后我们通过 extract_regex_pattern 函数，从结果中提取出了我们想要的正则表达式。\n\n## 在实例方法中使用大模型\n\n```python\nimport byzerllm\ndata = {\n    'name': 'Jane Doe',\n    'task_count': 3,\n    'tasks': [\n        {'name': 'Submit report', 'due_date': '2024-03-10'},\n        {'name': 'Finish project', 'due_date': '2024-03-15'},\n        {'name': 'Reply to emails', 'due_date': '2024-03-08'}\n    ]\n}\n\n\nclass RAG():\n    def __init__(self):        \n        self.llm = byzerllm.ByzerLLM()\n        self.llm.setup_template(model=\"deepseek_chat\",template=\"auto\")\n        self.llm.setup_default_model_name(\"deepseek_chat\")        \n    \n    @byzerllm.prompt(lambda self:self.llm)\n    def generate_answer(self,name,task_count,tasks)->str:\n        '''\n        Hello {{ name }},\n\n        This is a reminder that you have {{ task_count }} pending tasks:\n        {% for task in tasks %}\n        - Task: {{ task.name }} | Due: {{ task.due_date }}\n        {% endfor %}\n\n        Best regards,\n        Your Reminder System\n        '''        \n\nt = RAG()\n\nresponse = t.generate_answer(**data)\nprint(response)\n\n## 输出:\n## Hello Jane Doe,\n##I hope this message finds you well. I wanted to remind you of your 3 pending tasks to ensure you stay on track:\n## 1. **Submit report** - This task is due on **2024-03-10**. Please ensure that you allocat\n```\n\n这里我们给了个比较复杂的例子，但我们可以看到，给一个实例prompt方法和普通prompt 方法差异不大。\n唯一的区别是如果你希望在定义的时候就指定大模型，使用一个lambda函数返回实例的 llm 对象即可。\n\n```python\n@byzerllm.prompt(lambda self:self.llm)\n```\n\n你也可以不返回，在调用的时候通过 `with_llm` 方法指定 llm 对象。\n\n此外，这个例子也展示了如何通过jinja2模板语法，来处理复杂的结构化数据。\n\n## 通过 Python 代码处理复杂入参\n\n上面的一个例子中，我们通过 jinja2 模板语法，来处理复杂的结构化数据，但是有时候我们可能需要更加复杂的处理，这个时候我们可以通过 Python 代码来处理。\n\n```python\nimport byzerllm\n\ndata = {\n    'name': 'Jane Doe',\n    'task_count': 3,\n    'tasks': [\n        {'name': 'Submit report', 'due_date': '2024-03-10'},\n        {'name': 'Finish project', 'due_date': '2024-03-15'},\n        {'name': 'Reply to emails', 'due_date': '2024-03-08'}\n    ]\n}\n\n\nclass RAG():\n    def __init__(self):        \n        self.llm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n    \n    @byzerllm.prompt()\n    def generate_answer(self,name,task_count,tasks)->str:\n        '''\n        Hello {{ name }},\n\n        This is a reminder that you have {{ task_count }} pending tasks:\n            \n        {{ tasks }}\n\n        Best regards,\n        Your Reminder System\n        '''\n        \n        tasks_str = \"\\n\".join([f\"- Task: {task['name']} | Due: { task['due_date'] }\" for task in tasks])\n        return {\"tasks\": tasks_str}\n\nt = RAG()\n\nresponse = t.generate_answer.with_llm(t.llm).run(**data)\nprint(response)\n\n## Just a gentle nudge to keep you on track with your pending tasks. Here's a quick recap:....\n```\n\n在这个例子里，我们直接把 tasks 在方法体里进行处理，然后作为一个字符串返回，最够构建一个字典，字典的key为 tasks,然后\n你就可以在 docstring 里使用 `{{ tasks }}` 来引用这个字符串。\n\n这样对于很复杂的入参，就不用谢繁琐的 jinja2 模板语法了。\n\n## 如何自动实现一个方法\n\n比如我定义一个签名，但是我不想自己实现里面的逻辑，让大模型来实现。这个在 byzerllm 中叫 function impl。我们来看看怎么\n实现:\n\n```python\nimport pydantic\nclass Time(pydantic.BaseModel):\n    time: str = pydantic.Field(...,description=\"时间，时间格式为 yyyy-MM-dd\")\n\n\n@llm.impl()\ndef calculate_current_time()->Time:\n    '''\n    计算当前时间\n    '''\n    pass \n\n\ncalculate_current_time()\n#output: Time(time='2024-06-14')\n```\n\n在这个例子里，我们定义了一个 calculate_current_time 方法，但是我们没有实现里面的逻辑，我们通过 `@llm.impl()` 装饰器，让大模型来实现这个方法。\n为了避免每次都要“生成”这个方法，导致无法适用，我们提供了缓存，用户可以按如下方式打印速度：\n\n```python\nstart = time.monotonic()\ncalculate_current_time()\nprint(f\"first time cost: {time.monotonic()-start}\")\n\nstart = time.monotonic()\ncalculate_current_time()\nprint(f\"second time cost: {time.monotonic()-start}\")\n\n# output:\n# first time cost: 6.067266260739416\n# second time cost: 4.347506910562515e-05\n```\n可以看到，第一次执行花费了6s,第二次几乎是瞬间完成的，这是因为第一次执行的时候，我们实际上是在生成这个方法，第二次执行的时候，我们是执行已经生成好的代码，所以速度会非常快。你可以显示的调用 `llm.clear_impl_cache()` 清理掉函数缓存。\n\n## Stream 模式\n\n前面的例子都是一次性生成结果，但是有时候我们可能需要一个流式的输出，这个时候我们可能需要用底层一点的API来完成了：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\nv = llm.stream_chat_oai(model=\"deepseek_chat\",conversations=[{\n    \"role\":\"user\",\n    \"content\":\"你好，你是谁\",\n}],delta_mode=True)\n\nfor t in v:\n    print(t,flush=True)  \n\n# 你好\n# ！\n# 我\n# 是一个\n# 人工智能\n# 助手\n# ，\n# 旨在\n# 提供\n# 信息\n# 、\n# 解答\n# 问题....\n```\n\n如果你不想要流式输出，但是想用底层一点的API，你可以使用 `llm.chat_oai` 方法：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\nv = llm.chat_oai(model=\"deepseek_chat\",conversations=[{\n    \"role\":\"user\",\n    \"content\":\"你好，你是谁\",\n}])\n\nprint(v[0].output)\n## 你好！我是一个人工智能助手，旨在提供信息、解答问题和帮助用户解决问题。如果你有任何问题或需要帮助，请随时告诉我。\n```\n\n## Function Calling \n\nbyzerllm 可以不依赖模型自身就能提供 function calling 支持，我们来看个例子：\n\n\n```python\nfrom typing import List,Dict,Any,Annotated\nimport pydantic \nimport datetime\nfrom dateutil.relativedelta import relativedelta\n\ndef compute_date_range(count:Annotated[int,\"时间跨度，数值类型\"],\n                       unit:Annotated[str,\"时间单位，字符串类型\",{\"enum\":[\"day\",\"week\",\"month\",\"year\"]}])->List[str]:\n    '''\n    计算日期范围\n\n    Args:\n        count: 时间跨度，数值类型\n        unit: 时间单位，字符串类型，可选值为 day,week,month,year\n    '''        \n    now = datetime.datetime.now()\n    now_str = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n    if unit == \"day\":\n        return [(now - relativedelta(days=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"week\":\n        return [(now - relativedelta(weeks=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"month\":\n        return [(now - relativedelta(months=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"year\":\n        return [(now - relativedelta(years=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    return [\"\",\"\"]\n\ndef compute_now()->str:\n    '''\n    计算当前时间\n    '''\n    return datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n```\n\n我们定义了两个方法，一个是计算日期范围，一个是计算当前时间。\n\n现在我么可以来测试下，系统如何根据自然语言决定调用哪个方法：\n\n```python\nt = llm.chat_oai([{\n    \"content\":'''计算当前时间''',\n    \"role\":\"user\"    \n}],tools=[compute_date_range,compute_now],execute_tool=True)\n\nt[0].values\n\n## output: ['2024-06-14 15:18:02']\n```\n\n我们可以看到，他正确的选择了 compute_now 方法。\n\n接着我们再试一个：\n\n```python\nt = llm.chat_oai([{\n    \"content\":'''最近三个月趋势''',\n    \"role\":\"user\"    \n}],tools=[compute_date_range,compute_now],execute_tool=True)\n\nt[0].values\n\n## output: [['2024-03-14 15:19:13', '2024-06-14 15:19:13']]\n```\n\n模型正确的选择了 compute_date_range 方法。\n\n## 多模态\n\nbyerllm 也能很好的支持多模态的交互，而且统一了多模态大模型的接口，比如你可以用一样的方式使用 openai 或者 claude 的图片转文字能力， 或者一致的方式使用火山，azuer, openai的语音合成接口。\n\n### 图生文\n\n```python\nimport byzerllm\nfrom byzerllm.types import ImagePath\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\n\n\n@byzerllm.prompt()\ndef what_in_image(image_path: ImagePath) -> str:\n    \"\"\"\n    {{ image_path }}\n    这个图片里有什么？\n    \"\"\"    \n\n\nv = what_in_image.with_llm(vl_llm).run(\n    ImagePath(value=\"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\")\n)\nv\n## OUTPUT: 这张图片展示了多只可爱的猫咪，采用了艺术风格的绘画。猫咪们有不同的颜色和花纹，背景是浅棕色，上面还点缀着一些红色的花朵。整体画面给人一种温馨和谐的感觉\n```\n\n可以看到，我们只需要把 prompt 函数的图片地址入参使用 byzerllm.types.ImagePath里进行包装，就可以直接在 prompt 函数体里\n带上图片。\n\n或者你可以这样：\n\n```python\nimport byzerllm\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\n\n\n@byzerllm.prompt()\ndef what_in_image(image_path: str) -> str:\n    \"\"\"\n    {{ image }}\n    这个图片里有什么？\n    \"\"\"\n    return {\"image\": byzerllm.Image.load_image_from_path(image_path)}\n\n\nv = what_in_image.with_llm(vl_llm).run(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv\n```\n\n通过 `image_path` 参数，然后通过 `byzerllm.Image.load_image_from_path` 方法，转化为一个图片对象 image，最后在 prompt 函数体里\n使用 `{{ image }}` 引用这个图片对象。\n\n另外我们也是可以支持配置多张图片的。\n\n另外，我们也可以使用基础的 `llm.chat_oai` 方法来实现：\n\n```python\nimport byzerllm\nimport json\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\nimage = byzerllm.Image.load_image_from_path(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv = vl_llm.chat_oai(\n    conversations=[\n        {\n            \"role\": \"user\",\n            \"content\": json.dumps(\n                [{\"image\": image, \"text\": \"这个图片里有什么？\"}], ensure_ascii=False\n            ),\n        }\n    ]\n)\nv[0].output\n```\n\n还可以这么写：\n\n```python\nimport byzerllm\nimport json\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\nimage = byzerllm.Image.load_image_from_path(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv = vl_llm.chat_oai(\n    conversations=[\n        {\n            \"role\": \"user\",\n            \"content\": json.dumps(\n                [\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\"url\": image, \"detail\": \"high\"},\n                    },\n                    {\"text\": \"这个图片里有什么？\", \"type\": \"text\"},\n                ],\n                ensure_ascii=False,\n            ),\n        }\n    ]\n)\nv[0].output\n```\n\n### 语音合成\n\n这里以 openai 的 tts 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=tts-1 \\\n--model openai_tts\n```\n\n此外，byzerllm 支持 azure,火山引擎等 tts 语音合成引擎。\n\n接着你可以这么用：\n\n```python\nimport byzerllm\nimport base64\nimport json\n\nllm = byzerllm.ByzerLLM.from_default_model(\"openai_tts\")\n\n\nt = llm.chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\": json.dumps({\n        \"input\":\"hello, open_tts\",\n        \"voice\": \"alloy\",\n        \"response_format\": \"mp3\"\n    },ensure_ascii=False)\n}])\n\nwith open(\"voice.mp3\",\"wb\") as f:\n    f.write(base64.b64decode(t[0].output))\n```\n\ntts 模型生成没有prompt函数可以用，你需要直接使用 chat_oai。\n\n\n### 语音识别\n\n这里以 openai 的 whisper-1 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--worker_concurrency 10 \\\n--infer_params saas.model=whisper-1 saas.api_key=${MODEL_OPENAI_TOKEN} \\\n--model speech_to_text\n```\n\n语音识别的使用方式和图生文类似，我们可以直接在 prompt 函数体里带上音频文件。\n\n```python\nimport byzerllm\nimport json\nimport base64\nfrom byzerllm.types import AudioPath\n\nllm = byzerllm.ByzerLLM.from_default_model(\"speech_to_text\")\n\naudio_file = \"/Users/allwefantasy/videos/output_audio.mp3\"\n\n@byzerllm.prompt(llm=llm)\ndef audio_to_text(audio_file: AudioPath):\n    \"\"\"\n    {{ audio_file }}\n    \"\"\"\n\nv = audio_to_text(AudioPath(value=audio_file))\njson.loads(v)\n```\n输出的数据格式略微复杂：\n\n```\n{'text': 'In the last chapter, you and I started to step through the internal workings of a transformer. This is one of the key pieces of technology inside large language models, and a lot of other tools in the modern wave of AI.',\n 'task': 'transcribe',\n 'language': 'english',\n 'duration': 10.0,\n 'segments': [{'id': 0,\n   'seek': 0,\n   'start': 0.0,\n   'end': 4.78000020980835,\n   'text': ' In the last chapter, you and I started to step through the internal workings of a transformer.',\n   'tokens': [50364,\n    682,\n    264,\n    1036,\n    7187,\n    11,\n    291,\n    293,\n    286,\n    1409,\n    281,\n    1823,\n    807,\n    264,\n    6920,\n    589,\n    1109,\n    295,\n    257,\n    31782,\n    13,\n    50586],\n   'temperature': 0.0,\n   'avg_logprob': -0.28872039914131165,\n   'compression_ratio': 1.4220778942108154,\n   'no_speech_prob': 0.016033057123422623},\n  ....\n  {'id': 2,\n   'seek': 0,\n   'start': 8.579999923706055,\n   'end': 9.979999542236328,\n   'text': ' and a lot of other tools in the modern wave of AI.',\n   'tokens': [50759,\n    293,\n    257,\n    688,\n    295,\n    661,\n    3873,\n    294,\n    264,\n    4363,\n    5772,\n    295,\n    7318,\n    13,\n    50867],\n   'temperature': 0.0,\n   'avg_logprob': -0.28872039914131165,\n   'compression_ratio': 1.4220778942108154,\n   'no_speech_prob': 0.016033057123422623}],\n 'words': [{'word': 'In', 'start': 0.0, 'end': 0.18000000715255737},\n  {'word': 'the', 'start': 0.18000000715255737, 'end': 0.23999999463558197},\n  {'word': 'last', 'start': 0.23999999463558197, 'end': 0.5400000214576721},\n  {'word': 'chapter', 'start': 0.5400000214576721, 'end': 0.800000011920929},\n  ....\n  {'word': 'AI', 'start': 9.920000076293945, 'end': 9.979999542236328}]}\n```\n\n会输出每一句话以及每一个字所在的起始时间和截止时间。你可以根据需要来使用。\n\n### 文生图\n\n文生图和语音合成类似，首先要启动合适的模型,以openai 的 dall-e-3 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=dall-e-3 \\\n--model openai_image_gen\n```\n\n启动模型后，只需要记住几个模板参数即可使用，这里直接使用 chat_oai 方法来使用：\n\n```python\n\nimport byzerllm\nimport json\nimport base64\n\nllm = byzerllm.ByzerLLM.from_default_model(\"openai_image_gen\")\nt = llm.chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\": json.dumps({\n        \"input\":\"a white siamese cat\",\n        \"size\": \"1024x1024\",\n        \"quality\": \"standard\"\n    },ensure_ascii=False)\n}])\n\nwith open(\"output1.jpg\",\"wb\") as f:\n    f.write(base64.b64decode(t[0].output))\n\n\nimport matplotlib.pyplot as plt\n\nimage_path = \"output1.jpg\"\nimage = plt.imread(image_path)\n\nplt.imshow(image)\nplt.axis('off')\nplt.show()\n```\n\n## Prompt 函数的流式输出\n\nbyzerllm 底层支持流式输出，非 prompt 函数的用法是这样的：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\n\nv = llm.stream_chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\":\"讲一个100字的故事\"\n}])\n\nfor s  in v:\n    print(s[0], end=\"\")\n```\n\n如果你像用 prompt 函数，可以这么用：\n\n\n```python\nimport byzerllm\nimport json\nimport base64\nfrom typing import Generator\n\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\n\n@byzerllm.prompt()\ndef tell_story() -> Generator[str, None, None]:\n    '''\n    给我讲一个一百多字的故事\n    '''\n\nv = tell_story.with_llm(llm).run()    \nfor i in v:\n    print(i, end=\"\")\n```\n\n可以看到，和普通的 prompt 函数的区别在于，返回值是一个生成器，然后你可以通过 for 循环来获取结果。\n\n## 注意事项\n\n1. prompt函数方法体返回只能是dict，实际的返回类型和方法签名可以不一样，但是方法体返回只能是dict。\n2. 大部分情况prompt函数体为空，如果一定要有方法体，可以返回一个空字典。\n3. 调用prompt方法的时候，如果在@byzerllm.prompt()里没有指定llm对象，那么需要在调用的时候通过with_llm方法指定llm对象。\n\n\n\n\n\n\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/apps/byzer_storage/train_pt.py\nimport json\nimport os\nfrom typing import List, Dict, Any\nimport byzerllm\n\ndef train_pt(\n    self,\n    name: str,\n    memories: List[str],\n    options: Dict[str, Any],\n    dataset_dir: str,\n    loras_dir: str,\n    llama_model: str,\n):\n    data = []\n    min_samples = options.pop(\"min_samples\", 1000)\n\n    for memory in memories:\n        item = {\n            \"text\": memory,\n        }\n        data.append(item)\n\n    if len(data) < min_samples:\n        data = data * (min_samples // len(data) + 1)\n\n    with open(f\"{dataset_dir}/data.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(data, f, indent=2, ensure_ascii=False)\n\n    with open(f\"{dataset_dir}/dataset_info.json\", \"w\", encoding=\"utf-8\") as f:\n        r = {\"data\": {\"file_name\": \"data.json\", \"columns\": {\"prompt\": \"text\"}}}\n        f.write(json.dumps(r, indent=2, ensure_ascii=False))\n\n    args = dict(\n        stage=\"pt\",\n        do_train=True,\n        model_name_or_path=llama_model,\n        dataset=\"data\",\n        dataset_dir=dataset_dir,\n        cutoff_len=1024,\n        max_samples=1000000,\n        overwrite_cache=True,\n        preprocessing_num_workers=1,\n        template=\"llama3\",\n        finetuning_type=\"lora\",\n        lora_target=\"all\",\n        output_dir=f\"{loras_dir}/{name}\",\n        overwrite_output_dir=True,\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=4,\n        lr_scheduler_type=\"cosine\",\n        logging_steps=10,\n        warmup_ratio=0.1,\n        save_steps=1000,\n        plot_loss=True,\n        learning_rate=5e-5,\n        num_train_epochs=1000.0,\n        max_grad_norm=1.0,\n        quantization_bit=4,\n        loraplus_lr_ratio=16.0,\n        fp16=True,\n        ddp_timeout=180000000,\n    )\n    os.environ[\"WANDB_DISABLED\"] = \"true\"\n    from llamafactory.train import tuner\n\n    tuner.run_exp({**args, **options})\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/apps/byzer_storage/train_sft.py\nimport json\nimport os\nimport concurrent.futures\nfrom typing import List, Dict, Any\nimport byzerllm\nfrom loguru import logger\nfrom pydantic import BaseModel\nfrom byzerllm.apps.utils import TagExtractor, Tag\nfrom byzerllm.apps.byzer_storage.generate_sft_data import to_qa_pairs, read_alpaca_zh\n\n\ndef train_sft(\n    name: str,\n    memories: List[str],\n    options: Dict[str, Any],\n    dataset_dir: str,\n    loras_dir: str,\n    model_name: str,\n):\n    data = []\n    min_samples = options.pop(\"min_samples\", 1000)\n\n    if memories:\n        with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:\n            futures = []\n            llm = byzerllm.ByzerLLM().from_default_model(model_name)\n            for memory in memories:\n                futures.append(executor.submit(to_qa_pairs, memory, llm))\n\n            for future in concurrent.futures.as_completed(futures):\n                qa_pairs = future.result()\n                for qa_pair in qa_pairs:\n                    item = {\n                        \"instruction\": qa_pair.question,\n                        \"input\": \"\",\n                        \"output\": qa_pair.answer,\n                    }\n                    data.append(item)\n    else:\n        v = read_alpaca_zh()\n        data.extend(v)\n\n    if len(data) < min_samples:\n        data = data * (min_samples // len(data) + 1)\n\n    with open(f\"{dataset_dir}/data.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(data, f, indent=2, ensure_ascii=False)\n\n    with open(f\"{dataset_dir}/dataset_info.json\", \"w\", encoding=\"utf-8\") as f:\n        r = {\"data\": {\"file_name\": \"data.json\"}}\n        f.write(json.dumps(r, indent=2, ensure_ascii=False))\n\n    args = dict(\n        stage=\"sft\",\n        do_train=True,\n        model_name_or_path=model_name,\n        dataset=\"data\",\n        dataset_dir=dataset_dir,\n        cutoff_len=1024,\n        max_samples=1000000,\n        overwrite_cache=True,\n        preprocessing_num_workers=1,\n        template=\"llama3\",\n        finetuning_type=\"lora\",\n        lora_target=\"all\",\n        output_dir=f\"{loras_dir}/{name}\",\n        overwrite_output_dir=True,\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=4,\n        lr_scheduler_type=\"cosine\",\n        logging_steps=10,\n        warmup_ratio=0.1,\n        save_steps=1000,\n        plot_loss=True,\n        learning_rate=5e-5,\n        num_train_epochs=1000.0,\n        max_grad_norm=1.0,\n        quantization_bit=4,\n        loraplus_lr_ratio=16.0,\n        fp16=True,\n        ddp_timeout=180000000,\n    )\n    os.environ[\"WANDB_DISABLED\"] = \"true\"\n    from llamafactory.train import tuner\n\n    tuner.run_exp({**args, **options})\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/apps/byzer_storage/train_dpo.py\nimport json\nimport os\nfrom typing import List, Dict, Any\nimport byzerllm\n\n\n@byzerllm.prompt()\ndef generate_dpo_data(memory: str) -> str:\n    \"\"\"\n    根据以下内容，生成一个DPO（Direct Preference Optimization）训练数据样本。\n    每个样本应包含一个提示（prompt），一个首选回答（preferred_output）和一个拒绝回答（rejected_output）。\n    首选回答应该是高质量、有帮助的回答，而拒绝回答应该是质量较差或不太有帮助的回答。\n\n    内容：\n    {{memory}}\n\n    请生成DPO训练数据，并以JSON格式返回，格式如下：\n    {\n        \"prompt\": \"用户提问或指令\",\n        \"preferred_output\": \"高质量、有帮助的回答\",\n        \"rejected_output\": \"质量较差或不太有帮助的回答\"\n    }\n    \"\"\"\n\n\ndef train_dpo(\n    self,\n    name: str,\n    memories: List[str],\n    options: Dict[str, Any],\n    dataset_dir: str,\n    loras_dir: str,\n    llama_model: str,\n):\n    data = []\n    min_samples = options.pop(\"min_samples\", 1000)\n\n    llm = byzerllm.ByzerLLM().from_default_model(self.model_name)\n    for memory in memories:\n        dpo_sample = generate_dpo_data.with_llm(llm).run(memory)\n        data.append(json.loads(dpo_sample))\n\n    if len(data) < min_samples:\n        data = data * (min_samples // len(data) + 1)\n\n    with open(f\"{dataset_dir}/data.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(data, f, indent=2, ensure_ascii=False)\n\n    with open(f\"{dataset_dir}/dataset_info.json\", \"w\", encoding=\"utf-8\") as f:\n        r = {\"data\": {\"file_name\": \"data.json\"}}\n        f.write(json.dumps(r, indent=2, ensure_ascii=False))\n\n    args = dict(\n        stage=\"dpo\",\n        do_train=True,\n        model_name_or_path=llama_model,\n        dataset=\"data\",\n        dataset_dir=dataset_dir,\n        cutoff_len=1024,\n        max_samples=1000000,\n        overwrite_cache=True,\n        preprocessing_num_workers=1,\n        template=\"llama3\",\n        finetuning_type=\"lora\",\n        lora_target=\"all\",\n        output_dir=f\"{loras_dir}/{name}\",\n        overwrite_output_dir=True,\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=4,\n        lr_scheduler_type=\"cosine\",\n        logging_steps=10,\n        warmup_ratio=0.1,\n        save_steps=1000,\n        plot_loss=True,\n        learning_rate=5e-5,\n        num_train_epochs=1000.0,\n        max_grad_norm=1.0,\n        quantization_bit=4,\n        loraplus_lr_ratio=16.0,\n        fp16=True,\n        ddp_timeout=180000000,\n    )\n    os.environ[\"WANDB_DISABLED\"] = \"true\"\n    from llamafactory.train import tuner\n\n    tuner.run_exp({**args, **options})\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/apps/byzer_storage/generate_sft_data.py\nfrom typing import List, Dict, Any\nimport byzerllm\nfrom loguru import logger\nfrom pydantic import BaseModel\nfrom byzerllm.apps.utils import TagExtractor, Tag\nimport importlib\nimport json\n\n\ndef read_alpaca_zh():\n    with importlib.resources.path(\n        \"byzerllm.apps.byzer_storage\", \"alpaca_zh.json\"\n    ) as json_path:\n        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n            return json.load(f)\n\n\nclass QAPair(BaseModel):\n    question: str\n    answer: str\n\n\n@byzerllm.prompt()\ndef _convert_pretrain_text_to_instruction(text: str, num: int = 10) -> str:\n    \"\"\"\n    根据提供的信息，生成多个相关的问题，这些问题的回答，最终要能覆盖里面所有的信息,生成 {{num}} 个。\n\n    下面是一些信息：\n\n    {{text}}\n\n    现在请开始生成问题，请将每个问题使用<_question_></_question_>标签包裹，每个回答使用<_answer_></_answer_>标签包裹，最后\n    每个一组问题和回答使用<_group_></_group_>标签包裹。\n    \"\"\"\n\n\n@byzerllm.prompt()\ndef get_more(text: str, questions_and_answers: str, num: int = 10) -> str:\n    \"\"\"\n    下面是一些问答信息：\n\n    {{text}}\n\n    下面是已有的问题和回答：\n\n    {{questions_and_answers}}\n\n    下面是已经生成的问题和回答，请继续生成{{num}}个问题和回答，不要和前面的重复，请将每个问题使用<_question_></_question_>标签包裹，每个回答使用<_answer_></_answer_>标签包裹，最后\n    每个一组问题和回答使用<_group_></_group_>标签包裹。\n    \"\"\"\n\n\ndef to_qa_pairs(text: str, llm, num: int = 30) -> List[QAPair]:\n    logger.info(f\"Starting to generate QA pairs for text: {text[:100]}...\")\n\n    logger.info(\"Generating initial QA pairs...\")\n    v = _convert_pretrain_text_to_instruction.with_llm(llm).run(text)\n    logger.info(f\"Initial QA pairs generated. Length: {len(v)}\")\n\n    max_turns = int(num / 10)\n    if max_turns < 1:\n        max_turns = 1\n    logger.info(f\"Will generate additional QA pairs for {max_turns} turns\")\n\n    turn = 0\n    while turn < max_turns:\n        turn += 1\n        logger.info(f\"Generating additional QA pairs (turn {turn}/{max_turns})...\")\n        t = get_more.with_llm(llm).run(text, v)\n        v += t\n        logger.info(f\"Additional QA pairs generated. New total length: {len(v)}\")\n\n    logger.info(f\"Extracting QA pairs from generated text...{v}\")\n    root_tag = TagExtractor(v).extract()\n    qa_pairs = []\n    for i, item in enumerate(root_tag.content):\n        qas = item.content\n        if len(qas) == 2:\n            if (\n                qas[0].start_tag == \"<_question_>\"\n                and qas[0].end_tag == \"</_question_>\"\n                and qas[1].start_tag == \"<_answer_>\"\n                and qas[1].end_tag == \"</_answer_>\"\n            ):\n                qa_pairs.append(QAPair(question=qas[0].content, answer=qas[1].content))\n                logger.debug(\n                    f\"Extracted QA pair {i+1}: Q: {qas[0].content[:50]}... A: {qas[1].content[:50]}...\"\n                )\n            else:\n                logger.warning(\n                    f\"Skipping item {i+1} due to incorrect tags: {qas[0].start_tag}, {qas[0].end_tag}, {qas[1].start_tag}, {qas[1].end_tag}\"\n                )\n        else:\n            logger.warning(\n                f\"Skipping item {i+1} due to incorrect number of elements: {len(qas)}\"\n            )\n\n    logger.info(f\"Extracted {len(qa_pairs)} valid QA pairs.\")\n    return qa_pairs\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/apps/byzer_storage/memory_model_based.py\nimport asyncio\nfrom typing import List, Dict, Any\n\nimport ray.experimental\nfrom byzerllm.apps.byzer_storage.simple_api import ByzerStorage\nimport time\nimport json\nimport os\nimport concurrent.futures\nimport io\nimport sys\nfrom contextlib import redirect_stdout, redirect_stderr\nimport queue\nimport threading\nimport ray\nimport byzerllm\nfrom typing import List, Union, Dict\nfrom pydantic import BaseModel\nimport importlib\nfrom loguru import logger\n\nclass MemoryManager:\n    _queue = asyncio.Queue()\n    _is_processing = False\n\n    def __init__(\n        self,\n        storage: ByzerStorage,\n        base_dir: str,\n        model_name: str = \"deepseek_chat\",\n        remote: bool = True,\n    ):\n        self.storage = storage\n        home = os.path.expanduser(\"~\")\n        self.base_dir = base_dir or os.path.join(home, \".auto-coder\")\n        self.thread_pool = concurrent.futures.ThreadPoolExecutor(max_workers=5)\n        self.log_file = None\n        self.remote = remote\n        self.model_name = model_name\n\n    @classmethod\n    async def add_to_queue(cls, name: str, memories: List[str]):\n        await cls._queue.put((name, memories))\n        if not cls._is_processing:\n            asyncio.create_task(cls.process_queue())\n\n    @classmethod\n    async def process_queue(cls):\n        cls._is_processing = True\n        while not cls._queue.empty():\n            name, memories = await cls._queue.get()\n            instance = cls.get_instance()\n            await instance.memorize(name, memories)\n            cls._queue.task_done()\n        cls._is_processing = False\n\n    async def memorize(\n        self, name: str, memories: List[str], options: Dict[str, Any] = {}\n    ):\n        try:\n            loop = asyncio.get_running_loop()\n            await loop.run_in_executor(\n                self.thread_pool, self._memorize_with_logs, name, memories, options\n            )\n        finally:\n            if self.remote:\n                ray.actor.exit_actor()\n\n        print(f\"Memorization for {name} completed.\")\n\n    def _memorize_with_logs(\n        self, name: str, memories: List[str], options: Dict[str, Any] = {}\n    ):\n\n        if self.remote:\n            self._memorize(name, memories, options=options)\n            return\n\n        logs_dir = os.path.join(self.base_dir, \"storage\", \"logs\", \"memorize\")\n        os.makedirs(logs_dir, exist_ok=True)\n        log_file = os.path.join(logs_dir, f\"{name}.log\")\n\n        log_queue = queue.Queue()\n        stop_event = threading.Event()\n\n        # Start the log writer thread\n        log_writer_thread = threading.Thread(\n            target=self._log_writer, args=(log_queue, log_file, stop_event)\n        )\n        self.log_file = log_file\n        log_writer_thread.start()\n\n        class QueueStream:\n            def __init__(self, queue):\n                self.queue = queue\n\n            def write(self, msg):\n                self.queue.put(msg)\n\n            def flush(self):\n                pass\n\n        queue_stream = QueueStream(log_queue)\n\n        with redirect_stdout(queue_stream), redirect_stderr(queue_stream):\n            self._memorize(name, memories)\n\n        # Signal the log writer to stop and wait for it to finish\n        stop_event.set()\n        log_writer_thread.join()\n\n    def _log_writer(\n        self, log_queue: queue.Queue, log_file: str, stop_event: threading.Event\n    ):\n        with open(log_file, \"w\") as f:\n            while not stop_event.is_set() or not log_queue.empty():\n                try:\n                    msg = log_queue.get(timeout=0.1)\n                    f.write(msg)\n                    f.flush()\n                except queue.Empty:\n                    continue    \n\n    def _memorize(\n        self,\n        name: str,\n        memories: List[Union[str, Dict[str, Any]]],\n        options: Dict[str, Any] = {},\n    ):\n        data = []\n        stage = options.get(\"stage\", \"pt\")\n        min_samples = options.pop(\"min_samples\", 1000)\n        base_model_dir = os.path.join(self.base_dir, \"storage\", \"models\")\n        llama_model = os.path.join(\n            base_model_dir, \"meta-llama\", \"Meta-Llama-3-8B-Instruct-GPTQ\"\n        )\n\n        loras_dir = os.path.join(self.base_dir, \"storage\", \"loras\")\n        dataset_dir = os.path.join(self.base_dir, \"storage\", \"datasets\", name)\n\n        os.makedirs(loras_dir, exist_ok=True)\n        os.makedirs(dataset_dir, exist_ok=True)\n\n        if stage == \"pt\":\n            from .train_pt import train_pt\n            train_pt(self, name, memories, options, dataset_dir, loras_dir, llama_model)\n        elif stage == \"sft\":\n            from .train_sft import train_sft\n            train_sft(self, name, memories, options, dataset_dir, loras_dir, llama_model)\n        elif stage == \"dpo\":\n            from .train_dpo import train_dpo\n            train_dpo(self, name, memories, options, dataset_dir, loras_dir, llama_model)\n        else:\n            raise ValueError(f\"Unsupported stage: {stage}\")\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/apps/byzer_storage/train_pt.py\nimport json\nimport os\nfrom typing import List, Dict, Any\nimport byzerllm\n\ndef train_pt(\n    self,\n    name: str,\n    memories: List[str],\n    options: Dict[str, Any],\n    dataset_dir: str,\n    loras_dir: str,\n    llama_model: str,\n):\n    data = []\n    min_samples = options.pop(\"min_samples\", 1000)\n\n    for memory in memories:\n        item = {\n            \"text\": memory,\n        }\n        data.append(item)\n\n    if len(data) < min_samples:\n        data = data * (min_samples // len(data) + 1)\n\n    with open(f\"{dataset_dir}/data.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(data, f, indent=2, ensure_ascii=False)\n\n    with open(f\"{dataset_dir}/dataset_info.json\", \"w\", encoding=\"utf-8\") as f:\n        r = {\"data\": {\"file_name\": \"data.json\", \"columns\": {\"prompt\": \"text\"}}}\n        f.write(json.dumps(r, indent=2, ensure_ascii=False))\n\n    args = dict(\n        stage=\"pt\",\n        do_train=True,\n        model_name_or_path=llama_model,\n        dataset=\"data\",\n        dataset_dir=dataset_dir,\n        cutoff_len=1024,\n        max_samples=1000000,\n        overwrite_cache=True,\n        preprocessing_num_workers=1,\n        template=\"llama3\",\n        finetuning_type=\"lora\",\n        lora_target=\"all\",\n        output_dir=f\"{loras_dir}/{name}\",\n        overwrite_output_dir=True,\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=4,\n        lr_scheduler_type=\"cosine\",\n        logging_steps=10,\n        warmup_ratio=0.1,\n        save_steps=1000,\n        plot_loss=True,\n        learning_rate=5e-5,\n        num_train_epochs=1000.0,\n        max_grad_norm=1.0,\n        quantization_bit=4,\n        loraplus_lr_ratio=16.0,\n        fp16=True,\n        ddp_timeout=180000000,\n    )\n    os.environ[\"WANDB_DISABLED\"] = \"true\"\n    from llamafactory.train import tuner\n\n    tuner.run_exp({**args, **options})\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/apps/byzer_storage/train_sft.py\nimport json\nimport os\nimport concurrent.futures\nfrom typing import List, Dict, Any\nimport byzerllm\nfrom loguru import logger\nfrom pydantic import BaseModel\nfrom byzerllm.apps.utils import TagExtractor, Tag\nfrom byzerllm.apps.byzer_storage.generate_sft_data import to_qa_pairs, read_alpaca_zh\n\n\ndef train_sft(\n    name: str,\n    memories: List[str],\n    options: Dict[str, Any],\n    dataset_dir: str,\n    loras_dir: str,\n    model_name: str,\n):\n    data = []\n    min_samples = options.pop(\"min_samples\", 1000)\n\n    if memories:\n        with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:\n            futures = []\n            llm = byzerllm.ByzerLLM().from_default_model(model_name)\n            for memory in memories:\n                futures.append(executor.submit(to_qa_pairs, memory, llm))\n\n            for future in concurrent.futures.as_completed(futures):\n                qa_pairs = future.result()\n                for qa_pair in qa_pairs:\n                    item = {\n                        \"instruction\": qa_pair.question,\n                        \"input\": \"\",\n                        \"output\": qa_pair.answer,\n                    }\n                    data.append(item)\n    else:\n        v = read_alpaca_zh()\n        data.extend(v)\n\n    if len(data) < min_samples:\n        data = data * (min_samples // len(data) + 1)\n\n    with open(f\"{dataset_dir}/data.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(data, f, indent=2, ensure_ascii=False)\n\n    with open(f\"{dataset_dir}/dataset_info.json\", \"w\", encoding=\"utf-8\") as f:\n        r = {\"data\": {\"file_name\": \"data.json\"}}\n        f.write(json.dumps(r, indent=2, ensure_ascii=False))\n\n    args = dict(\n        stage=\"sft\",\n        do_train=True,\n        model_name_or_path=model_name,\n        dataset=\"data\",\n        dataset_dir=dataset_dir,\n        cutoff_len=1024,\n        max_samples=1000000,\n        overwrite_cache=True,\n        preprocessing_num_workers=1,\n        template=\"llama3\",\n        finetuning_type=\"lora\",\n        lora_target=\"all\",\n        output_dir=f\"{loras_dir}/{name}\",\n        overwrite_output_dir=True,\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=4,\n        lr_scheduler_type=\"cosine\",\n        logging_steps=10,\n        warmup_ratio=0.1,\n        save_steps=1000,\n        plot_loss=True,\n        learning_rate=5e-5,\n        num_train_epochs=1000.0,\n        max_grad_norm=1.0,\n        quantization_bit=4,\n        loraplus_lr_ratio=16.0,\n        fp16=True,\n        ddp_timeout=180000000,\n    )\n    os.environ[\"WANDB_DISABLED\"] = \"true\"\n    from llamafactory.train import tuner\n\n    tuner.run_exp({**args, **options})\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/apps/byzer_storage/train_dpo.py\nimport json\nimport os\nfrom typing import List, Dict, Any\nimport byzerllm\n\n\n@byzerllm.prompt()\ndef generate_dpo_data(memory: str) -> str:\n    \"\"\"\n    根据以下内容，生成一个DPO（Direct Preference Optimization）训练数据样本。\n    每个样本应包含一个提示（prompt），一个首选回答（preferred_output）和一个拒绝回答（rejected_output）。\n    首选回答应该是高质量、有帮助的回答，而拒绝回答应该是质量较差或不太有帮助的回答。\n\n    内容：\n    {{memory}}\n\n    请生成DPO训练数据，并以JSON格式返回，格式如下：\n    {\n        \"prompt\": \"用户提问或指令\",\n        \"preferred_output\": \"高质量、有帮助的回答\",\n        \"rejected_output\": \"质量较差或不太有帮助的回答\"\n    }\n    \"\"\"\n\n\ndef train_dpo(\n    self,\n    name: str,\n    memories: List[str],\n    options: Dict[str, Any],\n    dataset_dir: str,\n    loras_dir: str,\n    llama_model: str,\n):\n    data = []\n    min_samples = options.pop(\"min_samples\", 1000)\n\n    llm = byzerllm.ByzerLLM().from_default_model(self.model_name)\n    for memory in memories:\n        dpo_sample = generate_dpo_data.with_llm(llm).run(memory)\n        data.append(json.loads(dpo_sample))\n\n    if len(data) < min_samples:\n        data = data * (min_samples // len(data) + 1)\n\n    with open(f\"{dataset_dir}/data.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(data, f, indent=2, ensure_ascii=False)\n\n    with open(f\"{dataset_dir}/dataset_info.json\", \"w\", encoding=\"utf-8\") as f:\n        r = {\"data\": {\"file_name\": \"data.json\"}}\n        f.write(json.dumps(r, indent=2, ensure_ascii=False))\n\n    args = dict(\n        stage=\"dpo\",\n        do_train=True,\n        model_name_or_path=llama_model,\n        dataset=\"data\",\n        dataset_dir=dataset_dir,\n        cutoff_len=1024,\n        max_samples=1000000,\n        overwrite_cache=True,\n        preprocessing_num_workers=1,\n        template=\"llama3\",\n        finetuning_type=\"lora\",\n        lora_target=\"all\",\n        output_dir=f\"{loras_dir}/{name}\",\n        overwrite_output_dir=True,\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=4,\n        lr_scheduler_type=\"cosine\",\n        logging_steps=10,\n        warmup_ratio=0.1,\n        save_steps=1000,\n        plot_loss=True,\n        learning_rate=5e-5,\n        num_train_epochs=1000.0,\n        max_grad_norm=1.0,\n        quantization_bit=4,\n        loraplus_lr_ratio=16.0,\n        fp16=True,\n        ddp_timeout=180000000,\n    )\n    os.environ[\"WANDB_DISABLED\"] = \"true\"\n    from llamafactory.train import tuner\n\n    tuner.run_exp({**args, **options})\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/apps/byzer_storage/generate_sft_data.py\nfrom typing import List, Dict, Any\nimport byzerllm\nfrom loguru import logger\nfrom pydantic import BaseModel\nfrom byzerllm.apps.utils import TagExtractor, Tag\nimport importlib\nimport json\n\n\ndef read_alpaca_zh():\n    with importlib.resources.path(\n        \"byzerllm.apps.byzer_storage\", \"alpaca_zh.json\"\n    ) as json_path:\n        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n            return json.load(f)\n\n\nclass QAPair(BaseModel):\n    question: str\n    answer: str\n\n\n@byzerllm.prompt()\ndef _convert_pretrain_text_to_instruction(text: str, num: int = 10) -> str:\n    \"\"\"\n    根据提供的信息，生成多个相关的问题，这些问题的回答，最终要能覆盖里面所有的信息,生成 {{num}} 个。\n\n    下面是一些信息：\n\n    {{text}}\n\n    现在请开始生成问题，请将每个问题使用<_question_></_question_>标签包裹，每个回答使用<_answer_></_answer_>标签包裹，最后\n    每个一组问题和回答使用<_group_></_group_>标签包裹。\n    \"\"\"\n\n\n@byzerllm.prompt()\ndef get_more(text: str, questions_and_answers: str, num: int = 10) -> str:\n    \"\"\"\n    下面是一些问答信息：\n\n    {{text}}\n\n    下面是已有的问题和回答：\n\n    {{questions_and_answers}}\n\n    下面是已经生成的问题和回答，请继续生成{{num}}个问题和回答，不要和前面的重复，请将每个问题使用<_question_></_question_>标签包裹，每个回答使用<_answer_></_answer_>标签包裹，最后\n    每个一组问题和回答使用<_group_></_group_>标签包裹。\n    \"\"\"\n\n\ndef to_qa_pairs(text: str, llm, num: int = 30) -> List[QAPair]:\n    logger.info(f\"Starting to generate QA pairs for text: {text[:100]}...\")\n\n    logger.info(\"Generating initial QA pairs...\")\n    v = _convert_pretrain_text_to_instruction.with_llm(llm).run(text)\n    logger.info(f\"Initial QA pairs generated. Length: {len(v)}\")\n\n    max_turns = int(num / 10)\n    if max_turns < 1:\n        max_turns = 1\n    logger.info(f\"Will generate additional QA pairs for {max_turns} turns\")\n\n    turn = 0\n    while turn < max_turns:\n        turn += 1\n        logger.info(f\"Generating additional QA pairs (turn {turn}/{max_turns})...\")\n        t = get_more.with_llm(llm).run(text, v)\n        v += t\n        logger.info(f\"Additional QA pairs generated. New total length: {len(v)}\")\n\n    logger.info(f\"Extracting QA pairs from generated text...{v}\")\n    root_tag = TagExtractor(v).extract()\n    qa_pairs = []\n    for i, item in enumerate(root_tag.content):\n        qas = item.content\n        if len(qas) == 2:\n            if (\n                qas[0].start_tag == \"<_question_>\"\n                and qas[0].end_tag == \"</_question_>\"\n                and qas[1].start_tag == \"<_answer_>\"\n                and qas[1].end_tag == \"</_answer_>\"\n            ):\n                qa_pairs.append(QAPair(question=qas[0].content, answer=qas[1].content))\n                logger.debug(\n                    f\"Extracted QA pair {i+1}: Q: {qas[0].content[:50]}... A: {qas[1].content[:50]}...\"\n                )\n            else:\n                logger.warning(\n                    f\"Skipping item {i+1} due to incorrect tags: {qas[0].start_tag}, {qas[0].end_tag}, {qas[1].start_tag}, {qas[1].end_tag}\"\n                )\n        else:\n            logger.warning(\n                f\"Skipping item {i+1} due to incorrect number of elements: {len(qas)}\"\n            )\n\n    logger.info(f\"Extracted {len(qa_pairs)} valid QA pairs.\")\n    return qa_pairs\n\n\n     \n\n下面是用户的需求：\n\nto_qa_pairs 方法要支持缓存,text 参数做md5,该md5值作为缓存key, 产生的数据保存成json格式,放到当前用户 .byzerllm/cache 目录下\n"}, {"role": "assistant", "content": "To implement caching for the `to_qa_pairs` method, we need to make the following changes:\n\n1. Import necessary libraries for MD5 hashing and file operations.\n2. Create a function to generate the cache key using MD5.\n3. Modify the `to_qa_pairs` function to check for cached results before processing.\n4. Add a function to save and load cached results.\n\nHere are the *SEARCH/REPLACE* blocks to implement these changes:\n\n```python\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/apps/byzer_storage/generate_sft_data.py\n<<<<<<< SEARCH\nfrom typing import List, Dict, Any\nimport byzerllm\nfrom loguru import logger\nfrom pydantic import BaseModel\nfrom byzerllm.apps.utils import TagExtractor, Tag\nimport importlib\nimport json\n=======\nfrom typing import List, Dict, Any\nimport byzerllm\nfrom loguru import logger\nfrom pydantic import BaseModel\nfrom byzerllm.apps.utils import TagExtractor, Tag\nimport importlib\nimport json\nimport hashlib\nimport os\nfrom pathlib import Path\n>>>>>>> REPLACE\n```\n\n```python\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/apps/byzer_storage/generate_sft_data.py\n<<<<<<< SEARCH\ndef to_qa_pairs(text: str, llm, num: int = 30) -> List[QAPair]:\n    logger.info(f\"Starting to generate QA pairs for text: {text[:100]}...\")\n\n    logger.info(\"Generating initial QA pairs...\")\n    v = _convert_pretrain_text_to_instruction.with_llm(llm).run(text)\n    logger.info(f\"Initial QA pairs generated. Length: {len(v)}\")\n\n    max_turns = int(num / 10)\n    if max_turns < 1:\n        max_turns = 1\n    logger.info(f\"Will generate additional QA pairs for {max_turns} turns\")\n\n    turn = 0\n    while turn < max_turns:\n        turn += 1\n        logger.info(f\"Generating additional QA pairs (turn {turn}/{max_turns})...\")\n        t = get_more.with_llm(llm).run(text, v)\n        v += t\n        logger.info(f\"Additional QA pairs generated. New total length: {len(v)}\")\n\n    logger.info(f\"Extracting QA pairs from generated text...{v}\")\n    root_tag = TagExtractor(v).extract()\n    qa_pairs = []\n    for i, item in enumerate(root_tag.content):\n        qas = item.content\n        if len(qas) == 2:\n            if (\n                qas[0].start_tag == \"<_question_>\"\n                and qas[0].end_tag == \"</_question_>\"\n                and qas[1].start_tag == \"<_answer_>\"\n                and qas[1].end_tag == \"</_answer_>\"\n            ):\n                qa_pairs.append(QAPair(question=qas[0].content, answer=qas[1].content))\n                logger.debug(\n                    f\"Extracted QA pair {i+1}: Q: {qas[0].content[:50]}... A: {qas[1].content[:50]}...\"\n                )\n            else:\n                logger.warning(\n                    f\"Skipping item {i+1} due to incorrect tags: {qas[0].start_tag}, {qas[0].end_tag}, {qas[1].start_tag}, {qas[1].end_tag}\"\n                )\n        else:\n            logger.warning(\n                f\"Skipping item {i+1} due to incorrect number of elements: {len(qas)}\"\n            )\n\n    logger.info(f\"Extracted {len(qa_pairs)} valid QA pairs.\")\n    return qa_pairs\n=======\ndef generate_cache_key(text: str) -> str:\n    return hashlib.md5(text.encode()).hexdigest()\n\ndef get_cache_path() -> Path:\n    home = Path.home()\n    cache_dir = home / \".byzerllm\" / \"cache\"\n    cache_dir.mkdir(parents=True, exist_ok=True)\n    return cache_dir\n\ndef save_to_cache(key: str, data: List[QAPair]):\n    cache_path = get_cache_path() / f\"{key}.json\"\n    with cache_path.open(\"w\", encoding=\"utf-8\") as f:\n        json.dump([qa.dict() for qa in data], f, ensure_ascii=False, indent=2)\n\ndef load_from_cache(key: str) -> List[QAPair]:\n    cache_path = get_cache_path() / f\"{key}.json\"\n    if cache_path.exists():\n        with cache_path.open(\"r\", encoding=\"utf-8\") as f:\n            data = json.load(f)\n        return [QAPair(**item) for item in data]\n    return None\n\ndef to_qa_pairs(text: str, llm, num: int = 30) -> List[QAPair]:\n    cache_key = generate_cache_key(text)\n    cached_result = load_from_cache(cache_key)\n    \n    if cached_result:\n        logger.info(f\"Using cached result for text: {text[:100]}...\")\n        return cached_result\n\n    logger.info(f\"Starting to generate QA pairs for text: {text[:100]}...\")\n\n    logger.info(\"Generating initial QA pairs...\")\n    v = _convert_pretrain_text_to_instruction.with_llm(llm).run(text)\n    logger.info(f\"Initial QA pairs generated. Length: {len(v)}\")\n\n    max_turns = int(num / 10)\n    if max_turns < 1:\n        max_turns = 1\n    logger.info(f\"Will generate additional QA pairs for {max_turns} turns\")\n\n    turn = 0\n    while turn < max_turns:\n        turn += 1\n        logger.info(f\"Generating additional QA pairs (turn {turn}/{max_turns})...\")\n        t = get_more.with_llm(llm).run(text, v)\n        v += t\n        logger.info(f\"Additional QA pairs generated. New total length: {len(v)}\")\n\n    logger.info(f\"Extracting QA pairs from generated text...{v}\")\n    root_tag = TagExtractor(v).extract()\n    qa_pairs = []\n    for i, item in enumerate(root_tag.content):\n        qas = item.content\n        if len(qas) == 2:\n            if (\n                qas[0].start_tag == \"<_question_>\"\n                and qas[0].end_tag == \"</_question_>\"\n                and qas[1].start_tag == \"<_answer_>\"\n                and qas[1].end_tag == \"</_answer_>\"\n            ):\n                qa_pairs.append(QAPair(question=qas[0].content, answer=qas[1].content))\n                logger.debug(\n                    f\"Extracted QA pair {i+1}: Q: {qas[0].content[:50]}... A: {qas[1].content[:50]}...\"\n                )\n            else:\n                logger.warning(\n                    f\"Skipping item {i+1} due to incorrect tags: {qas[0].start_tag}, {qas[0].end_tag}, {qas[1].start_tag}, {qas[1].end_tag}\"\n                )\n        else:\n            logger.warning(\n                f\"Skipping item {i+1} due to incorrect number of elements: {len(qas)}\"\n            )\n\n    logger.info(f\"Extracted {len(qa_pairs)} valid QA pairs.\")\n    \n    # Save the result to cache\n    save_to_cache(cache_key, qa_pairs)\n    \n    return qa_pairs\n>>>>>>> REPLACE\n```\n\nThese changes implement the following:\n\n1. We've added imports for `hashlib`, `os`, and `Path` to handle MD5 hashing and file operations.\n\n2. We've created helper functions:\n   - `generate_cache_key`: Creates an MD5 hash of the input text.\n   - `get_cache_path`: Returns the path to the cache directory.\n   - `save_to_cache`: Saves the generated QA pairs to a JSON file in the cache directory.\n   - `load_from_cache`: Attempts to load cached QA pairs from a JSON file.\n\n3. In the `to_qa_pairs` function:\n   - We first check if there's a cached result for the given text.\n   - If a cached result exists, we return it immediately.\n   - If no cached result exists, we proceed with generating the QA pairs as before.\n   - After generating the QA pairs, we save them to the cache before returning.\n\nThese changes will implement caching for the `to_qa_pairs` function, storing the results in JSON format in the user's `.byzerllm/cache` directory, using an MD5 hash of the input text as the cache key."}], "model": "human", "yaml_file": "actions/009_chat_action.yml"}
{"instruction": "_memorize 方法添加日志,可以详细追踪流程", "conversations": [{"role": "user", "content": "\n如果你需要生成代码，对于每个需要更改的文件,你需要按 *SEARCH/REPLACE block* 的格式进行生成。\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The opening fence and code language, eg: ```python\n2. The file path alone on a line, starting with \"##File:\" and verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n*SEARCH/REPLACE* blocks will replace *all* matching occurrences.\nInclude enough lines to make the SEARCH blocks unique.\n\nInclude *ALL* the code being searched and replaced!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n下面我们来看一个例子：\n\n当前项目目录结构：\n1. 项目根目录： /tmp/projects/mathweb\n2. 项目子目录/文件列表(类似tree 命令输出)\nflask/\n    app.py\n    templates/\n        index.html\n    static/\n        style.css\n\n用户需求： Change get_factorial() to use math.factorial\n\n回答： To make this change we need to modify `/tmp/projects/mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n用户需求： Refactor hello() into its own file.\n\n回答：To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/hello.py\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/main.py\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n现在让我们开始一个新的任务:\n\n当前项目目录结构：\n1. 项目根目录： /Users/allwefantasy/projects/byzer-llm\n2. 项目子目录/文件列表(类似tree 命令输出)：\nsetup.py\ntests/\n    test_emb_rerank.py\n    test_deepseek_code.py\n    test_prompt.py\n    test_model_meta.py\n    __init__.py\n    test_transformers_deploy.py\n    test_data_analysis.py\n    test_yi.py\n    test_deploy.py\n    functions_suite.py\n    test_template.py\n    auto/\n        test_backend_llama_cpp.py\n        __init__.py\nsetup-machine/\n    convert_safetensor.py\nnotebooks/\n    test.py\nsrc/\n    byzerllm/\n        lang.py\n        store.py\n        version.py\n        log.py\n        __init__.py\n        types.py\n        easy_byzerllm_command.py\n        byzerllm_command.py\n        command_args.py\n        bark/\n            generation.py\n            __init__.py\n            model.py\n            api.py\n            model_fine.py\n            bark_voice.py\n        moss/\n            finetune_moss.py\n            __init__.py\n            moss_inference.py\n            models/\n                configuration_moss.py\n                quantization.py\n                custom_autotune.py\n                __init__.py\n                modeling_moss.py\n                tokenization_moss.py\n        llama2/\n            __init__.py\n        records/\n            __init__.py\n        visualglm/\n            __init__.py\n        auto/\n            __init__.py\n            backend_llama_cpp.py\n            backend_vllm.py\n            backend_ds.py\n        alpha_moss/\n            __init__.py\n        processor/\n            __init__.py\n        qwen/\n            __init__.py\n        utils/\n            json_repaire.py\n            object_store_ref_util.py\n            __init__.py\n            types.py\n            tokenizer.py\n            openai_utils.py\n            ray_utils.py\n            text_generator.py\n            emb.py\n            connect_ray.py\n            testing.py\n            langutil.py\n            nontext.py\n            fulltune/\n                deepspeed_trainner.py\n                __init__.py\n                launch.py\n                trainner.py\n                base_model/\n                    __init__.py\n                    configuration_baichuan.py\n                    modeling_baichuan.py\n                pretrain/\n                    __init__.py\n                    convert_to_transformers.py\n            metrics/\n                __init__.py\n            config/\n                __init__.py\n            retrieval/\n                rest.py\n                __init__.py\n                udf.py\n            sft/\n                argument.py\n                collator.py\n                qlora.py\n                __init__.py\n                model.py\n                dataset.py\n                loss.py\n                merge_lora.py\n                trainer.py\n            client/\n                message_utils.py\n                math_utils.py\n                byzerllm_client.py\n                parallel_utils.py\n                __init__.py\n                types.py\n                code_utils.py\n                img_utils.py\n                entrypoints/\n                    __init__.py\n                    openai/\n                        protocol.py\n                        __init__.py\n                        serve.py\n                        serving_chat.py\n                        api_server.py\n                        serving_engine.py\n                        serving_completion.py\n                        tool.py\n        chatglm2/\n            __init__.py\n        baichuan/\n            __init__.py\n        m3e/\n            __init__.py\n        stable_diffusion/\n            config.py\n            __init__.py\n            logger.py\n            shared.py\n            model.py\n            utils.py\n            images.py\n            diffusion/\n                __init__.py\n                embeddings.py\n                utils.py\n                networks/\n                    lyco.py\n                    __init__.py\n                upscalers/\n                    __init__.py\n                    multidiffusion.py\n                    samplers.py\n                piplines/\n                    __init__.py\n                    lpw.py\n                    diffusers.py\n            lib/\n                __init__.py\n                diffusers/\n                    __init__.py\n                    scheduler.py\n            api/\n                __init__.py\n                plugin.py\n                models/\n                    diffusion.py\n                    __init__.py\n                diffusion/\n                    __init__.py\n                    pipelines/\n                        __init__.py\n                        diffusers.py\n                events/\n                    generation.py\n                    __init__.py\n        bge/\n            __init__.py\n        bge_rerank/\n            __init__.py\n        apps/\n            client.py\n            qa_strategy.py\n            __init__.py\n            builder.py\n            vector_db.py\n            utils.py\n            qa.py\n            byzer_storage/\n                memory_model_based.py\n                command.py\n                env.py\n                train_pt.py\n                train_sft.py\n                __init__.py\n                train_dpo.py\n                simple_api.py\n                generate_sft_data.py\n            byzer_sql/\n                __init__.py\n            llama_index/\n                byzerai_vectordb.py\n                byzerai.py\n                byzerai_embedding.py\n                __init__.py\n                collection_manager.py\n                byzerai_index_store.py\n                byzerai_docstore.py\n                simple_retrieval.py\n                byzerai_kvstore.py\n            agent/\n                conversable_agent.py\n                registry.py\n                groupchat.py\n                user_proxy_agent.py\n                __init__.py\n                agent.py\n                extensions/\n                    simple_retrieval_client.py\n                    planner.py\n                    python_codesandbox_agent.py\n                    preview_file_agent.py\n                    llama_index_retrieval_agent.py\n                    visualization_agent.py\n                    assistant_agent.py\n                    load_data_agent.py\n                    retrieval_agent.py\n                    output_agent.py\n                    __init__.py\n                    byzer_engine_agent.py\n                    subquestion_agent.py\n                    sql_reviewer_agent.py\n                    spark_sql_agent.py\n                    rhetorical_agent.py\n                    data_analysis.py\n                    data_analysis_pipeline_agent.py\n                    common_agent.py\n                    query_rewrite/\n                        time.py\n                        rhetorical.py\n                        __init__.py\n                        condition.py\n                        context.py\n                store/\n                    memory_store.py\n                    __init__.py\n                    stores.py\n        saas/\n            __init__.py\n            chatglm/\n                __init__.py\n            sparkdesk/\n                __init__.py\n            gemini/\n                __init__.py\n            azure_openai/\n                __init__.py\n            azure/\n                __init__.py\n            qianfan/\n                __init__.py\n            claude/\n                __init__.py\n            qianwen/\n                __init__.py\n            volcengine/\n                __init__.py\n            minimax/\n                __init__.py\n            baichuan/\n                __init__.py\n            qianwen_vl/\n                __init__.py\n            official_openai/\n                __init__.py\n            zhipu/\n                __init__.py\n            aws_bedrock/\n                __init__.py\n            openai/\n                __init__.py\n        falcon/\n            __init__.py\n        chatglm6b/\n            trainer_seq2seq.py\n            arguments.py\n            __init__.py\n            finetune.py\n            tunning/\n                __init__.py\n                infer.py\n                finetune.py\n                utils/\n                    config.py\n                    __init__.py\n                    other.py\n                    common.py\n                    seq2seq.py\n        starcode/\n            __init__.py\n        qwen_vl_chat/\n            __init__.py\n        zephyr/\n            __init__.py\n        dolly/\n            generate.py\n            __init__.py\n            consts.py\n            dolly_inference.py\n            trainer.py\n        llama/\n            __init__.py\n        whisper/\n            whisper_inference.py\n            __init__.py\n下面是一些文件路径以及每个文件对应的源码：\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/apps/byzer_storage/memory_model_based.py\nimport asyncio\nfrom typing import List, Dict, Any\n\nimport ray.experimental\nfrom byzerllm.apps.byzer_storage.simple_api import ByzerStorage\nimport time\nimport json\nimport os\nimport concurrent.futures\nimport io\nimport sys\nfrom contextlib import redirect_stdout, redirect_stderr\nimport queue\nimport threading\nimport ray\nimport byzerllm\nfrom typing import List, Union, Dict\nfrom pydantic import BaseModel\nimport importlib\nfrom loguru import logger\n\n\nclass MemoryManager:\n    _queue = asyncio.Queue()\n    _is_processing = False\n\n    def __init__(\n        self,\n        storage: ByzerStorage,\n        base_dir: str,\n        model_name: str = \"deepseek_chat\",\n        remote: bool = True,\n    ):\n        self.storage = storage\n        home = os.path.expanduser(\"~\")\n        self.base_dir = base_dir or os.path.join(home, \".auto-coder\")\n        self.thread_pool = concurrent.futures.ThreadPoolExecutor(max_workers=5)\n        self.log_file = None\n        self.remote = remote\n        self.model_name = model_name\n\n    @classmethod\n    async def add_to_queue(cls, name: str, memories: List[str]):\n        await cls._queue.put((name, memories))\n        if not cls._is_processing:\n            asyncio.create_task(cls.process_queue())\n\n    @classmethod\n    async def process_queue(cls):\n        cls._is_processing = True\n        while not cls._queue.empty():\n            name, memories = await cls._queue.get()\n            instance = cls.get_instance()\n            await instance.memorize(name, memories)\n            cls._queue.task_done()\n        cls._is_processing = False\n\n    async def memorize(\n        self, name: str, memories: List[str], options: Dict[str, Any] = {}\n    ):\n        try:\n            loop = asyncio.get_running_loop()\n            await loop.run_in_executor(\n                self.thread_pool, self._memorize_with_logs, name, memories, options\n            )\n        finally:\n            if self.remote:\n                ray.actor.exit_actor()\n\n        print(f\"Memorization for {name} completed.\")\n\n    def _memorize_with_logs(\n        self, name: str, memories: List[str], options: Dict[str, Any] = {}\n    ):\n\n        if self.remote:\n            self._memorize(name, memories, options=options)\n            return\n\n        logs_dir = os.path.join(self.base_dir, \"storage\", \"logs\", \"memorize\")\n        os.makedirs(logs_dir, exist_ok=True)\n        log_file = os.path.join(logs_dir, f\"{name}.log\")\n\n        log_queue = queue.Queue()\n        stop_event = threading.Event()\n\n        # Start the log writer thread\n        log_writer_thread = threading.Thread(\n            target=self._log_writer, args=(log_queue, log_file, stop_event)\n        )\n        self.log_file = log_file\n        log_writer_thread.start()\n\n        class QueueStream:\n            def __init__(self, queue):\n                self.queue = queue\n\n            def write(self, msg):\n                self.queue.put(msg)\n\n            def flush(self):\n                pass\n\n        queue_stream = QueueStream(log_queue)\n\n        with redirect_stdout(queue_stream), redirect_stderr(queue_stream):\n            self._memorize(name, memories)\n\n        # Signal the log writer to stop and wait for it to finish\n        stop_event.set()\n        log_writer_thread.join()\n\n    def _log_writer(\n        self, log_queue: queue.Queue, log_file: str, stop_event: threading.Event\n    ):\n        with open(log_file, \"w\") as f:\n            while not stop_event.is_set() or not log_queue.empty():\n                try:\n                    msg = log_queue.get(timeout=0.1)\n                    f.write(msg)\n                    f.flush()\n                except queue.Empty:\n                    continue\n\n    def _memorize(\n        self,\n        name: str,\n        memories: List[Union[str, Dict[str, Any]]],\n        options: Dict[str, Any] = {},\n    ):\n\n        stage = options.get(\"stage\", \"pt\")\n        base_model_dir = os.path.join(self.base_dir, \"storage\", \"models\")\n        llama_model = os.path.join(\n            base_model_dir, \"meta-llama\", \"Meta-Llama-3-8B-Instruct-GPTQ\"\n        )\n\n        loras_dir = os.path.join(self.base_dir, \"storage\", \"loras\")\n        dataset_dir = os.path.join(self.base_dir, \"storage\", \"datasets\", name)\n\n        os.makedirs(loras_dir, exist_ok=True)\n        os.makedirs(dataset_dir, exist_ok=True)\n\n        if stage == \"pt\":\n            from byzerllm.apps.byzer_storage.train_pt import train_pt\n\n            train_pt(\n                name,\n                memories,\n                options,\n                dataset_dir,\n                loras_dir,\n                model_name=llama_model,\n                data_model_name=self.model_name,\n            )\n        elif stage == \"sft\":\n            from byzerllm.apps.byzer_storage.train_sft import train_sft\n\n            train_sft(\n                name,\n                memories,\n                options,\n                dataset_dir,\n                loras_dir,\n                model_name=llama_model,\n                data_model_name=self.model_name,\n            )\n        elif stage == \"dpo\":\n            from byzerllm.apps.byzer_storage.train_dpo import train_dpo\n\n            train_dpo(\n                name,\n                memories,\n                options,\n                dataset_dir,\n                loras_dir,\n                model_name=llama_model,\n                data_model_name=self.model_name,\n            )\n        else:\n            raise ValueError(f\"Unsupported stage: {stage}\")\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/docs/zh/003_byzerllm_大模型编程快速指南.md\n##  byzerllm 大模型编程快速指南\n\n本文示例 [Notebook](../../notebooks/003_byzerllm_大模型编程快速指南.ipynb)\n\n## 安装\n\n```bash\npip install -U byzerllm\nray start --head\n```\n## 启动一个模型代理\n\nbyzerllm 支持私有化模型或者SaaS模型的部署。\n\n这里以 deepseek 官方API 为例：\n\n```bash\neasy-byzerllm deploy deepseek-chat --token xxxxx --alias deepseek_chat\n```\n\n或者跬基流动API:\n\n```bash\neasy-byzerllm deploy alibaba/Qwen1.5-110B-Chat --token xxxxx --alias qwen110b_chat\n```\n\n将上面的 API KEY 替换成你们自己的。\n\n如果你想部署私有化模型或者对接 Ollama 等更多需求，参考 [002_使用byzerllm进行模型部署.md](./002_使用byzerllm进行模型部署.md) 或者 [README.md](../../README.md)。\n\n之后，你就可以在代码里使用  deepseek_chat 或者 qwen110b_chat  访问模型了。\n\n## hello world\n\n来和我们的大模型打个招呼:\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\n@byzerllm.prompt(llm=llm)\ndef hello(q:str) ->str:\n    '''\n    你好, {{ q }}\n    '''\n\ns = hello(\"你是谁\")    \nprint(s)\n\n## 输出:\n## '你好！我是一个人工智能助手，专门设计来回答问题、提供信息和帮助解决问题。如果你有任何疑问或需要帮助，请随时告诉我。'\n```\n\n恭喜，你和大模型成功打了招呼！\n\n可以看到，我们通过 `@byzerllm.prompt` 装饰器，将一个方法转换成了一个大模型的调用，然后这个方法的主题是一段文本，文本中\n使用了 jinja2 模板语法，来获得方法的参数。当正常调用该方法时，实际上就发起了和大模型的交互，并且返回了大模型的结果。\n\n在 byzerllm 中，我们把这种方法称之为 prompt 函数。\n\n## 查看发送给大模型的prompt\n\n很多情况你可能需要调试，查看自己的 prompt 渲染后到底是什么样子的，这个时候你可以通过如下方式\n获取渲染后的prompt:\n\n```python\nhello.prompt(\"你是谁\")\n## '你好, 你是谁'\n```            \n\n## 动态换一个模型\n\n前面的 hello 方法在初始化的时候，我们使用了默认的模型 deepseek_chat，如果我们想换一个模型，可以这样做：\n\n```python\nhello.with_llm(llm).run(\"你是谁\")\n## '你好！我是一个人工智能助手，专门设计来回答问题、提供信息和帮助解决问题。如果你有任何疑问或需要帮助，请随时告诉我。'\n```\n\n通过 with_llm 你可以设置一个新的 llm 对象，然后调用 run 方法，就可以使用新的模型了。\n\n## 超长文本生成\n\n我们知道，大模型一次生成的长度其实是有限的，如果你想生成超长文本，你可能需手动的不断获得\n生成结果，然后把他转化为输入，然后再次生成，这样的方式是比较麻烦的。\n\nbyzerllm 提供了更加易用的 API :\n\n```python\nimport byzerllm\nfrom byzerllm import ByzerLLM\n\nllm = ByzerLLM.from_default_model(\"deepseek_chat\")\n\n@byzerllm.prompt()\ndef tell_story() -> str:\n    \"\"\"\n    讲一个100字的故事。    \n    \"\"\"\n\n\ns = (\n    tell_story.with_llm(llm)\n    .with_response_markers()\n    .options({\"llm_config\": {\"max_length\": 10}})\n    .run()\n)\nprint(s)\n\n## 从前，森林里住着一只聪明的小狐狸。一天，它发现了一块闪闪发光的宝石。小狐狸决定用这块宝石帮助森林里的动物们。它用宝石的光芒指引迷路的小鹿找到了回家的路，用宝石的温暖治愈了受伤的小鸟。从此，小狐狸成了森林里的英雄，动物们都感激它的善良和智慧。\n```\n\n实际核心部分就是这一行：\n\n```python\ntell_story.with_llm(llm)\n    .with_response_markers()    \n    .run()\n```\n\n我们只需要调用 `with_response_markers` 方法，系统就会自动的帮我们生成超长文本。\n在上面的案例中，我们通过\n\n```python\n.options({\"llm_config\": {\"max_length\": 10}})\n```\n\n认为的限制大模型一次交互最多只能输出10个字符，但是系统依然自动完成了远超过10个字符的文本生成。\n\n## 对象输出\n\n前面我们的例子都是返回字符串，但是我们也可以返回对象，这样我们就可以更加灵活的处理返回结果。\n\n```python\nimport pydantic \n\nclass Story(pydantic.BaseModel):\n    '''\n    故事\n    '''\n\n    title: str = pydantic.Field(description=\"故事的标题\")\n    body: str = pydantic.Field(description=\"故事主体\")\n\n@byzerllm.prompt()\ndef tell_story()->Story:\n    '''\n    讲一个100字的故事。    \n    '''\n\ns = tell_story.with_llm(llm).run()\nprint(isinstance(s, Story))\nprint(s.title)\n\n## True\n## 勇敢的小鸟\n```\n\n可以看到，我们很轻松的将输出转化为格式化输出。\n\n## 自定义字段抽取\n\n前面的结构化输出，其实会消耗更多token,还有一种更加精准的结构化输出方式。\n比如让大模型生成一个正则表达式，但实际上大模型很难准确只输出一个正则表达式，这个时候我们可以通过自定义抽取函数来获取我们想要的结果。\n\n\n```python\nfrom loguru import logger\nimport re\n\n@byzerllm.prompt()\ndef generate_regex_pattern(desc: str) -> str:\n    \"\"\"\n    根据下面的描述生成一个正则表达式，要符合python re.compile 库的要求。\n\n    {{ desc }}\n\n    最后生成的正则表达式要在<REGEX></REGEX>标签对里。\n    \"\"\"    \n\ndef extract_regex_pattern(regex_block: str) -> str:    \n    pattern = re.search(r\"<REGEX>(.*)</REGEX>\", regex_block, re.DOTALL)\n    if pattern is None:\n        logger.warning(\"No regex pattern found in the generated block:\\n {regex_block}\")\n        raise None\n    return pattern.group(1)\n\npattern = \"匹配一个邮箱地址\"\nv = generate_regex_pattern.with_llm(llm).with_extractor(extract_regex_pattern).run(desc=pattern)\nprint(v)\n## ^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\n```\n\n在上面的例子里，我们根据一句话生成一个正则表达式。我们通过 `with_extractor` 方法，传入了一个自定义的抽取函数，这个函数会在大模型生成结果后，对结果进行处理，然后返回我们想要的结果。\n\n我们在 prompt 明确说了，生成的结果要放到 `<REGEX></REGEX>` 标签对里，然后我们通过 extract_regex_pattern 函数，从结果中提取出了我们想要的正则表达式。\n\n## 在实例方法中使用大模型\n\n```python\nimport byzerllm\ndata = {\n    'name': 'Jane Doe',\n    'task_count': 3,\n    'tasks': [\n        {'name': 'Submit report', 'due_date': '2024-03-10'},\n        {'name': 'Finish project', 'due_date': '2024-03-15'},\n        {'name': 'Reply to emails', 'due_date': '2024-03-08'}\n    ]\n}\n\n\nclass RAG():\n    def __init__(self):        \n        self.llm = byzerllm.ByzerLLM()\n        self.llm.setup_template(model=\"deepseek_chat\",template=\"auto\")\n        self.llm.setup_default_model_name(\"deepseek_chat\")        \n    \n    @byzerllm.prompt(lambda self:self.llm)\n    def generate_answer(self,name,task_count,tasks)->str:\n        '''\n        Hello {{ name }},\n\n        This is a reminder that you have {{ task_count }} pending tasks:\n        {% for task in tasks %}\n        - Task: {{ task.name }} | Due: {{ task.due_date }}\n        {% endfor %}\n\n        Best regards,\n        Your Reminder System\n        '''        \n\nt = RAG()\n\nresponse = t.generate_answer(**data)\nprint(response)\n\n## 输出:\n## Hello Jane Doe,\n##I hope this message finds you well. I wanted to remind you of your 3 pending tasks to ensure you stay on track:\n## 1. **Submit report** - This task is due on **2024-03-10**. Please ensure that you allocat\n```\n\n这里我们给了个比较复杂的例子，但我们可以看到，给一个实例prompt方法和普通prompt 方法差异不大。\n唯一的区别是如果你希望在定义的时候就指定大模型，使用一个lambda函数返回实例的 llm 对象即可。\n\n```python\n@byzerllm.prompt(lambda self:self.llm)\n```\n\n你也可以不返回，在调用的时候通过 `with_llm` 方法指定 llm 对象。\n\n此外，这个例子也展示了如何通过jinja2模板语法，来处理复杂的结构化数据。\n\n## 通过 Python 代码处理复杂入参\n\n上面的一个例子中，我们通过 jinja2 模板语法，来处理复杂的结构化数据，但是有时候我们可能需要更加复杂的处理，这个时候我们可以通过 Python 代码来处理。\n\n```python\nimport byzerllm\n\ndata = {\n    'name': 'Jane Doe',\n    'task_count': 3,\n    'tasks': [\n        {'name': 'Submit report', 'due_date': '2024-03-10'},\n        {'name': 'Finish project', 'due_date': '2024-03-15'},\n        {'name': 'Reply to emails', 'due_date': '2024-03-08'}\n    ]\n}\n\n\nclass RAG():\n    def __init__(self):        \n        self.llm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n    \n    @byzerllm.prompt()\n    def generate_answer(self,name,task_count,tasks)->str:\n        '''\n        Hello {{ name }},\n\n        This is a reminder that you have {{ task_count }} pending tasks:\n            \n        {{ tasks }}\n\n        Best regards,\n        Your Reminder System\n        '''\n        \n        tasks_str = \"\\n\".join([f\"- Task: {task['name']} | Due: { task['due_date'] }\" for task in tasks])\n        return {\"tasks\": tasks_str}\n\nt = RAG()\n\nresponse = t.generate_answer.with_llm(t.llm).run(**data)\nprint(response)\n\n## Just a gentle nudge to keep you on track with your pending tasks. Here's a quick recap:....\n```\n\n在这个例子里，我们直接把 tasks 在方法体里进行处理，然后作为一个字符串返回，最够构建一个字典，字典的key为 tasks,然后\n你就可以在 docstring 里使用 `{{ tasks }}` 来引用这个字符串。\n\n这样对于很复杂的入参，就不用谢繁琐的 jinja2 模板语法了。\n\n## 如何自动实现一个方法\n\n比如我定义一个签名，但是我不想自己实现里面的逻辑，让大模型来实现。这个在 byzerllm 中叫 function impl。我们来看看怎么\n实现:\n\n```python\nimport pydantic\nclass Time(pydantic.BaseModel):\n    time: str = pydantic.Field(...,description=\"时间，时间格式为 yyyy-MM-dd\")\n\n\n@llm.impl()\ndef calculate_current_time()->Time:\n    '''\n    计算当前时间\n    '''\n    pass \n\n\ncalculate_current_time()\n#output: Time(time='2024-06-14')\n```\n\n在这个例子里，我们定义了一个 calculate_current_time 方法，但是我们没有实现里面的逻辑，我们通过 `@llm.impl()` 装饰器，让大模型来实现这个方法。\n为了避免每次都要“生成”这个方法，导致无法适用，我们提供了缓存，用户可以按如下方式打印速度：\n\n```python\nstart = time.monotonic()\ncalculate_current_time()\nprint(f\"first time cost: {time.monotonic()-start}\")\n\nstart = time.monotonic()\ncalculate_current_time()\nprint(f\"second time cost: {time.monotonic()-start}\")\n\n# output:\n# first time cost: 6.067266260739416\n# second time cost: 4.347506910562515e-05\n```\n可以看到，第一次执行花费了6s,第二次几乎是瞬间完成的，这是因为第一次执行的时候，我们实际上是在生成这个方法，第二次执行的时候，我们是执行已经生成好的代码，所以速度会非常快。你可以显示的调用 `llm.clear_impl_cache()` 清理掉函数缓存。\n\n## Stream 模式\n\n前面的例子都是一次性生成结果，但是有时候我们可能需要一个流式的输出，这个时候我们可能需要用底层一点的API来完成了：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\nv = llm.stream_chat_oai(model=\"deepseek_chat\",conversations=[{\n    \"role\":\"user\",\n    \"content\":\"你好，你是谁\",\n}],delta_mode=True)\n\nfor t in v:\n    print(t,flush=True)  \n\n# 你好\n# ！\n# 我\n# 是一个\n# 人工智能\n# 助手\n# ，\n# 旨在\n# 提供\n# 信息\n# 、\n# 解答\n# 问题....\n```\n\n如果你不想要流式输出，但是想用底层一点的API，你可以使用 `llm.chat_oai` 方法：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\nv = llm.chat_oai(model=\"deepseek_chat\",conversations=[{\n    \"role\":\"user\",\n    \"content\":\"你好，你是谁\",\n}])\n\nprint(v[0].output)\n## 你好！我是一个人工智能助手，旨在提供信息、解答问题和帮助用户解决问题。如果你有任何问题或需要帮助，请随时告诉我。\n```\n\n## Function Calling \n\nbyzerllm 可以不依赖模型自身就能提供 function calling 支持，我们来看个例子：\n\n\n```python\nfrom typing import List,Dict,Any,Annotated\nimport pydantic \nimport datetime\nfrom dateutil.relativedelta import relativedelta\n\ndef compute_date_range(count:Annotated[int,\"时间跨度，数值类型\"],\n                       unit:Annotated[str,\"时间单位，字符串类型\",{\"enum\":[\"day\",\"week\",\"month\",\"year\"]}])->List[str]:\n    '''\n    计算日期范围\n\n    Args:\n        count: 时间跨度，数值类型\n        unit: 时间单位，字符串类型，可选值为 day,week,month,year\n    '''        \n    now = datetime.datetime.now()\n    now_str = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n    if unit == \"day\":\n        return [(now - relativedelta(days=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"week\":\n        return [(now - relativedelta(weeks=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"month\":\n        return [(now - relativedelta(months=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"year\":\n        return [(now - relativedelta(years=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    return [\"\",\"\"]\n\ndef compute_now()->str:\n    '''\n    计算当前时间\n    '''\n    return datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n```\n\n我们定义了两个方法，一个是计算日期范围，一个是计算当前时间。\n\n现在我么可以来测试下，系统如何根据自然语言决定调用哪个方法：\n\n```python\nt = llm.chat_oai([{\n    \"content\":'''计算当前时间''',\n    \"role\":\"user\"    \n}],tools=[compute_date_range,compute_now],execute_tool=True)\n\nt[0].values\n\n## output: ['2024-06-14 15:18:02']\n```\n\n我们可以看到，他正确的选择了 compute_now 方法。\n\n接着我们再试一个：\n\n```python\nt = llm.chat_oai([{\n    \"content\":'''最近三个月趋势''',\n    \"role\":\"user\"    \n}],tools=[compute_date_range,compute_now],execute_tool=True)\n\nt[0].values\n\n## output: [['2024-03-14 15:19:13', '2024-06-14 15:19:13']]\n```\n\n模型正确的选择了 compute_date_range 方法。\n\n## 多模态\n\nbyerllm 也能很好的支持多模态的交互，而且统一了多模态大模型的接口，比如你可以用一样的方式使用 openai 或者 claude 的图片转文字能力， 或者一致的方式使用火山，azuer, openai的语音合成接口。\n\n### 图生文\n\n```python\nimport byzerllm\nfrom byzerllm.types import ImagePath\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\n\n\n@byzerllm.prompt()\ndef what_in_image(image_path: ImagePath) -> str:\n    \"\"\"\n    {{ image_path }}\n    这个图片里有什么？\n    \"\"\"    \n\n\nv = what_in_image.with_llm(vl_llm).run(\n    ImagePath(value=\"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\")\n)\nv\n## OUTPUT: 这张图片展示了多只可爱的猫咪，采用了艺术风格的绘画。猫咪们有不同的颜色和花纹，背景是浅棕色，上面还点缀着一些红色的花朵。整体画面给人一种温馨和谐的感觉\n```\n\n可以看到，我们只需要把 prompt 函数的图片地址入参使用 byzerllm.types.ImagePath里进行包装，就可以直接在 prompt 函数体里\n带上图片。\n\n或者你可以这样：\n\n```python\nimport byzerllm\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\n\n\n@byzerllm.prompt()\ndef what_in_image(image_path: str) -> str:\n    \"\"\"\n    {{ image }}\n    这个图片里有什么？\n    \"\"\"\n    return {\"image\": byzerllm.Image.load_image_from_path(image_path)}\n\n\nv = what_in_image.with_llm(vl_llm).run(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv\n```\n\n通过 `image_path` 参数，然后通过 `byzerllm.Image.load_image_from_path` 方法，转化为一个图片对象 image，最后在 prompt 函数体里\n使用 `{{ image }}` 引用这个图片对象。\n\n另外我们也是可以支持配置多张图片的。\n\n另外，我们也可以使用基础的 `llm.chat_oai` 方法来实现：\n\n```python\nimport byzerllm\nimport json\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\nimage = byzerllm.Image.load_image_from_path(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv = vl_llm.chat_oai(\n    conversations=[\n        {\n            \"role\": \"user\",\n            \"content\": json.dumps(\n                [{\"image\": image, \"text\": \"这个图片里有什么？\"}], ensure_ascii=False\n            ),\n        }\n    ]\n)\nv[0].output\n```\n\n还可以这么写：\n\n```python\nimport byzerllm\nimport json\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\nimage = byzerllm.Image.load_image_from_path(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv = vl_llm.chat_oai(\n    conversations=[\n        {\n            \"role\": \"user\",\n            \"content\": json.dumps(\n                [\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\"url\": image, \"detail\": \"high\"},\n                    },\n                    {\"text\": \"这个图片里有什么？\", \"type\": \"text\"},\n                ],\n                ensure_ascii=False,\n            ),\n        }\n    ]\n)\nv[0].output\n```\n\n### 语音合成\n\n这里以 openai 的 tts 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=tts-1 \\\n--model openai_tts\n```\n\n此外，byzerllm 支持 azure,火山引擎等 tts 语音合成引擎。\n\n接着你可以这么用：\n\n```python\nimport byzerllm\nimport base64\nimport json\n\nllm = byzerllm.ByzerLLM.from_default_model(\"openai_tts\")\n\n\nt = llm.chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\": json.dumps({\n        \"input\":\"hello, open_tts\",\n        \"voice\": \"alloy\",\n        \"response_format\": \"mp3\"\n    },ensure_ascii=False)\n}])\n\nwith open(\"voice.mp3\",\"wb\") as f:\n    f.write(base64.b64decode(t[0].output))\n```\n\ntts 模型生成没有prompt函数可以用，你需要直接使用 chat_oai。\n\n\n### 语音识别\n\n这里以 openai 的 whisper-1 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--worker_concurrency 10 \\\n--infer_params saas.model=whisper-1 saas.api_key=${MODEL_OPENAI_TOKEN} \\\n--model speech_to_text\n```\n\n语音识别的使用方式和图生文类似，我们可以直接在 prompt 函数体里带上音频文件。\n\n```python\nimport byzerllm\nimport json\nimport base64\nfrom byzerllm.types import AudioPath\n\nllm = byzerllm.ByzerLLM.from_default_model(\"speech_to_text\")\n\naudio_file = \"/Users/allwefantasy/videos/output_audio.mp3\"\n\n@byzerllm.prompt(llm=llm)\ndef audio_to_text(audio_file: AudioPath):\n    \"\"\"\n    {{ audio_file }}\n    \"\"\"\n\nv = audio_to_text(AudioPath(value=audio_file))\njson.loads(v)\n```\n输出的数据格式略微复杂：\n\n```\n{'text': 'In the last chapter, you and I started to step through the internal workings of a transformer. This is one of the key pieces of technology inside large language models, and a lot of other tools in the modern wave of AI.',\n 'task': 'transcribe',\n 'language': 'english',\n 'duration': 10.0,\n 'segments': [{'id': 0,\n   'seek': 0,\n   'start': 0.0,\n   'end': 4.78000020980835,\n   'text': ' In the last chapter, you and I started to step through the internal workings of a transformer.',\n   'tokens': [50364,\n    682,\n    264,\n    1036,\n    7187,\n    11,\n    291,\n    293,\n    286,\n    1409,\n    281,\n    1823,\n    807,\n    264,\n    6920,\n    589,\n    1109,\n    295,\n    257,\n    31782,\n    13,\n    50586],\n   'temperature': 0.0,\n   'avg_logprob': -0.28872039914131165,\n   'compression_ratio': 1.4220778942108154,\n   'no_speech_prob': 0.016033057123422623},\n  ....\n  {'id': 2,\n   'seek': 0,\n   'start': 8.579999923706055,\n   'end': 9.979999542236328,\n   'text': ' and a lot of other tools in the modern wave of AI.',\n   'tokens': [50759,\n    293,\n    257,\n    688,\n    295,\n    661,\n    3873,\n    294,\n    264,\n    4363,\n    5772,\n    295,\n    7318,\n    13,\n    50867],\n   'temperature': 0.0,\n   'avg_logprob': -0.28872039914131165,\n   'compression_ratio': 1.4220778942108154,\n   'no_speech_prob': 0.016033057123422623}],\n 'words': [{'word': 'In', 'start': 0.0, 'end': 0.18000000715255737},\n  {'word': 'the', 'start': 0.18000000715255737, 'end': 0.23999999463558197},\n  {'word': 'last', 'start': 0.23999999463558197, 'end': 0.5400000214576721},\n  {'word': 'chapter', 'start': 0.5400000214576721, 'end': 0.800000011920929},\n  ....\n  {'word': 'AI', 'start': 9.920000076293945, 'end': 9.979999542236328}]}\n```\n\n会输出每一句话以及每一个字所在的起始时间和截止时间。你可以根据需要来使用。\n\n### 文生图\n\n文生图和语音合成类似，首先要启动合适的模型,以openai 的 dall-e-3 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=dall-e-3 \\\n--model openai_image_gen\n```\n\n启动模型后，只需要记住几个模板参数即可使用，这里直接使用 chat_oai 方法来使用：\n\n```python\n\nimport byzerllm\nimport json\nimport base64\n\nllm = byzerllm.ByzerLLM.from_default_model(\"openai_image_gen\")\nt = llm.chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\": json.dumps({\n        \"input\":\"a white siamese cat\",\n        \"size\": \"1024x1024\",\n        \"quality\": \"standard\"\n    },ensure_ascii=False)\n}])\n\nwith open(\"output1.jpg\",\"wb\") as f:\n    f.write(base64.b64decode(t[0].output))\n\n\nimport matplotlib.pyplot as plt\n\nimage_path = \"output1.jpg\"\nimage = plt.imread(image_path)\n\nplt.imshow(image)\nplt.axis('off')\nplt.show()\n```\n\n## Prompt 函数的流式输出\n\nbyzerllm 底层支持流式输出，非 prompt 函数的用法是这样的：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\n\nv = llm.stream_chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\":\"讲一个100字的故事\"\n}])\n\nfor s  in v:\n    print(s[0], end=\"\")\n```\n\n如果你像用 prompt 函数，可以这么用：\n\n\n```python\nimport byzerllm\nimport json\nimport base64\nfrom typing import Generator\n\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\n\n@byzerllm.prompt()\ndef tell_story() -> Generator[str, None, None]:\n    '''\n    给我讲一个一百多字的故事\n    '''\n\nv = tell_story.with_llm(llm).run()    \nfor i in v:\n    print(i, end=\"\")\n```\n\n可以看到，和普通的 prompt 函数的区别在于，返回值是一个生成器，然后你可以通过 for 循环来获取结果。\n\n## 注意事项\n\n1. prompt函数方法体返回只能是dict，实际的返回类型和方法签名可以不一样，但是方法体返回只能是dict。\n2. 大部分情况prompt函数体为空，如果一定要有方法体，可以返回一个空字典。\n3. 调用prompt方法的时候，如果在@byzerllm.prompt()里没有指定llm对象，那么需要在调用的时候通过with_llm方法指定llm对象。\n\n\n\n\n\n\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/apps/byzer_storage/train_pt.py\nimport json\nimport os\nfrom typing import List, Dict, Any\n\n\ndef train_pt(\n    name: str,\n    memories: List[str],\n    options: Dict[str, Any],\n    dataset_dir: str,\n    loras_dir: str,\n    data_model_name: str,\n    model_name: str,\n):\n    data = []\n    min_samples = options.pop(\"min_samples\", 1000)\n\n    for memory in memories:\n        item = {\n            \"text\": memory,\n        }\n        data.append(item)\n\n    if len(data) < min_samples:\n        data = data * (min_samples // len(data) + 1)\n\n    with open(f\"{dataset_dir}/data.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(data, f, indent=2, ensure_ascii=False)\n\n    with open(f\"{dataset_dir}/dataset_info.json\", \"w\", encoding=\"utf-8\") as f:\n        r = {\"data\": {\"file_name\": \"data.json\", \"columns\": {\"prompt\": \"text\"}}}\n        f.write(json.dumps(r, indent=2, ensure_ascii=False))\n\n    args = dict(\n        stage=\"pt\",\n        do_train=True,\n        model_name_or_path=model_name,\n        dataset=\"data\",\n        dataset_dir=dataset_dir,\n        cutoff_len=1024,\n        max_samples=1000000,\n        overwrite_cache=True,\n        preprocessing_num_workers=1,\n        template=\"llama3\",\n        finetuning_type=\"lora\",\n        lora_target=\"all\",\n        output_dir=f\"{loras_dir}/{name}\",\n        overwrite_output_dir=True,\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=4,\n        lr_scheduler_type=\"cosine\",\n        logging_steps=10,\n        warmup_ratio=0.1,\n        save_steps=1000,\n        plot_loss=True,\n        learning_rate=5e-5,\n        num_train_epochs=1000.0,\n        max_grad_norm=1.0,\n        quantization_bit=4,\n        loraplus_lr_ratio=16.0,\n        fp16=True,\n        ddp_timeout=180000000,\n    )\n    os.environ[\"WANDB_DISABLED\"] = \"true\"\n    from llamafactory.train import tuner\n\n    tuner.run_exp({**args, **options})\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/apps/byzer_storage/train_sft.py\nimport json\nimport os\nimport concurrent.futures\nfrom typing import List, Dict, Any\nimport byzerllm\nfrom byzerllm.apps.byzer_storage.generate_sft_data import to_qa_pairs, read_alpaca_zh\n\n\ndef train_sft(\n    name: str,\n    memories: List[str],\n    options: Dict[str, Any],\n    dataset_dir: str,\n    loras_dir: str,\n    data_model_name: str,\n    model_name: str,\n):\n    data = []\n    min_samples = options.pop(\"min_samples\", 1000)\n\n    if memories:\n        with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:\n            futures = []\n            llm = byzerllm.ByzerLLM().from_default_model(data_model_name)\n            for memory in memories:\n                futures.append(executor.submit(to_qa_pairs, memory, llm))\n\n            for future in concurrent.futures.as_completed(futures):\n                qa_pairs = future.result()\n                for qa_pair in qa_pairs:\n                    item = {\n                        \"instruction\": qa_pair.question,\n                        \"input\": \"\",\n                        \"output\": qa_pair.answer,\n                    }\n                    data.append(item)\n    else:\n        v = read_alpaca_zh()\n        data.extend(v)\n\n    if len(data) < min_samples:\n        data = data * (min_samples // len(data) + 1)\n\n    with open(f\"{dataset_dir}/data.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(data, f, indent=2, ensure_ascii=False)\n\n    with open(f\"{dataset_dir}/dataset_info.json\", \"w\", encoding=\"utf-8\") as f:\n        r = {\"data\": {\"file_name\": \"data.json\"}}\n        f.write(json.dumps(r, indent=2, ensure_ascii=False))\n\n    args = dict(\n        stage=\"sft\",\n        do_train=True,\n        model_name_or_path=model_name,\n        dataset=\"data\",\n        dataset_dir=dataset_dir,\n        cutoff_len=1024,\n        max_samples=1000000,\n        overwrite_cache=True,\n        preprocessing_num_workers=1,\n        template=\"llama3\",\n        finetuning_type=\"lora\",\n        lora_target=\"all\",\n        output_dir=f\"{loras_dir}/{name}\",\n        overwrite_output_dir=True,\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=4,\n        lr_scheduler_type=\"cosine\",\n        logging_steps=10,\n        warmup_ratio=0.1,\n        save_steps=1000,\n        plot_loss=True,\n        learning_rate=5e-5,\n        num_train_epochs=1000.0,\n        max_grad_norm=1.0,\n        quantization_bit=4,\n        loraplus_lr_ratio=16.0,\n        fp16=True,\n        ddp_timeout=180000000,\n    )\n    os.environ[\"WANDB_DISABLED\"] = \"true\"\n    from llamafactory.train import tuner\n\n    tuner.run_exp({**args, **options})\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/apps/byzer_storage/train_dpo.py\nimport json\nimport os\nfrom typing import List, Dict, Any\nimport byzerllm\nimport concurrent.futures\nfrom byzerllm.apps.byzer_storage.generate_sft_data import to_qa_pairs\n\n\ndef train_dpo(    \n    name: str,\n    memories: List[str],\n    options: Dict[str, Any],\n    dataset_dir: str,\n    loras_dir: str,\n    model_name: str,\n    data_model_name: str,\n):\n    data = []\n    min_samples = options.pop(\"min_samples\", 1000)\n\n    if memories:\n        with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:\n            futures = []\n            data_llm = byzerllm.ByzerLLM().from_default_model(data_model_name)\n            current_llm = byzerllm.ByzerLLM().from_default_model(model_name)\n            for memory in memories:\n                futures.append(executor.submit(to_qa_pairs, memory, data_llm))\n\n            for future in concurrent.futures.as_completed(futures):\n                qa_pairs = future.result()\n                for qa_pair in qa_pairs:\n                    response = current_llm.chat_oai(\n                        conversations=[{\"role\": \"user\", \"content\": qa_pair.question}]\n                    )\n                    v = response[0].output\n                    item = (\n                        {\n                            \"conversations\": [\n                                {\n                                    \"from\": \"human\",\n                                    \"value\": qa_pair.question,\n                                }\n                            ],\n                            \"chosen\": {\n                                \"from\": \"gpt\",\n                                \"value\": qa_pair.answer,\n                            },\n                            \"rejected\": {\n                                \"from\": \"gpt\",\n                                \"value\": v,\n                            },\n                        },\n                    )\n                    data.append(item)\n\n    if len(data) < min_samples:\n        data = data * (min_samples // len(data) + 1)\n\n    with open(f\"{dataset_dir}/data.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(data, f, indent=2, ensure_ascii=False)\n\n    with open(f\"{dataset_dir}/dataset_info.json\", \"w\", encoding=\"utf-8\") as f:\n        r = {\n            \"data\": {\n                \"file_name\": \"data.json\",\n                \"ranking\": True,\n                \"columns\": {\n                    \"prompt\": \"instruction\",\n                    \"query\": \"input\",\n                    \"chosen\": \"chosen\",\n                    \"rejected\": \"rejected\",\n                },\n            }\n        }\n        f.write(json.dumps(r, indent=2, ensure_ascii=False))\n\n    args = dict(\n        stage=\"dpo\",\n        do_train=True,\n        model_name_or_path=model_name,\n        dataset=\"data\",\n        dataset_dir=dataset_dir,\n        cutoff_len=1024,\n        max_samples=1000000,\n        overwrite_cache=True,\n        preprocessing_num_workers=1,\n        template=\"llama3\",\n        finetuning_type=\"lora\",\n        lora_target=\"all\",\n        output_dir=f\"{loras_dir}/{name}\",\n        overwrite_output_dir=True,\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=4,\n        lr_scheduler_type=\"cosine\",\n        logging_steps=10,\n        warmup_ratio=0.1,\n        save_steps=1000,\n        plot_loss=True,\n        learning_rate=5e-5,\n        num_train_epochs=1000.0,\n        max_grad_norm=1.0,\n        quantization_bit=4,\n        loraplus_lr_ratio=16.0,\n        fp16=True,\n        ddp_timeout=180000000,\n    )\n    os.environ[\"WANDB_DISABLED\"] = \"true\"\n    from llamafactory.train import tuner\n\n    tuner.run_exp({**args, **options})\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/apps/byzer_storage/generate_sft_data.py\nfrom typing import List, Dict, Any\nimport byzerllm\nfrom loguru import logger\nfrom pydantic import BaseModel\nfrom byzerllm.apps.utils import TagExtractor, Tag\nimport importlib\nimport json\nimport hashlib\nimport os\nfrom pathlib import Path\n\n\ndef read_alpaca_zh():\n    with importlib.resources.path(\n        \"byzerllm.apps.byzer_storage\", \"alpaca_zh.json\"\n    ) as json_path:\n        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n            return json.load(f)\n\n\nclass QAPair(BaseModel):\n    question: str\n    answer: str\n\n\n@byzerllm.prompt()\ndef _convert_pretrain_text_to_instruction(text: str, num: int = 10) -> str:\n    \"\"\"\n    根据提供的信息，生成多个相关的问题，这些问题的回答，最终要能覆盖里面所有的信息,生成 {{num}} 个。\n\n    下面是一些信息：\n\n    {{text}}\n\n    现在请开始生成问题，请将每个问题使用<_question_></_question_>标签包裹，每个回答使用<_answer_></_answer_>标签包裹，最后\n    每个一组问题和回答使用<_group_></_group_>标签包裹。\n    \"\"\"\n\n\n@byzerllm.prompt()\ndef get_more(text: str, questions_and_answers: str, num: int = 10) -> str:\n    \"\"\"\n    下面是一些问答信息：\n\n    {{text}}\n\n    下面是已有的问题和回答：\n\n    {{questions_and_answers}}\n\n    下面是已经生成的问题和回答，请继续生成{{num}}个问题和回答，不要和前面的重复，请将每个问题使用<_question_></_question_>标签包裹，每个回答使用<_answer_></_answer_>标签包裹，最后\n    每个一组问题和回答使用<_group_></_group_>标签包裹。\n    \"\"\"\n\n\ndef generate_cache_key(text: str) -> str:\n    return hashlib.md5(text.encode()).hexdigest()\n\ndef get_cache_path() -> Path:\n    home = Path.home()\n    cache_dir = home / \".byzerllm\" / \"cache\" / \"llm_generated_data\"\n    cache_dir.mkdir(parents=True, exist_ok=True)\n    return cache_dir\n\ndef save_to_cache(key: str, data: List[QAPair]):\n    cache_path = get_cache_path() / f\"{key}.json\"\n    with cache_path.open(\"w\", encoding=\"utf-8\") as f:\n        json.dump([qa.dict() for qa in data], f, ensure_ascii=False, indent=2)\n\ndef load_from_cache(key: str) -> List[QAPair]:\n    cache_path = get_cache_path() / f\"{key}.json\"\n    if cache_path.exists():\n        with cache_path.open(\"r\", encoding=\"utf-8\") as f:\n            data = json.load(f)\n        return [QAPair(**item) for item in data]\n    return None\n\ndef to_qa_pairs(text: str, llm, num: int = 30) -> List[QAPair]:\n    cache_key = generate_cache_key(text)\n    cached_result = load_from_cache(cache_key)\n    \n    if cached_result:\n        logger.info(f\"Using cached result for text: {text[:100]}...\")\n        return cached_result\n\n    logger.info(f\"Starting to generate QA pairs for text: {text[:100]}...\")\n\n    logger.info(\"Generating initial QA pairs...\")\n    v = _convert_pretrain_text_to_instruction.with_llm(llm).run(text)\n    logger.info(f\"Initial QA pairs generated. Length: {len(v)}\")\n\n    max_turns = int(num / 10)\n    if max_turns < 1:\n        max_turns = 1\n    logger.info(f\"Will generate additional QA pairs for {max_turns} turns\")\n\n    turn = 0\n    while turn < max_turns:\n        turn += 1\n        logger.info(f\"Generating additional QA pairs (turn {turn}/{max_turns})...\")\n        t = get_more.with_llm(llm).run(text, v)\n        v += t\n        logger.info(f\"Additional QA pairs generated. New total length: {len(v)}\")\n\n    logger.info(f\"Extracting QA pairs from generated text...{v}\")\n    root_tag = TagExtractor(v).extract()\n    qa_pairs = []\n    for i, item in enumerate(root_tag.content):\n        qas = item.content\n        if len(qas) == 2:\n            if (\n                qas[0].start_tag == \"<_question_>\"\n                and qas[0].end_tag == \"</_question_>\"\n                and qas[1].start_tag == \"<_answer_>\"\n                and qas[1].end_tag == \"</_answer_>\"\n            ):\n                qa_pairs.append(QAPair(question=qas[0].content, answer=qas[1].content))\n                logger.debug(\n                    f\"Extracted QA pair {i+1}: Q: {qas[0].content[:50]}... A: {qas[1].content[:50]}...\"\n                )\n            else:\n                logger.warning(\n                    f\"Skipping item {i+1} due to incorrect tags: {qas[0].start_tag}, {qas[0].end_tag}, {qas[1].start_tag}, {qas[1].end_tag}\"\n                )\n        else:\n            logger.warning(\n                f\"Skipping item {i+1} due to incorrect number of elements: {len(qas)}\"\n            )\n\n    logger.info(f\"Extracted {len(qa_pairs)} valid QA pairs.\")\n    \n    # Save the result to cache\n    save_to_cache(cache_key, qa_pairs)\n    \n    return qa_pairs\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/apps/byzer_storage/memory_model_based.py\nimport asyncio\nfrom typing import List, Dict, Any\n\nimport ray.experimental\nfrom byzerllm.apps.byzer_storage.simple_api import ByzerStorage\nimport time\nimport json\nimport os\nimport concurrent.futures\nimport io\nimport sys\nfrom contextlib import redirect_stdout, redirect_stderr\nimport queue\nimport threading\nimport ray\nimport byzerllm\nfrom typing import List, Union, Dict\nfrom pydantic import BaseModel\nimport importlib\nfrom loguru import logger\n\n\nclass MemoryManager:\n    _queue = asyncio.Queue()\n    _is_processing = False\n\n    def __init__(\n        self,\n        storage: ByzerStorage,\n        base_dir: str,\n        model_name: str = \"deepseek_chat\",\n        remote: bool = True,\n    ):\n        self.storage = storage\n        home = os.path.expanduser(\"~\")\n        self.base_dir = base_dir or os.path.join(home, \".auto-coder\")\n        self.thread_pool = concurrent.futures.ThreadPoolExecutor(max_workers=5)\n        self.log_file = None\n        self.remote = remote\n        self.model_name = model_name\n\n    @classmethod\n    async def add_to_queue(cls, name: str, memories: List[str]):\n        await cls._queue.put((name, memories))\n        if not cls._is_processing:\n            asyncio.create_task(cls.process_queue())\n\n    @classmethod\n    async def process_queue(cls):\n        cls._is_processing = True\n        while not cls._queue.empty():\n            name, memories = await cls._queue.get()\n            instance = cls.get_instance()\n            await instance.memorize(name, memories)\n            cls._queue.task_done()\n        cls._is_processing = False\n\n    async def memorize(\n        self, name: str, memories: List[str], options: Dict[str, Any] = {}\n    ):\n        try:\n            loop = asyncio.get_running_loop()\n            await loop.run_in_executor(\n                self.thread_pool, self._memorize_with_logs, name, memories, options\n            )\n        finally:\n            if self.remote:\n                ray.actor.exit_actor()\n\n        print(f\"Memorization for {name} completed.\")\n\n    def _memorize_with_logs(\n        self, name: str, memories: List[str], options: Dict[str, Any] = {}\n    ):\n\n        if self.remote:\n            self._memorize(name, memories, options=options)\n            return\n\n        logs_dir = os.path.join(self.base_dir, \"storage\", \"logs\", \"memorize\")\n        os.makedirs(logs_dir, exist_ok=True)\n        log_file = os.path.join(logs_dir, f\"{name}.log\")\n\n        log_queue = queue.Queue()\n        stop_event = threading.Event()\n\n        # Start the log writer thread\n        log_writer_thread = threading.Thread(\n            target=self._log_writer, args=(log_queue, log_file, stop_event)\n        )\n        self.log_file = log_file\n        log_writer_thread.start()\n\n        class QueueStream:\n            def __init__(self, queue):\n                self.queue = queue\n\n            def write(self, msg):\n                self.queue.put(msg)\n\n            def flush(self):\n                pass\n\n        queue_stream = QueueStream(log_queue)\n\n        with redirect_stdout(queue_stream), redirect_stderr(queue_stream):\n            self._memorize(name, memories)\n\n        # Signal the log writer to stop and wait for it to finish\n        stop_event.set()\n        log_writer_thread.join()\n\n    def _log_writer(\n        self, log_queue: queue.Queue, log_file: str, stop_event: threading.Event\n    ):\n        with open(log_file, \"w\") as f:\n            while not stop_event.is_set() or not log_queue.empty():\n                try:\n                    msg = log_queue.get(timeout=0.1)\n                    f.write(msg)\n                    f.flush()\n                except queue.Empty:\n                    continue\n\n    def _memorize(\n        self,\n        name: str,\n        memories: List[Union[str, Dict[str, Any]]],\n        options: Dict[str, Any] = {},\n    ):\n\n        stage = options.get(\"stage\", \"pt\")\n        base_model_dir = os.path.join(self.base_dir, \"storage\", \"models\")\n        llama_model = os.path.join(\n            base_model_dir, \"meta-llama\", \"Meta-Llama-3-8B-Instruct-GPTQ\"\n        )\n\n        loras_dir = os.path.join(self.base_dir, \"storage\", \"loras\")\n        dataset_dir = os.path.join(self.base_dir, \"storage\", \"datasets\", name)\n\n        os.makedirs(loras_dir, exist_ok=True)\n        os.makedirs(dataset_dir, exist_ok=True)\n\n        if stage == \"pt\":\n            from byzerllm.apps.byzer_storage.train_pt import train_pt\n\n            train_pt(\n                name,\n                memories,\n                options,\n                dataset_dir,\n                loras_dir,\n                model_name=llama_model,\n                data_model_name=self.model_name,\n            )\n        elif stage == \"sft\":\n            from byzerllm.apps.byzer_storage.train_sft import train_sft\n\n            train_sft(\n                name,\n                memories,\n                options,\n                dataset_dir,\n                loras_dir,\n                model_name=llama_model,\n                data_model_name=self.model_name,\n            )\n        elif stage == \"dpo\":\n            from byzerllm.apps.byzer_storage.train_dpo import train_dpo\n\n            train_dpo(\n                name,\n                memories,\n                options,\n                dataset_dir,\n                loras_dir,\n                model_name=llama_model,\n                data_model_name=self.model_name,\n            )\n        else:\n            raise ValueError(f\"Unsupported stage: {stage}\")\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/apps/byzer_storage/train_pt.py\nimport json\nimport os\nfrom typing import List, Dict, Any\n\n\ndef train_pt(\n    name: str,\n    memories: List[str],\n    options: Dict[str, Any],\n    dataset_dir: str,\n    loras_dir: str,\n    data_model_name: str,\n    model_name: str,\n):\n    data = []\n    min_samples = options.pop(\"min_samples\", 1000)\n\n    for memory in memories:\n        item = {\n            \"text\": memory,\n        }\n        data.append(item)\n\n    if len(data) < min_samples:\n        data = data * (min_samples // len(data) + 1)\n\n    with open(f\"{dataset_dir}/data.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(data, f, indent=2, ensure_ascii=False)\n\n    with open(f\"{dataset_dir}/dataset_info.json\", \"w\", encoding=\"utf-8\") as f:\n        r = {\"data\": {\"file_name\": \"data.json\", \"columns\": {\"prompt\": \"text\"}}}\n        f.write(json.dumps(r, indent=2, ensure_ascii=False))\n\n    args = dict(\n        stage=\"pt\",\n        do_train=True,\n        model_name_or_path=model_name,\n        dataset=\"data\",\n        dataset_dir=dataset_dir,\n        cutoff_len=1024,\n        max_samples=1000000,\n        overwrite_cache=True,\n        preprocessing_num_workers=1,\n        template=\"llama3\",\n        finetuning_type=\"lora\",\n        lora_target=\"all\",\n        output_dir=f\"{loras_dir}/{name}\",\n        overwrite_output_dir=True,\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=4,\n        lr_scheduler_type=\"cosine\",\n        logging_steps=10,\n        warmup_ratio=0.1,\n        save_steps=1000,\n        plot_loss=True,\n        learning_rate=5e-5,\n        num_train_epochs=1000.0,\n        max_grad_norm=1.0,\n        quantization_bit=4,\n        loraplus_lr_ratio=16.0,\n        fp16=True,\n        ddp_timeout=180000000,\n    )\n    os.environ[\"WANDB_DISABLED\"] = \"true\"\n    from llamafactory.train import tuner\n\n    tuner.run_exp({**args, **options})\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/apps/byzer_storage/train_sft.py\nimport json\nimport os\nimport concurrent.futures\nfrom typing import List, Dict, Any\nimport byzerllm\nfrom byzerllm.apps.byzer_storage.generate_sft_data import to_qa_pairs, read_alpaca_zh\n\n\ndef train_sft(\n    name: str,\n    memories: List[str],\n    options: Dict[str, Any],\n    dataset_dir: str,\n    loras_dir: str,\n    data_model_name: str,\n    model_name: str,\n):\n    data = []\n    min_samples = options.pop(\"min_samples\", 1000)\n\n    if memories:\n        with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:\n            futures = []\n            llm = byzerllm.ByzerLLM().from_default_model(data_model_name)\n            for memory in memories:\n                futures.append(executor.submit(to_qa_pairs, memory, llm))\n\n            for future in concurrent.futures.as_completed(futures):\n                qa_pairs = future.result()\n                for qa_pair in qa_pairs:\n                    item = {\n                        \"instruction\": qa_pair.question,\n                        \"input\": \"\",\n                        \"output\": qa_pair.answer,\n                    }\n                    data.append(item)\n    else:\n        v = read_alpaca_zh()\n        data.extend(v)\n\n    if len(data) < min_samples:\n        data = data * (min_samples // len(data) + 1)\n\n    with open(f\"{dataset_dir}/data.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(data, f, indent=2, ensure_ascii=False)\n\n    with open(f\"{dataset_dir}/dataset_info.json\", \"w\", encoding=\"utf-8\") as f:\n        r = {\"data\": {\"file_name\": \"data.json\"}}\n        f.write(json.dumps(r, indent=2, ensure_ascii=False))\n\n    args = dict(\n        stage=\"sft\",\n        do_train=True,\n        model_name_or_path=model_name,\n        dataset=\"data\",\n        dataset_dir=dataset_dir,\n        cutoff_len=1024,\n        max_samples=1000000,\n        overwrite_cache=True,\n        preprocessing_num_workers=1,\n        template=\"llama3\",\n        finetuning_type=\"lora\",\n        lora_target=\"all\",\n        output_dir=f\"{loras_dir}/{name}\",\n        overwrite_output_dir=True,\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=4,\n        lr_scheduler_type=\"cosine\",\n        logging_steps=10,\n        warmup_ratio=0.1,\n        save_steps=1000,\n        plot_loss=True,\n        learning_rate=5e-5,\n        num_train_epochs=1000.0,\n        max_grad_norm=1.0,\n        quantization_bit=4,\n        loraplus_lr_ratio=16.0,\n        fp16=True,\n        ddp_timeout=180000000,\n    )\n    os.environ[\"WANDB_DISABLED\"] = \"true\"\n    from llamafactory.train import tuner\n\n    tuner.run_exp({**args, **options})\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/apps/byzer_storage/train_dpo.py\nimport json\nimport os\nfrom typing import List, Dict, Any\nimport byzerllm\nimport concurrent.futures\nfrom byzerllm.apps.byzer_storage.generate_sft_data import to_qa_pairs\n\n\ndef train_dpo(    \n    name: str,\n    memories: List[str],\n    options: Dict[str, Any],\n    dataset_dir: str,\n    loras_dir: str,\n    model_name: str,\n    data_model_name: str,\n):\n    data = []\n    min_samples = options.pop(\"min_samples\", 1000)\n\n    if memories:\n        with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:\n            futures = []\n            data_llm = byzerllm.ByzerLLM().from_default_model(data_model_name)\n            current_llm = byzerllm.ByzerLLM().from_default_model(model_name)\n            for memory in memories:\n                futures.append(executor.submit(to_qa_pairs, memory, data_llm))\n\n            for future in concurrent.futures.as_completed(futures):\n                qa_pairs = future.result()\n                for qa_pair in qa_pairs:\n                    response = current_llm.chat_oai(\n                        conversations=[{\"role\": \"user\", \"content\": qa_pair.question}]\n                    )\n                    v = response[0].output\n                    item = (\n                        {\n                            \"conversations\": [\n                                {\n                                    \"from\": \"human\",\n                                    \"value\": qa_pair.question,\n                                }\n                            ],\n                            \"chosen\": {\n                                \"from\": \"gpt\",\n                                \"value\": qa_pair.answer,\n                            },\n                            \"rejected\": {\n                                \"from\": \"gpt\",\n                                \"value\": v,\n                            },\n                        },\n                    )\n                    data.append(item)\n\n    if len(data) < min_samples:\n        data = data * (min_samples // len(data) + 1)\n\n    with open(f\"{dataset_dir}/data.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(data, f, indent=2, ensure_ascii=False)\n\n    with open(f\"{dataset_dir}/dataset_info.json\", \"w\", encoding=\"utf-8\") as f:\n        r = {\n            \"data\": {\n                \"file_name\": \"data.json\",\n                \"ranking\": True,\n                \"columns\": {\n                    \"prompt\": \"instruction\",\n                    \"query\": \"input\",\n                    \"chosen\": \"chosen\",\n                    \"rejected\": \"rejected\",\n                },\n            }\n        }\n        f.write(json.dumps(r, indent=2, ensure_ascii=False))\n\n    args = dict(\n        stage=\"dpo\",\n        do_train=True,\n        model_name_or_path=model_name,\n        dataset=\"data\",\n        dataset_dir=dataset_dir,\n        cutoff_len=1024,\n        max_samples=1000000,\n        overwrite_cache=True,\n        preprocessing_num_workers=1,\n        template=\"llama3\",\n        finetuning_type=\"lora\",\n        lora_target=\"all\",\n        output_dir=f\"{loras_dir}/{name}\",\n        overwrite_output_dir=True,\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=4,\n        lr_scheduler_type=\"cosine\",\n        logging_steps=10,\n        warmup_ratio=0.1,\n        save_steps=1000,\n        plot_loss=True,\n        learning_rate=5e-5,\n        num_train_epochs=1000.0,\n        max_grad_norm=1.0,\n        quantization_bit=4,\n        loraplus_lr_ratio=16.0,\n        fp16=True,\n        ddp_timeout=180000000,\n    )\n    os.environ[\"WANDB_DISABLED\"] = \"true\"\n    from llamafactory.train import tuner\n\n    tuner.run_exp({**args, **options})\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/apps/byzer_storage/generate_sft_data.py\nfrom typing import List, Dict, Any\nimport byzerllm\nfrom loguru import logger\nfrom pydantic import BaseModel\nfrom byzerllm.apps.utils import TagExtractor, Tag\nimport importlib\nimport json\nimport hashlib\nimport os\nfrom pathlib import Path\n\n\ndef read_alpaca_zh():\n    with importlib.resources.path(\n        \"byzerllm.apps.byzer_storage\", \"alpaca_zh.json\"\n    ) as json_path:\n        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n            return json.load(f)\n\n\nclass QAPair(BaseModel):\n    question: str\n    answer: str\n\n\n@byzerllm.prompt()\ndef _convert_pretrain_text_to_instruction(text: str, num: int = 10) -> str:\n    \"\"\"\n    根据提供的信息，生成多个相关的问题，这些问题的回答，最终要能覆盖里面所有的信息,生成 {{num}} 个。\n\n    下面是一些信息：\n\n    {{text}}\n\n    现在请开始生成问题，请将每个问题使用<_question_></_question_>标签包裹，每个回答使用<_answer_></_answer_>标签包裹，最后\n    每个一组问题和回答使用<_group_></_group_>标签包裹。\n    \"\"\"\n\n\n@byzerllm.prompt()\ndef get_more(text: str, questions_and_answers: str, num: int = 10) -> str:\n    \"\"\"\n    下面是一些问答信息：\n\n    {{text}}\n\n    下面是已有的问题和回答：\n\n    {{questions_and_answers}}\n\n    下面是已经生成的问题和回答，请继续生成{{num}}个问题和回答，不要和前面的重复，请将每个问题使用<_question_></_question_>标签包裹，每个回答使用<_answer_></_answer_>标签包裹，最后\n    每个一组问题和回答使用<_group_></_group_>标签包裹。\n    \"\"\"\n\n\ndef generate_cache_key(text: str) -> str:\n    return hashlib.md5(text.encode()).hexdigest()\n\ndef get_cache_path() -> Path:\n    home = Path.home()\n    cache_dir = home / \".byzerllm\" / \"cache\" / \"llm_generated_data\"\n    cache_dir.mkdir(parents=True, exist_ok=True)\n    return cache_dir\n\ndef save_to_cache(key: str, data: List[QAPair]):\n    cache_path = get_cache_path() / f\"{key}.json\"\n    with cache_path.open(\"w\", encoding=\"utf-8\") as f:\n        json.dump([qa.dict() for qa in data], f, ensure_ascii=False, indent=2)\n\ndef load_from_cache(key: str) -> List[QAPair]:\n    cache_path = get_cache_path() / f\"{key}.json\"\n    if cache_path.exists():\n        with cache_path.open(\"r\", encoding=\"utf-8\") as f:\n            data = json.load(f)\n        return [QAPair(**item) for item in data]\n    return None\n\ndef to_qa_pairs(text: str, llm, num: int = 30) -> List[QAPair]:\n    cache_key = generate_cache_key(text)\n    cached_result = load_from_cache(cache_key)\n    \n    if cached_result:\n        logger.info(f\"Using cached result for text: {text[:100]}...\")\n        return cached_result\n\n    logger.info(f\"Starting to generate QA pairs for text: {text[:100]}...\")\n\n    logger.info(\"Generating initial QA pairs...\")\n    v = _convert_pretrain_text_to_instruction.with_llm(llm).run(text)\n    logger.info(f\"Initial QA pairs generated. Length: {len(v)}\")\n\n    max_turns = int(num / 10)\n    if max_turns < 1:\n        max_turns = 1\n    logger.info(f\"Will generate additional QA pairs for {max_turns} turns\")\n\n    turn = 0\n    while turn < max_turns:\n        turn += 1\n        logger.info(f\"Generating additional QA pairs (turn {turn}/{max_turns})...\")\n        t = get_more.with_llm(llm).run(text, v)\n        v += t\n        logger.info(f\"Additional QA pairs generated. New total length: {len(v)}\")\n\n    logger.info(f\"Extracting QA pairs from generated text...{v}\")\n    root_tag = TagExtractor(v).extract()\n    qa_pairs = []\n    for i, item in enumerate(root_tag.content):\n        qas = item.content\n        if len(qas) == 2:\n            if (\n                qas[0].start_tag == \"<_question_>\"\n                and qas[0].end_tag == \"</_question_>\"\n                and qas[1].start_tag == \"<_answer_>\"\n                and qas[1].end_tag == \"</_answer_>\"\n            ):\n                qa_pairs.append(QAPair(question=qas[0].content, answer=qas[1].content))\n                logger.debug(\n                    f\"Extracted QA pair {i+1}: Q: {qas[0].content[:50]}... A: {qas[1].content[:50]}...\"\n                )\n            else:\n                logger.warning(\n                    f\"Skipping item {i+1} due to incorrect tags: {qas[0].start_tag}, {qas[0].end_tag}, {qas[1].start_tag}, {qas[1].end_tag}\"\n                )\n        else:\n            logger.warning(\n                f\"Skipping item {i+1} due to incorrect number of elements: {len(qas)}\"\n            )\n\n    logger.info(f\"Extracted {len(qa_pairs)} valid QA pairs.\")\n    \n    # Save the result to cache\n    save_to_cache(cache_key, qa_pairs)\n    \n    return qa_pairs\n\n\n     \n\n下面是用户的需求：\n\n_memorize 方法添加日志,可以详细追踪流程\n"}, {"role": "assistant", "content": "为了给 _memorize 方法添加详细的日志以追踪流程，我们需要对该方法进行修改。我们可以使用 Python 的 logging 模块来添加日志。下面是修改后的代码：\n\n```python\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/apps/byzer_storage/memory_model_based.py\n<<<<<<< SEARCH\nimport asyncio\nfrom typing import List, Dict, Any\n\nimport ray.experimental\nfrom byzerllm.apps.byzer_storage.simple_api import ByzerStorage\nimport time\nimport json\nimport os\nimport concurrent.futures\nimport io\nimport sys\nfrom contextlib import redirect_stdout, redirect_stderr\nimport queue\nimport threading\nimport ray\nimport byzerllm\nfrom typing import List, Union, Dict\nfrom pydantic import BaseModel\nimport importlib\nfrom loguru import logger\n=======\nimport asyncio\nfrom typing import List, Dict, Any\n\nimport ray.experimental\nfrom byzerllm.apps.byzer_storage.simple_api import ByzerStorage\nimport time\nimport json\nimport os\nimport concurrent.futures\nimport io\nimport sys\nfrom contextlib import redirect_stdout, redirect_stderr\nimport queue\nimport threading\nimport ray\nimport byzerllm\nfrom typing import List, Union, Dict\nfrom pydantic import BaseModel\nimport importlib\nfrom loguru import logger\nimport logging\n>>>>>>> REPLACE\n```\n\n```python\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/apps/byzer_storage/memory_model_based.py\n<<<<<<< SEARCH\n    def _memorize(\n        self,\n        name: str,\n        memories: List[Union[str, Dict[str, Any]]],\n        options: Dict[str, Any] = {},\n    ):\n\n        stage = options.get(\"stage\", \"pt\")\n        base_model_dir = os.path.join(self.base_dir, \"storage\", \"models\")\n        llama_model = os.path.join(\n            base_model_dir, \"meta-llama\", \"Meta-Llama-3-8B-Instruct-GPTQ\"\n        )\n\n        loras_dir = os.path.join(self.base_dir, \"storage\", \"loras\")\n        dataset_dir = os.path.join(self.base_dir, \"storage\", \"datasets\", name)\n\n        os.makedirs(loras_dir, exist_ok=True)\n        os.makedirs(dataset_dir, exist_ok=True)\n\n        if stage == \"pt\":\n            from byzerllm.apps.byzer_storage.train_pt import train_pt\n\n            train_pt(\n                name,\n                memories,\n                options,\n                dataset_dir,\n                loras_dir,\n                model_name=llama_model,\n                data_model_name=self.model_name,\n            )\n        elif stage == \"sft\":\n            from byzerllm.apps.byzer_storage.train_sft import train_sft\n\n            train_sft(\n                name,\n                memories,\n                options,\n                dataset_dir,\n                loras_dir,\n                model_name=llama_model,\n                data_model_name=self.model_name,\n            )\n        elif stage == \"dpo\":\n            from byzerllm.apps.byzer_storage.train_dpo import train_dpo\n\n            train_dpo(\n                name,\n                memories,\n                options,\n                dataset_dir,\n                loras_dir,\n                model_name=llama_model,\n                data_model_name=self.model_name,\n            )\n        else:\n            raise ValueError(f\"Unsupported stage: {stage}\")\n=======\n    def _memorize(\n        self,\n        name: str,\n        memories: List[Union[str, Dict[str, Any]]],\n        options: Dict[str, Any] = {},\n    ):\n        logging.info(f\"Starting _memorize for {name}\")\n        logging.info(f\"Memories count: {len(memories)}\")\n        logging.info(f\"Options: {options}\")\n\n        stage = options.get(\"stage\", \"pt\")\n        logging.info(f\"Selected stage: {stage}\")\n\n        base_model_dir = os.path.join(self.base_dir, \"storage\", \"models\")\n        llama_model = os.path.join(\n            base_model_dir, \"meta-llama\", \"Meta-Llama-3-8B-Instruct-GPTQ\"\n        )\n        logging.info(f\"Using model: {llama_model}\")\n\n        loras_dir = os.path.join(self.base_dir, \"storage\", \"loras\")\n        dataset_dir = os.path.join(self.base_dir, \"storage\", \"datasets\", name)\n\n        logging.info(f\"Creating directories: {loras_dir} and {dataset_dir}\")\n        os.makedirs(loras_dir, exist_ok=True)\n        os.makedirs(dataset_dir, exist_ok=True)\n\n        if stage == \"pt\":\n            logging.info(\"Starting pre-training stage\")\n            from byzerllm.apps.byzer_storage.train_pt import train_pt\n\n            train_pt(\n                name,\n                memories,\n                options,\n                dataset_dir,\n                loras_dir,\n                model_name=llama_model,\n                data_model_name=self.model_name,\n            )\n        elif stage == \"sft\":\n            logging.info(\"Starting SFT stage\")\n            from byzerllm.apps.byzer_storage.train_sft import train_sft\n\n            train_sft(\n                name,\n                memories,\n                options,\n                dataset_dir,\n                loras_dir,\n                model_name=llama_model,\n                data_model_name=self.model_name,\n            )\n        elif stage == \"dpo\":\n            logging.info(\"Starting DPO stage\")\n            from byzerllm.apps.byzer_storage.train_dpo import train_dpo\n\n            train_dpo(\n                name,\n                memories,\n                options,\n                dataset_dir,\n                loras_dir,\n                model_name=llama_model,\n                data_model_name=self.model_name,\n            )\n        else:\n            logging.error(f\"Unsupported stage: {stage}\")\n            raise ValueError(f\"Unsupported stage: {stage}\")\n\n        logging.info(f\"_memorize completed for {name}\")\n>>>>>>> REPLACE\n```\n\n这些修改添加了详细的日志记录，可以帮助你追踪 _memorize 方法的执行流程。主要的变化包括：\n\n1. 在文件顶部导入了 logging 模块。\n\n2. 在 _memorize 方法的开始添加了日志，记录了方法的开始、记忆的数量和选项。\n\n3. 记录了选择的训练阶段。\n\n4. 记录了使用的模型路径。\n\n5. 记录了创建目录的操作。\n\n6. 在每个训练阶段（pt、sft、dpo）开始前添加了日志。\n\n7. 如果遇到不支持的阶段，会记录一个错误日志。\n\n8. 在方法结束时添加了完成日志。\n\n这些日志会帮助你更好地理解方法的执行流程，包括它在什么时候开始、使用了哪些选项、选择了哪个训练阶段、使用了哪个模型、创建了哪些目录，以及每个训练阶段的开始和结束。\n\n如果你想要更详细的日志，你还可以考虑在每个训练函数（train_pt、train_sft、train_dpo）中也添加类似的日志记录。这样你就可以追踪到训练过程中的更多细节。\n\n此外，你可能还需要在类的 __init__ 方法中配置日志记录器，例如：\n\n```python\ndef __init__(self, ...):\n    ...\n    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n    ...\n```\n\n这将确保日志被正确地配置和输出。"}], "model": "human", "yaml_file": "actions/010_chat_action.yml"}
{"instruction": "train_dpo 添加合适的日志方便追踪流程", "conversations": [{"role": "user", "content": "\n如果你需要生成代码，对于每个需要更改的文件,你需要按 *SEARCH/REPLACE block* 的格式进行生成。\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The opening fence and code language, eg: ```python\n2. The file path alone on a line, starting with \"##File:\" and verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n*SEARCH/REPLACE* blocks will replace *all* matching occurrences.\nInclude enough lines to make the SEARCH blocks unique.\n\nInclude *ALL* the code being searched and replaced!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n下面我们来看一个例子：\n\n当前项目目录结构：\n1. 项目根目录： /tmp/projects/mathweb\n2. 项目子目录/文件列表(类似tree 命令输出)\nflask/\n    app.py\n    templates/\n        index.html\n    static/\n        style.css\n\n用户需求： Change get_factorial() to use math.factorial\n\n回答： To make this change we need to modify `/tmp/projects/mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n用户需求： Refactor hello() into its own file.\n\n回答：To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/hello.py\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/main.py\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n现在让我们开始一个新的任务:\n\n当前项目目录结构：\n1. 项目根目录： /Users/allwefantasy/projects/byzer-llm\n2. 项目子目录/文件列表(类似tree 命令输出)：\nsetup.py\ntests/\n    test_emb_rerank.py\n    test_deepseek_code.py\n    test_prompt.py\n    test_model_meta.py\n    __init__.py\n    test_transformers_deploy.py\n    test_data_analysis.py\n    test_yi.py\n    test_deploy.py\n    functions_suite.py\n    test_template.py\n    auto/\n        test_backend_llama_cpp.py\n        __init__.py\nsetup-machine/\n    convert_safetensor.py\nnotebooks/\n    test.py\nsrc/\n    byzerllm/\n        lang.py\n        store.py\n        version.py\n        log.py\n        __init__.py\n        types.py\n        easy_byzerllm_command.py\n        byzerllm_command.py\n        command_args.py\n        bark/\n            generation.py\n            __init__.py\n            model.py\n            api.py\n            model_fine.py\n            bark_voice.py\n        moss/\n            finetune_moss.py\n            __init__.py\n            moss_inference.py\n            models/\n                configuration_moss.py\n                quantization.py\n                custom_autotune.py\n                __init__.py\n                modeling_moss.py\n                tokenization_moss.py\n        llama2/\n            __init__.py\n        records/\n            __init__.py\n        visualglm/\n            __init__.py\n        auto/\n            __init__.py\n            backend_llama_cpp.py\n            backend_vllm.py\n            backend_ds.py\n        alpha_moss/\n            __init__.py\n        processor/\n            __init__.py\n        qwen/\n            __init__.py\n        utils/\n            json_repaire.py\n            object_store_ref_util.py\n            __init__.py\n            types.py\n            tokenizer.py\n            openai_utils.py\n            ray_utils.py\n            text_generator.py\n            emb.py\n            connect_ray.py\n            testing.py\n            langutil.py\n            nontext.py\n            fulltune/\n                deepspeed_trainner.py\n                __init__.py\n                launch.py\n                trainner.py\n                base_model/\n                    __init__.py\n                    configuration_baichuan.py\n                    modeling_baichuan.py\n                pretrain/\n                    __init__.py\n                    convert_to_transformers.py\n            metrics/\n                __init__.py\n            config/\n                __init__.py\n            retrieval/\n                rest.py\n                __init__.py\n                udf.py\n            sft/\n                argument.py\n                collator.py\n                qlora.py\n                __init__.py\n                model.py\n                dataset.py\n                loss.py\n                merge_lora.py\n                trainer.py\n            client/\n                message_utils.py\n                math_utils.py\n                byzerllm_client.py\n                parallel_utils.py\n                __init__.py\n                types.py\n                code_utils.py\n                img_utils.py\n                entrypoints/\n                    __init__.py\n                    openai/\n                        protocol.py\n                        __init__.py\n                        serve.py\n                        serving_chat.py\n                        api_server.py\n                        serving_engine.py\n                        serving_completion.py\n                        tool.py\n        chatglm2/\n            __init__.py\n        baichuan/\n            __init__.py\n        m3e/\n            __init__.py\n        stable_diffusion/\n            config.py\n            __init__.py\n            logger.py\n            shared.py\n            model.py\n            utils.py\n            images.py\n            diffusion/\n                __init__.py\n                embeddings.py\n                utils.py\n                networks/\n                    lyco.py\n                    __init__.py\n                upscalers/\n                    __init__.py\n                    multidiffusion.py\n                    samplers.py\n                piplines/\n                    __init__.py\n                    lpw.py\n                    diffusers.py\n            lib/\n                __init__.py\n                diffusers/\n                    __init__.py\n                    scheduler.py\n            api/\n                __init__.py\n                plugin.py\n                models/\n                    diffusion.py\n                    __init__.py\n                diffusion/\n                    __init__.py\n                    pipelines/\n                        __init__.py\n                        diffusers.py\n                events/\n                    generation.py\n                    __init__.py\n        bge/\n            __init__.py\n        bge_rerank/\n            __init__.py\n        apps/\n            client.py\n            qa_strategy.py\n            __init__.py\n            builder.py\n            vector_db.py\n            utils.py\n            qa.py\n            byzer_storage/\n                memory_model_based.py\n                command.py\n                env.py\n                train_pt.py\n                train_sft.py\n                __init__.py\n                train_dpo.py\n                simple_api.py\n                generate_sft_data.py\n            byzer_sql/\n                __init__.py\n            llama_index/\n                byzerai_vectordb.py\n                byzerai.py\n                byzerai_embedding.py\n                __init__.py\n                collection_manager.py\n                byzerai_index_store.py\n                byzerai_docstore.py\n                simple_retrieval.py\n                byzerai_kvstore.py\n            agent/\n                conversable_agent.py\n                registry.py\n                groupchat.py\n                user_proxy_agent.py\n                __init__.py\n                agent.py\n                extensions/\n                    simple_retrieval_client.py\n                    planner.py\n                    python_codesandbox_agent.py\n                    preview_file_agent.py\n                    llama_index_retrieval_agent.py\n                    visualization_agent.py\n                    assistant_agent.py\n                    load_data_agent.py\n                    retrieval_agent.py\n                    output_agent.py\n                    __init__.py\n                    byzer_engine_agent.py\n                    subquestion_agent.py\n                    sql_reviewer_agent.py\n                    spark_sql_agent.py\n                    rhetorical_agent.py\n                    data_analysis.py\n                    data_analysis_pipeline_agent.py\n                    common_agent.py\n                    query_rewrite/\n                        time.py\n                        rhetorical.py\n                        __init__.py\n                        condition.py\n                        context.py\n                store/\n                    memory_store.py\n                    __init__.py\n                    stores.py\n        saas/\n            __init__.py\n            chatglm/\n                __init__.py\n            sparkdesk/\n                __init__.py\n            gemini/\n                __init__.py\n            azure_openai/\n                __init__.py\n            azure/\n                __init__.py\n            qianfan/\n                __init__.py\n            claude/\n                __init__.py\n            qianwen/\n                __init__.py\n            volcengine/\n                __init__.py\n            minimax/\n                __init__.py\n            baichuan/\n                __init__.py\n            qianwen_vl/\n                __init__.py\n            official_openai/\n                __init__.py\n            zhipu/\n                __init__.py\n            aws_bedrock/\n                __init__.py\n            openai/\n                __init__.py\n        falcon/\n            __init__.py\n        chatglm6b/\n            trainer_seq2seq.py\n            arguments.py\n            __init__.py\n            finetune.py\n            tunning/\n                __init__.py\n                infer.py\n                finetune.py\n                utils/\n                    config.py\n                    __init__.py\n                    other.py\n                    common.py\n                    seq2seq.py\n        starcode/\n            __init__.py\n        qwen_vl_chat/\n            __init__.py\n        zephyr/\n            __init__.py\n        dolly/\n            generate.py\n            __init__.py\n            consts.py\n            dolly_inference.py\n            trainer.py\n        llama/\n            __init__.py\n        whisper/\n            whisper_inference.py\n            __init__.py\n下面是一些文件路径以及每个文件对应的源码：\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/apps/byzer_storage/memory_model_based.py\nimport asyncio\nfrom typing import List, Dict, Any\n\nimport ray.experimental\nfrom byzerllm.apps.byzer_storage.simple_api import ByzerStorage\nimport time\nimport json\nimport os\nimport concurrent.futures\nimport io\nimport sys\nfrom contextlib import redirect_stdout, redirect_stderr\nimport queue\nimport threading\nimport ray\nimport byzerllm\nfrom typing import List, Union, Dict\nfrom pydantic import BaseModel\nimport importlib\nfrom loguru import logger\n\n\nclass MemoryManager:\n    _queue = asyncio.Queue()\n    _is_processing = False\n\n    def __init__(\n        self,\n        storage: ByzerStorage,\n        base_dir: str,\n        model_name: str = \"deepseek_chat\",\n        remote: bool = True,\n    ):\n        self.storage = storage\n        home = os.path.expanduser(\"~\")\n        self.base_dir = base_dir or os.path.join(home, \".auto-coder\")\n        self.thread_pool = concurrent.futures.ThreadPoolExecutor(max_workers=5)\n        self.log_file = None\n        self.remote = remote\n        self.model_name = model_name\n\n    @classmethod\n    async def add_to_queue(cls, name: str, memories: List[str]):\n        await cls._queue.put((name, memories))\n        if not cls._is_processing:\n            asyncio.create_task(cls.process_queue())\n\n    @classmethod\n    async def process_queue(cls):\n        cls._is_processing = True\n        while not cls._queue.empty():\n            name, memories = await cls._queue.get()\n            instance = cls.get_instance()\n            await instance.memorize(name, memories)\n            cls._queue.task_done()\n        cls._is_processing = False\n\n    async def memorize(\n        self, name: str, memories: List[str], options: Dict[str, Any] = {}\n    ):\n        try:\n            loop = asyncio.get_running_loop()\n            await loop.run_in_executor(\n                self.thread_pool, self._memorize_with_logs, name, memories, options\n            )\n        finally:\n            if self.remote:\n                ray.actor.exit_actor()\n\n        print(f\"Memorization for {name} completed.\")\n\n    def _memorize_with_logs(\n        self, name: str, memories: List[str], options: Dict[str, Any] = {}\n    ):\n\n        if self.remote:\n            self._memorize(name, memories, options=options)\n            return\n\n        logs_dir = os.path.join(self.base_dir, \"storage\", \"logs\", \"memorize\")\n        os.makedirs(logs_dir, exist_ok=True)\n        log_file = os.path.join(logs_dir, f\"{name}.log\")\n\n        log_queue = queue.Queue()\n        stop_event = threading.Event()\n\n        # Start the log writer thread\n        log_writer_thread = threading.Thread(\n            target=self._log_writer, args=(log_queue, log_file, stop_event)\n        )\n        self.log_file = log_file\n        log_writer_thread.start()\n\n        class QueueStream:\n            def __init__(self, queue):\n                self.queue = queue\n\n            def write(self, msg):\n                self.queue.put(msg)\n\n            def flush(self):\n                pass\n\n        queue_stream = QueueStream(log_queue)\n\n        with redirect_stdout(queue_stream), redirect_stderr(queue_stream):\n            self._memorize(name, memories)\n\n        # Signal the log writer to stop and wait for it to finish\n        stop_event.set()\n        log_writer_thread.join()\n\n    def _log_writer(\n        self, log_queue: queue.Queue, log_file: str, stop_event: threading.Event\n    ):\n        with open(log_file, \"w\") as f:\n            while not stop_event.is_set() or not log_queue.empty():\n                try:\n                    msg = log_queue.get(timeout=0.1)\n                    f.write(msg)\n                    f.flush()\n                except queue.Empty:\n                    continue\n\n    def _memorize(\n        self,\n        name: str,\n        memories: List[Union[str, Dict[str, Any]]],\n        options: Dict[str, Any] = {},\n    ):\n        logger.info(f\"Starting _memorize for {name}\")\n        logger.info(f\"Memories count: {len(memories)}\")\n        logger.info(f\"Options: {options}\")\n\n        stage = options.get(\"stage\", \"pt\")\n        logger.info(f\"Selected stage: {stage}\")\n\n        base_model_dir = os.path.join(self.base_dir, \"storage\", \"models\")\n        llama_model = os.path.join(\n            base_model_dir, \"meta-llama\", \"Meta-Llama-3-8B-Instruct-GPTQ\"\n        )\n        logger.info(f\"Using model: {llama_model}\")\n\n        loras_dir = os.path.join(self.base_dir, \"storage\", \"loras\")\n        dataset_dir = os.path.join(self.base_dir, \"storage\", \"datasets\", name)\n\n        logger.info(f\"Creating directories: {loras_dir} and {dataset_dir}\")\n        os.makedirs(loras_dir, exist_ok=True)\n        os.makedirs(dataset_dir, exist_ok=True)\n\n        if stage == \"pt\":\n            logger.info(\"Starting pre-training stage\")\n            from byzerllm.apps.byzer_storage.train_pt import train_pt\n\n            train_pt(\n                name,\n                memories,\n                options,\n                dataset_dir,\n                loras_dir,\n                model_name=llama_model,\n                data_model_name=self.model_name,\n            )\n        elif stage == \"sft\":\n            logger.info(\"Starting SFT stage\")\n            from byzerllm.apps.byzer_storage.train_sft import train_sft\n\n            train_sft(\n                name,\n                memories,\n                options,\n                dataset_dir,\n                loras_dir,\n                model_name=llama_model,\n                data_model_name=self.model_name,\n            )\n        elif stage == \"dpo\":\n            logger.info(\"Starting DPO stage\")\n            from byzerllm.apps.byzer_storage.train_dpo import train_dpo\n\n            train_dpo(\n                name,\n                memories,\n                options,\n                dataset_dir,\n                loras_dir,\n                model_name=llama_model,\n                data_model_name=self.model_name,\n            )\n        else:\n            logger.error(f\"Unsupported stage: {stage}\")\n            raise ValueError(f\"Unsupported stage: {stage}\")\n\n        logger.info(f\"_memorize completed for {name}\")\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/docs/zh/003_byzerllm_大模型编程快速指南.md\n##  byzerllm 大模型编程快速指南\n\n本文示例 [Notebook](../../notebooks/003_byzerllm_大模型编程快速指南.ipynb)\n\n## 安装\n\n```bash\npip install -U byzerllm\nray start --head\n```\n## 启动一个模型代理\n\nbyzerllm 支持私有化模型或者SaaS模型的部署。\n\n这里以 deepseek 官方API 为例：\n\n```bash\neasy-byzerllm deploy deepseek-chat --token xxxxx --alias deepseek_chat\n```\n\n或者跬基流动API:\n\n```bash\neasy-byzerllm deploy alibaba/Qwen1.5-110B-Chat --token xxxxx --alias qwen110b_chat\n```\n\n将上面的 API KEY 替换成你们自己的。\n\n如果你想部署私有化模型或者对接 Ollama 等更多需求，参考 [002_使用byzerllm进行模型部署.md](./002_使用byzerllm进行模型部署.md) 或者 [README.md](../../README.md)。\n\n之后，你就可以在代码里使用  deepseek_chat 或者 qwen110b_chat  访问模型了。\n\n## hello world\n\n来和我们的大模型打个招呼:\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\n@byzerllm.prompt(llm=llm)\ndef hello(q:str) ->str:\n    '''\n    你好, {{ q }}\n    '''\n\ns = hello(\"你是谁\")    \nprint(s)\n\n## 输出:\n## '你好！我是一个人工智能助手，专门设计来回答问题、提供信息和帮助解决问题。如果你有任何疑问或需要帮助，请随时告诉我。'\n```\n\n恭喜，你和大模型成功打了招呼！\n\n可以看到，我们通过 `@byzerllm.prompt` 装饰器，将一个方法转换成了一个大模型的调用，然后这个方法的主题是一段文本，文本中\n使用了 jinja2 模板语法，来获得方法的参数。当正常调用该方法时，实际上就发起了和大模型的交互，并且返回了大模型的结果。\n\n在 byzerllm 中，我们把这种方法称之为 prompt 函数。\n\n## 查看发送给大模型的prompt\n\n很多情况你可能需要调试，查看自己的 prompt 渲染后到底是什么样子的，这个时候你可以通过如下方式\n获取渲染后的prompt:\n\n```python\nhello.prompt(\"你是谁\")\n## '你好, 你是谁'\n```            \n\n## 动态换一个模型\n\n前面的 hello 方法在初始化的时候，我们使用了默认的模型 deepseek_chat，如果我们想换一个模型，可以这样做：\n\n```python\nhello.with_llm(llm).run(\"你是谁\")\n## '你好！我是一个人工智能助手，专门设计来回答问题、提供信息和帮助解决问题。如果你有任何疑问或需要帮助，请随时告诉我。'\n```\n\n通过 with_llm 你可以设置一个新的 llm 对象，然后调用 run 方法，就可以使用新的模型了。\n\n## 超长文本生成\n\n我们知道，大模型一次生成的长度其实是有限的，如果你想生成超长文本，你可能需手动的不断获得\n生成结果，然后把他转化为输入，然后再次生成，这样的方式是比较麻烦的。\n\nbyzerllm 提供了更加易用的 API :\n\n```python\nimport byzerllm\nfrom byzerllm import ByzerLLM\n\nllm = ByzerLLM.from_default_model(\"deepseek_chat\")\n\n@byzerllm.prompt()\ndef tell_story() -> str:\n    \"\"\"\n    讲一个100字的故事。    \n    \"\"\"\n\n\ns = (\n    tell_story.with_llm(llm)\n    .with_response_markers()\n    .options({\"llm_config\": {\"max_length\": 10}})\n    .run()\n)\nprint(s)\n\n## 从前，森林里住着一只聪明的小狐狸。一天，它发现了一块闪闪发光的宝石。小狐狸决定用这块宝石帮助森林里的动物们。它用宝石的光芒指引迷路的小鹿找到了回家的路，用宝石的温暖治愈了受伤的小鸟。从此，小狐狸成了森林里的英雄，动物们都感激它的善良和智慧。\n```\n\n实际核心部分就是这一行：\n\n```python\ntell_story.with_llm(llm)\n    .with_response_markers()    \n    .run()\n```\n\n我们只需要调用 `with_response_markers` 方法，系统就会自动的帮我们生成超长文本。\n在上面的案例中，我们通过\n\n```python\n.options({\"llm_config\": {\"max_length\": 10}})\n```\n\n认为的限制大模型一次交互最多只能输出10个字符，但是系统依然自动完成了远超过10个字符的文本生成。\n\n## 对象输出\n\n前面我们的例子都是返回字符串，但是我们也可以返回对象，这样我们就可以更加灵活的处理返回结果。\n\n```python\nimport pydantic \n\nclass Story(pydantic.BaseModel):\n    '''\n    故事\n    '''\n\n    title: str = pydantic.Field(description=\"故事的标题\")\n    body: str = pydantic.Field(description=\"故事主体\")\n\n@byzerllm.prompt()\ndef tell_story()->Story:\n    '''\n    讲一个100字的故事。    \n    '''\n\ns = tell_story.with_llm(llm).run()\nprint(isinstance(s, Story))\nprint(s.title)\n\n## True\n## 勇敢的小鸟\n```\n\n可以看到，我们很轻松的将输出转化为格式化输出。\n\n## 自定义字段抽取\n\n前面的结构化输出，其实会消耗更多token,还有一种更加精准的结构化输出方式。\n比如让大模型生成一个正则表达式，但实际上大模型很难准确只输出一个正则表达式，这个时候我们可以通过自定义抽取函数来获取我们想要的结果。\n\n\n```python\nfrom loguru import logger\nimport re\n\n@byzerllm.prompt()\ndef generate_regex_pattern(desc: str) -> str:\n    \"\"\"\n    根据下面的描述生成一个正则表达式，要符合python re.compile 库的要求。\n\n    {{ desc }}\n\n    最后生成的正则表达式要在<REGEX></REGEX>标签对里。\n    \"\"\"    \n\ndef extract_regex_pattern(regex_block: str) -> str:    \n    pattern = re.search(r\"<REGEX>(.*)</REGEX>\", regex_block, re.DOTALL)\n    if pattern is None:\n        logger.warning(\"No regex pattern found in the generated block:\\n {regex_block}\")\n        raise None\n    return pattern.group(1)\n\npattern = \"匹配一个邮箱地址\"\nv = generate_regex_pattern.with_llm(llm).with_extractor(extract_regex_pattern).run(desc=pattern)\nprint(v)\n## ^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\n```\n\n在上面的例子里，我们根据一句话生成一个正则表达式。我们通过 `with_extractor` 方法，传入了一个自定义的抽取函数，这个函数会在大模型生成结果后，对结果进行处理，然后返回我们想要的结果。\n\n我们在 prompt 明确说了，生成的结果要放到 `<REGEX></REGEX>` 标签对里，然后我们通过 extract_regex_pattern 函数，从结果中提取出了我们想要的正则表达式。\n\n## 在实例方法中使用大模型\n\n```python\nimport byzerllm\ndata = {\n    'name': 'Jane Doe',\n    'task_count': 3,\n    'tasks': [\n        {'name': 'Submit report', 'due_date': '2024-03-10'},\n        {'name': 'Finish project', 'due_date': '2024-03-15'},\n        {'name': 'Reply to emails', 'due_date': '2024-03-08'}\n    ]\n}\n\n\nclass RAG():\n    def __init__(self):        \n        self.llm = byzerllm.ByzerLLM()\n        self.llm.setup_template(model=\"deepseek_chat\",template=\"auto\")\n        self.llm.setup_default_model_name(\"deepseek_chat\")        \n    \n    @byzerllm.prompt(lambda self:self.llm)\n    def generate_answer(self,name,task_count,tasks)->str:\n        '''\n        Hello {{ name }},\n\n        This is a reminder that you have {{ task_count }} pending tasks:\n        {% for task in tasks %}\n        - Task: {{ task.name }} | Due: {{ task.due_date }}\n        {% endfor %}\n\n        Best regards,\n        Your Reminder System\n        '''        \n\nt = RAG()\n\nresponse = t.generate_answer(**data)\nprint(response)\n\n## 输出:\n## Hello Jane Doe,\n##I hope this message finds you well. I wanted to remind you of your 3 pending tasks to ensure you stay on track:\n## 1. **Submit report** - This task is due on **2024-03-10**. Please ensure that you allocat\n```\n\n这里我们给了个比较复杂的例子，但我们可以看到，给一个实例prompt方法和普通prompt 方法差异不大。\n唯一的区别是如果你希望在定义的时候就指定大模型，使用一个lambda函数返回实例的 llm 对象即可。\n\n```python\n@byzerllm.prompt(lambda self:self.llm)\n```\n\n你也可以不返回，在调用的时候通过 `with_llm` 方法指定 llm 对象。\n\n此外，这个例子也展示了如何通过jinja2模板语法，来处理复杂的结构化数据。\n\n## 通过 Python 代码处理复杂入参\n\n上面的一个例子中，我们通过 jinja2 模板语法，来处理复杂的结构化数据，但是有时候我们可能需要更加复杂的处理，这个时候我们可以通过 Python 代码来处理。\n\n```python\nimport byzerllm\n\ndata = {\n    'name': 'Jane Doe',\n    'task_count': 3,\n    'tasks': [\n        {'name': 'Submit report', 'due_date': '2024-03-10'},\n        {'name': 'Finish project', 'due_date': '2024-03-15'},\n        {'name': 'Reply to emails', 'due_date': '2024-03-08'}\n    ]\n}\n\n\nclass RAG():\n    def __init__(self):        \n        self.llm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n    \n    @byzerllm.prompt()\n    def generate_answer(self,name,task_count,tasks)->str:\n        '''\n        Hello {{ name }},\n\n        This is a reminder that you have {{ task_count }} pending tasks:\n            \n        {{ tasks }}\n\n        Best regards,\n        Your Reminder System\n        '''\n        \n        tasks_str = \"\\n\".join([f\"- Task: {task['name']} | Due: { task['due_date'] }\" for task in tasks])\n        return {\"tasks\": tasks_str}\n\nt = RAG()\n\nresponse = t.generate_answer.with_llm(t.llm).run(**data)\nprint(response)\n\n## Just a gentle nudge to keep you on track with your pending tasks. Here's a quick recap:....\n```\n\n在这个例子里，我们直接把 tasks 在方法体里进行处理，然后作为一个字符串返回，最够构建一个字典，字典的key为 tasks,然后\n你就可以在 docstring 里使用 `{{ tasks }}` 来引用这个字符串。\n\n这样对于很复杂的入参，就不用谢繁琐的 jinja2 模板语法了。\n\n## 如何自动实现一个方法\n\n比如我定义一个签名，但是我不想自己实现里面的逻辑，让大模型来实现。这个在 byzerllm 中叫 function impl。我们来看看怎么\n实现:\n\n```python\nimport pydantic\nclass Time(pydantic.BaseModel):\n    time: str = pydantic.Field(...,description=\"时间，时间格式为 yyyy-MM-dd\")\n\n\n@llm.impl()\ndef calculate_current_time()->Time:\n    '''\n    计算当前时间\n    '''\n    pass \n\n\ncalculate_current_time()\n#output: Time(time='2024-06-14')\n```\n\n在这个例子里，我们定义了一个 calculate_current_time 方法，但是我们没有实现里面的逻辑，我们通过 `@llm.impl()` 装饰器，让大模型来实现这个方法。\n为了避免每次都要“生成”这个方法，导致无法适用，我们提供了缓存，用户可以按如下方式打印速度：\n\n```python\nstart = time.monotonic()\ncalculate_current_time()\nprint(f\"first time cost: {time.monotonic()-start}\")\n\nstart = time.monotonic()\ncalculate_current_time()\nprint(f\"second time cost: {time.monotonic()-start}\")\n\n# output:\n# first time cost: 6.067266260739416\n# second time cost: 4.347506910562515e-05\n```\n可以看到，第一次执行花费了6s,第二次几乎是瞬间完成的，这是因为第一次执行的时候，我们实际上是在生成这个方法，第二次执行的时候，我们是执行已经生成好的代码，所以速度会非常快。你可以显示的调用 `llm.clear_impl_cache()` 清理掉函数缓存。\n\n## Stream 模式\n\n前面的例子都是一次性生成结果，但是有时候我们可能需要一个流式的输出，这个时候我们可能需要用底层一点的API来完成了：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\nv = llm.stream_chat_oai(model=\"deepseek_chat\",conversations=[{\n    \"role\":\"user\",\n    \"content\":\"你好，你是谁\",\n}],delta_mode=True)\n\nfor t in v:\n    print(t,flush=True)  \n\n# 你好\n# ！\n# 我\n# 是一个\n# 人工智能\n# 助手\n# ，\n# 旨在\n# 提供\n# 信息\n# 、\n# 解答\n# 问题....\n```\n\n如果你不想要流式输出，但是想用底层一点的API，你可以使用 `llm.chat_oai` 方法：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\nv = llm.chat_oai(model=\"deepseek_chat\",conversations=[{\n    \"role\":\"user\",\n    \"content\":\"你好，你是谁\",\n}])\n\nprint(v[0].output)\n## 你好！我是一个人工智能助手，旨在提供信息、解答问题和帮助用户解决问题。如果你有任何问题或需要帮助，请随时告诉我。\n```\n\n## Function Calling \n\nbyzerllm 可以不依赖模型自身就能提供 function calling 支持，我们来看个例子：\n\n\n```python\nfrom typing import List,Dict,Any,Annotated\nimport pydantic \nimport datetime\nfrom dateutil.relativedelta import relativedelta\n\ndef compute_date_range(count:Annotated[int,\"时间跨度，数值类型\"],\n                       unit:Annotated[str,\"时间单位，字符串类型\",{\"enum\":[\"day\",\"week\",\"month\",\"year\"]}])->List[str]:\n    '''\n    计算日期范围\n\n    Args:\n        count: 时间跨度，数值类型\n        unit: 时间单位，字符串类型，可选值为 day,week,month,year\n    '''        \n    now = datetime.datetime.now()\n    now_str = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n    if unit == \"day\":\n        return [(now - relativedelta(days=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"week\":\n        return [(now - relativedelta(weeks=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"month\":\n        return [(now - relativedelta(months=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"year\":\n        return [(now - relativedelta(years=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    return [\"\",\"\"]\n\ndef compute_now()->str:\n    '''\n    计算当前时间\n    '''\n    return datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n```\n\n我们定义了两个方法，一个是计算日期范围，一个是计算当前时间。\n\n现在我么可以来测试下，系统如何根据自然语言决定调用哪个方法：\n\n```python\nt = llm.chat_oai([{\n    \"content\":'''计算当前时间''',\n    \"role\":\"user\"    \n}],tools=[compute_date_range,compute_now],execute_tool=True)\n\nt[0].values\n\n## output: ['2024-06-14 15:18:02']\n```\n\n我们可以看到，他正确的选择了 compute_now 方法。\n\n接着我们再试一个：\n\n```python\nt = llm.chat_oai([{\n    \"content\":'''最近三个月趋势''',\n    \"role\":\"user\"    \n}],tools=[compute_date_range,compute_now],execute_tool=True)\n\nt[0].values\n\n## output: [['2024-03-14 15:19:13', '2024-06-14 15:19:13']]\n```\n\n模型正确的选择了 compute_date_range 方法。\n\n## 多模态\n\nbyerllm 也能很好的支持多模态的交互，而且统一了多模态大模型的接口，比如你可以用一样的方式使用 openai 或者 claude 的图片转文字能力， 或者一致的方式使用火山，azuer, openai的语音合成接口。\n\n### 图生文\n\n```python\nimport byzerllm\nfrom byzerllm.types import ImagePath\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\n\n\n@byzerllm.prompt()\ndef what_in_image(image_path: ImagePath) -> str:\n    \"\"\"\n    {{ image_path }}\n    这个图片里有什么？\n    \"\"\"    \n\n\nv = what_in_image.with_llm(vl_llm).run(\n    ImagePath(value=\"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\")\n)\nv\n## OUTPUT: 这张图片展示了多只可爱的猫咪，采用了艺术风格的绘画。猫咪们有不同的颜色和花纹，背景是浅棕色，上面还点缀着一些红色的花朵。整体画面给人一种温馨和谐的感觉\n```\n\n可以看到，我们只需要把 prompt 函数的图片地址入参使用 byzerllm.types.ImagePath里进行包装，就可以直接在 prompt 函数体里\n带上图片。\n\n或者你可以这样：\n\n```python\nimport byzerllm\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\n\n\n@byzerllm.prompt()\ndef what_in_image(image_path: str) -> str:\n    \"\"\"\n    {{ image }}\n    这个图片里有什么？\n    \"\"\"\n    return {\"image\": byzerllm.Image.load_image_from_path(image_path)}\n\n\nv = what_in_image.with_llm(vl_llm).run(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv\n```\n\n通过 `image_path` 参数，然后通过 `byzerllm.Image.load_image_from_path` 方法，转化为一个图片对象 image，最后在 prompt 函数体里\n使用 `{{ image }}` 引用这个图片对象。\n\n另外我们也是可以支持配置多张图片的。\n\n另外，我们也可以使用基础的 `llm.chat_oai` 方法来实现：\n\n```python\nimport byzerllm\nimport json\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\nimage = byzerllm.Image.load_image_from_path(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv = vl_llm.chat_oai(\n    conversations=[\n        {\n            \"role\": \"user\",\n            \"content\": json.dumps(\n                [{\"image\": image, \"text\": \"这个图片里有什么？\"}], ensure_ascii=False\n            ),\n        }\n    ]\n)\nv[0].output\n```\n\n还可以这么写：\n\n```python\nimport byzerllm\nimport json\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\nimage = byzerllm.Image.load_image_from_path(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv = vl_llm.chat_oai(\n    conversations=[\n        {\n            \"role\": \"user\",\n            \"content\": json.dumps(\n                [\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\"url\": image, \"detail\": \"high\"},\n                    },\n                    {\"text\": \"这个图片里有什么？\", \"type\": \"text\"},\n                ],\n                ensure_ascii=False,\n            ),\n        }\n    ]\n)\nv[0].output\n```\n\n### 语音合成\n\n这里以 openai 的 tts 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=tts-1 \\\n--model openai_tts\n```\n\n此外，byzerllm 支持 azure,火山引擎等 tts 语音合成引擎。\n\n接着你可以这么用：\n\n```python\nimport byzerllm\nimport base64\nimport json\n\nllm = byzerllm.ByzerLLM.from_default_model(\"openai_tts\")\n\n\nt = llm.chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\": json.dumps({\n        \"input\":\"hello, open_tts\",\n        \"voice\": \"alloy\",\n        \"response_format\": \"mp3\"\n    },ensure_ascii=False)\n}])\n\nwith open(\"voice.mp3\",\"wb\") as f:\n    f.write(base64.b64decode(t[0].output))\n```\n\ntts 模型生成没有prompt函数可以用，你需要直接使用 chat_oai。\n\n\n### 语音识别\n\n这里以 openai 的 whisper-1 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--worker_concurrency 10 \\\n--infer_params saas.model=whisper-1 saas.api_key=${MODEL_OPENAI_TOKEN} \\\n--model speech_to_text\n```\n\n语音识别的使用方式和图生文类似，我们可以直接在 prompt 函数体里带上音频文件。\n\n```python\nimport byzerllm\nimport json\nimport base64\nfrom byzerllm.types import AudioPath\n\nllm = byzerllm.ByzerLLM.from_default_model(\"speech_to_text\")\n\naudio_file = \"/Users/allwefantasy/videos/output_audio.mp3\"\n\n@byzerllm.prompt(llm=llm)\ndef audio_to_text(audio_file: AudioPath):\n    \"\"\"\n    {{ audio_file }}\n    \"\"\"\n\nv = audio_to_text(AudioPath(value=audio_file))\njson.loads(v)\n```\n输出的数据格式略微复杂：\n\n```\n{'text': 'In the last chapter, you and I started to step through the internal workings of a transformer. This is one of the key pieces of technology inside large language models, and a lot of other tools in the modern wave of AI.',\n 'task': 'transcribe',\n 'language': 'english',\n 'duration': 10.0,\n 'segments': [{'id': 0,\n   'seek': 0,\n   'start': 0.0,\n   'end': 4.78000020980835,\n   'text': ' In the last chapter, you and I started to step through the internal workings of a transformer.',\n   'tokens': [50364,\n    682,\n    264,\n    1036,\n    7187,\n    11,\n    291,\n    293,\n    286,\n    1409,\n    281,\n    1823,\n    807,\n    264,\n    6920,\n    589,\n    1109,\n    295,\n    257,\n    31782,\n    13,\n    50586],\n   'temperature': 0.0,\n   'avg_logprob': -0.28872039914131165,\n   'compression_ratio': 1.4220778942108154,\n   'no_speech_prob': 0.016033057123422623},\n  ....\n  {'id': 2,\n   'seek': 0,\n   'start': 8.579999923706055,\n   'end': 9.979999542236328,\n   'text': ' and a lot of other tools in the modern wave of AI.',\n   'tokens': [50759,\n    293,\n    257,\n    688,\n    295,\n    661,\n    3873,\n    294,\n    264,\n    4363,\n    5772,\n    295,\n    7318,\n    13,\n    50867],\n   'temperature': 0.0,\n   'avg_logprob': -0.28872039914131165,\n   'compression_ratio': 1.4220778942108154,\n   'no_speech_prob': 0.016033057123422623}],\n 'words': [{'word': 'In', 'start': 0.0, 'end': 0.18000000715255737},\n  {'word': 'the', 'start': 0.18000000715255737, 'end': 0.23999999463558197},\n  {'word': 'last', 'start': 0.23999999463558197, 'end': 0.5400000214576721},\n  {'word': 'chapter', 'start': 0.5400000214576721, 'end': 0.800000011920929},\n  ....\n  {'word': 'AI', 'start': 9.920000076293945, 'end': 9.979999542236328}]}\n```\n\n会输出每一句话以及每一个字所在的起始时间和截止时间。你可以根据需要来使用。\n\n### 文生图\n\n文生图和语音合成类似，首先要启动合适的模型,以openai 的 dall-e-3 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=dall-e-3 \\\n--model openai_image_gen\n```\n\n启动模型后，只需要记住几个模板参数即可使用，这里直接使用 chat_oai 方法来使用：\n\n```python\n\nimport byzerllm\nimport json\nimport base64\n\nllm = byzerllm.ByzerLLM.from_default_model(\"openai_image_gen\")\nt = llm.chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\": json.dumps({\n        \"input\":\"a white siamese cat\",\n        \"size\": \"1024x1024\",\n        \"quality\": \"standard\"\n    },ensure_ascii=False)\n}])\n\nwith open(\"output1.jpg\",\"wb\") as f:\n    f.write(base64.b64decode(t[0].output))\n\n\nimport matplotlib.pyplot as plt\n\nimage_path = \"output1.jpg\"\nimage = plt.imread(image_path)\n\nplt.imshow(image)\nplt.axis('off')\nplt.show()\n```\n\n## Prompt 函数的流式输出\n\nbyzerllm 底层支持流式输出，非 prompt 函数的用法是这样的：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\n\nv = llm.stream_chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\":\"讲一个100字的故事\"\n}])\n\nfor s  in v:\n    print(s[0], end=\"\")\n```\n\n如果你像用 prompt 函数，可以这么用：\n\n\n```python\nimport byzerllm\nimport json\nimport base64\nfrom typing import Generator\n\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\n\n@byzerllm.prompt()\ndef tell_story() -> Generator[str, None, None]:\n    '''\n    给我讲一个一百多字的故事\n    '''\n\nv = tell_story.with_llm(llm).run()    \nfor i in v:\n    print(i, end=\"\")\n```\n\n可以看到，和普通的 prompt 函数的区别在于，返回值是一个生成器，然后你可以通过 for 循环来获取结果。\n\n## 注意事项\n\n1. prompt函数方法体返回只能是dict，实际的返回类型和方法签名可以不一样，但是方法体返回只能是dict。\n2. 大部分情况prompt函数体为空，如果一定要有方法体，可以返回一个空字典。\n3. 调用prompt方法的时候，如果在@byzerllm.prompt()里没有指定llm对象，那么需要在调用的时候通过with_llm方法指定llm对象。\n\n\n\n\n\n\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/apps/byzer_storage/train_pt.py\nimport json\nimport os\nfrom typing import List, Dict, Any\n\n\ndef train_pt(\n    name: str,\n    memories: List[str],\n    options: Dict[str, Any],\n    dataset_dir: str,\n    loras_dir: str,\n    data_model_name: str,\n    model_name: str,\n):\n    data = []\n    min_samples = options.pop(\"min_samples\", 1000)\n\n    for memory in memories:\n        item = {\n            \"text\": memory,\n        }\n        data.append(item)\n\n    if len(data) < min_samples:\n        data = data * (min_samples // len(data) + 1)\n\n    with open(f\"{dataset_dir}/data.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(data, f, indent=2, ensure_ascii=False)\n\n    with open(f\"{dataset_dir}/dataset_info.json\", \"w\", encoding=\"utf-8\") as f:\n        r = {\"data\": {\"file_name\": \"data.json\", \"columns\": {\"prompt\": \"text\"}}}\n        f.write(json.dumps(r, indent=2, ensure_ascii=False))\n\n    args = dict(\n        stage=\"pt\",\n        do_train=True,\n        model_name_or_path=model_name,\n        dataset=\"data\",\n        dataset_dir=dataset_dir,\n        cutoff_len=1024,\n        max_samples=1000000,\n        overwrite_cache=True,\n        preprocessing_num_workers=1,\n        template=\"llama3\",\n        finetuning_type=\"lora\",\n        lora_target=\"all\",\n        output_dir=f\"{loras_dir}/{name}\",\n        overwrite_output_dir=True,\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=4,\n        lr_scheduler_type=\"cosine\",\n        logging_steps=10,\n        warmup_ratio=0.1,\n        save_steps=1000,\n        plot_loss=True,\n        learning_rate=5e-5,\n        num_train_epochs=1000.0,\n        max_grad_norm=1.0,\n        quantization_bit=4,\n        loraplus_lr_ratio=16.0,\n        fp16=True,\n        ddp_timeout=180000000,\n    )\n    os.environ[\"WANDB_DISABLED\"] = \"true\"\n    from llamafactory.train import tuner\n\n    tuner.run_exp({**args, **options})\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/apps/byzer_storage/train_sft.py\nimport json\nimport os\nimport concurrent.futures\nfrom typing import List, Dict, Any\nimport byzerllm\nfrom byzerllm.apps.byzer_storage.generate_sft_data import to_qa_pairs, read_alpaca_zh\n\n\ndef train_sft(\n    name: str,\n    memories: List[str],\n    options: Dict[str, Any],\n    dataset_dir: str,\n    loras_dir: str,\n    data_model_name: str,\n    model_name: str,\n):\n    data = []\n    min_samples = options.pop(\"min_samples\", 1000)\n\n    if memories:\n        with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:\n            futures = []\n            llm = byzerllm.ByzerLLM().from_default_model(data_model_name)\n            for memory in memories:\n                futures.append(executor.submit(to_qa_pairs, memory, llm))\n\n            for future in concurrent.futures.as_completed(futures):\n                qa_pairs = future.result()\n                for qa_pair in qa_pairs:\n                    item = {\n                        \"instruction\": qa_pair.question,\n                        \"input\": \"\",\n                        \"output\": qa_pair.answer,\n                    }\n                    data.append(item)\n    else:\n        v = read_alpaca_zh()\n        data.extend(v)\n\n    if len(data) < min_samples:\n        data = data * (min_samples // len(data) + 1)\n\n    with open(f\"{dataset_dir}/data.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(data, f, indent=2, ensure_ascii=False)\n\n    with open(f\"{dataset_dir}/dataset_info.json\", \"w\", encoding=\"utf-8\") as f:\n        r = {\"data\": {\"file_name\": \"data.json\"}}\n        f.write(json.dumps(r, indent=2, ensure_ascii=False))\n\n    args = dict(\n        stage=\"sft\",\n        do_train=True,\n        model_name_or_path=model_name,\n        dataset=\"data\",\n        dataset_dir=dataset_dir,\n        cutoff_len=1024,\n        max_samples=1000000,\n        overwrite_cache=True,\n        preprocessing_num_workers=1,\n        template=\"llama3\",\n        finetuning_type=\"lora\",\n        lora_target=\"all\",\n        output_dir=f\"{loras_dir}/{name}\",\n        overwrite_output_dir=True,\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=4,\n        lr_scheduler_type=\"cosine\",\n        logging_steps=10,\n        warmup_ratio=0.1,\n        save_steps=1000,\n        plot_loss=True,\n        learning_rate=5e-5,\n        num_train_epochs=1000.0,\n        max_grad_norm=1.0,\n        quantization_bit=4,\n        loraplus_lr_ratio=16.0,\n        fp16=True,\n        ddp_timeout=180000000,\n    )\n    os.environ[\"WANDB_DISABLED\"] = \"true\"\n    from llamafactory.train import tuner\n\n    tuner.run_exp({**args, **options})\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/apps/byzer_storage/train_dpo.py\nimport json\nimport os\nfrom typing import List, Dict, Any\nimport byzerllm\nimport concurrent.futures\nfrom byzerllm.apps.byzer_storage.generate_sft_data import to_qa_pairs\n\n\ndef train_dpo(    \n    name: str,\n    memories: List[str],\n    options: Dict[str, Any],\n    dataset_dir: str,\n    loras_dir: str,\n    model_name: str,\n    data_model_name: str,\n):\n    data = []\n    min_samples = options.pop(\"min_samples\", 1000)\n\n    if memories:\n        with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:\n            futures = []\n            data_llm = byzerllm.ByzerLLM().from_default_model(data_model_name)\n            current_llm = byzerllm.ByzerLLM().from_default_model(model_name)\n            for memory in memories:\n                futures.append(executor.submit(to_qa_pairs, memory, data_llm))\n\n            for future in concurrent.futures.as_completed(futures):\n                qa_pairs = future.result()\n                for qa_pair in qa_pairs:\n                    response = current_llm.chat_oai(\n                        conversations=[{\"role\": \"user\", \"content\": qa_pair.question}]\n                    )\n                    v = response[0].output\n                    item = (\n                        {\n                            \"conversations\": [\n                                {\n                                    \"from\": \"human\",\n                                    \"value\": qa_pair.question,\n                                }\n                            ],\n                            \"chosen\": {\n                                \"from\": \"gpt\",\n                                \"value\": qa_pair.answer,\n                            },\n                            \"rejected\": {\n                                \"from\": \"gpt\",\n                                \"value\": v,\n                            },\n                        },\n                    )\n                    data.append(item)\n\n    if len(data) < min_samples:\n        data = data * (min_samples // len(data) + 1)\n\n    with open(f\"{dataset_dir}/data.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(data, f, indent=2, ensure_ascii=False)\n\n    with open(f\"{dataset_dir}/dataset_info.json\", \"w\", encoding=\"utf-8\") as f:\n        r = {\n            \"data\": {\n                \"file_name\": \"data.json\",\n                \"ranking\": True,\n                \"columns\": {\n                    \"prompt\": \"instruction\",\n                    \"query\": \"input\",\n                    \"chosen\": \"chosen\",\n                    \"rejected\": \"rejected\",\n                },\n            }\n        }\n        f.write(json.dumps(r, indent=2, ensure_ascii=False))\n\n    args = dict(\n        stage=\"dpo\",\n        do_train=True,\n        model_name_or_path=model_name,\n        dataset=\"data\",\n        dataset_dir=dataset_dir,\n        cutoff_len=1024,\n        max_samples=1000000,\n        overwrite_cache=True,\n        preprocessing_num_workers=1,\n        template=\"llama3\",\n        finetuning_type=\"lora\",\n        lora_target=\"all\",\n        output_dir=f\"{loras_dir}/{name}\",\n        overwrite_output_dir=True,\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=4,\n        lr_scheduler_type=\"cosine\",\n        logging_steps=10,\n        warmup_ratio=0.1,\n        save_steps=1000,\n        plot_loss=True,\n        learning_rate=5e-5,\n        num_train_epochs=1000.0,\n        max_grad_norm=1.0,\n        quantization_bit=4,\n        loraplus_lr_ratio=16.0,\n        fp16=True,\n        ddp_timeout=180000000,\n    )\n    os.environ[\"WANDB_DISABLED\"] = \"true\"\n    from llamafactory.train import tuner\n\n    tuner.run_exp({**args, **options})\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/apps/byzer_storage/generate_sft_data.py\nfrom typing import List, Dict, Any\nimport byzerllm\nfrom loguru import logger\nfrom pydantic import BaseModel\nfrom byzerllm.apps.utils import TagExtractor, Tag\nimport importlib\nimport json\nimport hashlib\nimport os\nfrom pathlib import Path\n\n\ndef read_alpaca_zh():\n    with importlib.resources.path(\n        \"byzerllm.apps.byzer_storage\", \"alpaca_zh.json\"\n    ) as json_path:\n        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n            return json.load(f)\n\n\nclass QAPair(BaseModel):\n    question: str\n    answer: str\n\n\n@byzerllm.prompt()\ndef _convert_pretrain_text_to_instruction(text: str, num: int = 10) -> str:\n    \"\"\"\n    根据提供的信息，生成多个相关的问题，这些问题的回答，最终要能覆盖里面所有的信息,生成 {{num}} 个。\n\n    下面是一些信息：\n\n    {{text}}\n\n    现在请开始生成问题，请将每个问题使用<_question_></_question_>标签包裹，每个回答使用<_answer_></_answer_>标签包裹，最后\n    每个一组问题和回答使用<_group_></_group_>标签包裹。\n    \"\"\"\n\n\n@byzerllm.prompt()\ndef get_more(text: str, questions_and_answers: str, num: int = 10) -> str:\n    \"\"\"\n    下面是一些问答信息：\n\n    {{text}}\n\n    下面是已有的问题和回答：\n\n    {{questions_and_answers}}\n\n    下面是已经生成的问题和回答，请继续生成{{num}}个问题和回答，不要和前面的重复，请将每个问题使用<_question_></_question_>标签包裹，每个回答使用<_answer_></_answer_>标签包裹，最后\n    每个一组问题和回答使用<_group_></_group_>标签包裹。\n    \"\"\"\n\n\ndef generate_cache_key(text: str) -> str:\n    return hashlib.md5(text.encode()).hexdigest()\n\ndef get_cache_path() -> Path:\n    home = Path.home()\n    cache_dir = home / \".byzerllm\" / \"cache\" / \"llm_generated_data\"\n    cache_dir.mkdir(parents=True, exist_ok=True)\n    return cache_dir\n\ndef save_to_cache(key: str, data: List[QAPair]):\n    cache_path = get_cache_path() / f\"{key}.json\"\n    with cache_path.open(\"w\", encoding=\"utf-8\") as f:\n        json.dump([qa.dict() for qa in data], f, ensure_ascii=False, indent=2)\n\ndef load_from_cache(key: str) -> List[QAPair]:\n    cache_path = get_cache_path() / f\"{key}.json\"\n    if cache_path.exists():\n        with cache_path.open(\"r\", encoding=\"utf-8\") as f:\n            data = json.load(f)\n        return [QAPair(**item) for item in data]\n    return None\n\ndef to_qa_pairs(text: str, llm, num: int = 30) -> List[QAPair]:\n    cache_key = generate_cache_key(text)\n    cached_result = load_from_cache(cache_key)\n    \n    if cached_result:\n        logger.info(f\"Using cached result for text: {text[:100]}...\")\n        return cached_result\n\n    logger.info(f\"Starting to generate QA pairs for text: {text[:100]}...\")\n\n    logger.info(\"Generating initial QA pairs...\")\n    v = _convert_pretrain_text_to_instruction.with_llm(llm).run(text)\n    logger.info(f\"Initial QA pairs generated. Length: {len(v)}\")\n\n    max_turns = int(num / 10)\n    if max_turns < 1:\n        max_turns = 1\n    logger.info(f\"Will generate additional QA pairs for {max_turns} turns\")\n\n    turn = 0\n    while turn < max_turns:\n        turn += 1\n        logger.info(f\"Generating additional QA pairs (turn {turn}/{max_turns})...\")\n        t = get_more.with_llm(llm).run(text, v)\n        v += t\n        logger.info(f\"Additional QA pairs generated. New total length: {len(v)}\")\n\n    logger.info(f\"Extracting QA pairs from generated text...{v}\")\n    root_tag = TagExtractor(v).extract()\n    qa_pairs = []\n    for i, item in enumerate(root_tag.content):\n        qas = item.content\n        if len(qas) == 2:\n            if (\n                qas[0].start_tag == \"<_question_>\"\n                and qas[0].end_tag == \"</_question_>\"\n                and qas[1].start_tag == \"<_answer_>\"\n                and qas[1].end_tag == \"</_answer_>\"\n            ):\n                qa_pairs.append(QAPair(question=qas[0].content, answer=qas[1].content))\n                logger.debug(\n                    f\"Extracted QA pair {i+1}: Q: {qas[0].content[:50]}... A: {qas[1].content[:50]}...\"\n                )\n            else:\n                logger.warning(\n                    f\"Skipping item {i+1} due to incorrect tags: {qas[0].start_tag}, {qas[0].end_tag}, {qas[1].start_tag}, {qas[1].end_tag}\"\n                )\n        else:\n            logger.warning(\n                f\"Skipping item {i+1} due to incorrect number of elements: {len(qas)}\"\n            )\n\n    logger.info(f\"Extracted {len(qa_pairs)} valid QA pairs.\")\n    \n    # Save the result to cache\n    save_to_cache(cache_key, qa_pairs)\n    \n    return qa_pairs\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/apps/byzer_storage/memory_model_based.py\nimport asyncio\nfrom typing import List, Dict, Any\n\nimport ray.experimental\nfrom byzerllm.apps.byzer_storage.simple_api import ByzerStorage\nimport time\nimport json\nimport os\nimport concurrent.futures\nimport io\nimport sys\nfrom contextlib import redirect_stdout, redirect_stderr\nimport queue\nimport threading\nimport ray\nimport byzerllm\nfrom typing import List, Union, Dict\nfrom pydantic import BaseModel\nimport importlib\nfrom loguru import logger\n\n\nclass MemoryManager:\n    _queue = asyncio.Queue()\n    _is_processing = False\n\n    def __init__(\n        self,\n        storage: ByzerStorage,\n        base_dir: str,\n        model_name: str = \"deepseek_chat\",\n        remote: bool = True,\n    ):\n        self.storage = storage\n        home = os.path.expanduser(\"~\")\n        self.base_dir = base_dir or os.path.join(home, \".auto-coder\")\n        self.thread_pool = concurrent.futures.ThreadPoolExecutor(max_workers=5)\n        self.log_file = None\n        self.remote = remote\n        self.model_name = model_name\n\n    @classmethod\n    async def add_to_queue(cls, name: str, memories: List[str]):\n        await cls._queue.put((name, memories))\n        if not cls._is_processing:\n            asyncio.create_task(cls.process_queue())\n\n    @classmethod\n    async def process_queue(cls):\n        cls._is_processing = True\n        while not cls._queue.empty():\n            name, memories = await cls._queue.get()\n            instance = cls.get_instance()\n            await instance.memorize(name, memories)\n            cls._queue.task_done()\n        cls._is_processing = False\n\n    async def memorize(\n        self, name: str, memories: List[str], options: Dict[str, Any] = {}\n    ):\n        try:\n            loop = asyncio.get_running_loop()\n            await loop.run_in_executor(\n                self.thread_pool, self._memorize_with_logs, name, memories, options\n            )\n        finally:\n            if self.remote:\n                ray.actor.exit_actor()\n\n        print(f\"Memorization for {name} completed.\")\n\n    def _memorize_with_logs(\n        self, name: str, memories: List[str], options: Dict[str, Any] = {}\n    ):\n\n        if self.remote:\n            self._memorize(name, memories, options=options)\n            return\n\n        logs_dir = os.path.join(self.base_dir, \"storage\", \"logs\", \"memorize\")\n        os.makedirs(logs_dir, exist_ok=True)\n        log_file = os.path.join(logs_dir, f\"{name}.log\")\n\n        log_queue = queue.Queue()\n        stop_event = threading.Event()\n\n        # Start the log writer thread\n        log_writer_thread = threading.Thread(\n            target=self._log_writer, args=(log_queue, log_file, stop_event)\n        )\n        self.log_file = log_file\n        log_writer_thread.start()\n\n        class QueueStream:\n            def __init__(self, queue):\n                self.queue = queue\n\n            def write(self, msg):\n                self.queue.put(msg)\n\n            def flush(self):\n                pass\n\n        queue_stream = QueueStream(log_queue)\n\n        with redirect_stdout(queue_stream), redirect_stderr(queue_stream):\n            self._memorize(name, memories)\n\n        # Signal the log writer to stop and wait for it to finish\n        stop_event.set()\n        log_writer_thread.join()\n\n    def _log_writer(\n        self, log_queue: queue.Queue, log_file: str, stop_event: threading.Event\n    ):\n        with open(log_file, \"w\") as f:\n            while not stop_event.is_set() or not log_queue.empty():\n                try:\n                    msg = log_queue.get(timeout=0.1)\n                    f.write(msg)\n                    f.flush()\n                except queue.Empty:\n                    continue\n\n    def _memorize(\n        self,\n        name: str,\n        memories: List[Union[str, Dict[str, Any]]],\n        options: Dict[str, Any] = {},\n    ):\n        logger.info(f\"Starting _memorize for {name}\")\n        logger.info(f\"Memories count: {len(memories)}\")\n        logger.info(f\"Options: {options}\")\n\n        stage = options.get(\"stage\", \"pt\")\n        logger.info(f\"Selected stage: {stage}\")\n\n        base_model_dir = os.path.join(self.base_dir, \"storage\", \"models\")\n        llama_model = os.path.join(\n            base_model_dir, \"meta-llama\", \"Meta-Llama-3-8B-Instruct-GPTQ\"\n        )\n        logger.info(f\"Using model: {llama_model}\")\n\n        loras_dir = os.path.join(self.base_dir, \"storage\", \"loras\")\n        dataset_dir = os.path.join(self.base_dir, \"storage\", \"datasets\", name)\n\n        logger.info(f\"Creating directories: {loras_dir} and {dataset_dir}\")\n        os.makedirs(loras_dir, exist_ok=True)\n        os.makedirs(dataset_dir, exist_ok=True)\n\n        if stage == \"pt\":\n            logger.info(\"Starting pre-training stage\")\n            from byzerllm.apps.byzer_storage.train_pt import train_pt\n\n            train_pt(\n                name,\n                memories,\n                options,\n                dataset_dir,\n                loras_dir,\n                model_name=llama_model,\n                data_model_name=self.model_name,\n            )\n        elif stage == \"sft\":\n            logger.info(\"Starting SFT stage\")\n            from byzerllm.apps.byzer_storage.train_sft import train_sft\n\n            train_sft(\n                name,\n                memories,\n                options,\n                dataset_dir,\n                loras_dir,\n                model_name=llama_model,\n                data_model_name=self.model_name,\n            )\n        elif stage == \"dpo\":\n            logger.info(\"Starting DPO stage\")\n            from byzerllm.apps.byzer_storage.train_dpo import train_dpo\n\n            train_dpo(\n                name,\n                memories,\n                options,\n                dataset_dir,\n                loras_dir,\n                model_name=llama_model,\n                data_model_name=self.model_name,\n            )\n        else:\n            logger.error(f\"Unsupported stage: {stage}\")\n            raise ValueError(f\"Unsupported stage: {stage}\")\n\n        logger.info(f\"_memorize completed for {name}\")\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/apps/byzer_storage/train_pt.py\nimport json\nimport os\nfrom typing import List, Dict, Any\n\n\ndef train_pt(\n    name: str,\n    memories: List[str],\n    options: Dict[str, Any],\n    dataset_dir: str,\n    loras_dir: str,\n    data_model_name: str,\n    model_name: str,\n):\n    data = []\n    min_samples = options.pop(\"min_samples\", 1000)\n\n    for memory in memories:\n        item = {\n            \"text\": memory,\n        }\n        data.append(item)\n\n    if len(data) < min_samples:\n        data = data * (min_samples // len(data) + 1)\n\n    with open(f\"{dataset_dir}/data.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(data, f, indent=2, ensure_ascii=False)\n\n    with open(f\"{dataset_dir}/dataset_info.json\", \"w\", encoding=\"utf-8\") as f:\n        r = {\"data\": {\"file_name\": \"data.json\", \"columns\": {\"prompt\": \"text\"}}}\n        f.write(json.dumps(r, indent=2, ensure_ascii=False))\n\n    args = dict(\n        stage=\"pt\",\n        do_train=True,\n        model_name_or_path=model_name,\n        dataset=\"data\",\n        dataset_dir=dataset_dir,\n        cutoff_len=1024,\n        max_samples=1000000,\n        overwrite_cache=True,\n        preprocessing_num_workers=1,\n        template=\"llama3\",\n        finetuning_type=\"lora\",\n        lora_target=\"all\",\n        output_dir=f\"{loras_dir}/{name}\",\n        overwrite_output_dir=True,\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=4,\n        lr_scheduler_type=\"cosine\",\n        logging_steps=10,\n        warmup_ratio=0.1,\n        save_steps=1000,\n        plot_loss=True,\n        learning_rate=5e-5,\n        num_train_epochs=1000.0,\n        max_grad_norm=1.0,\n        quantization_bit=4,\n        loraplus_lr_ratio=16.0,\n        fp16=True,\n        ddp_timeout=180000000,\n    )\n    os.environ[\"WANDB_DISABLED\"] = \"true\"\n    from llamafactory.train import tuner\n\n    tuner.run_exp({**args, **options})\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/apps/byzer_storage/train_sft.py\nimport json\nimport os\nimport concurrent.futures\nfrom typing import List, Dict, Any\nimport byzerllm\nfrom byzerllm.apps.byzer_storage.generate_sft_data import to_qa_pairs, read_alpaca_zh\n\n\ndef train_sft(\n    name: str,\n    memories: List[str],\n    options: Dict[str, Any],\n    dataset_dir: str,\n    loras_dir: str,\n    data_model_name: str,\n    model_name: str,\n):\n    data = []\n    min_samples = options.pop(\"min_samples\", 1000)\n\n    if memories:\n        with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:\n            futures = []\n            llm = byzerllm.ByzerLLM().from_default_model(data_model_name)\n            for memory in memories:\n                futures.append(executor.submit(to_qa_pairs, memory, llm))\n\n            for future in concurrent.futures.as_completed(futures):\n                qa_pairs = future.result()\n                for qa_pair in qa_pairs:\n                    item = {\n                        \"instruction\": qa_pair.question,\n                        \"input\": \"\",\n                        \"output\": qa_pair.answer,\n                    }\n                    data.append(item)\n    else:\n        v = read_alpaca_zh()\n        data.extend(v)\n\n    if len(data) < min_samples:\n        data = data * (min_samples // len(data) + 1)\n\n    with open(f\"{dataset_dir}/data.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(data, f, indent=2, ensure_ascii=False)\n\n    with open(f\"{dataset_dir}/dataset_info.json\", \"w\", encoding=\"utf-8\") as f:\n        r = {\"data\": {\"file_name\": \"data.json\"}}\n        f.write(json.dumps(r, indent=2, ensure_ascii=False))\n\n    args = dict(\n        stage=\"sft\",\n        do_train=True,\n        model_name_or_path=model_name,\n        dataset=\"data\",\n        dataset_dir=dataset_dir,\n        cutoff_len=1024,\n        max_samples=1000000,\n        overwrite_cache=True,\n        preprocessing_num_workers=1,\n        template=\"llama3\",\n        finetuning_type=\"lora\",\n        lora_target=\"all\",\n        output_dir=f\"{loras_dir}/{name}\",\n        overwrite_output_dir=True,\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=4,\n        lr_scheduler_type=\"cosine\",\n        logging_steps=10,\n        warmup_ratio=0.1,\n        save_steps=1000,\n        plot_loss=True,\n        learning_rate=5e-5,\n        num_train_epochs=1000.0,\n        max_grad_norm=1.0,\n        quantization_bit=4,\n        loraplus_lr_ratio=16.0,\n        fp16=True,\n        ddp_timeout=180000000,\n    )\n    os.environ[\"WANDB_DISABLED\"] = \"true\"\n    from llamafactory.train import tuner\n\n    tuner.run_exp({**args, **options})\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/apps/byzer_storage/train_dpo.py\nimport json\nimport os\nfrom typing import List, Dict, Any\nimport byzerllm\nimport concurrent.futures\nfrom byzerllm.apps.byzer_storage.generate_sft_data import to_qa_pairs\n\n\ndef train_dpo(    \n    name: str,\n    memories: List[str],\n    options: Dict[str, Any],\n    dataset_dir: str,\n    loras_dir: str,\n    model_name: str,\n    data_model_name: str,\n):\n    data = []\n    min_samples = options.pop(\"min_samples\", 1000)\n\n    if memories:\n        with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:\n            futures = []\n            data_llm = byzerllm.ByzerLLM().from_default_model(data_model_name)\n            current_llm = byzerllm.ByzerLLM().from_default_model(model_name)\n            for memory in memories:\n                futures.append(executor.submit(to_qa_pairs, memory, data_llm))\n\n            for future in concurrent.futures.as_completed(futures):\n                qa_pairs = future.result()\n                for qa_pair in qa_pairs:\n                    response = current_llm.chat_oai(\n                        conversations=[{\"role\": \"user\", \"content\": qa_pair.question}]\n                    )\n                    v = response[0].output\n                    item = (\n                        {\n                            \"conversations\": [\n                                {\n                                    \"from\": \"human\",\n                                    \"value\": qa_pair.question,\n                                }\n                            ],\n                            \"chosen\": {\n                                \"from\": \"gpt\",\n                                \"value\": qa_pair.answer,\n                            },\n                            \"rejected\": {\n                                \"from\": \"gpt\",\n                                \"value\": v,\n                            },\n                        },\n                    )\n                    data.append(item)\n\n    if len(data) < min_samples:\n        data = data * (min_samples // len(data) + 1)\n\n    with open(f\"{dataset_dir}/data.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(data, f, indent=2, ensure_ascii=False)\n\n    with open(f\"{dataset_dir}/dataset_info.json\", \"w\", encoding=\"utf-8\") as f:\n        r = {\n            \"data\": {\n                \"file_name\": \"data.json\",\n                \"ranking\": True,\n                \"columns\": {\n                    \"prompt\": \"instruction\",\n                    \"query\": \"input\",\n                    \"chosen\": \"chosen\",\n                    \"rejected\": \"rejected\",\n                },\n            }\n        }\n        f.write(json.dumps(r, indent=2, ensure_ascii=False))\n\n    args = dict(\n        stage=\"dpo\",\n        do_train=True,\n        model_name_or_path=model_name,\n        dataset=\"data\",\n        dataset_dir=dataset_dir,\n        cutoff_len=1024,\n        max_samples=1000000,\n        overwrite_cache=True,\n        preprocessing_num_workers=1,\n        template=\"llama3\",\n        finetuning_type=\"lora\",\n        lora_target=\"all\",\n        output_dir=f\"{loras_dir}/{name}\",\n        overwrite_output_dir=True,\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=4,\n        lr_scheduler_type=\"cosine\",\n        logging_steps=10,\n        warmup_ratio=0.1,\n        save_steps=1000,\n        plot_loss=True,\n        learning_rate=5e-5,\n        num_train_epochs=1000.0,\n        max_grad_norm=1.0,\n        quantization_bit=4,\n        loraplus_lr_ratio=16.0,\n        fp16=True,\n        ddp_timeout=180000000,\n    )\n    os.environ[\"WANDB_DISABLED\"] = \"true\"\n    from llamafactory.train import tuner\n\n    tuner.run_exp({**args, **options})\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/apps/byzer_storage/generate_sft_data.py\nfrom typing import List, Dict, Any\nimport byzerllm\nfrom loguru import logger\nfrom pydantic import BaseModel\nfrom byzerllm.apps.utils import TagExtractor, Tag\nimport importlib\nimport json\nimport hashlib\nimport os\nfrom pathlib import Path\n\n\ndef read_alpaca_zh():\n    with importlib.resources.path(\n        \"byzerllm.apps.byzer_storage\", \"alpaca_zh.json\"\n    ) as json_path:\n        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n            return json.load(f)\n\n\nclass QAPair(BaseModel):\n    question: str\n    answer: str\n\n\n@byzerllm.prompt()\ndef _convert_pretrain_text_to_instruction(text: str, num: int = 10) -> str:\n    \"\"\"\n    根据提供的信息，生成多个相关的问题，这些问题的回答，最终要能覆盖里面所有的信息,生成 {{num}} 个。\n\n    下面是一些信息：\n\n    {{text}}\n\n    现在请开始生成问题，请将每个问题使用<_question_></_question_>标签包裹，每个回答使用<_answer_></_answer_>标签包裹，最后\n    每个一组问题和回答使用<_group_></_group_>标签包裹。\n    \"\"\"\n\n\n@byzerllm.prompt()\ndef get_more(text: str, questions_and_answers: str, num: int = 10) -> str:\n    \"\"\"\n    下面是一些问答信息：\n\n    {{text}}\n\n    下面是已有的问题和回答：\n\n    {{questions_and_answers}}\n\n    下面是已经生成的问题和回答，请继续生成{{num}}个问题和回答，不要和前面的重复，请将每个问题使用<_question_></_question_>标签包裹，每个回答使用<_answer_></_answer_>标签包裹，最后\n    每个一组问题和回答使用<_group_></_group_>标签包裹。\n    \"\"\"\n\n\ndef generate_cache_key(text: str) -> str:\n    return hashlib.md5(text.encode()).hexdigest()\n\ndef get_cache_path() -> Path:\n    home = Path.home()\n    cache_dir = home / \".byzerllm\" / \"cache\" / \"llm_generated_data\"\n    cache_dir.mkdir(parents=True, exist_ok=True)\n    return cache_dir\n\ndef save_to_cache(key: str, data: List[QAPair]):\n    cache_path = get_cache_path() / f\"{key}.json\"\n    with cache_path.open(\"w\", encoding=\"utf-8\") as f:\n        json.dump([qa.dict() for qa in data], f, ensure_ascii=False, indent=2)\n\ndef load_from_cache(key: str) -> List[QAPair]:\n    cache_path = get_cache_path() / f\"{key}.json\"\n    if cache_path.exists():\n        with cache_path.open(\"r\", encoding=\"utf-8\") as f:\n            data = json.load(f)\n        return [QAPair(**item) for item in data]\n    return None\n\ndef to_qa_pairs(text: str, llm, num: int = 30) -> List[QAPair]:\n    cache_key = generate_cache_key(text)\n    cached_result = load_from_cache(cache_key)\n    \n    if cached_result:\n        logger.info(f\"Using cached result for text: {text[:100]}...\")\n        return cached_result\n\n    logger.info(f\"Starting to generate QA pairs for text: {text[:100]}...\")\n\n    logger.info(\"Generating initial QA pairs...\")\n    v = _convert_pretrain_text_to_instruction.with_llm(llm).run(text)\n    logger.info(f\"Initial QA pairs generated. Length: {len(v)}\")\n\n    max_turns = int(num / 10)\n    if max_turns < 1:\n        max_turns = 1\n    logger.info(f\"Will generate additional QA pairs for {max_turns} turns\")\n\n    turn = 0\n    while turn < max_turns:\n        turn += 1\n        logger.info(f\"Generating additional QA pairs (turn {turn}/{max_turns})...\")\n        t = get_more.with_llm(llm).run(text, v)\n        v += t\n        logger.info(f\"Additional QA pairs generated. New total length: {len(v)}\")\n\n    logger.info(f\"Extracting QA pairs from generated text...{v}\")\n    root_tag = TagExtractor(v).extract()\n    qa_pairs = []\n    for i, item in enumerate(root_tag.content):\n        qas = item.content\n        if len(qas) == 2:\n            if (\n                qas[0].start_tag == \"<_question_>\"\n                and qas[0].end_tag == \"</_question_>\"\n                and qas[1].start_tag == \"<_answer_>\"\n                and qas[1].end_tag == \"</_answer_>\"\n            ):\n                qa_pairs.append(QAPair(question=qas[0].content, answer=qas[1].content))\n                logger.debug(\n                    f\"Extracted QA pair {i+1}: Q: {qas[0].content[:50]}... A: {qas[1].content[:50]}...\"\n                )\n            else:\n                logger.warning(\n                    f\"Skipping item {i+1} due to incorrect tags: {qas[0].start_tag}, {qas[0].end_tag}, {qas[1].start_tag}, {qas[1].end_tag}\"\n                )\n        else:\n            logger.warning(\n                f\"Skipping item {i+1} due to incorrect number of elements: {len(qas)}\"\n            )\n\n    logger.info(f\"Extracted {len(qa_pairs)} valid QA pairs.\")\n    \n    # Save the result to cache\n    save_to_cache(cache_key, qa_pairs)\n    \n    return qa_pairs\n\n\n     \n\n下面是用户的需求：\n\ntrain_dpo 添加合适的日志方便追踪流程\n"}, {"role": "assistant", "content": "为了添加合适的日志以便于追踪 train_dpo 的流程，我们可以在关键步骤添加日志记录。以下是修改后的 train_dpo 函数：\n\n```python\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/apps/byzer_storage/train_dpo.py\n<<<<<<< SEARCH\nimport json\nimport os\nfrom typing import List, Dict, Any\nimport byzerllm\nimport concurrent.futures\nfrom byzerllm.apps.byzer_storage.generate_sft_data import to_qa_pairs\n\n\ndef train_dpo(    \n    name: str,\n    memories: List[str],\n    options: Dict[str, Any],\n    dataset_dir: str,\n    loras_dir: str,\n    model_name: str,\n    data_model_name: str,\n):\n    data = []\n    min_samples = options.pop(\"min_samples\", 1000)\n\n    if memories:\n        with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:\n            futures = []\n            data_llm = byzerllm.ByzerLLM().from_default_model(data_model_name)\n            current_llm = byzerllm.ByzerLLM().from_default_model(model_name)\n            for memory in memories:\n                futures.append(executor.submit(to_qa_pairs, memory, data_llm))\n\n            for future in concurrent.futures.as_completed(futures):\n                qa_pairs = future.result()\n                for qa_pair in qa_pairs:\n                    response = current_llm.chat_oai(\n                        conversations=[{\"role\": \"user\", \"content\": qa_pair.question}]\n                    )\n                    v = response[0].output\n                    item = (\n                        {\n                            \"conversations\": [\n                                {\n                                    \"from\": \"human\",\n                                    \"value\": qa_pair.question,\n                                }\n                            ],\n                            \"chosen\": {\n                                \"from\": \"gpt\",\n                                \"value\": qa_pair.answer,\n                            },\n                            \"rejected\": {\n                                \"from\": \"gpt\",\n                                \"value\": v,\n                            },\n                        },\n                    )\n                    data.append(item)\n\n    if len(data) < min_samples:\n        data = data * (min_samples // len(data) + 1)\n\n    with open(f\"{dataset_dir}/data.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(data, f, indent=2, ensure_ascii=False)\n\n    with open(f\"{dataset_dir}/dataset_info.json\", \"w\", encoding=\"utf-8\") as f:\n        r = {\n            \"data\": {\n                \"file_name\": \"data.json\",\n                \"ranking\": True,\n                \"columns\": {\n                    \"prompt\": \"instruction\",\n                    \"query\": \"input\",\n                    \"chosen\": \"chosen\",\n                    \"rejected\": \"rejected\",\n                },\n            }\n        }\n        f.write(json.dumps(r, indent=2, ensure_ascii=False))\n\n    args = dict(\n        stage=\"dpo\",\n        do_train=True,\n        model_name_or_path=model_name,\n        dataset=\"data\",\n        dataset_dir=dataset_dir,\n        cutoff_len=1024,\n        max_samples=1000000,\n        overwrite_cache=True,\n        preprocessing_num_workers=1,\n        template=\"llama3\",\n        finetuning_type=\"lora\",\n        lora_target=\"all\",\n        output_dir=f\"{loras_dir}/{name}\",\n        overwrite_output_dir=True,\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=4,\n        lr_scheduler_type=\"cosine\",\n        logging_steps=10,\n        warmup_ratio=0.1,\n        save_steps=1000,\n        plot_loss=True,\n        learning_rate=5e-5,\n        num_train_epochs=1000.0,\n        max_grad_norm=1.0,\n        quantization_bit=4,\n        loraplus_lr_ratio=16.0,\n        fp16=True,\n        ddp_timeout=180000000,\n    )\n    os.environ[\"WANDB_DISABLED\"] = \"true\"\n    from llamafactory.train import tuner\n\n    tuner.run_exp({**args, **options})\n=======\nimport json\nimport os\nfrom typing import List, Dict, Any\nimport byzerllm\nimport concurrent.futures\nfrom byzerllm.apps.byzer_storage.generate_sft_data import to_qa_pairs\nfrom loguru import logger\n\ndef train_dpo(    \n    name: str,\n    memories: List[str],\n    options: Dict[str, Any],\n    dataset_dir: str,\n    loras_dir: str,\n    model_name: str,\n    data_model_name: str,\n):\n    logger.info(f\"Starting DPO training for {name}\")\n    logger.info(f\"Using data model: {data_model_name} and target model: {model_name}\")\n    \n    data = []\n    min_samples = options.pop(\"min_samples\", 1000)\n    logger.info(f\"Minimum samples required: {min_samples}\")\n\n    if memories:\n        logger.info(f\"Processing {len(memories)} memories\")\n        with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:\n            futures = []\n            data_llm = byzerllm.ByzerLLM().from_default_model(data_model_name)\n            current_llm = byzerllm.ByzerLLM().from_default_model(model_name)\n            for memory in memories:\n                futures.append(executor.submit(to_qa_pairs, memory, data_llm))\n\n            for future in concurrent.futures.as_completed(futures):\n                qa_pairs = future.result()\n                logger.info(f\"Generated {len(qa_pairs)} QA pairs\")\n                for qa_pair in qa_pairs:\n                    logger.debug(f\"Processing QA pair: {qa_pair.question[:50]}...\")\n                    response = current_llm.chat_oai(\n                        conversations=[{\"role\": \"user\", \"content\": qa_pair.question}]\n                    )\n                    v = response[0].output\n                    item = (\n                        {\n                            \"conversations\": [\n                                {\n                                    \"from\": \"human\",\n                                    \"value\": qa_pair.question,\n                                }\n                            ],\n                            \"chosen\": {\n                                \"from\": \"gpt\",\n                                \"value\": qa_pair.answer,\n                            },\n                            \"rejected\": {\n                                \"from\": \"gpt\",\n                                \"value\": v,\n                            },\n                        },\n                    )\n                    data.append(item)\n\n    logger.info(f\"Total data points generated: {len(data)}\")\n    if len(data) < min_samples:\n        logger.warning(f\"Not enough samples. Duplicating data to reach {min_samples}\")\n        data = data * (min_samples // len(data) + 1)\n    logger.info(f\"Final number of data points: {len(data)}\")\n\n    logger.info(f\"Saving data to {dataset_dir}/data.json\")\n    with open(f\"{dataset_dir}/data.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(data, f, indent=2, ensure_ascii=False)\n\n    logger.info(f\"Saving dataset info to {dataset_dir}/dataset_info.json\")\n    with open(f\"{dataset_dir}/dataset_info.json\", \"w\", encoding=\"utf-8\") as f:\n        r = {\n            \"data\": {\n                \"file_name\": \"data.json\",\n                \"ranking\": True,\n                \"columns\": {\n                    \"prompt\": \"instruction\",\n                    \"query\": \"input\",\n                    \"chosen\": \"chosen\",\n                    \"rejected\": \"rejected\",\n                },\n            }\n        }\n        f.write(json.dumps(r, indent=2, ensure_ascii=False))\n\n    args = dict(\n        stage=\"dpo\",\n        do_train=True,\n        model_name_or_path=model_name,\n        dataset=\"data\",\n        dataset_dir=dataset_dir,\n        cutoff_len=1024,\n        max_samples=1000000,\n        overwrite_cache=True,\n        preprocessing_num_workers=1,\n        template=\"llama3\",\n        finetuning_type=\"lora\",\n        lora_target=\"all\",\n        output_dir=f\"{loras_dir}/{name}\",\n        overwrite_output_dir=True,\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=4,\n        lr_scheduler_type=\"cosine\",\n        logging_steps=10,\n        warmup_ratio=0.1,\n        save_steps=1000,\n        plot_loss=True,\n        learning_rate=5e-5,\n        num_train_epochs=1000.0,\n        max_grad_norm=1.0,\n        quantization_bit=4,\n        loraplus_lr_ratio=16.0,\n        fp16=True,\n        ddp_timeout=180000000,\n    )\n    logger.info(\"Setting up training arguments\")\n    for key, value in args.items():\n        logger.debug(f\"{key}: {value}\")\n\n    os.environ[\"WANDB_DISABLED\"] = \"true\"\n    logger.info(\"Starting DPO training\")\n    from llamafactory.train import tuner\n\n    tuner.run_exp({**args, **options})\n    logger.info(\"DPO training completed\")\n>>>>>>> REPLACE\n```\n\n这些修改添加了详细的日志记录，使用 loguru 库来记录关键步骤和重要信息。主要的改进包括：\n\n1. 在函数开始时记录训练的基本信息，如模型名称和数据模型名称。\n2. 记录内存处理和QA对生成的进度。\n3. 记录数据点的数量，包括是否需要复制数据以达到最小样本数。\n4. 记录文件保存的位置和内容。\n5. 详细记录训练参数。\n6. 在训练开始和结束时添加日志。\n\n这些日志将帮助您更好地追踪 DPO 训练的整个流程，便于调试和监控。如果需要更详细的日志，您可以将一些 `logger.debug` 语句改为 `logger.info`，或者在更多的地方添加日志记录。"}], "model": "human", "yaml_file": "actions/011_chat_action.yml"}
{"instruction": "使用正则抽取 start_tag,end_tag 包括的值,然后替换成标砖的image 标签来 包裹.", "conversations": [{"role": "user", "content": "\n如果你需要生成代码，对于每个需要更改的文件,你需要按 *SEARCH/REPLACE block* 的格式进行生成。\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The opening fence and code language, eg: ```python\n2. The file path alone on a line, starting with \"##File:\" and verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n*SEARCH/REPLACE* blocks will replace *all* matching occurrences.\nInclude enough lines to make the SEARCH blocks unique.\n\nInclude *ALL* the code being searched and replaced!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n下面我们来看一个例子：\n\n当前项目目录结构：\n1. 项目根目录： /tmp/projects/mathweb\n2. 项目子目录/文件列表(类似tree 命令输出)\nflask/\n    app.py\n    templates/\n        index.html\n    static/\n        style.css\n\n用户需求： Change get_factorial() to use math.factorial\n\n回答： To make this change we need to modify `/tmp/projects/mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n用户需求： Refactor hello() into its own file.\n\n回答：To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/hello.py\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/main.py\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n现在让我们开始一个新的任务:\n\n当前项目目录结构：\n1. 项目根目录： /Users/allwefantasy/projects/byzer-llm\n2. 项目子目录/文件列表(类似tree 命令输出)：\nsetup.py\ntests/\n    test_emb_rerank.py\n    test_deepseek_code.py\n    test_prompt.py\n    test_model_meta.py\n    __init__.py\n    test_transformers_deploy.py\n    test_data_analysis.py\n    test_yi.py\n    test_deploy.py\n    functions_suite.py\n    test_template.py\n    auto/\n        test_backend_llama_cpp.py\n        __init__.py\nsetup-machine/\n    convert_safetensor.py\nnotebooks/\n    test.py\nsrc/\n    byzerllm/\n        lang.py\n        store.py\n        version.py\n        log.py\n        __init__.py\n        types.py\n        easy_byzerllm_command.py\n        byzerllm_command.py\n        command_args.py\n        bark/\n            generation.py\n            __init__.py\n            model.py\n            api.py\n            model_fine.py\n            bark_voice.py\n        moss/\n            finetune_moss.py\n            __init__.py\n            moss_inference.py\n            models/\n                configuration_moss.py\n                quantization.py\n                custom_autotune.py\n                __init__.py\n                modeling_moss.py\n                tokenization_moss.py\n        llama2/\n            __init__.py\n        records/\n            __init__.py\n        visualglm/\n            __init__.py\n        auto/\n            __init__.py\n            backend_llama_cpp.py\n            backend_vllm.py\n            backend_ds.py\n        alpha_moss/\n            __init__.py\n        processor/\n            __init__.py\n        qwen/\n            __init__.py\n        utils/\n            json_repaire.py\n            object_store_ref_util.py\n            __init__.py\n            types.py\n            tokenizer.py\n            openai_utils.py\n            ray_utils.py\n            text_generator.py\n            emb.py\n            connect_ray.py\n            testing.py\n            langutil.py\n            nontext.py\n            fulltune/\n                deepspeed_trainner.py\n                __init__.py\n                launch.py\n                trainner.py\n                base_model/\n                    __init__.py\n                    configuration_baichuan.py\n                    modeling_baichuan.py\n                pretrain/\n                    __init__.py\n                    convert_to_transformers.py\n            metrics/\n                __init__.py\n            config/\n                __init__.py\n            retrieval/\n                rest.py\n                __init__.py\n                udf.py\n            sft/\n                argument.py\n                collator.py\n                qlora.py\n                __init__.py\n                model.py\n                dataset.py\n                loss.py\n                merge_lora.py\n                trainer.py\n            client/\n                message_utils.py\n                math_utils.py\n                byzerllm_client.py\n                parallel_utils.py\n                __init__.py\n                types.py\n                code_utils.py\n                img_utils.py\n                entrypoints/\n                    __init__.py\n                    openai/\n                        protocol.py\n                        __init__.py\n                        serve.py\n                        serving_chat.py\n                        api_server.py\n                        serving_engine.py\n                        serving_completion.py\n                        tool.py\n        chatglm2/\n            __init__.py\n        baichuan/\n            __init__.py\n        m3e/\n            __init__.py\n        stable_diffusion/\n            config.py\n            __init__.py\n            logger.py\n            shared.py\n            model.py\n            utils.py\n            images.py\n            diffusion/\n                __init__.py\n                embeddings.py\n                utils.py\n                networks/\n                    lyco.py\n                    __init__.py\n                upscalers/\n                    __init__.py\n                    multidiffusion.py\n                    samplers.py\n                piplines/\n                    __init__.py\n                    lpw.py\n                    diffusers.py\n            lib/\n                __init__.py\n                diffusers/\n                    __init__.py\n                    scheduler.py\n            api/\n                __init__.py\n                plugin.py\n                models/\n                    diffusion.py\n                    __init__.py\n                diffusion/\n                    __init__.py\n                    pipelines/\n                        __init__.py\n                        diffusers.py\n                events/\n                    generation.py\n                    __init__.py\n        bge/\n            __init__.py\n        bge_rerank/\n            __init__.py\n        apps/\n            client.py\n            qa_strategy.py\n            __init__.py\n            builder.py\n            vector_db.py\n            utils.py\n            qa.py\n            byzer_storage/\n                memory_model_based.py\n                command.py\n                env.py\n                train_pt.py\n                train_sft.py\n                __init__.py\n                train_dpo.py\n                simple_api.py\n                generate_sft_data.py\n            byzer_sql/\n                __init__.py\n            llama_index/\n                byzerai_vectordb.py\n                byzerai.py\n                byzerai_embedding.py\n                __init__.py\n                collection_manager.py\n                byzerai_index_store.py\n                byzerai_docstore.py\n                simple_retrieval.py\n                byzerai_kvstore.py\n            agent/\n                conversable_agent.py\n                registry.py\n                groupchat.py\n                user_proxy_agent.py\n                __init__.py\n                agent.py\n                extensions/\n                    simple_retrieval_client.py\n                    planner.py\n                    python_codesandbox_agent.py\n                    preview_file_agent.py\n                    llama_index_retrieval_agent.py\n                    visualization_agent.py\n                    assistant_agent.py\n                    load_data_agent.py\n                    retrieval_agent.py\n                    output_agent.py\n                    __init__.py\n                    byzer_engine_agent.py\n                    subquestion_agent.py\n                    sql_reviewer_agent.py\n                    spark_sql_agent.py\n                    rhetorical_agent.py\n                    data_analysis.py\n                    data_analysis_pipeline_agent.py\n                    common_agent.py\n                    query_rewrite/\n                        time.py\n                        rhetorical.py\n                        __init__.py\n                        condition.py\n                        context.py\n                store/\n                    memory_store.py\n                    __init__.py\n                    stores.py\n        saas/\n            __init__.py\n            chatglm/\n                __init__.py\n            sparkdesk/\n                __init__.py\n            gemini/\n                __init__.py\n            azure_openai/\n                __init__.py\n            azure/\n                __init__.py\n            qianfan/\n                __init__.py\n            claude/\n                __init__.py\n            qianwen/\n                __init__.py\n            volcengine/\n                __init__.py\n            minimax/\n                __init__.py\n            baichuan/\n                __init__.py\n            qianwen_vl/\n                __init__.py\n            official_openai/\n                __init__.py\n            zhipu/\n                __init__.py\n            aws_bedrock/\n                __init__.py\n            openai/\n                __init__.py\n        falcon/\n            __init__.py\n        chatglm6b/\n            trainer_seq2seq.py\n            arguments.py\n            __init__.py\n            finetune.py\n            tunning/\n                __init__.py\n                infer.py\n                finetune.py\n                utils/\n                    config.py\n                    __init__.py\n                    other.py\n                    common.py\n                    seq2seq.py\n        starcode/\n            __init__.py\n        qwen_vl_chat/\n            __init__.py\n        zephyr/\n            __init__.py\n        dolly/\n            generate.py\n            __init__.py\n            consts.py\n            dolly_inference.py\n            trainer.py\n        llama/\n            __init__.py\n        whisper/\n            whisper_inference.py\n            __init__.py\n下面是一些文件路径以及每个文件对应的源码：\n\n##File: /Users/allwefantasy/projects/byzer-llm/docs/zh/003_byzerllm_大模型编程快速指南.md\n##  byzerllm 大模型编程快速指南\n\n本文示例 [Notebook](../../notebooks/003_byzerllm_大模型编程快速指南.ipynb)\n\n## 安装\n\n```bash\npip install -U byzerllm\nray start --head\n```\n## 启动一个模型代理\n\nbyzerllm 支持私有化模型或者SaaS模型的部署。\n\n这里以 deepseek 官方API 为例：\n\n```bash\neasy-byzerllm deploy deepseek-chat --token xxxxx --alias deepseek_chat\n```\n\n或者跬基流动API:\n\n```bash\neasy-byzerllm deploy alibaba/Qwen1.5-110B-Chat --token xxxxx --alias qwen110b_chat\n```\n\n将上面的 API KEY 替换成你们自己的。\n\n如果你想部署私有化模型或者对接 Ollama 等更多需求，参考 [002_使用byzerllm进行模型部署.md](./002_使用byzerllm进行模型部署.md) 或者 [README.md](../../README.md)。\n\n之后，你就可以在代码里使用  deepseek_chat 或者 qwen110b_chat  访问模型了。\n\n## hello world\n\n来和我们的大模型打个招呼:\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\n@byzerllm.prompt(llm=llm)\ndef hello(q:str) ->str:\n    '''\n    你好, {{ q }}\n    '''\n\ns = hello(\"你是谁\")    \nprint(s)\n\n## 输出:\n## '你好！我是一个人工智能助手，专门设计来回答问题、提供信息和帮助解决问题。如果你有任何疑问或需要帮助，请随时告诉我。'\n```\n\n恭喜，你和大模型成功打了招呼！\n\n可以看到，我们通过 `@byzerllm.prompt` 装饰器，将一个方法转换成了一个大模型的调用，然后这个方法的主题是一段文本，文本中\n使用了 jinja2 模板语法，来获得方法的参数。当正常调用该方法时，实际上就发起了和大模型的交互，并且返回了大模型的结果。\n\n在 byzerllm 中，我们把这种方法称之为 prompt 函数。\n\n## 查看发送给大模型的prompt\n\n很多情况你可能需要调试，查看自己的 prompt 渲染后到底是什么样子的，这个时候你可以通过如下方式\n获取渲染后的prompt:\n\n```python\nhello.prompt(\"你是谁\")\n## '你好, 你是谁'\n```            \n\n## 动态换一个模型\n\n前面的 hello 方法在初始化的时候，我们使用了默认的模型 deepseek_chat，如果我们想换一个模型，可以这样做：\n\n```python\nhello.with_llm(llm).run(\"你是谁\")\n## '你好！我是一个人工智能助手，专门设计来回答问题、提供信息和帮助解决问题。如果你有任何疑问或需要帮助，请随时告诉我。'\n```\n\n通过 with_llm 你可以设置一个新的 llm 对象，然后调用 run 方法，就可以使用新的模型了。\n\n## 超长文本生成\n\n我们知道，大模型一次生成的长度其实是有限的，如果你想生成超长文本，你可能需手动的不断获得\n生成结果，然后把他转化为输入，然后再次生成，这样的方式是比较麻烦的。\n\nbyzerllm 提供了更加易用的 API :\n\n```python\nimport byzerllm\nfrom byzerllm import ByzerLLM\n\nllm = ByzerLLM.from_default_model(\"deepseek_chat\")\n\n@byzerllm.prompt()\ndef tell_story() -> str:\n    \"\"\"\n    讲一个100字的故事。    \n    \"\"\"\n\n\ns = (\n    tell_story.with_llm(llm)\n    .with_response_markers()\n    .options({\"llm_config\": {\"max_length\": 10}})\n    .run()\n)\nprint(s)\n\n## 从前，森林里住着一只聪明的小狐狸。一天，它发现了一块闪闪发光的宝石。小狐狸决定用这块宝石帮助森林里的动物们。它用宝石的光芒指引迷路的小鹿找到了回家的路，用宝石的温暖治愈了受伤的小鸟。从此，小狐狸成了森林里的英雄，动物们都感激它的善良和智慧。\n```\n\n实际核心部分就是这一行：\n\n```python\ntell_story.with_llm(llm)\n    .with_response_markers()    \n    .run()\n```\n\n我们只需要调用 `with_response_markers` 方法，系统就会自动的帮我们生成超长文本。\n在上面的案例中，我们通过\n\n```python\n.options({\"llm_config\": {\"max_length\": 10}})\n```\n\n认为的限制大模型一次交互最多只能输出10个字符，但是系统依然自动完成了远超过10个字符的文本生成。\n\n## 对象输出\n\n前面我们的例子都是返回字符串，但是我们也可以返回对象，这样我们就可以更加灵活的处理返回结果。\n\n```python\nimport pydantic \n\nclass Story(pydantic.BaseModel):\n    '''\n    故事\n    '''\n\n    title: str = pydantic.Field(description=\"故事的标题\")\n    body: str = pydantic.Field(description=\"故事主体\")\n\n@byzerllm.prompt()\ndef tell_story()->Story:\n    '''\n    讲一个100字的故事。    \n    '''\n\ns = tell_story.with_llm(llm).run()\nprint(isinstance(s, Story))\nprint(s.title)\n\n## True\n## 勇敢的小鸟\n```\n\n可以看到，我们很轻松的将输出转化为格式化输出。\n\n## 自定义字段抽取\n\n前面的结构化输出，其实会消耗更多token,还有一种更加精准的结构化输出方式。\n比如让大模型生成一个正则表达式，但实际上大模型很难准确只输出一个正则表达式，这个时候我们可以通过自定义抽取函数来获取我们想要的结果。\n\n\n```python\nfrom loguru import logger\nimport re\n\n@byzerllm.prompt()\ndef generate_regex_pattern(desc: str) -> str:\n    \"\"\"\n    根据下面的描述生成一个正则表达式，要符合python re.compile 库的要求。\n\n    {{ desc }}\n\n    最后生成的正则表达式要在<REGEX></REGEX>标签对里。\n    \"\"\"    \n\ndef extract_regex_pattern(regex_block: str) -> str:    \n    pattern = re.search(r\"<REGEX>(.*)</REGEX>\", regex_block, re.DOTALL)\n    if pattern is None:\n        logger.warning(\"No regex pattern found in the generated block:\\n {regex_block}\")\n        raise None\n    return pattern.group(1)\n\npattern = \"匹配一个邮箱地址\"\nv = generate_regex_pattern.with_llm(llm).with_extractor(extract_regex_pattern).run(desc=pattern)\nprint(v)\n## ^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\n```\n\n在上面的例子里，我们根据一句话生成一个正则表达式。我们通过 `with_extractor` 方法，传入了一个自定义的抽取函数，这个函数会在大模型生成结果后，对结果进行处理，然后返回我们想要的结果。\n\n我们在 prompt 明确说了，生成的结果要放到 `<REGEX></REGEX>` 标签对里，然后我们通过 extract_regex_pattern 函数，从结果中提取出了我们想要的正则表达式。\n\n## 在实例方法中使用大模型\n\n```python\nimport byzerllm\ndata = {\n    'name': 'Jane Doe',\n    'task_count': 3,\n    'tasks': [\n        {'name': 'Submit report', 'due_date': '2024-03-10'},\n        {'name': 'Finish project', 'due_date': '2024-03-15'},\n        {'name': 'Reply to emails', 'due_date': '2024-03-08'}\n    ]\n}\n\n\nclass RAG():\n    def __init__(self):        \n        self.llm = byzerllm.ByzerLLM()\n        self.llm.setup_template(model=\"deepseek_chat\",template=\"auto\")\n        self.llm.setup_default_model_name(\"deepseek_chat\")        \n    \n    @byzerllm.prompt(lambda self:self.llm)\n    def generate_answer(self,name,task_count,tasks)->str:\n        '''\n        Hello {{ name }},\n\n        This is a reminder that you have {{ task_count }} pending tasks:\n        {% for task in tasks %}\n        - Task: {{ task.name }} | Due: {{ task.due_date }}\n        {% endfor %}\n\n        Best regards,\n        Your Reminder System\n        '''        \n\nt = RAG()\n\nresponse = t.generate_answer(**data)\nprint(response)\n\n## 输出:\n## Hello Jane Doe,\n##I hope this message finds you well. I wanted to remind you of your 3 pending tasks to ensure you stay on track:\n## 1. **Submit report** - This task is due on **2024-03-10**. Please ensure that you allocat\n```\n\n这里我们给了个比较复杂的例子，但我们可以看到，给一个实例prompt方法和普通prompt 方法差异不大。\n唯一的区别是如果你希望在定义的时候就指定大模型，使用一个lambda函数返回实例的 llm 对象即可。\n\n```python\n@byzerllm.prompt(lambda self:self.llm)\n```\n\n你也可以不返回，在调用的时候通过 `with_llm` 方法指定 llm 对象。\n\n此外，这个例子也展示了如何通过jinja2模板语法，来处理复杂的结构化数据。\n\n## 通过 Python 代码处理复杂入参\n\n上面的一个例子中，我们通过 jinja2 模板语法，来处理复杂的结构化数据，但是有时候我们可能需要更加复杂的处理，这个时候我们可以通过 Python 代码来处理。\n\n```python\nimport byzerllm\n\ndata = {\n    'name': 'Jane Doe',\n    'task_count': 3,\n    'tasks': [\n        {'name': 'Submit report', 'due_date': '2024-03-10'},\n        {'name': 'Finish project', 'due_date': '2024-03-15'},\n        {'name': 'Reply to emails', 'due_date': '2024-03-08'}\n    ]\n}\n\n\nclass RAG():\n    def __init__(self):        \n        self.llm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n    \n    @byzerllm.prompt()\n    def generate_answer(self,name,task_count,tasks)->str:\n        '''\n        Hello {{ name }},\n\n        This is a reminder that you have {{ task_count }} pending tasks:\n            \n        {{ tasks }}\n\n        Best regards,\n        Your Reminder System\n        '''\n        \n        tasks_str = \"\\n\".join([f\"- Task: {task['name']} | Due: { task['due_date'] }\" for task in tasks])\n        return {\"tasks\": tasks_str}\n\nt = RAG()\n\nresponse = t.generate_answer.with_llm(t.llm).run(**data)\nprint(response)\n\n## Just a gentle nudge to keep you on track with your pending tasks. Here's a quick recap:....\n```\n\n在这个例子里，我们直接把 tasks 在方法体里进行处理，然后作为一个字符串返回，最够构建一个字典，字典的key为 tasks,然后\n你就可以在 docstring 里使用 `{{ tasks }}` 来引用这个字符串。\n\n这样对于很复杂的入参，就不用谢繁琐的 jinja2 模板语法了。\n\n## 如何自动实现一个方法\n\n比如我定义一个签名，但是我不想自己实现里面的逻辑，让大模型来实现。这个在 byzerllm 中叫 function impl。我们来看看怎么\n实现:\n\n```python\nimport pydantic\nclass Time(pydantic.BaseModel):\n    time: str = pydantic.Field(...,description=\"时间，时间格式为 yyyy-MM-dd\")\n\n\n@llm.impl()\ndef calculate_current_time()->Time:\n    '''\n    计算当前时间\n    '''\n    pass \n\n\ncalculate_current_time()\n#output: Time(time='2024-06-14')\n```\n\n在这个例子里，我们定义了一个 calculate_current_time 方法，但是我们没有实现里面的逻辑，我们通过 `@llm.impl()` 装饰器，让大模型来实现这个方法。\n为了避免每次都要“生成”这个方法，导致无法适用，我们提供了缓存，用户可以按如下方式打印速度：\n\n```python\nstart = time.monotonic()\ncalculate_current_time()\nprint(f\"first time cost: {time.monotonic()-start}\")\n\nstart = time.monotonic()\ncalculate_current_time()\nprint(f\"second time cost: {time.monotonic()-start}\")\n\n# output:\n# first time cost: 6.067266260739416\n# second time cost: 4.347506910562515e-05\n```\n可以看到，第一次执行花费了6s,第二次几乎是瞬间完成的，这是因为第一次执行的时候，我们实际上是在生成这个方法，第二次执行的时候，我们是执行已经生成好的代码，所以速度会非常快。你可以显示的调用 `llm.clear_impl_cache()` 清理掉函数缓存。\n\n## Stream 模式\n\n前面的例子都是一次性生成结果，但是有时候我们可能需要一个流式的输出，这个时候我们可能需要用底层一点的API来完成了：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\nv = llm.stream_chat_oai(model=\"deepseek_chat\",conversations=[{\n    \"role\":\"user\",\n    \"content\":\"你好，你是谁\",\n}],delta_mode=True)\n\nfor t in v:\n    print(t,flush=True)  \n\n# 你好\n# ！\n# 我\n# 是一个\n# 人工智能\n# 助手\n# ，\n# 旨在\n# 提供\n# 信息\n# 、\n# 解答\n# 问题....\n```\n\n如果你不想要流式输出，但是想用底层一点的API，你可以使用 `llm.chat_oai` 方法：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(model=\"deepseek_chat\")\n\nv = llm.chat_oai(model=\"deepseek_chat\",conversations=[{\n    \"role\":\"user\",\n    \"content\":\"你好，你是谁\",\n}])\n\nprint(v[0].output)\n## 你好！我是一个人工智能助手，旨在提供信息、解答问题和帮助用户解决问题。如果你有任何问题或需要帮助，请随时告诉我。\n```\n\n## Function Calling \n\nbyzerllm 可以不依赖模型自身就能提供 function calling 支持，我们来看个例子：\n\n\n```python\nfrom typing import List,Dict,Any,Annotated\nimport pydantic \nimport datetime\nfrom dateutil.relativedelta import relativedelta\n\ndef compute_date_range(count:Annotated[int,\"时间跨度，数值类型\"],\n                       unit:Annotated[str,\"时间单位，字符串类型\",{\"enum\":[\"day\",\"week\",\"month\",\"year\"]}])->List[str]:\n    '''\n    计算日期范围\n\n    Args:\n        count: 时间跨度，数值类型\n        unit: 时间单位，字符串类型，可选值为 day,week,month,year\n    '''        \n    now = datetime.datetime.now()\n    now_str = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n    if unit == \"day\":\n        return [(now - relativedelta(days=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"week\":\n        return [(now - relativedelta(weeks=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"month\":\n        return [(now - relativedelta(months=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    elif unit == \"year\":\n        return [(now - relativedelta(years=count)).strftime(\"%Y-%m-%d %H:%M:%S\"),now_str]\n    return [\"\",\"\"]\n\ndef compute_now()->str:\n    '''\n    计算当前时间\n    '''\n    return datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n```\n\n我们定义了两个方法，一个是计算日期范围，一个是计算当前时间。\n\n现在我么可以来测试下，系统如何根据自然语言决定调用哪个方法：\n\n```python\nt = llm.chat_oai([{\n    \"content\":'''计算当前时间''',\n    \"role\":\"user\"    \n}],tools=[compute_date_range,compute_now],execute_tool=True)\n\nt[0].values\n\n## output: ['2024-06-14 15:18:02']\n```\n\n我们可以看到，他正确的选择了 compute_now 方法。\n\n接着我们再试一个：\n\n```python\nt = llm.chat_oai([{\n    \"content\":'''最近三个月趋势''',\n    \"role\":\"user\"    \n}],tools=[compute_date_range,compute_now],execute_tool=True)\n\nt[0].values\n\n## output: [['2024-03-14 15:19:13', '2024-06-14 15:19:13']]\n```\n\n模型正确的选择了 compute_date_range 方法。\n\n## 多模态\n\nbyerllm 也能很好的支持多模态的交互，而且统一了多模态大模型的接口，比如你可以用一样的方式使用 openai 或者 claude 的图片转文字能力， 或者一致的方式使用火山，azuer, openai的语音合成接口。\n\n### 图生文\n\n```python\nimport byzerllm\nfrom byzerllm.types import ImagePath\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\n\n\n@byzerllm.prompt()\ndef what_in_image(image_path: ImagePath) -> str:\n    \"\"\"\n    {{ image_path }}\n    这个图片里有什么？\n    \"\"\"    \n\n\nv = what_in_image.with_llm(vl_llm).run(\n    ImagePath(value=\"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\")\n)\nv\n## OUTPUT: 这张图片展示了多只可爱的猫咪，采用了艺术风格的绘画。猫咪们有不同的颜色和花纹，背景是浅棕色，上面还点缀着一些红色的花朵。整体画面给人一种温馨和谐的感觉\n```\n\n可以看到，我们只需要把 prompt 函数的图片地址入参使用 byzerllm.types.ImagePath里进行包装，就可以直接在 prompt 函数体里\n带上图片。\n\n或者你可以这样：\n\n```python\nimport byzerllm\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\n\n\n@byzerllm.prompt()\ndef what_in_image(image_path: str) -> str:\n    \"\"\"\n    {{ image }}\n    这个图片里有什么？\n    \"\"\"\n    return {\"image\": byzerllm.Image.load_image_from_path(image_path)}\n\n\nv = what_in_image.with_llm(vl_llm).run(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv\n```\n\n通过 `image_path` 参数，然后通过 `byzerllm.Image.load_image_from_path` 方法，转化为一个图片对象 image，最后在 prompt 函数体里\n使用 `{{ image }}` 引用这个图片对象。\n\n另外我们也是可以支持配置多张图片的。\n\n另外，我们也可以使用基础的 `llm.chat_oai` 方法来实现：\n\n```python\nimport byzerllm\nimport json\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\nimage = byzerllm.Image.load_image_from_path(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv = vl_llm.chat_oai(\n    conversations=[\n        {\n            \"role\": \"user\",\n            \"content\": json.dumps(\n                [{\"image\": image, \"text\": \"这个图片里有什么？\"}], ensure_ascii=False\n            ),\n        }\n    ]\n)\nv[0].output\n```\n\n还可以这么写：\n\n```python\nimport byzerllm\nimport json\n\nvl_llm = byzerllm.ByzerLLM.from_default_model(\"gpt4o_mini_chat\")\nimage = byzerllm.Image.load_image_from_path(\n    \"/Users/allwefantasy/projects/byzer-llm/images/cat1.png\"\n)\nv = vl_llm.chat_oai(\n    conversations=[\n        {\n            \"role\": \"user\",\n            \"content\": json.dumps(\n                [\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\"url\": image, \"detail\": \"high\"},\n                    },\n                    {\"text\": \"这个图片里有什么？\", \"type\": \"text\"},\n                ],\n                ensure_ascii=False,\n            ),\n        }\n    ]\n)\nv[0].output\n```\n\n### 语音合成\n\n这里以 openai 的 tts 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=tts-1 \\\n--model openai_tts\n```\n\n此外，byzerllm 支持 azure,火山引擎等 tts 语音合成引擎。\n\n接着你可以这么用：\n\n```python\nimport byzerllm\nimport base64\nimport json\n\nllm = byzerllm.ByzerLLM.from_default_model(\"openai_tts\")\n\n\nt = llm.chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\": json.dumps({\n        \"input\":\"hello, open_tts\",\n        \"voice\": \"alloy\",\n        \"response_format\": \"mp3\"\n    },ensure_ascii=False)\n}])\n\nwith open(\"voice.mp3\",\"wb\") as f:\n    f.write(base64.b64decode(t[0].output))\n```\n\ntts 模型生成没有prompt函数可以用，你需要直接使用 chat_oai。\n\n\n### 语音识别\n\n这里以 openai 的 whisper-1 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--worker_concurrency 10 \\\n--infer_params saas.model=whisper-1 saas.api_key=${MODEL_OPENAI_TOKEN} \\\n--model speech_to_text\n```\n\n语音识别的使用方式和图生文类似，我们可以直接在 prompt 函数体里带上音频文件。\n\n```python\nimport byzerllm\nimport json\nimport base64\nfrom byzerllm.types import AudioPath\n\nllm = byzerllm.ByzerLLM.from_default_model(\"speech_to_text\")\n\naudio_file = \"/Users/allwefantasy/videos/output_audio.mp3\"\n\n@byzerllm.prompt(llm=llm)\ndef audio_to_text(audio_file: AudioPath):\n    \"\"\"\n    {{ audio_file }}\n    \"\"\"\n\nv = audio_to_text(AudioPath(value=audio_file))\njson.loads(v)\n```\n输出的数据格式略微复杂：\n\n```\n{'text': 'In the last chapter, you and I started to step through the internal workings of a transformer. This is one of the key pieces of technology inside large language models, and a lot of other tools in the modern wave of AI.',\n 'task': 'transcribe',\n 'language': 'english',\n 'duration': 10.0,\n 'segments': [{'id': 0,\n   'seek': 0,\n   'start': 0.0,\n   'end': 4.78000020980835,\n   'text': ' In the last chapter, you and I started to step through the internal workings of a transformer.',\n   'tokens': [50364,\n    682,\n    264,\n    1036,\n    7187,\n    11,\n    291,\n    293,\n    286,\n    1409,\n    281,\n    1823,\n    807,\n    264,\n    6920,\n    589,\n    1109,\n    295,\n    257,\n    31782,\n    13,\n    50586],\n   'temperature': 0.0,\n   'avg_logprob': -0.28872039914131165,\n   'compression_ratio': 1.4220778942108154,\n   'no_speech_prob': 0.016033057123422623},\n  ....\n  {'id': 2,\n   'seek': 0,\n   'start': 8.579999923706055,\n   'end': 9.979999542236328,\n   'text': ' and a lot of other tools in the modern wave of AI.',\n   'tokens': [50759,\n    293,\n    257,\n    688,\n    295,\n    661,\n    3873,\n    294,\n    264,\n    4363,\n    5772,\n    295,\n    7318,\n    13,\n    50867],\n   'temperature': 0.0,\n   'avg_logprob': -0.28872039914131165,\n   'compression_ratio': 1.4220778942108154,\n   'no_speech_prob': 0.016033057123422623}],\n 'words': [{'word': 'In', 'start': 0.0, 'end': 0.18000000715255737},\n  {'word': 'the', 'start': 0.18000000715255737, 'end': 0.23999999463558197},\n  {'word': 'last', 'start': 0.23999999463558197, 'end': 0.5400000214576721},\n  {'word': 'chapter', 'start': 0.5400000214576721, 'end': 0.800000011920929},\n  ....\n  {'word': 'AI', 'start': 9.920000076293945, 'end': 9.979999542236328}]}\n```\n\n会输出每一句话以及每一个字所在的起始时间和截止时间。你可以根据需要来使用。\n\n### 文生图\n\n文生图和语音合成类似，首先要启动合适的模型,以openai 的 dall-e-3 为例：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=dall-e-3 \\\n--model openai_image_gen\n```\n\n启动模型后，只需要记住几个模板参数即可使用，这里直接使用 chat_oai 方法来使用：\n\n```python\n\nimport byzerllm\nimport json\nimport base64\n\nllm = byzerllm.ByzerLLM.from_default_model(\"openai_image_gen\")\nt = llm.chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\": json.dumps({\n        \"input\":\"a white siamese cat\",\n        \"size\": \"1024x1024\",\n        \"quality\": \"standard\"\n    },ensure_ascii=False)\n}])\n\nwith open(\"output1.jpg\",\"wb\") as f:\n    f.write(base64.b64decode(t[0].output))\n\n\nimport matplotlib.pyplot as plt\n\nimage_path = \"output1.jpg\"\nimage = plt.imread(image_path)\n\nplt.imshow(image)\nplt.axis('off')\nplt.show()\n```\n\n## Prompt 函数的流式输出\n\nbyzerllm 底层支持流式输出，非 prompt 函数的用法是这样的：\n\n```python\nimport byzerllm\n\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\n\nv = llm.stream_chat_oai(conversations=[{\n    \"role\":\"user\",\n    \"content\":\"讲一个100字的故事\"\n}])\n\nfor s  in v:\n    print(s[0], end=\"\")\n```\n\n如果你像用 prompt 函数，可以这么用：\n\n\n```python\nimport byzerllm\nimport json\nimport base64\nfrom typing import Generator\n\nllm = byzerllm.ByzerLLM.from_default_model(\"deepseek_chat\")\n\n@byzerllm.prompt()\ndef tell_story() -> Generator[str, None, None]:\n    '''\n    给我讲一个一百多字的故事\n    '''\n\nv = tell_story.with_llm(llm).run()    \nfor i in v:\n    print(i, end=\"\")\n```\n\n可以看到，和普通的 prompt 函数的区别在于，返回值是一个生成器，然后你可以通过 for 循环来获取结果。\n\n## 注意事项\n\n1. prompt函数方法体返回只能是dict，实际的返回类型和方法签名可以不一样，但是方法体返回只能是dict。\n2. 大部分情况prompt函数体为空，如果一定要有方法体，可以返回一个空字典。\n3. 调用prompt方法的时候，如果在@byzerllm.prompt()里没有指定llm对象，那么需要在调用的时候通过with_llm方法指定llm对象。\n\n\n\n\n\n\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/utils/nontext.py\nfrom typing import List, Dict, Any, Union, Optional\nimport pydantic\nimport base64\nimport os\n\n\nclass Tag(pydantic.BaseModel):\n    start_tag: str\n    end_tag: str\n    content: Union[str, List[\"Tag\"], \"Tag\"]\n    parent: Optional[\"Tag\"] = None\n\n\nclass TagExtractor:\n    def __init__(self, text: str):\n        self.text = text\n        self.pos = -1\n        self.len = len(text)\n        self.root_tag = Tag(start_tag=\"<_ROOT_>\", end_tag=\"</_ROOT_>\", content=[])\n        self.current_tag = None\n        self.is_extracted = False\n\n    def peek(self) -> str:\n        if self.pos + 1 < self.len:\n            return self.text[self.pos + 1]\n        return None\n\n    def peek2(self) -> str:\n        if self.pos + 1 < self.len:\n            return self.text[self.pos + 2]\n        return None\n\n    def peek3(self) -> str:\n        if self.pos + 1 < self.len:\n            return self.text[self.pos + 3]\n        return None\n\n    def next(self) -> str:\n        if self.pos < self.len - 1:\n            self.pos += 1\n            char = self.text[self.pos]\n            return char\n        return None\n\n    def is_full_tag(self) -> bool:\n        return self.current_tag.start_tag and self.current_tag.end_tag\n\n    def is_start_tag(self) -> bool:\n        return self.peek() and self.peek() == \"<\" and self.peek2() == \"_\"\n\n    def extract_start_tag(self) -> str:\n        tag = \"\"\n\n        while self.peek() and self.peek() != \">\":\n            tag += self.next()\n        tag += self.next()\n\n        if self.current_tag is None or self.current_tag == self.root_tag:\n            self.current_tag = Tag(\n                start_tag=tag, end_tag=\"\", content=\"\", parent=self.root_tag\n            )\n            self.root_tag.content.append(self.current_tag)\n        ## 当前tag已经闭合，找到当前tag的父tag,遇到新tag\n        elif self.is_full_tag():\n            parent = self.current_tag.parent or self.current_tag\n            current_tag = Tag(\n                start_tag=tag,\n                end_tag=\"\",\n                content=\"\",\n                parent=parent,\n            )\n            if isinstance(parent.content, list):\n                parent.content.append(current_tag)\n            else:\n                s = self.current_tag.content\n                parent.content = []\n                if s:\n                    parent.content.append(s)\n                parent.content.append(current_tag)\n            self.current_tag = current_tag\n        ## 当前tag还没有闭合，直接添加到当前tag的content中\n        elif not self.is_full_tag():\n            current_tag = Tag(\n                start_tag=tag, end_tag=\"\", content=\"\", parent=self.current_tag\n            )\n            if isinstance(self.current_tag.content, list):\n                self.current_tag.content.append(current_tag)\n            else:\n                s = self.current_tag.content\n                self.current_tag.content = []\n                if s:\n                    self.current_tag.content.append(s)\n                self.current_tag.content.append(current_tag)\n            self.current_tag = current_tag\n\n        return tag\n\n    def is_end_tag(self) -> bool:\n        return (\n            self.peek()\n            and self.peek() == \"<\"\n            and self.peek2() == \"/\"\n            and self.peek3() == \"_\"\n        )\n\n    def extract_end_tag(self) -> str:\n        tag = \"\"\n        while self.peek() and self.peek() != \">\":\n            tag += self.next()\n        tag += self.next()\n        self.current_tag.end_tag = tag\n        self.current_tag = self.current_tag.parent\n        return tag\n\n    def consume_blank(self):\n        while (\n            self.peek() == \"\\n\"\n            or self.peek() == \" \"\n            or self.peek() == \"\\t\"\n            or self.peek() == \"\\r\"\n        ):\n            self.next()\n\n    def is_in_tag(self) -> bool:\n        return (\n            self.current_tag\n            and self.current_tag.start_tag\n            and not self.current_tag.end_tag\n        )\n\n    def is_tag_content(self) -> bool:\n        if not self.root_tag:\n            return False\n        temp_pos = self.pos\n        self.consume_blank()\n        if self.is_start_tag():\n            self.pos = temp_pos\n            return True\n        if self.is_end_tag():\n            self.pos = temp_pos\n            return True\n        return False\n\n    def extract_str_content(self) -> str:\n        content = \"\"\n        while not self.is_start_tag() and not self.is_end_tag():\n            if self.peek() is None:\n                break\n            content += self.next()\n\n        self.current_tag.content = content\n        return content\n\n    def is_not_in_tag_str(self) -> bool:\n        if not self.root_tag:\n            return True\n        if not self.current_tag:\n            return True\n        if not self.current_tag.start_tag and not self.current_tag.end_tag:\n            return True\n        if self.current_tag.start_tag and self.current_tag.end_tag:\n            return True\n        return False\n\n    def extract_content_not_in_tag(self) -> str:\n        content = \"\"\n        while self.peek() and not self.is_start_tag() and not self.is_end_tag():\n            content += self.next()\n        return content\n\n    def extract(self) -> Union[Tag]:\n        if self.is_extracted:\n            return self.root_tag\n        while True:\n            if self.pos == self.len - 1:\n                break\n            if self.is_start_tag():\n                self.extract_start_tag()\n            elif self.is_end_tag():\n                self.extract_end_tag()\n            elif self.is_in_tag():\n                if self.is_tag_content():\n                    self.consume_blank()\n                    continue\n                else:\n                    self.extract_str_content()\n            elif self.is_not_in_tag_str():\n                self.extract_content_not_in_tag()\n        self.is_extracted = True\n        return self.root_tag\n\n\nclass Image(TagExtractor):\n\n    def __init__(self, text: str):\n        super().__init__(text)\n        self.is_extracted = False\n\n    def has_image(self) -> bool:\n        self.extract()\n        for item in self.root_tag.content:\n            if (\n                isinstance(item, Tag)\n                and item.start_tag == \"<_image_>\"\n                and item.end_tag == \"</_image_>\"\n                and item.content.startswith(\"data:image/\")\n            ):\n                return True\n        return False\n    \n    def convert_image_paths_from(text:str,start_tag:str=\"<img>\",end_tag:str=\"</img>\"):\n        pass\n\n    @staticmethod\n    def extract_image_paths(text: str, to_base64: bool = False) -> List[str]:\n        v = Image(text)\n        v.extract()\n        result = []\n        for item in v.root_tag.content:\n            if (\n                isinstance(item, Tag)\n                and item.start_tag in [\"<_image_>\", \"<_img_>\"]\n                and item.end_tag in [\"</_image_>\", \"</_img_>\"]\n                and not item.content.startswith(\"data:image/\")\n            ):\n                if to_base64:\n                    result.append(Image.load_image_from_path(item.content.strip()))\n                else:\n                    result.append(item.content.strip())\n        return result\n\n    @staticmethod\n    def load_image_from_path(path: str) -> str:\n        \"\"\"\n        Load image from path and return base64 image data\n        \"\"\"\n\n        file_extension = os.path.splitext(path)[1][1:].lower()\n        image_type = file_extension\n\n        with open(path, \"rb\") as image_file:\n            encoded_string = base64.b64encode(image_file.read()).decode(\"utf-8\")\n        return f\"<_image_>data:image/{image_type};base64,{encoded_string}</_image_>\"\n\n    @staticmethod\n    def save_image_to_path(\n        image_data: str, output_path: str, auto_fix_suffix: bool = False\n    ) -> str:\n        \"\"\"\n        Save base64 image data as image file\n        \"\"\"\n        import base64\n        import os\n\n        # Extract image type and base64 data\n        data_prefix = \"data:image/\"\n        base64_prefix = \";base64,\"\n        if not image_data.startswith(data_prefix):\n            raise ValueError(\"Invalid image data format\")\n\n        format_end = image_data.index(base64_prefix)\n        image_type = image_data[len(data_prefix) : format_end]\n        base64_data = image_data[format_end + len(base64_prefix) :]\n\n        # Decode the base64 image data\n        image_data = base64.b64decode(base64_data)\n\n        # Check and fix file extension if necessary\n        file_name, file_extension = os.path.splitext(output_path)\n        correct_extension = f\".{image_type}\"\n\n        if auto_fix_suffix and file_extension.lower() != correct_extension.lower():\n            output_path = file_name + correct_extension\n\n        # Write the image data to the file\n        with open(output_path, \"wb\") as image_file:\n            image_file.write(image_data)\n\n        return output_path\n\n    def to_content(self) -> List[Dict[str, str]]:\n        self.extract()\n\n        result = []\n        current_item = {}\n\n        for item in self.root_tag.content:\n            if isinstance(item, Tag) and item.start_tag == \"<_image_>\":\n                if current_item:\n                    result.append(current_item)\n                    current_item = {}\n                current_item[\"image\"] = item.content\n\n        if current_item:\n            result.append(current_item)\n\n        new_text = self.text\n        for res in result:\n            new_text = new_text.replace(f\"<_image_>{res['image']}</_image_>\", \"\")\n\n        result.append({\"text\": new_text.strip()})\n\n        # for item in result:\n        #     if \"image\" in item:\n        #         print(item[\"image\"][0:100] + \"...\")\n        #     else:\n        #         print(item[\"text\"][0:100] + \"...\")\n        return result\n\n\nclass Audio(TagExtractor):\n    def __init__(self, text: str):\n        super().__init__(text)\n        self.is_extracted = False\n\n    def has_audio(self) -> bool:\n        self.extract()\n        for item in self.root_tag.content:\n            if (\n                isinstance(item, Tag)\n                and item.start_tag == \"<_audio_>\"\n                and item.end_tag == \"</_audio_>\"\n                and item.content.startswith(\"data:audio/\")\n            ):\n                return True\n        return False\n    \n    @staticmethod\n    def extract_audio_paths(text: str, to_base64: bool = False) -> List[str]:\n        v = Image(text)\n        v.extract()\n        result = []\n        for item in v.root_tag.content:\n            if (\n                isinstance(item, Tag)\n                and item.start_tag in [\"<_audio_>\"]\n                and item.end_tag in [\"</_audio_>\"]\n                and not item.content.startswith(\"data:audio/\")\n            ):\n                if to_base64:\n                    result.append(Audio.load_audio_from_path(item.content.strip()))\n                else:\n                    result.append(item.content.strip())\n        return result\n\n    @staticmethod\n    def load_audio_from_path(path: str) -> str:\n        \"\"\"\n        Load audio from path and return base64 audio data\n        \"\"\"\n        file_extension = os.path.splitext(path)[1][1:].lower()\n        audio_type = file_extension\n\n        with open(path, \"rb\") as audio_file:\n            encoded_string = base64.b64encode(audio_file.read()).decode(\"utf-8\")\n        return f\"<_audio_>data:audio/{audio_type};base64,{encoded_string}</_audio_>\"\n\n    @staticmethod\n    def save_audio_to_path(\n        audio_data: str, output_path: str, auto_fix_suffix: bool = False\n    ) -> str:\n        \"\"\"\n        Save base64 audio data as audio file\n        \"\"\"\n        # Extract audio type and base64 data\n        data_prefix = \"data:audio/\"\n        base64_prefix = \";base64,\"\n        if not audio_data.startswith(data_prefix):\n            raise ValueError(\"Invalid audio data format\")\n\n        format_end = audio_data.index(base64_prefix)\n        audio_type = audio_data[len(data_prefix) : format_end]\n        base64_data = audio_data[format_end + len(base64_prefix) :]\n\n        # Decode the base64 audio data\n        audio_data = base64.b64decode(base64_data)\n\n        # Check and fix file extension if necessary\n        file_name, file_extension = os.path.splitext(output_path)\n        correct_extension = f\".{audio_type}\"\n\n        if auto_fix_suffix and file_extension.lower() != correct_extension.lower():\n            output_path = file_name + correct_extension\n\n        # Write the audio data to the file\n        with open(output_path, \"wb\") as audio_file:\n            audio_file.write(audio_data)\n\n        return output_path\n\n    def to_content(self) -> Dict[str, Any]:\n        self.extract()\n        current_item = {}\n        counter = 0\n        for item in self.root_tag.content:\n            if isinstance(item, Tag) and item.start_tag == \"<_audio_>\":\n                if counter > 0:\n                    raise ValueError(\"Only one audio file is allowed\")\n                counter += 1\n                current_item[\"audio\"] = item.content\n\n        return current_item\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/utils/nontext.py\nfrom typing import List, Dict, Any, Union, Optional\nimport pydantic\nimport base64\nimport os\n\n\nclass Tag(pydantic.BaseModel):\n    start_tag: str\n    end_tag: str\n    content: Union[str, List[\"Tag\"], \"Tag\"]\n    parent: Optional[\"Tag\"] = None\n\n\nclass TagExtractor:\n    def __init__(self, text: str):\n        self.text = text\n        self.pos = -1\n        self.len = len(text)\n        self.root_tag = Tag(start_tag=\"<_ROOT_>\", end_tag=\"</_ROOT_>\", content=[])\n        self.current_tag = None\n        self.is_extracted = False\n\n    def peek(self) -> str:\n        if self.pos + 1 < self.len:\n            return self.text[self.pos + 1]\n        return None\n\n    def peek2(self) -> str:\n        if self.pos + 1 < self.len:\n            return self.text[self.pos + 2]\n        return None\n\n    def peek3(self) -> str:\n        if self.pos + 1 < self.len:\n            return self.text[self.pos + 3]\n        return None\n\n    def next(self) -> str:\n        if self.pos < self.len - 1:\n            self.pos += 1\n            char = self.text[self.pos]\n            return char\n        return None\n\n    def is_full_tag(self) -> bool:\n        return self.current_tag.start_tag and self.current_tag.end_tag\n\n    def is_start_tag(self) -> bool:\n        return self.peek() and self.peek() == \"<\" and self.peek2() == \"_\"\n\n    def extract_start_tag(self) -> str:\n        tag = \"\"\n\n        while self.peek() and self.peek() != \">\":\n            tag += self.next()\n        tag += self.next()\n\n        if self.current_tag is None or self.current_tag == self.root_tag:\n            self.current_tag = Tag(\n                start_tag=tag, end_tag=\"\", content=\"\", parent=self.root_tag\n            )\n            self.root_tag.content.append(self.current_tag)\n        ## 当前tag已经闭合，找到当前tag的父tag,遇到新tag\n        elif self.is_full_tag():\n            parent = self.current_tag.parent or self.current_tag\n            current_tag = Tag(\n                start_tag=tag,\n                end_tag=\"\",\n                content=\"\",\n                parent=parent,\n            )\n            if isinstance(parent.content, list):\n                parent.content.append(current_tag)\n            else:\n                s = self.current_tag.content\n                parent.content = []\n                if s:\n                    parent.content.append(s)\n                parent.content.append(current_tag)\n            self.current_tag = current_tag\n        ## 当前tag还没有闭合，直接添加到当前tag的content中\n        elif not self.is_full_tag():\n            current_tag = Tag(\n                start_tag=tag, end_tag=\"\", content=\"\", parent=self.current_tag\n            )\n            if isinstance(self.current_tag.content, list):\n                self.current_tag.content.append(current_tag)\n            else:\n                s = self.current_tag.content\n                self.current_tag.content = []\n                if s:\n                    self.current_tag.content.append(s)\n                self.current_tag.content.append(current_tag)\n            self.current_tag = current_tag\n\n        return tag\n\n    def is_end_tag(self) -> bool:\n        return (\n            self.peek()\n            and self.peek() == \"<\"\n            and self.peek2() == \"/\"\n            and self.peek3() == \"_\"\n        )\n\n    def extract_end_tag(self) -> str:\n        tag = \"\"\n        while self.peek() and self.peek() != \">\":\n            tag += self.next()\n        tag += self.next()\n        self.current_tag.end_tag = tag\n        self.current_tag = self.current_tag.parent\n        return tag\n\n    def consume_blank(self):\n        while (\n            self.peek() == \"\\n\"\n            or self.peek() == \" \"\n            or self.peek() == \"\\t\"\n            or self.peek() == \"\\r\"\n        ):\n            self.next()\n\n    def is_in_tag(self) -> bool:\n        return (\n            self.current_tag\n            and self.current_tag.start_tag\n            and not self.current_tag.end_tag\n        )\n\n    def is_tag_content(self) -> bool:\n        if not self.root_tag:\n            return False\n        temp_pos = self.pos\n        self.consume_blank()\n        if self.is_start_tag():\n            self.pos = temp_pos\n            return True\n        if self.is_end_tag():\n            self.pos = temp_pos\n            return True\n        return False\n\n    def extract_str_content(self) -> str:\n        content = \"\"\n        while not self.is_start_tag() and not self.is_end_tag():\n            if self.peek() is None:\n                break\n            content += self.next()\n\n        self.current_tag.content = content\n        return content\n\n    def is_not_in_tag_str(self) -> bool:\n        if not self.root_tag:\n            return True\n        if not self.current_tag:\n            return True\n        if not self.current_tag.start_tag and not self.current_tag.end_tag:\n            return True\n        if self.current_tag.start_tag and self.current_tag.end_tag:\n            return True\n        return False\n\n    def extract_content_not_in_tag(self) -> str:\n        content = \"\"\n        while self.peek() and not self.is_start_tag() and not self.is_end_tag():\n            content += self.next()\n        return content\n\n    def extract(self) -> Union[Tag]:\n        if self.is_extracted:\n            return self.root_tag\n        while True:\n            if self.pos == self.len - 1:\n                break\n            if self.is_start_tag():\n                self.extract_start_tag()\n            elif self.is_end_tag():\n                self.extract_end_tag()\n            elif self.is_in_tag():\n                if self.is_tag_content():\n                    self.consume_blank()\n                    continue\n                else:\n                    self.extract_str_content()\n            elif self.is_not_in_tag_str():\n                self.extract_content_not_in_tag()\n        self.is_extracted = True\n        return self.root_tag\n\n\nclass Image(TagExtractor):\n\n    def __init__(self, text: str):\n        super().__init__(text)\n        self.is_extracted = False\n\n    def has_image(self) -> bool:\n        self.extract()\n        for item in self.root_tag.content:\n            if (\n                isinstance(item, Tag)\n                and item.start_tag == \"<_image_>\"\n                and item.end_tag == \"</_image_>\"\n                and item.content.startswith(\"data:image/\")\n            ):\n                return True\n        return False\n    \n    def convert_image_paths_from(text:str,start_tag:str=\"<img>\",end_tag:str=\"</img>\"):\n        pass\n\n    @staticmethod\n    def extract_image_paths(text: str, to_base64: bool = False) -> List[str]:\n        v = Image(text)\n        v.extract()\n        result = []\n        for item in v.root_tag.content:\n            if (\n                isinstance(item, Tag)\n                and item.start_tag in [\"<_image_>\", \"<_img_>\"]\n                and item.end_tag in [\"</_image_>\", \"</_img_>\"]\n                and not item.content.startswith(\"data:image/\")\n            ):\n                if to_base64:\n                    result.append(Image.load_image_from_path(item.content.strip()))\n                else:\n                    result.append(item.content.strip())\n        return result\n\n    @staticmethod\n    def load_image_from_path(path: str) -> str:\n        \"\"\"\n        Load image from path and return base64 image data\n        \"\"\"\n\n        file_extension = os.path.splitext(path)[1][1:].lower()\n        image_type = file_extension\n\n        with open(path, \"rb\") as image_file:\n            encoded_string = base64.b64encode(image_file.read()).decode(\"utf-8\")\n        return f\"<_image_>data:image/{image_type};base64,{encoded_string}</_image_>\"\n\n    @staticmethod\n    def save_image_to_path(\n        image_data: str, output_path: str, auto_fix_suffix: bool = False\n    ) -> str:\n        \"\"\"\n        Save base64 image data as image file\n        \"\"\"\n        import base64\n        import os\n\n        # Extract image type and base64 data\n        data_prefix = \"data:image/\"\n        base64_prefix = \";base64,\"\n        if not image_data.startswith(data_prefix):\n            raise ValueError(\"Invalid image data format\")\n\n        format_end = image_data.index(base64_prefix)\n        image_type = image_data[len(data_prefix) : format_end]\n        base64_data = image_data[format_end + len(base64_prefix) :]\n\n        # Decode the base64 image data\n        image_data = base64.b64decode(base64_data)\n\n        # Check and fix file extension if necessary\n        file_name, file_extension = os.path.splitext(output_path)\n        correct_extension = f\".{image_type}\"\n\n        if auto_fix_suffix and file_extension.lower() != correct_extension.lower():\n            output_path = file_name + correct_extension\n\n        # Write the image data to the file\n        with open(output_path, \"wb\") as image_file:\n            image_file.write(image_data)\n\n        return output_path\n\n    def to_content(self) -> List[Dict[str, str]]:\n        self.extract()\n\n        result = []\n        current_item = {}\n\n        for item in self.root_tag.content:\n            if isinstance(item, Tag) and item.start_tag == \"<_image_>\":\n                if current_item:\n                    result.append(current_item)\n                    current_item = {}\n                current_item[\"image\"] = item.content\n\n        if current_item:\n            result.append(current_item)\n\n        new_text = self.text\n        for res in result:\n            new_text = new_text.replace(f\"<_image_>{res['image']}</_image_>\", \"\")\n\n        result.append({\"text\": new_text.strip()})\n\n        # for item in result:\n        #     if \"image\" in item:\n        #         print(item[\"image\"][0:100] + \"...\")\n        #     else:\n        #         print(item[\"text\"][0:100] + \"...\")\n        return result\n\n\nclass Audio(TagExtractor):\n    def __init__(self, text: str):\n        super().__init__(text)\n        self.is_extracted = False\n\n    def has_audio(self) -> bool:\n        self.extract()\n        for item in self.root_tag.content:\n            if (\n                isinstance(item, Tag)\n                and item.start_tag == \"<_audio_>\"\n                and item.end_tag == \"</_audio_>\"\n                and item.content.startswith(\"data:audio/\")\n            ):\n                return True\n        return False\n    \n    @staticmethod\n    def extract_audio_paths(text: str, to_base64: bool = False) -> List[str]:\n        v = Image(text)\n        v.extract()\n        result = []\n        for item in v.root_tag.content:\n            if (\n                isinstance(item, Tag)\n                and item.start_tag in [\"<_audio_>\"]\n                and item.end_tag in [\"</_audio_>\"]\n                and not item.content.startswith(\"data:audio/\")\n            ):\n                if to_base64:\n                    result.append(Audio.load_audio_from_path(item.content.strip()))\n                else:\n                    result.append(item.content.strip())\n        return result\n\n    @staticmethod\n    def load_audio_from_path(path: str) -> str:\n        \"\"\"\n        Load audio from path and return base64 audio data\n        \"\"\"\n        file_extension = os.path.splitext(path)[1][1:].lower()\n        audio_type = file_extension\n\n        with open(path, \"rb\") as audio_file:\n            encoded_string = base64.b64encode(audio_file.read()).decode(\"utf-8\")\n        return f\"<_audio_>data:audio/{audio_type};base64,{encoded_string}</_audio_>\"\n\n    @staticmethod\n    def save_audio_to_path(\n        audio_data: str, output_path: str, auto_fix_suffix: bool = False\n    ) -> str:\n        \"\"\"\n        Save base64 audio data as audio file\n        \"\"\"\n        # Extract audio type and base64 data\n        data_prefix = \"data:audio/\"\n        base64_prefix = \";base64,\"\n        if not audio_data.startswith(data_prefix):\n            raise ValueError(\"Invalid audio data format\")\n\n        format_end = audio_data.index(base64_prefix)\n        audio_type = audio_data[len(data_prefix) : format_end]\n        base64_data = audio_data[format_end + len(base64_prefix) :]\n\n        # Decode the base64 audio data\n        audio_data = base64.b64decode(base64_data)\n\n        # Check and fix file extension if necessary\n        file_name, file_extension = os.path.splitext(output_path)\n        correct_extension = f\".{audio_type}\"\n\n        if auto_fix_suffix and file_extension.lower() != correct_extension.lower():\n            output_path = file_name + correct_extension\n\n        # Write the audio data to the file\n        with open(output_path, \"wb\") as audio_file:\n            audio_file.write(audio_data)\n\n        return output_path\n\n    def to_content(self) -> Dict[str, Any]:\n        self.extract()\n        current_item = {}\n        counter = 0\n        for item in self.root_tag.content:\n            if isinstance(item, Tag) and item.start_tag == \"<_audio_>\":\n                if counter > 0:\n                    raise ValueError(\"Only one audio file is allowed\")\n                counter += 1\n                current_item[\"audio\"] = item.content\n\n        return current_item\n\n\n     \n\n下面是用户的需求：\n\n使用正则抽取 start_tag,end_tag 包括的值,然后替换成标砖的image 标签来 包裹.\n"}, {"role": "assistant", "content": "为了满足用户的需求，我们需要修改 `Image` 类，添加一个新的静态方法 `convert_image_paths_from`。这个方法将使用正则表达式来抽取指定标签之间的内容，然后将其替换为标准的 `<_image_>` 标签。以下是实现这个功能的代码：\n\n```python\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/utils/nontext.py\n<<<<<<< SEARCH\n    def convert_image_paths_from(text:str,start_tag:str=\"<img>\",end_tag:str=\"</img>\"):\n        pass\n=======\n    @staticmethod\n    def convert_image_paths_from(text: str, start_tag: str = \"<img>\", end_tag: str = \"</img>\") -> str:\n        import re\n        pattern = re.escape(start_tag) + r\"(.*?)\" + re.escape(end_tag)\n        \n        def replace_func(match):\n            content = match.group(1).strip()\n            if content.startswith(\"data:image/\"):\n                return f\"<_image_>{content}</_image_>\"\n            else:\n                return f\"<_image_>{Image.load_image_from_path(content)}</_image_>\"\n        \n        return re.sub(pattern, replace_func, text, flags=re.DOTALL)\n>>>>>>> REPLACE\n```\n\n这个新方法 `convert_image_paths_from` 做了以下几件事：\n\n1. 它接受三个参数：要处理的文本、开始标签和结束标签。默认的标签是 `<img>` 和 `</img>`。\n\n2. 它使用 `re.escape()` 来转义开始和结束标签，确保特殊字符不会影响正则表达式。\n\n3. 它创建一个正则表达式模式，用于匹配开始标签和结束标签之间的所有内容。\n\n4. 它定义了一个替换函数 `replace_func`，这个函数检查匹配的内容：\n   - 如果内容已经是 base64 编码的图片数据（以 \"data:image/\" 开头），它直接将其包装在 `<_image_>` 标签中。\n   - 如果内容是文件路径，它使用 `Image.load_image_from_path()` 方法来加载图片并转换为 base64 格式，然后包装在 `<_image_>` 标签中。\n\n5. 最后，它使用 `re.sub()` 来替换所有匹配的内容，应用 `replace_func` 函数。\n\n使用这个新方法的例子：\n\n```python\ntext = \"\"\"\n这是一张图片：<img>/path/to/image.jpg</img>\n这是另一张图片：<img>data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAACklEQVR4nGMAAQAABQABDQottAAAAABJRU5ErkJggg==</img>\n\"\"\"\n\nconverted_text = Image.convert_image_paths_from(text)\nprint(converted_text)\n```\n\n这个实现满足了用户的需求，它可以：\n1. 使用正则表达式抽取指定的开始标签和结束标签之间的内容。\n2. 将抽取的内容替换成标准的 `<_image_>` 标签包裹的格式。\n3. 如果内容是文件路径，它会自动加载图片并转换为 base64 格式。\n4. 如果内容已经是 base64 编码的图片数据，它会直接使用这些数据。\n\n这个方法提供了很大的灵活性，允许用户指定自定义的开始和结束标签，同时保持了与 `Image` 类其他方法的一致性。"}], "model": "human", "yaml_file": "actions/015_chat_action.yml"}
{"instruction": "参考 web_ui.py的代码,按  whisper/__init__.py 的接口规范来实现 sensevoice/__init__.py. 务必要理解后再去实现.", "conversations": [{"role": "user", "content": "\n如果你需要生成代码，对于每个需要更改的文件,你需要按 *SEARCH/REPLACE block* 的格式进行生成。\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The opening fence and code language, eg: ```python\n2. The file path alone on a line, starting with \"##File:\" and verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n*SEARCH/REPLACE* blocks will replace *all* matching occurrences.\nInclude enough lines to make the SEARCH blocks unique.\n\nInclude *ALL* the code being searched and replaced!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n下面我们来看一个例子：\n\n当前项目目录结构：\n1. 项目根目录： /tmp/projects/mathweb\n2. 项目子目录/文件列表(类似tree 命令输出)\nflask/\n    app.py\n    templates/\n        index.html\n    static/\n        style.css\n\n用户需求： Change get_factorial() to use math.factorial\n\n回答： To make this change we need to modify `/tmp/projects/mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n用户需求： Refactor hello() into its own file.\n\n回答：To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/hello.py\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/main.py\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n现在让我们开始一个新的任务:\n\n当前项目目录结构：\n1. 项目根目录： /Users/allwefantasy/projects/byzer-llm\n2. 项目子目录/文件列表(类似tree 命令输出)：\nsetup.py\ntests/\n    test_emb_rerank.py\n    test_deepseek_code.py\n    test_prompt.py\n    test_model_meta.py\n    __init__.py\n    test_transformers_deploy.py\n    test_data_analysis.py\n    test_yi.py\n    test_deploy.py\n    functions_suite.py\n    test_template.py\n    auto/\n        test_backend_llama_cpp.py\n        __init__.py\nsetup-machine/\n    convert_safetensor.py\nnotebooks/\n    test.py\nsrc/\n    byzerllm/\n        lang.py\n        store.py\n        version.py\n        log.py\n        __init__.py\n        types.py\n        easy_byzerllm_command.py\n        byzerllm_command.py\n        command_args.py\n        bark/\n            generation.py\n            __init__.py\n            model.py\n            api.py\n            model_fine.py\n            bark_voice.py\n        moss/\n            finetune_moss.py\n            __init__.py\n            moss_inference.py\n            models/\n                configuration_moss.py\n                quantization.py\n                custom_autotune.py\n                __init__.py\n                modeling_moss.py\n                tokenization_moss.py\n        llama2/\n            __init__.py\n        records/\n            __init__.py\n        visualglm/\n            __init__.py\n        auto/\n            __init__.py\n            backend_llama_cpp.py\n            backend_vllm.py\n            backend_ds.py\n        alpha_moss/\n            __init__.py\n        processor/\n            __init__.py\n        qwen/\n            __init__.py\n        utils/\n            json_repaire.py\n            object_store_ref_util.py\n            __init__.py\n            types.py\n            tokenizer.py\n            openai_utils.py\n            ray_utils.py\n            text_generator.py\n            emb.py\n            connect_ray.py\n            testing.py\n            langutil.py\n            nontext.py\n            fulltune/\n                deepspeed_trainner.py\n                __init__.py\n                launch.py\n                trainner.py\n                base_model/\n                    __init__.py\n                    configuration_baichuan.py\n                    modeling_baichuan.py\n                pretrain/\n                    __init__.py\n                    convert_to_transformers.py\n            metrics/\n                __init__.py\n            config/\n                __init__.py\n            retrieval/\n                rest.py\n                __init__.py\n                udf.py\n            sft/\n                argument.py\n                collator.py\n                qlora.py\n                __init__.py\n                model.py\n                dataset.py\n                loss.py\n                merge_lora.py\n                trainer.py\n            client/\n                message_utils.py\n                math_utils.py\n                byzerllm_client.py\n                parallel_utils.py\n                __init__.py\n                types.py\n                code_utils.py\n                img_utils.py\n                entrypoints/\n                    __init__.py\n                    openai/\n                        protocol.py\n                        __init__.py\n                        serve.py\n                        serving_chat.py\n                        api_server.py\n                        serving_engine.py\n                        serving_completion.py\n                        tool.py\n        chatglm2/\n            __init__.py\n        baichuan/\n            __init__.py\n        m3e/\n            __init__.py\n        stable_diffusion/\n            config.py\n            __init__.py\n            logger.py\n            shared.py\n            model.py\n            utils.py\n            images.py\n            diffusion/\n                __init__.py\n                embeddings.py\n                utils.py\n                networks/\n                    lyco.py\n                    __init__.py\n                upscalers/\n                    __init__.py\n                    multidiffusion.py\n                    samplers.py\n                piplines/\n                    __init__.py\n                    lpw.py\n                    diffusers.py\n            lib/\n                __init__.py\n                diffusers/\n                    __init__.py\n                    scheduler.py\n            api/\n                __init__.py\n                plugin.py\n                models/\n                    diffusion.py\n                    __init__.py\n                diffusion/\n                    __init__.py\n                    pipelines/\n                        __init__.py\n                        diffusers.py\n                events/\n                    generation.py\n                    __init__.py\n        bge/\n            __init__.py\n        bge_rerank/\n            __init__.py\n        apps/\n            client.py\n            qa_strategy.py\n            __init__.py\n            builder.py\n            vector_db.py\n            utils.py\n            qa.py\n            byzer_storage/\n                memory_model_based.py\n                command.py\n                env.py\n                train_pt.py\n                train_sft.py\n                __init__.py\n                train_dpo.py\n                simple_api.py\n                generate_sft_data.py\n            byzer_sql/\n                __init__.py\n            llama_index/\n                byzerai_vectordb.py\n                byzerai.py\n                byzerai_embedding.py\n                __init__.py\n                collection_manager.py\n                byzerai_index_store.py\n                byzerai_docstore.py\n                simple_retrieval.py\n                byzerai_kvstore.py\n            agent/\n                conversable_agent.py\n                registry.py\n                groupchat.py\n                user_proxy_agent.py\n                __init__.py\n                agent.py\n                extensions/\n                    simple_retrieval_client.py\n                    planner.py\n                    python_codesandbox_agent.py\n                    preview_file_agent.py\n                    llama_index_retrieval_agent.py\n                    visualization_agent.py\n                    assistant_agent.py\n                    load_data_agent.py\n                    retrieval_agent.py\n                    output_agent.py\n                    __init__.py\n                    byzer_engine_agent.py\n                    subquestion_agent.py\n                    sql_reviewer_agent.py\n                    spark_sql_agent.py\n                    rhetorical_agent.py\n                    data_analysis.py\n                    data_analysis_pipeline_agent.py\n                    common_agent.py\n                    query_rewrite/\n                        time.py\n                        rhetorical.py\n                        __init__.py\n                        condition.py\n                        context.py\n                store/\n                    memory_store.py\n                    __init__.py\n                    stores.py\n        saas/\n            __init__.py\n            chatglm/\n                __init__.py\n            sparkdesk/\n                __init__.py\n            gemini/\n                __init__.py\n            azure_openai/\n                __init__.py\n            azure/\n                __init__.py\n            qianfan/\n                __init__.py\n            claude/\n                __init__.py\n            qianwen/\n                __init__.py\n            volcengine/\n                __init__.py\n            minimax/\n                __init__.py\n            baichuan/\n                __init__.py\n            qianwen_vl/\n                __init__.py\n            official_openai/\n                __init__.py\n            zhipu/\n                __init__.py\n            aws_bedrock/\n                __init__.py\n            openai/\n                __init__.py\n        falcon/\n            __init__.py\n        chatglm6b/\n            trainer_seq2seq.py\n            arguments.py\n            __init__.py\n            finetune.py\n            tunning/\n                __init__.py\n                infer.py\n                finetune.py\n                utils/\n                    config.py\n                    __init__.py\n                    other.py\n                    common.py\n                    seq2seq.py\n        starcode/\n            __init__.py\n        qwen_vl_chat/\n            __init__.py\n        zephyr/\n            __init__.py\n        dolly/\n            generate.py\n            __init__.py\n            consts.py\n            dolly_inference.py\n            trainer.py\n        llama/\n            __init__.py\n        whisper/\n            whisper_inference.py\n            __init__.py\n下面是一些文件路径以及每个文件对应的源码：\n\n##File: /Users/allwefantasy/projects/SenseVoice/webui.py\n# coding=utf-8\n\nimport os\nimport librosa\nimport base64\nimport io\nimport gradio as gr\nimport re\n\nimport numpy as np\nimport torch\nimport torchaudio\n\n\nfrom funasr import AutoModel\n\nmodel = \"iic/SenseVoiceSmall\"\nmodel = AutoModel(model=model,\n\t\t\t\t  vad_model=\"iic/speech_fsmn_vad_zh-cn-16k-common-pytorch\",\n\t\t\t\t  vad_kwargs={\"max_single_segment_time\": 30000},\n\t\t\t\t  trust_remote_code=True,\n\t\t\t\t  )\n\nimport re\n\nemo_dict = {\n\t\"<|HAPPY|>\": \"😊\",\n\t\"<|SAD|>\": \"😔\",\n\t\"<|ANGRY|>\": \"😡\",\n\t\"<|NEUTRAL|>\": \"\",\n\t\"<|FEARFUL|>\": \"😰\",\n\t\"<|DISGUSTED|>\": \"🤢\",\n\t\"<|SURPRISED|>\": \"😮\",\n}\n\nevent_dict = {\n\t\"<|BGM|>\": \"🎼\",\n\t\"<|Speech|>\": \"\",\n\t\"<|Applause|>\": \"👏\",\n\t\"<|Laughter|>\": \"😀\",\n\t\"<|Cry|>\": \"😭\",\n\t\"<|Sneeze|>\": \"🤧\",\n\t\"<|Breath|>\": \"\",\n\t\"<|Cough|>\": \"🤧\",\n}\n\nemoji_dict = {\n\t\"<|nospeech|><|Event_UNK|>\": \"❓\",\n\t\"<|zh|>\": \"\",\n\t\"<|en|>\": \"\",\n\t\"<|yue|>\": \"\",\n\t\"<|ja|>\": \"\",\n\t\"<|ko|>\": \"\",\n\t\"<|nospeech|>\": \"\",\n\t\"<|HAPPY|>\": \"😊\",\n\t\"<|SAD|>\": \"😔\",\n\t\"<|ANGRY|>\": \"😡\",\n\t\"<|NEUTRAL|>\": \"\",\n\t\"<|BGM|>\": \"🎼\",\n\t\"<|Speech|>\": \"\",\n\t\"<|Applause|>\": \"👏\",\n\t\"<|Laughter|>\": \"😀\",\n\t\"<|FEARFUL|>\": \"😰\",\n\t\"<|DISGUSTED|>\": \"🤢\",\n\t\"<|SURPRISED|>\": \"😮\",\n\t\"<|Cry|>\": \"😭\",\n\t\"<|EMO_UNKNOWN|>\": \"\",\n\t\"<|Sneeze|>\": \"🤧\",\n\t\"<|Breath|>\": \"\",\n\t\"<|Cough|>\": \"😷\",\n\t\"<|Sing|>\": \"\",\n\t\"<|Speech_Noise|>\": \"\",\n\t\"<|withitn|>\": \"\",\n\t\"<|woitn|>\": \"\",\n\t\"<|GBG|>\": \"\",\n\t\"<|Event_UNK|>\": \"\",\n}\n\nlang_dict =  {\n    \"<|zh|>\": \"<|lang|>\",\n    \"<|en|>\": \"<|lang|>\",\n    \"<|yue|>\": \"<|lang|>\",\n    \"<|ja|>\": \"<|lang|>\",\n    \"<|ko|>\": \"<|lang|>\",\n    \"<|nospeech|>\": \"<|lang|>\",\n}\n\nemo_set = {\"😊\", \"😔\", \"😡\", \"😰\", \"🤢\", \"😮\"}\nevent_set = {\"🎼\", \"👏\", \"😀\", \"😭\", \"🤧\", \"😷\",}\n\ndef format_str(s):\n\tfor sptk in emoji_dict:\n\t\ts = s.replace(sptk, emoji_dict[sptk])\n\treturn s\n\n\ndef format_str_v2(s):\n\tsptk_dict = {}\n\tfor sptk in emoji_dict:\n\t\tsptk_dict[sptk] = s.count(sptk)\n\t\ts = s.replace(sptk, \"\")\n\temo = \"<|NEUTRAL|>\"\n\tfor e in emo_dict:\n\t\tif sptk_dict[e] > sptk_dict[emo]:\n\t\t\temo = e\n\tfor e in event_dict:\n\t\tif sptk_dict[e] > 0:\n\t\t\ts = event_dict[e] + s\n\ts = s + emo_dict[emo]\n\n\tfor emoji in emo_set.union(event_set):\n\t\ts = s.replace(\" \" + emoji, emoji)\n\t\ts = s.replace(emoji + \" \", emoji)\n\treturn s.strip()\n\ndef format_str_v3(s):\n\tdef get_emo(s):\n\t\treturn s[-1] if s[-1] in emo_set else None\n\tdef get_event(s):\n\t\treturn s[0] if s[0] in event_set else None\n\n\ts = s.replace(\"<|nospeech|><|Event_UNK|>\", \"❓\")\n\tfor lang in lang_dict:\n\t\ts = s.replace(lang, \"<|lang|>\")\n\ts_list = [format_str_v2(s_i).strip(\" \") for s_i in s.split(\"<|lang|>\")]\n\tnew_s = \" \" + s_list[0]\n\tcur_ent_event = get_event(new_s)\n\tfor i in range(1, len(s_list)):\n\t\tif len(s_list[i]) == 0:\n\t\t\tcontinue\n\t\tif get_event(s_list[i]) == cur_ent_event and get_event(s_list[i]) != None:\n\t\t\ts_list[i] = s_list[i][1:]\n\t\t#else:\n\t\tcur_ent_event = get_event(s_list[i])\n\t\tif get_emo(s_list[i]) != None and get_emo(s_list[i]) == get_emo(new_s):\n\t\t\tnew_s = new_s[:-1]\n\t\tnew_s += s_list[i].strip().lstrip()\n\tnew_s = new_s.replace(\"The.\", \" \")\n\treturn new_s.strip()\n\ndef model_inference(input_wav, language, fs=16000):\n\t# task_abbr = {\"Speech Recognition\": \"ASR\", \"Rich Text Transcription\": (\"ASR\", \"AED\", \"SER\")}\n\tlanguage_abbr = {\"auto\": \"auto\", \"zh\": \"zh\", \"en\": \"en\", \"yue\": \"yue\", \"ja\": \"ja\", \"ko\": \"ko\",\n\t\t\t\t\t \"nospeech\": \"nospeech\"}\n\t\n\t# task = \"Speech Recognition\" if task is None else task\n\tlanguage = \"auto\" if len(language) < 1 else language\n\tselected_language = language_abbr[language]\n\t# selected_task = task_abbr.get(task)\n\t\n\t# print(f\"input_wav: {type(input_wav)}, {input_wav[1].shape}, {input_wav}\")\n\t\n\tif isinstance(input_wav, tuple):\n\t\tfs, input_wav = input_wav\n\t\tinput_wav = input_wav.astype(np.float32) / np.iinfo(np.int16).max\n\t\tif len(input_wav.shape) > 1:\n\t\t\tinput_wav = input_wav.mean(-1)\n\t\tif fs != 16000:\n\t\t\tprint(f\"audio_fs: {fs}\")\n\t\t\tresampler = torchaudio.transforms.Resample(fs, 16000)\n\t\t\tinput_wav_t = torch.from_numpy(input_wav).to(torch.float32)\n\t\t\tinput_wav = resampler(input_wav_t[None, :])[0, :].numpy()\n\t\n\t\n\tmerge_vad = True #False if selected_task == \"ASR\" else True\n\tprint(f\"language: {language}, merge_vad: {merge_vad}\")\n\ttext = model.generate(input=input_wav,\n\t\t\t\t\t\t  cache={},\n\t\t\t\t\t\t  language=language,\n\t\t\t\t\t\t  use_itn=True,\n\t\t\t\t\t\t  batch_size_s=60, merge_vad=merge_vad)\n\t\n\tprint(text)\n\ttext = text[0][\"text\"]\n\ttext = format_str_v3(text)\n\t\n\tprint(text)\n\t\n\treturn text\n\n\naudio_examples = [\n    [\"example/zh.mp3\", \"zh\"],\n    [\"example/yue.mp3\", \"yue\"],\n    [\"example/en.mp3\", \"en\"],\n    [\"example/ja.mp3\", \"ja\"],\n    [\"example/ko.mp3\", \"ko\"],\n    [\"example/emo_1.wav\", \"auto\"],\n    [\"example/emo_2.wav\", \"auto\"],\n    [\"example/emo_3.wav\", \"auto\"],\n    #[\"example/emo_4.wav\", \"auto\"],\n    #[\"example/event_1.wav\", \"auto\"],\n    #[\"example/event_2.wav\", \"auto\"],\n    #[\"example/event_3.wav\", \"auto\"],\n    [\"example/rich_1.wav\", \"auto\"],\n    [\"example/rich_2.wav\", \"auto\"],\n    #[\"example/rich_3.wav\", \"auto\"],\n    [\"example/longwav_1.wav\", \"auto\"],\n    [\"example/longwav_2.wav\", \"auto\"],\n    [\"example/longwav_3.wav\", \"auto\"],\n    #[\"example/longwav_4.wav\", \"auto\"],\n]\n\n\n\nhtml_content = \"\"\"\n<div>\n    <h2 style=\"font-size: 22px;margin-left: 0px;\">Voice Understanding Model: SenseVoice-Small</h2>\n    <p style=\"font-size: 18px;margin-left: 20px;\">SenseVoice-Small is an encoder-only speech foundation model designed for rapid voice understanding. It encompasses a variety of features including automatic speech recognition (ASR), spoken language identification (LID), speech emotion recognition (SER), and acoustic event detection (AED). SenseVoice-Small supports multilingual recognition for Chinese, English, Cantonese, Japanese, and Korean. Additionally, it offers exceptionally low inference latency, performing 7 times faster than Whisper-small and 17 times faster than Whisper-large.</p>\n    <h2 style=\"font-size: 22px;margin-left: 0px;\">Usage</h2> <p style=\"font-size: 18px;margin-left: 20px;\">Upload an audio file or input through a microphone, then select the task and language. the audio is transcribed into corresponding text along with associated emotions (😊 happy, 😡 angry/exicting, 😔 sad) and types of sound events (😀 laughter, 🎼 music, 👏 applause, 🤧 cough&sneeze, 😭 cry). The event labels are placed in the front of the text and the emotion are in the back of the text.</p>\n\t<p style=\"font-size: 18px;margin-left: 20px;\">Recommended audio input duration is below 30 seconds. For audio longer than 30 seconds, local deployment is recommended.</p>\n\t<h2 style=\"font-size: 22px;margin-left: 0px;\">Repo</h2>\n\t<p style=\"font-size: 18px;margin-left: 20px;\"><a href=\"https://github.com/FunAudioLLM/SenseVoice\" target=\"_blank\">SenseVoice</a>: multilingual speech understanding model</p>\n\t<p style=\"font-size: 18px;margin-left: 20px;\"><a href=\"https://github.com/modelscope/FunASR\" target=\"_blank\">FunASR</a>: fundamental speech recognition toolkit</p>\n\t<p style=\"font-size: 18px;margin-left: 20px;\"><a href=\"https://github.com/FunAudioLLM/CosyVoice\" target=\"_blank\">CosyVoice</a>: high-quality multilingual TTS model</p>\n</div>\n\"\"\"\n\n\ndef launch():\n\twith gr.Blocks(theme=gr.themes.Soft()) as demo:\n\t\t# gr.Markdown(description)\n\t\tgr.HTML(html_content)\n\t\twith gr.Row():\n\t\t\twith gr.Column():\n\t\t\t\taudio_inputs = gr.Audio(label=\"Upload audio or use the microphone\")\n\t\t\t\t\n\t\t\t\twith gr.Accordion(\"Configuration\"):\n\t\t\t\t\tlanguage_inputs = gr.Dropdown(choices=[\"auto\", \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"],\n\t\t\t\t\t\t\t\t\t\t\t\t  value=\"auto\",\n\t\t\t\t\t\t\t\t\t\t\t\t  label=\"Language\")\n\t\t\t\tfn_button = gr.Button(\"Start\", variant=\"primary\")\n\t\t\t\ttext_outputs = gr.Textbox(label=\"Results\")\n\t\t\tgr.Examples(examples=audio_examples, inputs=[audio_inputs, language_inputs], examples_per_page=20)\n\t\t\n\t\tfn_button.click(model_inference, inputs=[audio_inputs, language_inputs], outputs=text_outputs)\n\n\tdemo.launch()\n\n\nif __name__ == \"__main__\":\n\t# iface.launch()\n\tlaunch()\n\n\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/saas/official_openai/__init__.py\nimport time\nfrom typing import List, Tuple, Dict, Any, Union\nimport httpx\nfrom openai import OpenAI\nimport base64\nimport io\nimport json\nimport ray\nfrom byzerllm.utils.types import (\n    BlockVLLMStreamServer,\n    StreamOutputs,\n    SingleOutput,\n    SingleOutputMeta,\n    BlockBinaryStreamServer,\n)\nfrom byzerllm.utils.langutil import asyncfy_with_semaphore\nimport threading\nimport asyncio\nimport traceback\nimport uuid\nimport tempfile        \n\nclass CustomSaasAPI:\n\n    def __init__(self, infer_params: Dict[str, str]) -> None:\n\n        self.api_key = infer_params[\"saas.api_key\"]\n        self.model = infer_params.get(\"saas.model\", \"gpt-3.5-turbo-1106\")\n\n        other_params = {}\n\n        if \"saas.api_base\" in infer_params:\n            other_params[\"api_base\"] = infer_params[\"saas.api_base\"]\n\n        if \"saas.api_version\" in infer_params:\n            other_params[\"api_version\"] = infer_params[\"saas.api_version\"]\n\n        if \"saas.api_type\" in infer_params:\n            other_params[\"api_type\"] = infer_params[\"saas.api_type\"]\n\n        if \"saas.base_url\" in infer_params:\n            other_params[\"base_url\"] = infer_params[\"saas.base_url\"]\n\n        if \"saas.timeout\" in infer_params:\n            other_params[\"timeout\"] = float(infer_params[\"saas.timeout\"])\n\n        self.max_retries = int(infer_params.get(\"saas.max_retries\", 10))\n\n        self.meta = {\n            \"model_deploy_type\": \"saas\",\n            \"backend\": \"saas\",\n            \"support_stream\": True,\n            \"model_name\": self.model,\n        }\n\n        self.meta[\"embedding_mode\"] = \"embedding\" in self.model.lower()\n\n        self.proxies = infer_params.get(\"saas.proxies\", None)\n        self.local_address = infer_params.get(\"saas.local_address\", \"0.0.0.0\")\n\n        if not self.proxies:\n            self.client = OpenAI(**other_params, api_key=self.api_key)\n        else:\n            self.client = OpenAI(\n                **other_params,\n                api_key=self.api_key,\n                http_client=httpx.Client(\n                    proxies=self.proxies,\n                    transport=httpx.HTTPTransport(local_address=self.local_address),\n                ),\n            )\n\n        try:\n            ray.get_actor(\"BLOCK_VLLM_STREAM_SERVER\")\n        except ValueError:\n            try:\n                ray.remote(BlockVLLMStreamServer).options(\n                    name=\"BLOCK_VLLM_STREAM_SERVER\",\n                    lifetime=\"detached\",\n                    max_concurrency=1000,\n                ).remote()\n            except Exception as e:\n                pass\n        try:\n            ray.get_actor(\"BlockBinaryStreamServer\")\n        except ValueError:\n            try:\n                ray.remote(BlockBinaryStreamServer).options(\n                    name=\"BlockBinaryStreamServer\",\n                    lifetime=\"detached\",\n                    max_concurrency=1000,\n                ).remote()\n            except Exception as e:\n                pass\n\n    # saas/proprietary\n    def get_meta(self):\n        return [self.meta]\n\n    def process_input(self, ins: Union[str, List[Dict[str, Any]], Dict[str, Any]]):\n\n        if isinstance(ins, list) or isinstance(ins, dict):\n            return ins\n\n        content = []\n        try:\n            ins_json = json.loads(ins)\n        except:\n            return ins\n\n        ## speech\n        if isinstance(ins_json, dict):\n            return ins_json\n\n        content = []\n        #     [\n        #     {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n        #     {\n        #       \"type\": \"image_url\",\n        #       \"image_url\": {\n        #         \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n        #         \"detail\": \"high\"\n        #       },\n        #     },\n        #   ],\n        for item in ins_json:\n            # for format like this: {\"image\": \"xxxxxx\", \"text\": \"What’s in this image?\",\"detail\":\"high\"}\n            # or {\"image\": \"xxxxxx\"}, {\"text\": \"What’s in this image?\"}\n            if (\"image\" in item or \"image_url\" in item) and \"type\" not in item:\n                image_data = item.get(\"image\", item.get(\"image_url\", \"\"))\n                ## \"data:image/jpeg;base64,\"\n                if not image_data.startswith(\"data:\"):\n                    image_data = \"data:image/jpeg;base64,\" + image_data\n\n                ## get the other fields except image/text/image_url\n                other_fields = {\n                    k: v\n                    for k, v in item.items()\n                    if k not in [\"image\", \"text\", \"image_url\"]\n                }\n                content.append(\n                    {\n                        \"image_url\": {\"url\": image_data, **other_fields},\n                        \"type\": \"image_url\",\n                    }\n                )            \n\n            if \"text\" in item and \"type\" not in item:\n                text_data = item[\"text\"]\n                content.append({\"text\": text_data, \"type\": \"text\"})\n\n            ## for format like this: {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n            ## {\"type\": \"image_url\", \"image_url\": {\"url\":\"\",\"detail\":\"high\"}}\n            ## this is the standard format, just return it\n            if \"type\" in item and item[\"type\"] == \"text\":\n                content.append(item)\n\n            if \"type\" in item and item[\"type\"] == \"image_url\":\n                content.append(item)\n\n        if not content:\n            return ins\n\n        return content\n\n    async def async_embed_query(self, ins: str, **kwargs):\n        resp = await asyncfy_with_semaphore(\n            lambda: self.client.embeddings.create(input=[ins], model=self.model)\n        )()\n        embedding = resp.data[0].embedding\n        usage = resp.usage\n        return (\n            embedding,\n            {\n                \"metadata\": {\n                    \"input_tokens_count\": usage.prompt_tokens,\n                    \"generated_tokens_count\": 0,\n                }\n            },\n        )\n\n    def embed_query(self, ins: str, **kwargs):\n        resp = self.client.embeddings.create(input=[ins], model=self.model)\n        embedding = resp.data[0].embedding\n        usage = resp.usage\n        return (\n            embedding,\n            {\n                \"metadata\": {\n                    \"input_tokens_count\": usage.prompt_tokens,\n                    \"generated_tokens_count\": 0,\n                }\n            },\n        )\n\n    async def async_text_to_speech(\n        self, stream: bool, ins: str, voice: str, chunk_size: int = None, **kwargs\n    ):\n        if stream:\n            server = ray.get_actor(\"BlockBinaryStreamServer\")\n            request_id = [None]\n\n            def writer():\n                try:\n                    request_id[0] = str(uuid.uuid4())\n                    with self.client.with_streaming_response.audio.speech.create(\n                        model=self.model, voice=voice, input=ins, **kwargs\n                    ) as response:\n                        for chunk in response.iter_bytes(chunk_size):\n                            input_tokens_count = 0\n                            generated_tokens_count = 0\n                            ray.get(\n                                server.add_item.remote(\n                                    request_id[0],\n                                    StreamOutputs(\n                                        outputs=[\n                                            SingleOutput(\n                                                text=chunk,\n                                                metadata=SingleOutputMeta(\n                                                    input_tokens_count=input_tokens_count,\n                                                    generated_tokens_count=generated_tokens_count,\n                                                ),\n                                            )\n                                        ]\n                                    ),\n                                )\n                            )\n                except:\n                    traceback.print_exc()\n                ray.get(server.mark_done.remote(request_id[0]))\n\n            threading.Thread(target=writer, daemon=True).start()\n\n            time_count = 10 * 100\n            while request_id[0] is None and time_count > 0:\n                time.sleep(0.01)\n                time_count -= 1\n\n            if request_id[0] is None:\n                raise Exception(\"Failed to get request id\")\n\n            def write_running():\n                return ray.get(server.add_item.remote(request_id[0], \"RUNNING\"))\n\n            await asyncio.to_thread(write_running)\n            return [\n                (\n                    \"\",\n                    {\n                        \"metadata\": {\n                            \"request_id\": request_id[0],\n                            \"stream_server\": \"BlockBinaryStreamServer\",\n                        }\n                    },\n                )\n            ]\n\n        start_time = time.monotonic()\n        with io.BytesIO() as output:\n            async with asyncfy_with_semaphore(\n                lambda: self.client.with_streaming_response.audio.speech.create(\n                    model=self.model, voice=voice, input=ins, **kwargs\n                )\n            )() as response:\n                for chunk in response.iter_bytes():\n                    output.write(chunk)\n\n            base64_audio = base64.b64encode(output.getvalue()).decode()\n            time_cost = time.monotonic() - start_time\n            return [\n                (\n                    base64_audio,\n                    {\n                        \"metadata\": {\n                            \"request_id\": \"\",\n                            \"input_tokens_count\": 0,\n                            \"generated_tokens_count\": 0,\n                            \"time_cost\": time_cost,\n                            \"first_token_time\": 0,\n                            \"speed\": 0,\n                        }\n                    },\n                )\n            ]\n\n    async def async_speech_to_text(\n        self,\n        audio: str,\n        response_format: str = \"verbose_json\",\n        timestamp_granularities: List[str] = [\"word\",\"segment\"],\n    ):               \n        # Extract audio format and base64 data\n        data_prefix = \"data:audio/\"\n        base64_prefix = \";base64,\"\n        if not audio.startswith(data_prefix):\n            raise ValueError(\"Invalid audio data format\")\n        \n        format_end = audio.index(base64_prefix)\n        audio_format = audio[len(data_prefix):format_end]\n        base64_data = audio[format_end + len(base64_prefix):]\n        \n        # Decode the base64 audio data\n        audio_data = base64.b64decode(base64_data)\n\n        # Create a temporary file with the correct extension\n        with tempfile.NamedTemporaryFile(delete=False, suffix=f'.{audio_format}') as temp_audio_file:\n            temp_audio_file.write(audio_data)\n            temp_audio_file_path = temp_audio_file.name\n\n        try:\n            start_time = time.monotonic()\n                   \n            with open(temp_audio_file_path, \"rb\") as audio_file:                \n                transcription = self.client.audio.transcriptions.create(\n                    model=self.model or \"whisper-1\",\n                    file=audio_file,\n                    response_format=response_format,\n                    timestamp_granularities=timestamp_granularities,\n                )\n            time_cost = time.monotonic() - start_time\n\n            return [\n                (\n                    json.dumps(transcription.to_dict(), ensure_ascii=True),\n                    {\n                        \"metadata\": {\n                            \"request_id\": \"\",\n                            \"input_tokens_count\": 0,\n                            \"generated_tokens_count\": 0,\n                            \"time_cost\": time_cost,\n                            \"first_token_time\": 0,\n                            \"speed\": 0,\n                        }\n                    },\n                )\n            ]\n        finally:\n            # Clean up the temporary file\n            import os\n            os.unlink(temp_audio_file_path)\n\n    def image_to_text(self, ins: str, **kwargs):\n        pass\n\n    async def async_text_to_image(\n        self, stream: bool, input: str, size: str, quality: str, n: int, **kwargs\n    ):\n        if stream:\n            raise Exception(\"Stream not supported for text to image\")\n        start_time = time.monotonic()\n        response = await asyncfy_with_semaphore(\n            lambda: self.client.images.generate(\n                model=self.model,\n                prompt=input,\n                size=size,\n                quality=quality,\n                n=1,\n                response_format=\"b64_json\",\n                **kwargs,\n            )\n        )()\n        time_cost = time.monotonic() - start_time\n        base64_image = response.data[0].b64_json\n        return [\n            (\n                base64_image,\n                {\n                    \"metadata\": {\n                        \"request_id\": \"\",\n                        \"input_tokens_count\": 0,\n                        \"generated_tokens_count\": 0,\n                        \"time_cost\": time_cost,\n                        \"first_token_time\": 0,\n                        \"speed\": 0,\n                    }\n                },\n            )\n        ]\n\n    def text_to_text(self, ins: str, **kwargs):\n        pass\n\n    async def async_stream_chat(\n        self,\n        tokenizer,\n        ins: str,\n        his: List[Dict[str, Any]] = [],\n        max_length: int = 4096,\n        top_p: float = 0.7,\n        temperature: float = 0.9,\n        **kwargs,\n    ):\n\n        model = self.model\n\n        if \"model\" in kwargs:\n            model = kwargs[\"model\"]\n\n        messages = [\n            {\"role\": message[\"role\"], \"content\": self.process_input(message[\"content\"])}\n            for message in his\n        ] + [{\"role\": \"user\", \"content\": self.process_input(ins)}]\n\n        stream = kwargs.get(\"stream\", False)\n\n        ## content = [\n        ##    \"voice\": \"alloy\",\"input\": \"Hello, World!\",response_format: \"mp3\"]\n        last_message = messages[-1][\"content\"]\n\n        if isinstance(last_message, dict) and \"voice\" in last_message:\n            voice = last_message[\"voice\"]\n            response_format = last_message.get(\"response_format\", \"mp3\")\n            chunk_size = last_message.get(\"chunk_size\", None)\n            input = last_message[\"input\"]\n            return await self.async_text_to_speech(\n                stream=stream,\n                ins=input,\n                voice=voice,\n                chunk_size=chunk_size,\n                response_format=response_format,\n            )\n\n        if isinstance(last_message, dict) and \"input\" in last_message:\n            input = last_message[\"input\"]\n            size = last_message.get(\"size\", \"1024x1024\")\n            quality = last_message.get(\"quality\", \"standard\")\n            n = last_message.get(\"n\", 1)\n            return await self.async_text_to_image(\n                stream=stream, input=input, size=size, quality=quality, n=n\n            )\n\n        if isinstance(last_message, dict) and \"audio\" in last_message:            \n            audio_data = last_message.get(\"audio\", \"\")            \n            tpe = last_message.get(\"type\",\"wav\")\n            if not audio_data.startswith(\"data:\"):\n                audio_data = f\"data:audio/${tpe};base64,\" + audio_data                \n            return await self.async_speech_to_text(audio=audio_data)\n        \n        server = ray.get_actor(\"BLOCK_VLLM_STREAM_SERVER\")\n        request_id = [None]\n\n        def writer():\n            try:\n                r = \"\"\n                response = self.client.chat.completions.create(\n                    messages=messages,\n                    model=model,\n                    stream=True,\n                    max_tokens=max_length,\n                    temperature=temperature,\n                    top_p=top_p,\n                )\n                # input_tokens_count = 0\n                # generated_tokens_count = 0\n\n                request_id[0] = str(uuid.uuid4())\n\n                for chunk in response:\n                    content = chunk.choices[0].delta.content or \"\"\n                    r += content\n                    if hasattr(chunk, \"usage\") and chunk.usage:\n                        input_tokens_count = chunk.usage.prompt_tokens\n                        generated_tokens_count = chunk.usage.completion_tokens\n                    else:\n                        input_tokens_count = 0\n                        generated_tokens_count = 0\n                    ray.get(\n                        server.add_item.remote(\n                            request_id[0],\n                            StreamOutputs(\n                                outputs=[\n                                    SingleOutput(\n                                        text=r,\n                                        metadata=SingleOutputMeta(\n                                            input_tokens_count=input_tokens_count,\n                                            generated_tokens_count=generated_tokens_count,\n                                        ),\n                                    )\n                                ]\n                            ),\n                        )\n                    )\n            except:\n                traceback.print_exc()\n            ray.get(server.mark_done.remote(request_id[0]))\n\n        if stream:\n            threading.Thread(target=writer, daemon=True).start()\n\n            time_count = 60 * 100\n            while request_id[0] is None and time_count > 0:\n                time.sleep(0.01)\n                time_count -= 1\n\n            if request_id[0] is None:\n                raise Exception(\"Failed to get request id\")\n\n            def write_running():\n                return ray.get(server.add_item.remote(request_id[0], \"RUNNING\"))\n\n            await asyncio.to_thread(write_running)\n            return [\n                (\n                    \"\",\n                    {\n                        \"metadata\": {\n                            \"request_id\": request_id[0],\n                            \"stream_server\": \"BLOCK_VLLM_STREAM_SERVER\",\n                        }\n                    },\n                )\n            ]\n        else:\n            try:\n                start_time = time.monotonic()\n                response = await asyncfy_with_semaphore(\n                    lambda: self.client.chat.completions.create(\n                        messages=messages,\n                        model=model,\n                        max_tokens=max_length,\n                        temperature=temperature,\n                        top_p=top_p,\n                    )\n                )()\n\n                generated_text = response.choices[0].message.content\n                generated_tokens_count = response.usage.completion_tokens\n                input_tokens_count = response.usage.prompt_tokens\n                time_cost = time.monotonic() - start_time\n                return [\n                    (\n                        generated_text,\n                        {\n                            \"metadata\": {\n                                \"request_id\": response.id,\n                                \"input_tokens_count\": input_tokens_count,\n                                \"generated_tokens_count\": generated_tokens_count,\n                                \"time_cost\": time_cost,\n                                \"first_token_time\": 0,\n                                \"speed\": float(generated_tokens_count) / time_cost,\n                            }\n                        },\n                    )\n                ]\n            except Exception as e:\n                print(f\"Error: {e}\")\n                raise e\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/whisper/__init__.py\nimport whisper\nfrom typing import Dict, List, Tuple\nfrom byzerllm.utils.types import StopSequencesCriteria\n\nfrom typing import Dict, Any, List, Union\nfrom pyjava.storage import streaming_tar as STar\nimport json\nimport base64\nimport tempfile\nimport time\n\n\ndef get_meta(self):\n    return [\n        {\n            \"model_deploy_type\": \"proprietary\",\n            \"backend\": \"transformers\",\n            \"message_format\": True\n        }\n    ]\n\n\ndef process_input(ins: Union[str, List[Dict[str, Any]], Dict[str, Any]]):\n    # print(ins) \n    if isinstance(ins, list) or isinstance(ins, dict):\n        return ins\n\n    content = []\n    try:\n        ins_json = json.loads(ins)\n    except:\n        return ins\n\n    ## speech\n    if isinstance(ins_json, dict):\n        return ins_json\n\n    content = []\n    #     [\n    #     {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n    #     {\n    #       \"type\": \"image_url\",\n    #       \"image_url\": {\n    #         \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n    #         \"detail\": \"high\"\n    #       },\n    #     },\n    #   ],\n    for item in ins_json:\n        # for format like this: {\"image\": \"xxxxxx\", \"text\": \"What’s in this image?\",\"detail\":\"high\"}\n        # or {\"image\": \"xxxxxx\"}, {\"text\": \"What’s in this image?\"}\n        if (\"image\" in item or \"image_url\" in item) and \"type\" not in item:\n            image_data = item.get(\"image\", item.get(\"image_url\", \"\"))\n            ## \"data:image/jpeg;base64,\"\n            if not image_data.startswith(\"data:\"):\n                image_data = \"data:image/jpeg;base64,\" + image_data\n\n            ## get the other fields except image/text/image_url\n            other_fields = {\n                k: v for k, v in item.items() if k not in [\"image\", \"text\", \"image_url\"]\n            }\n            content.append(\n                {\n                    \"image_url\": {\"url\": image_data, **other_fields},\n                    \"type\": \"image_url\",\n                }\n            )\n\n        if \"text\" in item and \"type\" not in item:\n            text_data = item[\"text\"]\n            content.append({\"text\": text_data, \"type\": \"text\"})\n\n        ## for format like this: {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n        ## {\"type\": \"image_url\", \"image_url\": {\"url\":\"\",\"detail\":\"high\"}}\n        ## this is the standard format, just return it\n        if \"type\" in item and item[\"type\"] == \"text\":\n            content.append(item)\n\n        if \"type\" in item and item[\"type\"] == \"image_url\":\n            content.append(item)\n\n    if not content:\n        return ins\n\n    return content\n\n\ndef stream_chat(\n    self,\n    tokenizer,\n    ins: str,\n    his: List[Dict[str, str]] = [],\n    max_length: int = 4090,\n    top_p: float = 0.95,\n    temperature: float = 0.1,\n    **kwargs,\n):\n    messages = [\n        {\"role\": message[\"role\"], \"content\": process_input(message[\"content\"])}\n        for message in his\n    ] + [{\"role\": \"user\", \"content\": process_input(ins)}]\n\n    audio_input = messages[-1][\"content\"]\n\n    audio_data = audio_input.get(\"audio\", \"\")\n    audio = audio_input[\"audio\"]\n    response_format = audio_input.get(\"response_format\", \"json\")\n    timestamp_granularities = audio_input.get(\n        \"timestamp_granularities\", [\"word\", \"segment\"]\n    )\n    data_prefix = \"data:audio/\"\n    base64_prefix = \";base64,\"\n    if not audio.startswith(data_prefix):\n        raise ValueError(\"Invalid audio data format\")\n\n    format_end = audio.index(base64_prefix)\n    audio_format = audio[len(data_prefix) : format_end]\n    base64_data = audio[format_end + len(base64_prefix) :]\n\n    # Decode the base64 audio data\n    audio_data = base64.b64decode(base64_data)\n\n    # Create a temporary file with the correct extension\n    with tempfile.NamedTemporaryFile(\n        delete=False, suffix=f\".{audio_format}\"\n    ) as temp_audio_file:\n        temp_audio_file.write(audio_data)\n        temp_audio_file_path = temp_audio_file.name\n\n    try:\n        start_time = time.monotonic()\n        word_timestamps = \"word\" in timestamp_granularities\n        \n        result = self.transcribe(temp_audio_file_path, word_timestamps=word_timestamps)        \n            \n        time_cost = time.monotonic() - start_time\n        return [\n            (\n                json.dumps(result, ensure_ascii=True),\n                {\n                    \"metadata\": {\n                        \"request_id\": \"\",\n                        \"input_tokens_count\": 0,\n                        \"generated_tokens_count\": 0,\n                        \"time_cost\": time_cost,\n                        \"first_token_time\": 0,\n                        \"speed\": 0,\n                    }\n                },\n            )\n        ]\n    finally:\n        # Clean up the temporary file\n        import os\n\n        os.unlink(temp_audio_file_path)\n\n\ndef init_model(\n    model_dir, infer_params: Dict[str, str] = {}, sys_conf: Dict[str, str] = {}\n):\n    model = whisper.load_model(\"large\", download_root=model_dir)\n    import types\n\n    model.stream_chat = types.MethodType(stream_chat, model)\n    model.get_meta = types.MethodType(get_meta, model)\n    return (model, None)\n\n\n     \n\n下面是用户的需求：\n\n参考 web_ui.py的代码,按  whisper/__init__.py 的接口规范来实现 sensevoice/__init__.py. 务必要理解后再去实现.\n"}, {"role": "assistant", "content": "我理解您的要求。我将按照 SEARCH/REPLACE 格式重新输出代码修改。这里是 sensevoice/__init__.py 的实现：\n\n```python\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/sensevoice/__init__.py\n<<<<<<< SEARCH\n=======\nimport json\nimport base64\nimport tempfile\nimport time\nfrom typing import Dict, List, Union, Any\nfrom funasr import AutoModel\n\ndef get_meta(self):\n    return [\n        {\n            \"model_deploy_type\": \"proprietary\",\n            \"backend\": \"funasr\",\n            \"message_format\": True\n        }\n    ]\n\ndef process_input(ins: Union[str, List[Dict[str, Any]], Dict[str, Any]]):\n    if isinstance(ins, list) or isinstance(ins, dict):\n        return ins\n\n    try:\n        ins_json = json.loads(ins)\n    except:\n        return ins\n\n    if isinstance(ins_json, dict):\n        return ins_json\n\n    content = []\n    for item in ins_json:\n        if \"audio\" in item and \"type\" not in item:\n            audio_data = item.get(\"audio\", \"\")\n            if not audio_data.startswith(\"data:\"):\n                audio_data = f\"data:audio/wav;base64,{audio_data}\"\n            content.append({\"audio\": audio_data, \"type\": \"audio\"})\n\n        if \"text\" in item and \"type\" not in item:\n            content.append({\"text\": item[\"text\"], \"type\": \"text\"})\n\n        if \"type\" in item:\n            content.append(item)\n\n    return content if content else ins\n\ndef stream_chat(\n    self,\n    tokenizer,\n    ins: str,\n    his: List[Dict[str, str]] = [],\n    max_length: int = 4090,\n    top_p: float = 0.95,\n    temperature: float = 0.1,\n    **kwargs,\n):\n    messages = [\n        {\"role\": message[\"role\"], \"content\": process_input(message[\"content\"])}\n        for message in his\n    ] + [{\"role\": \"user\", \"content\": process_input(ins)}]\n\n    audio_input = messages[-1][\"content\"]\n\n    audio = audio_input[\"audio\"]\n    data_prefix = \"data:audio/\"\n    base64_prefix = \";base64,\"\n    if not audio.startswith(data_prefix):\n        raise ValueError(\"Invalid audio data format\")\n\n    format_end = audio.index(base64_prefix)\n    audio_format = audio[len(data_prefix):format_end]\n    base64_data = audio[format_end + len(base64_prefix):]\n\n    audio_data = base64.b64decode(base64_data)\n\n    with tempfile.NamedTemporaryFile(delete=False, suffix=f\".{audio_format}\") as temp_audio_file:\n        temp_audio_file.write(audio_data)\n        temp_audio_file_path = temp_audio_file.name\n\n    try:\n        start_time = time.monotonic()\n        \n        result = self.generate(input=temp_audio_file_path, cache={})\n        \n        time_cost = time.monotonic() - start_time\n        \n        formatted_result = format_str_v3(result[0][\"text\"])\n        \n        return [\n            (\n                json.dumps({\"text\": formatted_result}, ensure_ascii=False),\n                {\n                    \"metadata\": {\n                        \"request_id\": \"\",\n                        \"input_tokens_count\": 0,\n                        \"generated_tokens_count\": 0,\n                        \"time_cost\": time_cost,\n                        \"first_token_time\": 0,\n                        \"speed\": 0,\n                    }\n                },\n            )\n        ]\n    finally:\n        import os\n        os.unlink(temp_audio_file_path)\n\ndef init_model(\n    model_dir, infer_params: Dict[str, str] = {}, sys_conf: Dict[str, str] = {}\n):\n    model = AutoModel(\n        model=\"iic/SenseVoiceSmall\",\n        vad_model=\"iic/speech_fsmn_vad_zh-cn-16k-common-pytorch\",\n        vad_kwargs={\"max_single_segment_time\": 30000},\n        trust_remote_code=True,\n    )\n    \n    import types\n\n    model.stream_chat = types.MethodType(stream_chat, model)\n    model.get_meta = types.MethodType(get_meta, model)\n    \n    return (model, None)\n\n# Helper functions from web_ui.py\nemo_dict = {\n    \"<|HAPPY|>\": \"😊\", \"<|SAD|>\": \"😔\", \"<|ANGRY|>\": \"😡\", \"<|NEUTRAL|>\": \"\",\n    \"<|FEARFUL|>\": \"😰\", \"<|DISGUSTED|>\": \"🤢\", \"<|SURPRISED|>\": \"😮\",\n}\n\nevent_dict = {\n    \"<|BGM|>\": \"🎼\", \"<|Speech|>\": \"\", \"<|Applause|>\": \"👏\", \"<|Laughter|>\": \"��\",\n    \"<|Cry|>\": \"😭\", \"<|Sneeze|>\": \"🤧\", \"<|Breath|>\": \"\", \"<|Cough|>\": \"🤧\",\n}\n\nemoji_dict = {\n    \"<|nospeech|><|Event_UNK|>\": \"❓\", \"<|zh|>\": \"\", \"<|en|>\": \"\", \"<|yue|>\": \"\",\n    \"<|ja|>\": \"\", \"<|ko|>\": \"\", \"<|nospeech|>\": \"\", \"<|HAPPY|>\": \"😊\", \"<|SAD|>\": \"😔\",\n    \"<|ANGRY|>\": \"😡\", \"<|NEUTRAL|>\": \"\", \"<|BGM|>\": \"🎼\", \"<|Speech|>\": \"\",\n    \"<|Applause|>\": \"👏\", \"<|Laughter|>\": \"😀\", \"<|FEARFUL|>\": \"😰\", \"<|DISGUSTED|>\": \"🤢\",\n    \"<|SURPRISED|>\": \"😮\", \"<|Cry|>\": \"😭\", \"<|EMO_UNKNOWN|>\": \"\", \"<|Sneeze|>\": \"🤧\",\n    \"<|Breath|>\": \"\", \"<|Cough|>\": \"😷\", \"<|Sing|>\": \"\", \"<|Speech_Noise|>\": \"\",\n    \"<|withitn|>\": \"\", \"<|woitn|>\": \"\", \"<|GBG|>\": \"\", \"<|Event_UNK|>\": \"\",\n}\n\nlang_dict = {\n    \"<|zh|>\": \"<|lang|>\", \"<|en|>\": \"<|lang|>\", \"<|yue|>\": \"<|lang|>\",\n    \"<|ja|>\": \"<|lang|>\", \"<|ko|>\": \"<|lang|>\", \"<|nospeech|>\": \"<|lang|>\",\n}\n\nemo_set = {\"😊\", \"😔\", \"😡\", \"😰\", \"🤢\", \"😮\"}\nevent_set = {\"🎼\", \"👏\", \"😀\", \"😭\", \"🤧\", \"😷\"}\n\ndef format_str(s):\n    for sptk in emoji_dict:\n        s = s.replace(sptk, emoji_dict[sptk])\n    return s\n\ndef format_str_v2(s):\n    sptk_dict = {sptk: s.count(sptk) for sptk in emoji_dict}\n    s = ''.join(c for c in s if c not in emoji_dict.values())\n    emo = max(emo_dict, key=lambda e: sptk_dict[e])\n    event = ''.join(event_dict[e] for e in event_dict if sptk_dict[e] > 0)\n    s = event + s + emo_dict[emo]\n    for emoji in emo_set.union(event_set):\n        s = s.replace(\" \" + emoji, emoji).replace(emoji + \" \", emoji)\n    return s.strip()\n\ndef format_str_v3(s):\n    def get_emo(s):\n        return s[-1] if s[-1] in emo_set else None\n    def get_event(s):\n        return s[0] if s[0] in event_set else None\n\n    s = s.replace(\"<|nospeech|><|Event_UNK|>\", \"❓\")\n    for lang in lang_dict:\n        s = s.replace(lang, \"<|lang|>\")\n    s_list = [format_str_v2(s_i).strip() for s_i in s.split(\"<|lang|>\")]\n    new_s = \" \" + s_list[0]\n    cur_ent_event = get_event(new_s)\n    for i in range(1, len(s_list)):\n        if len(s_list[i]) == 0:\n            continue\n        if get_event(s_list[i]) == cur_ent_event and get_event(s_list[i]) is not None:\n            s_list[i] = s_list[i][1:]\n        cur_ent_event = get_event(s_list[i])\n        if get_emo(s_list[i]) is not None and get_emo(s_list[i]) == get_emo(new_s):\n            new_s = new_s[:-1]\n        new_s += s_list[i].strip()\n    new_s = new_s.replace(\"The.\", \" \")\n    return new_s.strip()\n>>>>>>> REPLACE\n```\n\n这个 SEARCH/REPLACE 块将创建一个新的 sensevoice/__init__.py 文件，其中包含了所有必要的功能。它遵循了 whisper/__init__.py 的接口规范，同时借鉴了 web_ui.py 中的一些功能来处理 SenseVoice 模型的输出。\n\n如果您需要对这个实现进行任何调整或有任何问题，请随时告诉我。"}], "model": "human", "yaml_file": "actions/016_chat_action.yml"}
{"instruction": "sensevoice 中 infer_params 获取vad_model, 如果为空,则不传递 vad_model相关参数.", "conversations": [{"role": "user", "content": "\n如果你需要生成代码，对于每个需要更改的文件,你需要按 *SEARCH/REPLACE block* 的格式进行生成。\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The opening fence and code language, eg: ```python\n2. The file path alone on a line, starting with \"##File:\" and verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n*SEARCH/REPLACE* blocks will replace *all* matching occurrences.\nInclude enough lines to make the SEARCH blocks unique.\n\nInclude *ALL* the code being searched and replaced!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n下面我们来看一个例子：\n\n当前项目目录结构：\n1. 项目根目录： /tmp/projects/mathweb\n2. 项目子目录/文件列表(类似tree 命令输出)\nflask/\n    app.py\n    templates/\n        index.html\n    static/\n        style.css\n\n用户需求： Change get_factorial() to use math.factorial\n\n回答： To make this change we need to modify `/tmp/projects/mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n用户需求： Refactor hello() into its own file.\n\n回答：To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/hello.py\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/main.py\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n现在让我们开始一个新的任务:\n\n当前项目目录结构：\n1. 项目根目录： /Users/allwefantasy/projects/byzer-llm\n2. 项目子目录/文件列表(类似tree 命令输出)：\nsetup.py\ntests/\n    test_emb_rerank.py\n    test_deepseek_code.py\n    test_prompt.py\n    test_model_meta.py\n    __init__.py\n    test_transformers_deploy.py\n    test_data_analysis.py\n    test_yi.py\n    test_deploy.py\n    functions_suite.py\n    test_template.py\n    auto/\n        test_backend_llama_cpp.py\n        __init__.py\nsetup-machine/\n    convert_safetensor.py\nnotebooks/\n    test.py\nsrc/\n    byzerllm/\n        lang.py\n        store.py\n        version.py\n        log.py\n        __init__.py\n        types.py\n        easy_byzerllm_command.py\n        byzerllm_command.py\n        command_args.py\n        bark/\n            generation.py\n            __init__.py\n            model.py\n            api.py\n            model_fine.py\n            bark_voice.py\n        moss/\n            finetune_moss.py\n            __init__.py\n            moss_inference.py\n            models/\n                configuration_moss.py\n                quantization.py\n                custom_autotune.py\n                __init__.py\n                modeling_moss.py\n                tokenization_moss.py\n        llama2/\n            __init__.py\n        records/\n            __init__.py\n        visualglm/\n            __init__.py\n        auto/\n            __init__.py\n            backend_llama_cpp.py\n            backend_vllm.py\n            backend_ds.py\n        alpha_moss/\n            __init__.py\n        processor/\n            __init__.py\n        sensevoice/\n            __init__.py\n        qwen/\n            __init__.py\n        utils/\n            json_repaire.py\n            object_store_ref_util.py\n            __init__.py\n            types.py\n            tokenizer.py\n            openai_utils.py\n            ray_utils.py\n            text_generator.py\n            emb.py\n            connect_ray.py\n            testing.py\n            langutil.py\n            nontext.py\n            fulltune/\n                deepspeed_trainner.py\n                __init__.py\n                launch.py\n                trainner.py\n                base_model/\n                    __init__.py\n                    configuration_baichuan.py\n                    modeling_baichuan.py\n                pretrain/\n                    __init__.py\n                    convert_to_transformers.py\n            metrics/\n                __init__.py\n            config/\n                __init__.py\n            retrieval/\n                rest.py\n                __init__.py\n                udf.py\n            sft/\n                argument.py\n                collator.py\n                qlora.py\n                __init__.py\n                model.py\n                dataset.py\n                loss.py\n                merge_lora.py\n                trainer.py\n            client/\n                message_utils.py\n                math_utils.py\n                byzerllm_client.py\n                parallel_utils.py\n                __init__.py\n                types.py\n                code_utils.py\n                img_utils.py\n                entrypoints/\n                    __init__.py\n                    openai/\n                        protocol.py\n                        __init__.py\n                        serve.py\n                        serving_chat.py\n                        api_server.py\n                        serving_engine.py\n                        serving_completion.py\n                        tool.py\n        chatglm2/\n            __init__.py\n        baichuan/\n            __init__.py\n        m3e/\n            __init__.py\n        stable_diffusion/\n            config.py\n            __init__.py\n            logger.py\n            shared.py\n            model.py\n            utils.py\n            images.py\n            diffusion/\n                __init__.py\n                embeddings.py\n                utils.py\n                networks/\n                    lyco.py\n                    __init__.py\n                upscalers/\n                    __init__.py\n                    multidiffusion.py\n                    samplers.py\n                piplines/\n                    __init__.py\n                    lpw.py\n                    diffusers.py\n            lib/\n                __init__.py\n                diffusers/\n                    __init__.py\n                    scheduler.py\n            api/\n                __init__.py\n                plugin.py\n                models/\n                    diffusion.py\n                    __init__.py\n                diffusion/\n                    __init__.py\n                    pipelines/\n                        __init__.py\n                        diffusers.py\n                events/\n                    generation.py\n                    __init__.py\n        bge/\n            __init__.py\n        bge_rerank/\n            __init__.py\n        apps/\n            client.py\n            qa_strategy.py\n            __init__.py\n            builder.py\n            vector_db.py\n            utils.py\n            qa.py\n            byzer_storage/\n                memory_model_based.py\n                command.py\n                env.py\n                train_pt.py\n                train_sft.py\n                __init__.py\n                train_dpo.py\n                simple_api.py\n                generate_sft_data.py\n            byzer_sql/\n                __init__.py\n            llama_index/\n                byzerai_vectordb.py\n                byzerai.py\n                byzerai_embedding.py\n                __init__.py\n                collection_manager.py\n                byzerai_index_store.py\n                byzerai_docstore.py\n                simple_retrieval.py\n                byzerai_kvstore.py\n            agent/\n                conversable_agent.py\n                registry.py\n                groupchat.py\n                user_proxy_agent.py\n                __init__.py\n                agent.py\n                extensions/\n                    simple_retrieval_client.py\n                    planner.py\n                    python_codesandbox_agent.py\n                    preview_file_agent.py\n                    llama_index_retrieval_agent.py\n                    visualization_agent.py\n                    assistant_agent.py\n                    load_data_agent.py\n                    retrieval_agent.py\n                    output_agent.py\n                    __init__.py\n                    byzer_engine_agent.py\n                    subquestion_agent.py\n                    sql_reviewer_agent.py\n                    spark_sql_agent.py\n                    rhetorical_agent.py\n                    data_analysis.py\n                    data_analysis_pipeline_agent.py\n                    common_agent.py\n                    query_rewrite/\n                        time.py\n                        rhetorical.py\n                        __init__.py\n                        condition.py\n                        context.py\n                store/\n                    memory_store.py\n                    __init__.py\n                    stores.py\n        saas/\n            __init__.py\n            chatglm/\n                __init__.py\n            sparkdesk/\n                __init__.py\n            gemini/\n                __init__.py\n            azure_openai/\n                __init__.py\n            azure/\n                __init__.py\n            qianfan/\n                __init__.py\n            claude/\n                __init__.py\n            qianwen/\n                __init__.py\n            volcengine/\n                __init__.py\n            minimax/\n                __init__.py\n            baichuan/\n                __init__.py\n            qianwen_vl/\n                __init__.py\n            official_openai/\n                __init__.py\n            zhipu/\n                __init__.py\n            aws_bedrock/\n                __init__.py\n            openai/\n                __init__.py\n        falcon/\n            __init__.py\n        chatglm6b/\n            trainer_seq2seq.py\n            arguments.py\n            __init__.py\n            finetune.py\n            tunning/\n                __init__.py\n                infer.py\n                finetune.py\n                utils/\n                    config.py\n                    __init__.py\n                    other.py\n                    common.py\n                    seq2seq.py\n        starcode/\n            __init__.py\n        qwen_vl_chat/\n            __init__.py\n        zephyr/\n            __init__.py\n        dolly/\n            generate.py\n            __init__.py\n            consts.py\n            dolly_inference.py\n            trainer.py\n        llama/\n            __init__.py\n        whisper/\n            whisper_inference.py\n            __init__.py\n下面是一些文件路径以及每个文件对应的源码：\n\n##File: /Users/allwefantasy/projects/SenseVoice/webui.py\n# coding=utf-8\n\nimport os\nimport librosa\nimport base64\nimport io\nimport gradio as gr\nimport re\n\nimport numpy as np\nimport torch\nimport torchaudio\n\n\nfrom funasr import AutoModel\n\nmodel = \"iic/SenseVoiceSmall\"\nmodel = AutoModel(model=model,\n\t\t\t\t  vad_model=\"iic/speech_fsmn_vad_zh-cn-16k-common-pytorch\",\n\t\t\t\t  vad_kwargs={\"max_single_segment_time\": 30000},\n\t\t\t\t  trust_remote_code=True,\n\t\t\t\t  )\n\nimport re\n\nemo_dict = {\n\t\"<|HAPPY|>\": \"😊\",\n\t\"<|SAD|>\": \"😔\",\n\t\"<|ANGRY|>\": \"😡\",\n\t\"<|NEUTRAL|>\": \"\",\n\t\"<|FEARFUL|>\": \"😰\",\n\t\"<|DISGUSTED|>\": \"🤢\",\n\t\"<|SURPRISED|>\": \"😮\",\n}\n\nevent_dict = {\n\t\"<|BGM|>\": \"🎼\",\n\t\"<|Speech|>\": \"\",\n\t\"<|Applause|>\": \"👏\",\n\t\"<|Laughter|>\": \"😀\",\n\t\"<|Cry|>\": \"😭\",\n\t\"<|Sneeze|>\": \"🤧\",\n\t\"<|Breath|>\": \"\",\n\t\"<|Cough|>\": \"🤧\",\n}\n\nemoji_dict = {\n\t\"<|nospeech|><|Event_UNK|>\": \"❓\",\n\t\"<|zh|>\": \"\",\n\t\"<|en|>\": \"\",\n\t\"<|yue|>\": \"\",\n\t\"<|ja|>\": \"\",\n\t\"<|ko|>\": \"\",\n\t\"<|nospeech|>\": \"\",\n\t\"<|HAPPY|>\": \"😊\",\n\t\"<|SAD|>\": \"😔\",\n\t\"<|ANGRY|>\": \"😡\",\n\t\"<|NEUTRAL|>\": \"\",\n\t\"<|BGM|>\": \"🎼\",\n\t\"<|Speech|>\": \"\",\n\t\"<|Applause|>\": \"👏\",\n\t\"<|Laughter|>\": \"😀\",\n\t\"<|FEARFUL|>\": \"😰\",\n\t\"<|DISGUSTED|>\": \"🤢\",\n\t\"<|SURPRISED|>\": \"😮\",\n\t\"<|Cry|>\": \"😭\",\n\t\"<|EMO_UNKNOWN|>\": \"\",\n\t\"<|Sneeze|>\": \"🤧\",\n\t\"<|Breath|>\": \"\",\n\t\"<|Cough|>\": \"😷\",\n\t\"<|Sing|>\": \"\",\n\t\"<|Speech_Noise|>\": \"\",\n\t\"<|withitn|>\": \"\",\n\t\"<|woitn|>\": \"\",\n\t\"<|GBG|>\": \"\",\n\t\"<|Event_UNK|>\": \"\",\n}\n\nlang_dict =  {\n    \"<|zh|>\": \"<|lang|>\",\n    \"<|en|>\": \"<|lang|>\",\n    \"<|yue|>\": \"<|lang|>\",\n    \"<|ja|>\": \"<|lang|>\",\n    \"<|ko|>\": \"<|lang|>\",\n    \"<|nospeech|>\": \"<|lang|>\",\n}\n\nemo_set = {\"😊\", \"😔\", \"😡\", \"😰\", \"🤢\", \"😮\"}\nevent_set = {\"🎼\", \"👏\", \"😀\", \"😭\", \"🤧\", \"😷\",}\n\ndef format_str(s):\n\tfor sptk in emoji_dict:\n\t\ts = s.replace(sptk, emoji_dict[sptk])\n\treturn s\n\n\ndef format_str_v2(s):\n\tsptk_dict = {}\n\tfor sptk in emoji_dict:\n\t\tsptk_dict[sptk] = s.count(sptk)\n\t\ts = s.replace(sptk, \"\")\n\temo = \"<|NEUTRAL|>\"\n\tfor e in emo_dict:\n\t\tif sptk_dict[e] > sptk_dict[emo]:\n\t\t\temo = e\n\tfor e in event_dict:\n\t\tif sptk_dict[e] > 0:\n\t\t\ts = event_dict[e] + s\n\ts = s + emo_dict[emo]\n\n\tfor emoji in emo_set.union(event_set):\n\t\ts = s.replace(\" \" + emoji, emoji)\n\t\ts = s.replace(emoji + \" \", emoji)\n\treturn s.strip()\n\ndef format_str_v3(s):\n\tdef get_emo(s):\n\t\treturn s[-1] if s[-1] in emo_set else None\n\tdef get_event(s):\n\t\treturn s[0] if s[0] in event_set else None\n\n\ts = s.replace(\"<|nospeech|><|Event_UNK|>\", \"❓\")\n\tfor lang in lang_dict:\n\t\ts = s.replace(lang, \"<|lang|>\")\n\ts_list = [format_str_v2(s_i).strip(\" \") for s_i in s.split(\"<|lang|>\")]\n\tnew_s = \" \" + s_list[0]\n\tcur_ent_event = get_event(new_s)\n\tfor i in range(1, len(s_list)):\n\t\tif len(s_list[i]) == 0:\n\t\t\tcontinue\n\t\tif get_event(s_list[i]) == cur_ent_event and get_event(s_list[i]) != None:\n\t\t\ts_list[i] = s_list[i][1:]\n\t\t#else:\n\t\tcur_ent_event = get_event(s_list[i])\n\t\tif get_emo(s_list[i]) != None and get_emo(s_list[i]) == get_emo(new_s):\n\t\t\tnew_s = new_s[:-1]\n\t\tnew_s += s_list[i].strip().lstrip()\n\tnew_s = new_s.replace(\"The.\", \" \")\n\treturn new_s.strip()\n\ndef model_inference(input_wav, language, fs=16000):\n\t# task_abbr = {\"Speech Recognition\": \"ASR\", \"Rich Text Transcription\": (\"ASR\", \"AED\", \"SER\")}\n\tlanguage_abbr = {\"auto\": \"auto\", \"zh\": \"zh\", \"en\": \"en\", \"yue\": \"yue\", \"ja\": \"ja\", \"ko\": \"ko\",\n\t\t\t\t\t \"nospeech\": \"nospeech\"}\n\t\n\t# task = \"Speech Recognition\" if task is None else task\n\tlanguage = \"auto\" if len(language) < 1 else language\n\tselected_language = language_abbr[language]\n\t# selected_task = task_abbr.get(task)\n\t\n\t# print(f\"input_wav: {type(input_wav)}, {input_wav[1].shape}, {input_wav}\")\n\t\n\tif isinstance(input_wav, tuple):\n\t\tfs, input_wav = input_wav\n\t\tinput_wav = input_wav.astype(np.float32) / np.iinfo(np.int16).max\n\t\tif len(input_wav.shape) > 1:\n\t\t\tinput_wav = input_wav.mean(-1)\n\t\tif fs != 16000:\n\t\t\tprint(f\"audio_fs: {fs}\")\n\t\t\tresampler = torchaudio.transforms.Resample(fs, 16000)\n\t\t\tinput_wav_t = torch.from_numpy(input_wav).to(torch.float32)\n\t\t\tinput_wav = resampler(input_wav_t[None, :])[0, :].numpy()\n\t\n\t\n\tmerge_vad = True #False if selected_task == \"ASR\" else True\n\tprint(f\"language: {language}, merge_vad: {merge_vad}\")\n\ttext = model.generate(input=input_wav,\n\t\t\t\t\t\t  cache={},\n\t\t\t\t\t\t  language=language,\n\t\t\t\t\t\t  use_itn=True,\n\t\t\t\t\t\t  batch_size_s=60, merge_vad=merge_vad)\n\t\n\tprint(text)\n\ttext = text[0][\"text\"]\n\ttext = format_str_v3(text)\n\t\n\tprint(text)\n\t\n\treturn text\n\n\naudio_examples = [\n    [\"example/zh.mp3\", \"zh\"],\n    [\"example/yue.mp3\", \"yue\"],\n    [\"example/en.mp3\", \"en\"],\n    [\"example/ja.mp3\", \"ja\"],\n    [\"example/ko.mp3\", \"ko\"],\n    [\"example/emo_1.wav\", \"auto\"],\n    [\"example/emo_2.wav\", \"auto\"],\n    [\"example/emo_3.wav\", \"auto\"],\n    #[\"example/emo_4.wav\", \"auto\"],\n    #[\"example/event_1.wav\", \"auto\"],\n    #[\"example/event_2.wav\", \"auto\"],\n    #[\"example/event_3.wav\", \"auto\"],\n    [\"example/rich_1.wav\", \"auto\"],\n    [\"example/rich_2.wav\", \"auto\"],\n    #[\"example/rich_3.wav\", \"auto\"],\n    [\"example/longwav_1.wav\", \"auto\"],\n    [\"example/longwav_2.wav\", \"auto\"],\n    [\"example/longwav_3.wav\", \"auto\"],\n    #[\"example/longwav_4.wav\", \"auto\"],\n]\n\n\n\nhtml_content = \"\"\"\n<div>\n    <h2 style=\"font-size: 22px;margin-left: 0px;\">Voice Understanding Model: SenseVoice-Small</h2>\n    <p style=\"font-size: 18px;margin-left: 20px;\">SenseVoice-Small is an encoder-only speech foundation model designed for rapid voice understanding. It encompasses a variety of features including automatic speech recognition (ASR), spoken language identification (LID), speech emotion recognition (SER), and acoustic event detection (AED). SenseVoice-Small supports multilingual recognition for Chinese, English, Cantonese, Japanese, and Korean. Additionally, it offers exceptionally low inference latency, performing 7 times faster than Whisper-small and 17 times faster than Whisper-large.</p>\n    <h2 style=\"font-size: 22px;margin-left: 0px;\">Usage</h2> <p style=\"font-size: 18px;margin-left: 20px;\">Upload an audio file or input through a microphone, then select the task and language. the audio is transcribed into corresponding text along with associated emotions (😊 happy, 😡 angry/exicting, 😔 sad) and types of sound events (😀 laughter, 🎼 music, 👏 applause, 🤧 cough&sneeze, 😭 cry). The event labels are placed in the front of the text and the emotion are in the back of the text.</p>\n\t<p style=\"font-size: 18px;margin-left: 20px;\">Recommended audio input duration is below 30 seconds. For audio longer than 30 seconds, local deployment is recommended.</p>\n\t<h2 style=\"font-size: 22px;margin-left: 0px;\">Repo</h2>\n\t<p style=\"font-size: 18px;margin-left: 20px;\"><a href=\"https://github.com/FunAudioLLM/SenseVoice\" target=\"_blank\">SenseVoice</a>: multilingual speech understanding model</p>\n\t<p style=\"font-size: 18px;margin-left: 20px;\"><a href=\"https://github.com/modelscope/FunASR\" target=\"_blank\">FunASR</a>: fundamental speech recognition toolkit</p>\n\t<p style=\"font-size: 18px;margin-left: 20px;\"><a href=\"https://github.com/FunAudioLLM/CosyVoice\" target=\"_blank\">CosyVoice</a>: high-quality multilingual TTS model</p>\n</div>\n\"\"\"\n\n\ndef launch():\n\twith gr.Blocks(theme=gr.themes.Soft()) as demo:\n\t\t# gr.Markdown(description)\n\t\tgr.HTML(html_content)\n\t\twith gr.Row():\n\t\t\twith gr.Column():\n\t\t\t\taudio_inputs = gr.Audio(label=\"Upload audio or use the microphone\")\n\t\t\t\t\n\t\t\t\twith gr.Accordion(\"Configuration\"):\n\t\t\t\t\tlanguage_inputs = gr.Dropdown(choices=[\"auto\", \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"],\n\t\t\t\t\t\t\t\t\t\t\t\t  value=\"auto\",\n\t\t\t\t\t\t\t\t\t\t\t\t  label=\"Language\")\n\t\t\t\tfn_button = gr.Button(\"Start\", variant=\"primary\")\n\t\t\t\ttext_outputs = gr.Textbox(label=\"Results\")\n\t\t\tgr.Examples(examples=audio_examples, inputs=[audio_inputs, language_inputs], examples_per_page=20)\n\t\t\n\t\tfn_button.click(model_inference, inputs=[audio_inputs, language_inputs], outputs=text_outputs)\n\n\tdemo.launch()\n\n\nif __name__ == \"__main__\":\n\t# iface.launch()\n\tlaunch()\n\n\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/saas/official_openai/__init__.py\nimport time\nfrom typing import List, Tuple, Dict, Any, Union\nimport httpx\nfrom openai import OpenAI\nimport base64\nimport io\nimport json\nimport ray\nfrom byzerllm.utils.types import (\n    BlockVLLMStreamServer,\n    StreamOutputs,\n    SingleOutput,\n    SingleOutputMeta,\n    BlockBinaryStreamServer,\n)\nfrom byzerllm.utils.langutil import asyncfy_with_semaphore\nimport threading\nimport asyncio\nimport traceback\nimport uuid\nimport tempfile        \n\nclass CustomSaasAPI:\n\n    def __init__(self, infer_params: Dict[str, str]) -> None:\n\n        self.api_key = infer_params[\"saas.api_key\"]\n        self.model = infer_params.get(\"saas.model\", \"gpt-3.5-turbo-1106\")\n\n        other_params = {}\n\n        if \"saas.api_base\" in infer_params:\n            other_params[\"api_base\"] = infer_params[\"saas.api_base\"]\n\n        if \"saas.api_version\" in infer_params:\n            other_params[\"api_version\"] = infer_params[\"saas.api_version\"]\n\n        if \"saas.api_type\" in infer_params:\n            other_params[\"api_type\"] = infer_params[\"saas.api_type\"]\n\n        if \"saas.base_url\" in infer_params:\n            other_params[\"base_url\"] = infer_params[\"saas.base_url\"]\n\n        if \"saas.timeout\" in infer_params:\n            other_params[\"timeout\"] = float(infer_params[\"saas.timeout\"])\n\n        self.max_retries = int(infer_params.get(\"saas.max_retries\", 10))\n\n        self.meta = {\n            \"model_deploy_type\": \"saas\",\n            \"backend\": \"saas\",\n            \"support_stream\": True,\n            \"model_name\": self.model,\n        }\n\n        self.meta[\"embedding_mode\"] = \"embedding\" in self.model.lower()\n\n        self.proxies = infer_params.get(\"saas.proxies\", None)\n        self.local_address = infer_params.get(\"saas.local_address\", \"0.0.0.0\")\n\n        if not self.proxies:\n            self.client = OpenAI(**other_params, api_key=self.api_key)\n        else:\n            self.client = OpenAI(\n                **other_params,\n                api_key=self.api_key,\n                http_client=httpx.Client(\n                    proxies=self.proxies,\n                    transport=httpx.HTTPTransport(local_address=self.local_address),\n                ),\n            )\n\n        try:\n            ray.get_actor(\"BLOCK_VLLM_STREAM_SERVER\")\n        except ValueError:\n            try:\n                ray.remote(BlockVLLMStreamServer).options(\n                    name=\"BLOCK_VLLM_STREAM_SERVER\",\n                    lifetime=\"detached\",\n                    max_concurrency=1000,\n                ).remote()\n            except Exception as e:\n                pass\n        try:\n            ray.get_actor(\"BlockBinaryStreamServer\")\n        except ValueError:\n            try:\n                ray.remote(BlockBinaryStreamServer).options(\n                    name=\"BlockBinaryStreamServer\",\n                    lifetime=\"detached\",\n                    max_concurrency=1000,\n                ).remote()\n            except Exception as e:\n                pass\n\n    # saas/proprietary\n    def get_meta(self):\n        return [self.meta]\n\n    def process_input(self, ins: Union[str, List[Dict[str, Any]], Dict[str, Any]]):\n\n        if isinstance(ins, list) or isinstance(ins, dict):\n            return ins\n\n        content = []\n        try:\n            ins_json = json.loads(ins)\n        except:\n            return ins\n\n        ## speech\n        if isinstance(ins_json, dict):\n            return ins_json\n\n        content = []\n        #     [\n        #     {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n        #     {\n        #       \"type\": \"image_url\",\n        #       \"image_url\": {\n        #         \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n        #         \"detail\": \"high\"\n        #       },\n        #     },\n        #   ],\n        for item in ins_json:\n            # for format like this: {\"image\": \"xxxxxx\", \"text\": \"What’s in this image?\",\"detail\":\"high\"}\n            # or {\"image\": \"xxxxxx\"}, {\"text\": \"What’s in this image?\"}\n            if (\"image\" in item or \"image_url\" in item) and \"type\" not in item:\n                image_data = item.get(\"image\", item.get(\"image_url\", \"\"))\n                ## \"data:image/jpeg;base64,\"\n                if not image_data.startswith(\"data:\"):\n                    image_data = \"data:image/jpeg;base64,\" + image_data\n\n                ## get the other fields except image/text/image_url\n                other_fields = {\n                    k: v\n                    for k, v in item.items()\n                    if k not in [\"image\", \"text\", \"image_url\"]\n                }\n                content.append(\n                    {\n                        \"image_url\": {\"url\": image_data, **other_fields},\n                        \"type\": \"image_url\",\n                    }\n                )            \n\n            if \"text\" in item and \"type\" not in item:\n                text_data = item[\"text\"]\n                content.append({\"text\": text_data, \"type\": \"text\"})\n\n            ## for format like this: {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n            ## {\"type\": \"image_url\", \"image_url\": {\"url\":\"\",\"detail\":\"high\"}}\n            ## this is the standard format, just return it\n            if \"type\" in item and item[\"type\"] == \"text\":\n                content.append(item)\n\n            if \"type\" in item and item[\"type\"] == \"image_url\":\n                content.append(item)\n\n        if not content:\n            return ins\n\n        return content\n\n    async def async_embed_query(self, ins: str, **kwargs):\n        resp = await asyncfy_with_semaphore(\n            lambda: self.client.embeddings.create(input=[ins], model=self.model)\n        )()\n        embedding = resp.data[0].embedding\n        usage = resp.usage\n        return (\n            embedding,\n            {\n                \"metadata\": {\n                    \"input_tokens_count\": usage.prompt_tokens,\n                    \"generated_tokens_count\": 0,\n                }\n            },\n        )\n\n    def embed_query(self, ins: str, **kwargs):\n        resp = self.client.embeddings.create(input=[ins], model=self.model)\n        embedding = resp.data[0].embedding\n        usage = resp.usage\n        return (\n            embedding,\n            {\n                \"metadata\": {\n                    \"input_tokens_count\": usage.prompt_tokens,\n                    \"generated_tokens_count\": 0,\n                }\n            },\n        )\n\n    async def async_text_to_speech(\n        self, stream: bool, ins: str, voice: str, chunk_size: int = None, **kwargs\n    ):\n        if stream:\n            server = ray.get_actor(\"BlockBinaryStreamServer\")\n            request_id = [None]\n\n            def writer():\n                try:\n                    request_id[0] = str(uuid.uuid4())\n                    with self.client.with_streaming_response.audio.speech.create(\n                        model=self.model, voice=voice, input=ins, **kwargs\n                    ) as response:\n                        for chunk in response.iter_bytes(chunk_size):\n                            input_tokens_count = 0\n                            generated_tokens_count = 0\n                            ray.get(\n                                server.add_item.remote(\n                                    request_id[0],\n                                    StreamOutputs(\n                                        outputs=[\n                                            SingleOutput(\n                                                text=chunk,\n                                                metadata=SingleOutputMeta(\n                                                    input_tokens_count=input_tokens_count,\n                                                    generated_tokens_count=generated_tokens_count,\n                                                ),\n                                            )\n                                        ]\n                                    ),\n                                )\n                            )\n                except:\n                    traceback.print_exc()\n                ray.get(server.mark_done.remote(request_id[0]))\n\n            threading.Thread(target=writer, daemon=True).start()\n\n            time_count = 10 * 100\n            while request_id[0] is None and time_count > 0:\n                time.sleep(0.01)\n                time_count -= 1\n\n            if request_id[0] is None:\n                raise Exception(\"Failed to get request id\")\n\n            def write_running():\n                return ray.get(server.add_item.remote(request_id[0], \"RUNNING\"))\n\n            await asyncio.to_thread(write_running)\n            return [\n                (\n                    \"\",\n                    {\n                        \"metadata\": {\n                            \"request_id\": request_id[0],\n                            \"stream_server\": \"BlockBinaryStreamServer\",\n                        }\n                    },\n                )\n            ]\n\n        start_time = time.monotonic()\n        with io.BytesIO() as output:\n            async with asyncfy_with_semaphore(\n                lambda: self.client.with_streaming_response.audio.speech.create(\n                    model=self.model, voice=voice, input=ins, **kwargs\n                )\n            )() as response:\n                for chunk in response.iter_bytes():\n                    output.write(chunk)\n\n            base64_audio = base64.b64encode(output.getvalue()).decode()\n            time_cost = time.monotonic() - start_time\n            return [\n                (\n                    base64_audio,\n                    {\n                        \"metadata\": {\n                            \"request_id\": \"\",\n                            \"input_tokens_count\": 0,\n                            \"generated_tokens_count\": 0,\n                            \"time_cost\": time_cost,\n                            \"first_token_time\": 0,\n                            \"speed\": 0,\n                        }\n                    },\n                )\n            ]\n\n    async def async_speech_to_text(\n        self,\n        audio: str,\n        response_format: str = \"verbose_json\",\n        timestamp_granularities: List[str] = [\"word\",\"segment\"],\n    ):               \n        # Extract audio format and base64 data\n        data_prefix = \"data:audio/\"\n        base64_prefix = \";base64,\"\n        if not audio.startswith(data_prefix):\n            raise ValueError(\"Invalid audio data format\")\n        \n        format_end = audio.index(base64_prefix)\n        audio_format = audio[len(data_prefix):format_end]\n        base64_data = audio[format_end + len(base64_prefix):]\n        \n        # Decode the base64 audio data\n        audio_data = base64.b64decode(base64_data)\n\n        # Create a temporary file with the correct extension\n        with tempfile.NamedTemporaryFile(delete=False, suffix=f'.{audio_format}') as temp_audio_file:\n            temp_audio_file.write(audio_data)\n            temp_audio_file_path = temp_audio_file.name\n\n        try:\n            start_time = time.monotonic()\n                   \n            with open(temp_audio_file_path, \"rb\") as audio_file:                \n                transcription = self.client.audio.transcriptions.create(\n                    model=self.model or \"whisper-1\",\n                    file=audio_file,\n                    response_format=response_format,\n                    timestamp_granularities=timestamp_granularities,\n                )\n            time_cost = time.monotonic() - start_time\n\n            return [\n                (\n                    json.dumps(transcription.to_dict(), ensure_ascii=True),\n                    {\n                        \"metadata\": {\n                            \"request_id\": \"\",\n                            \"input_tokens_count\": 0,\n                            \"generated_tokens_count\": 0,\n                            \"time_cost\": time_cost,\n                            \"first_token_time\": 0,\n                            \"speed\": 0,\n                        }\n                    },\n                )\n            ]\n        finally:\n            # Clean up the temporary file\n            import os\n            os.unlink(temp_audio_file_path)\n\n    def image_to_text(self, ins: str, **kwargs):\n        pass\n\n    async def async_text_to_image(\n        self, stream: bool, input: str, size: str, quality: str, n: int, **kwargs\n    ):\n        if stream:\n            raise Exception(\"Stream not supported for text to image\")\n        start_time = time.monotonic()\n        response = await asyncfy_with_semaphore(\n            lambda: self.client.images.generate(\n                model=self.model,\n                prompt=input,\n                size=size,\n                quality=quality,\n                n=1,\n                response_format=\"b64_json\",\n                **kwargs,\n            )\n        )()\n        time_cost = time.monotonic() - start_time\n        base64_image = response.data[0].b64_json\n        return [\n            (\n                base64_image,\n                {\n                    \"metadata\": {\n                        \"request_id\": \"\",\n                        \"input_tokens_count\": 0,\n                        \"generated_tokens_count\": 0,\n                        \"time_cost\": time_cost,\n                        \"first_token_time\": 0,\n                        \"speed\": 0,\n                    }\n                },\n            )\n        ]\n\n    def text_to_text(self, ins: str, **kwargs):\n        pass\n\n    async def async_stream_chat(\n        self,\n        tokenizer,\n        ins: str,\n        his: List[Dict[str, Any]] = [],\n        max_length: int = 4096,\n        top_p: float = 0.7,\n        temperature: float = 0.9,\n        **kwargs,\n    ):\n\n        model = self.model\n\n        if \"model\" in kwargs:\n            model = kwargs[\"model\"]\n\n        messages = [\n            {\"role\": message[\"role\"], \"content\": self.process_input(message[\"content\"])}\n            for message in his\n        ] + [{\"role\": \"user\", \"content\": self.process_input(ins)}]\n\n        stream = kwargs.get(\"stream\", False)\n\n        ## content = [\n        ##    \"voice\": \"alloy\",\"input\": \"Hello, World!\",response_format: \"mp3\"]\n        last_message = messages[-1][\"content\"]\n\n        if isinstance(last_message, dict) and \"voice\" in last_message:\n            voice = last_message[\"voice\"]\n            response_format = last_message.get(\"response_format\", \"mp3\")\n            chunk_size = last_message.get(\"chunk_size\", None)\n            input = last_message[\"input\"]\n            return await self.async_text_to_speech(\n                stream=stream,\n                ins=input,\n                voice=voice,\n                chunk_size=chunk_size,\n                response_format=response_format,\n            )\n\n        if isinstance(last_message, dict) and \"input\" in last_message:\n            input = last_message[\"input\"]\n            size = last_message.get(\"size\", \"1024x1024\")\n            quality = last_message.get(\"quality\", \"standard\")\n            n = last_message.get(\"n\", 1)\n            return await self.async_text_to_image(\n                stream=stream, input=input, size=size, quality=quality, n=n\n            )\n\n        if isinstance(last_message, dict) and \"audio\" in last_message:            \n            audio_data = last_message.get(\"audio\", \"\")            \n            tpe = last_message.get(\"type\",\"wav\")\n            if not audio_data.startswith(\"data:\"):\n                audio_data = f\"data:audio/${tpe};base64,\" + audio_data                \n            return await self.async_speech_to_text(audio=audio_data)\n        \n        server = ray.get_actor(\"BLOCK_VLLM_STREAM_SERVER\")\n        request_id = [None]\n\n        def writer():\n            try:\n                r = \"\"\n                response = self.client.chat.completions.create(\n                    messages=messages,\n                    model=model,\n                    stream=True,\n                    max_tokens=max_length,\n                    temperature=temperature,\n                    top_p=top_p,\n                )\n                # input_tokens_count = 0\n                # generated_tokens_count = 0\n\n                request_id[0] = str(uuid.uuid4())\n\n                for chunk in response:\n                    content = chunk.choices[0].delta.content or \"\"\n                    r += content\n                    if hasattr(chunk, \"usage\") and chunk.usage:\n                        input_tokens_count = chunk.usage.prompt_tokens\n                        generated_tokens_count = chunk.usage.completion_tokens\n                    else:\n                        input_tokens_count = 0\n                        generated_tokens_count = 0\n                    ray.get(\n                        server.add_item.remote(\n                            request_id[0],\n                            StreamOutputs(\n                                outputs=[\n                                    SingleOutput(\n                                        text=r,\n                                        metadata=SingleOutputMeta(\n                                            input_tokens_count=input_tokens_count,\n                                            generated_tokens_count=generated_tokens_count,\n                                        ),\n                                    )\n                                ]\n                            ),\n                        )\n                    )\n            except:\n                traceback.print_exc()\n            ray.get(server.mark_done.remote(request_id[0]))\n\n        if stream:\n            threading.Thread(target=writer, daemon=True).start()\n\n            time_count = 60 * 100\n            while request_id[0] is None and time_count > 0:\n                time.sleep(0.01)\n                time_count -= 1\n\n            if request_id[0] is None:\n                raise Exception(\"Failed to get request id\")\n\n            def write_running():\n                return ray.get(server.add_item.remote(request_id[0], \"RUNNING\"))\n\n            await asyncio.to_thread(write_running)\n            return [\n                (\n                    \"\",\n                    {\n                        \"metadata\": {\n                            \"request_id\": request_id[0],\n                            \"stream_server\": \"BLOCK_VLLM_STREAM_SERVER\",\n                        }\n                    },\n                )\n            ]\n        else:\n            try:\n                start_time = time.monotonic()\n                response = await asyncfy_with_semaphore(\n                    lambda: self.client.chat.completions.create(\n                        messages=messages,\n                        model=model,\n                        max_tokens=max_length,\n                        temperature=temperature,\n                        top_p=top_p,\n                    )\n                )()\n\n                generated_text = response.choices[0].message.content\n                generated_tokens_count = response.usage.completion_tokens\n                input_tokens_count = response.usage.prompt_tokens\n                time_cost = time.monotonic() - start_time\n                return [\n                    (\n                        generated_text,\n                        {\n                            \"metadata\": {\n                                \"request_id\": response.id,\n                                \"input_tokens_count\": input_tokens_count,\n                                \"generated_tokens_count\": generated_tokens_count,\n                                \"time_cost\": time_cost,\n                                \"first_token_time\": 0,\n                                \"speed\": float(generated_tokens_count) / time_cost,\n                            }\n                        },\n                    )\n                ]\n            except Exception as e:\n                print(f\"Error: {e}\")\n                raise e\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/whisper/__init__.py\nimport whisper\nfrom typing import Dict, List, Tuple\nfrom byzerllm.utils.types import StopSequencesCriteria\n\nfrom typing import Dict, Any, List, Union\nfrom pyjava.storage import streaming_tar as STar\nimport json\nimport base64\nimport tempfile\nimport time\n\n\ndef get_meta(self):\n    return [\n        {\n            \"model_deploy_type\": \"proprietary\",\n            \"backend\": \"transformers\",\n            \"message_format\": True\n        }\n    ]\n\n\ndef process_input(ins: Union[str, List[Dict[str, Any]], Dict[str, Any]]):\n    # print(ins) \n    if isinstance(ins, list) or isinstance(ins, dict):\n        return ins\n\n    content = []\n    try:\n        ins_json = json.loads(ins)\n    except:\n        return ins\n\n    ## speech\n    if isinstance(ins_json, dict):\n        return ins_json\n\n    content = []\n    #     [\n    #     {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n    #     {\n    #       \"type\": \"image_url\",\n    #       \"image_url\": {\n    #         \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n    #         \"detail\": \"high\"\n    #       },\n    #     },\n    #   ],\n    for item in ins_json:\n        # for format like this: {\"image\": \"xxxxxx\", \"text\": \"What’s in this image?\",\"detail\":\"high\"}\n        # or {\"image\": \"xxxxxx\"}, {\"text\": \"What’s in this image?\"}\n        if (\"image\" in item or \"image_url\" in item) and \"type\" not in item:\n            image_data = item.get(\"image\", item.get(\"image_url\", \"\"))\n            ## \"data:image/jpeg;base64,\"\n            if not image_data.startswith(\"data:\"):\n                image_data = \"data:image/jpeg;base64,\" + image_data\n\n            ## get the other fields except image/text/image_url\n            other_fields = {\n                k: v for k, v in item.items() if k not in [\"image\", \"text\", \"image_url\"]\n            }\n            content.append(\n                {\n                    \"image_url\": {\"url\": image_data, **other_fields},\n                    \"type\": \"image_url\",\n                }\n            )\n\n        if \"text\" in item and \"type\" not in item:\n            text_data = item[\"text\"]\n            content.append({\"text\": text_data, \"type\": \"text\"})\n\n        ## for format like this: {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n        ## {\"type\": \"image_url\", \"image_url\": {\"url\":\"\",\"detail\":\"high\"}}\n        ## this is the standard format, just return it\n        if \"type\" in item and item[\"type\"] == \"text\":\n            content.append(item)\n\n        if \"type\" in item and item[\"type\"] == \"image_url\":\n            content.append(item)\n\n    if not content:\n        return ins\n\n    return content\n\n\ndef stream_chat(\n    self,\n    tokenizer,\n    ins: str,\n    his: List[Dict[str, str]] = [],\n    max_length: int = 4090,\n    top_p: float = 0.95,\n    temperature: float = 0.1,\n    **kwargs,\n):\n    messages = [\n        {\"role\": message[\"role\"], \"content\": process_input(message[\"content\"])}\n        for message in his\n    ] + [{\"role\": \"user\", \"content\": process_input(ins)}]\n\n    audio_input = messages[-1][\"content\"]\n\n    audio_data = audio_input.get(\"audio\", \"\")\n    audio = audio_input[\"audio\"]\n    response_format = audio_input.get(\"response_format\", \"json\")\n    timestamp_granularities = audio_input.get(\n        \"timestamp_granularities\", [\"word\", \"segment\"]\n    )\n    data_prefix = \"data:audio/\"\n    base64_prefix = \";base64,\"\n    if not audio.startswith(data_prefix):\n        raise ValueError(\"Invalid audio data format\")\n\n    format_end = audio.index(base64_prefix)\n    audio_format = audio[len(data_prefix) : format_end]\n    base64_data = audio[format_end + len(base64_prefix) :]\n\n    # Decode the base64 audio data\n    audio_data = base64.b64decode(base64_data)\n\n    # Create a temporary file with the correct extension\n    with tempfile.NamedTemporaryFile(\n        delete=False, suffix=f\".{audio_format}\"\n    ) as temp_audio_file:\n        temp_audio_file.write(audio_data)\n        temp_audio_file_path = temp_audio_file.name\n\n    try:\n        start_time = time.monotonic()\n        word_timestamps = \"word\" in timestamp_granularities\n        \n        result = self.transcribe(temp_audio_file_path, word_timestamps=word_timestamps)        \n            \n        time_cost = time.monotonic() - start_time\n        return [\n            (\n                json.dumps(result, ensure_ascii=True),\n                {\n                    \"metadata\": {\n                        \"request_id\": \"\",\n                        \"input_tokens_count\": 0,\n                        \"generated_tokens_count\": 0,\n                        \"time_cost\": time_cost,\n                        \"first_token_time\": 0,\n                        \"speed\": 0,\n                    }\n                },\n            )\n        ]\n    finally:\n        # Clean up the temporary file\n        import os\n\n        os.unlink(temp_audio_file_path)\n\n\ndef init_model(\n    model_dir, infer_params: Dict[str, str] = {}, sys_conf: Dict[str, str] = {}\n):\n    model = whisper.load_model(\"large\", download_root=model_dir)\n    import types\n\n    model.stream_chat = types.MethodType(stream_chat, model)\n    model.get_meta = types.MethodType(get_meta, model)\n    return (model, None)\n\n\n     \n\n下面是用户的需求：\n\nsensevoice 中 infer_params 获取vad_model, 如果为空,则不传递 vad_model相关参数.\n"}, {"role": "assistant", "content": "好的，我理解您的需求。我们需要修改 `init_model` 函数，以便在初始化 SenseVoice 模型时考虑 `vad_model` 参数。如果 `infer_params` 中提供了 `vad_model`，我们就使用它；如果没有提供，我们就不传递相关参数。让我们修改代码来实现这一点。\n\n这里是修改后的 `init_model` 函数：\n\n```python\n##File: /Users/allwefantasy/projects/SenseVoice/webui.py\n<<<<<<< SEARCH\nfrom funasr import AutoModel\n\nmodel = \"iic/SenseVoiceSmall\"\nmodel = AutoModel(model=model,\n\t\t\t\t  vad_model=\"iic/speech_fsmn_vad_zh-cn-16k-common-pytorch\",\n\t\t\t\t  vad_kwargs={\"max_single_segment_time\": 30000},\n\t\t\t\t  trust_remote_code=True,\n\t\t\t\t  )\n=======\nfrom funasr import AutoModel\n\nmodel = \"iic/SenseVoiceSmall\"\nmodel_kwargs = {\n    \"model\": model,\n    \"trust_remote_code\": True,\n}\n\n# Check if vad_model is provided in infer_params\nif \"vad_model\" in infer_params:\n    model_kwargs[\"vad_model\"] = infer_params[\"vad_model\"]\n    model_kwargs[\"vad_kwargs\"] = {\"max_single_segment_time\": 30000}\n\nmodel = AutoModel(**model_kwargs)\n>>>>>>> REPLACE\n```\n\n这个修改做了以下几点改变：\n\n1. 我们创建了一个 `model_kwargs` 字典，其中包含了基本的模型参数。\n\n2. 我们检查 `infer_params` 中是否包含 `vad_model`。\n\n3. 如果 `vad_model` 存在，我们将其添加到 `model_kwargs` 中，同时也添加 `vad_kwargs`。\n\n4. 最后，我们使用 `**model_kwargs` 来初始化 AutoModel，这样就可以根据是否提供 `vad_model` 来决定是否传递相关参数。\n\n这样修改后，如果 `infer_params` 中没有提供 `vad_model`，就不会传递 `vad_model` 和 `vad_kwargs` 参数给 AutoModel。如果提供了 `vad_model`，则会像原来一样使用这些参数。\n\n请注意，这个修改假设 `infer_params` 是在 `init_model` 函数中可用的。如果不是这样，您可能需要调整代码结构，确保 `infer_params` 在需要的地方可用。"}], "model": "human", "yaml_file": "actions/018_chat_action.yml"}
{"instruction": "@/Users/allwefantasy/projects/byzer-llm/src/byzerllm/sensevoice/__init__.py 中的 中从 vad_model 需要从 infer_params 获取,如果没有,则不传递给 AutoModel", "conversations": [{"role": "user", "content": "\n如果你需要生成代码，对于每个需要更改的文件,你需要按 *SEARCH/REPLACE block* 的格式进行生成。\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The opening fence and code language, eg: ```python\n2. The file path alone on a line, starting with \"##File:\" and verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n*SEARCH/REPLACE* blocks will replace *all* matching occurrences.\nInclude enough lines to make the SEARCH blocks unique.\n\nInclude *ALL* the code being searched and replaced!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n下面我们来看一个例子：\n\n当前项目目录结构：\n1. 项目根目录： /tmp/projects/mathweb\n2. 项目子目录/文件列表(类似tree 命令输出)\nflask/\n    app.py\n    templates/\n        index.html\n    static/\n        style.css\n\n用户需求： Change get_factorial() to use math.factorial\n\n回答： To make this change we need to modify `/tmp/projects/mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n用户需求： Refactor hello() into its own file.\n\n回答：To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/hello.py\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/main.py\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n现在让我们开始一个新的任务:\n\n当前项目目录结构：\n1. 项目根目录： /Users/allwefantasy/projects/byzer-llm\n2. 项目子目录/文件列表(类似tree 命令输出)：\nsetup.py\ntests/\n    test_emb_rerank.py\n    test_deepseek_code.py\n    test_prompt.py\n    test_model_meta.py\n    __init__.py\n    test_transformers_deploy.py\n    test_data_analysis.py\n    test_yi.py\n    test_deploy.py\n    functions_suite.py\n    test_template.py\n    auto/\n        test_backend_llama_cpp.py\n        __init__.py\nsetup-machine/\n    convert_safetensor.py\nnotebooks/\n    test.py\nsrc/\n    byzerllm/\n        lang.py\n        store.py\n        version.py\n        log.py\n        __init__.py\n        types.py\n        easy_byzerllm_command.py\n        byzerllm_command.py\n        command_args.py\n        bark/\n            generation.py\n            __init__.py\n            model.py\n            api.py\n            model_fine.py\n            bark_voice.py\n        moss/\n            finetune_moss.py\n            __init__.py\n            moss_inference.py\n            models/\n                configuration_moss.py\n                quantization.py\n                custom_autotune.py\n                __init__.py\n                modeling_moss.py\n                tokenization_moss.py\n        llama2/\n            __init__.py\n        records/\n            __init__.py\n        visualglm/\n            __init__.py\n        auto/\n            __init__.py\n            backend_llama_cpp.py\n            backend_vllm.py\n            backend_ds.py\n        alpha_moss/\n            __init__.py\n        processor/\n            __init__.py\n        sensevoice/\n            __init__.py\n        qwen/\n            __init__.py\n        utils/\n            json_repaire.py\n            object_store_ref_util.py\n            __init__.py\n            types.py\n            tokenizer.py\n            openai_utils.py\n            ray_utils.py\n            text_generator.py\n            emb.py\n            connect_ray.py\n            testing.py\n            langutil.py\n            nontext.py\n            fulltune/\n                deepspeed_trainner.py\n                __init__.py\n                launch.py\n                trainner.py\n                base_model/\n                    __init__.py\n                    configuration_baichuan.py\n                    modeling_baichuan.py\n                pretrain/\n                    __init__.py\n                    convert_to_transformers.py\n            metrics/\n                __init__.py\n            config/\n                __init__.py\n            retrieval/\n                rest.py\n                __init__.py\n                udf.py\n            sft/\n                argument.py\n                collator.py\n                qlora.py\n                __init__.py\n                model.py\n                dataset.py\n                loss.py\n                merge_lora.py\n                trainer.py\n            client/\n                message_utils.py\n                math_utils.py\n                byzerllm_client.py\n                parallel_utils.py\n                __init__.py\n                types.py\n                code_utils.py\n                img_utils.py\n                entrypoints/\n                    __init__.py\n                    openai/\n                        protocol.py\n                        __init__.py\n                        serve.py\n                        serving_chat.py\n                        api_server.py\n                        serving_engine.py\n                        serving_completion.py\n                        tool.py\n        chatglm2/\n            __init__.py\n        baichuan/\n            __init__.py\n        m3e/\n            __init__.py\n        stable_diffusion/\n            config.py\n            __init__.py\n            logger.py\n            shared.py\n            model.py\n            utils.py\n            images.py\n            diffusion/\n                __init__.py\n                embeddings.py\n                utils.py\n                networks/\n                    lyco.py\n                    __init__.py\n                upscalers/\n                    __init__.py\n                    multidiffusion.py\n                    samplers.py\n                piplines/\n                    __init__.py\n                    lpw.py\n                    diffusers.py\n            lib/\n                __init__.py\n                diffusers/\n                    __init__.py\n                    scheduler.py\n            api/\n                __init__.py\n                plugin.py\n                models/\n                    diffusion.py\n                    __init__.py\n                diffusion/\n                    __init__.py\n                    pipelines/\n                        __init__.py\n                        diffusers.py\n                events/\n                    generation.py\n                    __init__.py\n        bge/\n            __init__.py\n        bge_rerank/\n            __init__.py\n        apps/\n            client.py\n            qa_strategy.py\n            __init__.py\n            builder.py\n            vector_db.py\n            utils.py\n            qa.py\n            byzer_storage/\n                memory_model_based.py\n                command.py\n                env.py\n                train_pt.py\n                train_sft.py\n                __init__.py\n                train_dpo.py\n                simple_api.py\n                generate_sft_data.py\n            byzer_sql/\n                __init__.py\n            llama_index/\n                byzerai_vectordb.py\n                byzerai.py\n                byzerai_embedding.py\n                __init__.py\n                collection_manager.py\n                byzerai_index_store.py\n                byzerai_docstore.py\n                simple_retrieval.py\n                byzerai_kvstore.py\n            agent/\n                conversable_agent.py\n                registry.py\n                groupchat.py\n                user_proxy_agent.py\n                __init__.py\n                agent.py\n                extensions/\n                    simple_retrieval_client.py\n                    planner.py\n                    python_codesandbox_agent.py\n                    preview_file_agent.py\n                    llama_index_retrieval_agent.py\n                    visualization_agent.py\n                    assistant_agent.py\n                    load_data_agent.py\n                    retrieval_agent.py\n                    output_agent.py\n                    __init__.py\n                    byzer_engine_agent.py\n                    subquestion_agent.py\n                    sql_reviewer_agent.py\n                    spark_sql_agent.py\n                    rhetorical_agent.py\n                    data_analysis.py\n                    data_analysis_pipeline_agent.py\n                    common_agent.py\n                    query_rewrite/\n                        time.py\n                        rhetorical.py\n                        __init__.py\n                        condition.py\n                        context.py\n                store/\n                    memory_store.py\n                    __init__.py\n                    stores.py\n        saas/\n            __init__.py\n            chatglm/\n                __init__.py\n            sparkdesk/\n                __init__.py\n            gemini/\n                __init__.py\n            azure_openai/\n                __init__.py\n            azure/\n                __init__.py\n            qianfan/\n                __init__.py\n            claude/\n                __init__.py\n            qianwen/\n                __init__.py\n            volcengine/\n                __init__.py\n            minimax/\n                __init__.py\n            baichuan/\n                __init__.py\n            qianwen_vl/\n                __init__.py\n            official_openai/\n                __init__.py\n            zhipu/\n                __init__.py\n            aws_bedrock/\n                __init__.py\n            openai/\n                __init__.py\n        falcon/\n            __init__.py\n        chatglm6b/\n            trainer_seq2seq.py\n            arguments.py\n            __init__.py\n            finetune.py\n            tunning/\n                __init__.py\n                infer.py\n                finetune.py\n                utils/\n                    config.py\n                    __init__.py\n                    other.py\n                    common.py\n                    seq2seq.py\n        starcode/\n            __init__.py\n        qwen_vl_chat/\n            __init__.py\n        zephyr/\n            __init__.py\n        dolly/\n            generate.py\n            __init__.py\n            consts.py\n            dolly_inference.py\n            trainer.py\n        llama/\n            __init__.py\n        whisper/\n            whisper_inference.py\n            __init__.py\n下面是一些文件路径以及每个文件对应的源码：\n\n##File: /Users/allwefantasy/projects/SenseVoice/webui.py\n# coding=utf-8\n\nimport os\nimport librosa\nimport base64\nimport io\nimport gradio as gr\nimport re\n\nimport numpy as np\nimport torch\nimport torchaudio\n\n\nfrom funasr import AutoModel\n\nmodel = \"iic/SenseVoiceSmall\"\nmodel_kwargs = {\n    \"model\": model,\n    \"trust_remote_code\": True,\n}\n\n# Check if vad_model is provided in infer_params\nif \"vad_model\" in infer_params:\n    model_kwargs[\"vad_model\"] = infer_params[\"vad_model\"]\n    model_kwargs[\"vad_kwargs\"] = {\"max_single_segment_time\": 30000}\n\nmodel = AutoModel(**model_kwargs)\n\nimport re\n\nemo_dict = {\n\t\"<|HAPPY|>\": \"😊\",\n\t\"<|SAD|>\": \"😔\",\n\t\"<|ANGRY|>\": \"😡\",\n\t\"<|NEUTRAL|>\": \"\",\n\t\"<|FEARFUL|>\": \"😰\",\n\t\"<|DISGUSTED|>\": \"🤢\",\n\t\"<|SURPRISED|>\": \"😮\",\n}\n\nevent_dict = {\n\t\"<|BGM|>\": \"🎼\",\n\t\"<|Speech|>\": \"\",\n\t\"<|Applause|>\": \"👏\",\n\t\"<|Laughter|>\": \"😀\",\n\t\"<|Cry|>\": \"😭\",\n\t\"<|Sneeze|>\": \"🤧\",\n\t\"<|Breath|>\": \"\",\n\t\"<|Cough|>\": \"🤧\",\n}\n\nemoji_dict = {\n\t\"<|nospeech|><|Event_UNK|>\": \"❓\",\n\t\"<|zh|>\": \"\",\n\t\"<|en|>\": \"\",\n\t\"<|yue|>\": \"\",\n\t\"<|ja|>\": \"\",\n\t\"<|ko|>\": \"\",\n\t\"<|nospeech|>\": \"\",\n\t\"<|HAPPY|>\": \"😊\",\n\t\"<|SAD|>\": \"😔\",\n\t\"<|ANGRY|>\": \"😡\",\n\t\"<|NEUTRAL|>\": \"\",\n\t\"<|BGM|>\": \"🎼\",\n\t\"<|Speech|>\": \"\",\n\t\"<|Applause|>\": \"👏\",\n\t\"<|Laughter|>\": \"😀\",\n\t\"<|FEARFUL|>\": \"😰\",\n\t\"<|DISGUSTED|>\": \"🤢\",\n\t\"<|SURPRISED|>\": \"😮\",\n\t\"<|Cry|>\": \"😭\",\n\t\"<|EMO_UNKNOWN|>\": \"\",\n\t\"<|Sneeze|>\": \"🤧\",\n\t\"<|Breath|>\": \"\",\n\t\"<|Cough|>\": \"😷\",\n\t\"<|Sing|>\": \"\",\n\t\"<|Speech_Noise|>\": \"\",\n\t\"<|withitn|>\": \"\",\n\t\"<|woitn|>\": \"\",\n\t\"<|GBG|>\": \"\",\n\t\"<|Event_UNK|>\": \"\",\n}\n\nlang_dict =  {\n    \"<|zh|>\": \"<|lang|>\",\n    \"<|en|>\": \"<|lang|>\",\n    \"<|yue|>\": \"<|lang|>\",\n    \"<|ja|>\": \"<|lang|>\",\n    \"<|ko|>\": \"<|lang|>\",\n    \"<|nospeech|>\": \"<|lang|>\",\n}\n\nemo_set = {\"😊\", \"😔\", \"😡\", \"😰\", \"🤢\", \"😮\"}\nevent_set = {\"🎼\", \"👏\", \"😀\", \"😭\", \"🤧\", \"😷\",}\n\ndef format_str(s):\n\tfor sptk in emoji_dict:\n\t\ts = s.replace(sptk, emoji_dict[sptk])\n\treturn s\n\n\ndef format_str_v2(s):\n\tsptk_dict = {}\n\tfor sptk in emoji_dict:\n\t\tsptk_dict[sptk] = s.count(sptk)\n\t\ts = s.replace(sptk, \"\")\n\temo = \"<|NEUTRAL|>\"\n\tfor e in emo_dict:\n\t\tif sptk_dict[e] > sptk_dict[emo]:\n\t\t\temo = e\n\tfor e in event_dict:\n\t\tif sptk_dict[e] > 0:\n\t\t\ts = event_dict[e] + s\n\ts = s + emo_dict[emo]\n\n\tfor emoji in emo_set.union(event_set):\n\t\ts = s.replace(\" \" + emoji, emoji)\n\t\ts = s.replace(emoji + \" \", emoji)\n\treturn s.strip()\n\ndef format_str_v3(s):\n\tdef get_emo(s):\n\t\treturn s[-1] if s[-1] in emo_set else None\n\tdef get_event(s):\n\t\treturn s[0] if s[0] in event_set else None\n\n\ts = s.replace(\"<|nospeech|><|Event_UNK|>\", \"❓\")\n\tfor lang in lang_dict:\n\t\ts = s.replace(lang, \"<|lang|>\")\n\ts_list = [format_str_v2(s_i).strip(\" \") for s_i in s.split(\"<|lang|>\")]\n\tnew_s = \" \" + s_list[0]\n\tcur_ent_event = get_event(new_s)\n\tfor i in range(1, len(s_list)):\n\t\tif len(s_list[i]) == 0:\n\t\t\tcontinue\n\t\tif get_event(s_list[i]) == cur_ent_event and get_event(s_list[i]) != None:\n\t\t\ts_list[i] = s_list[i][1:]\n\t\t#else:\n\t\tcur_ent_event = get_event(s_list[i])\n\t\tif get_emo(s_list[i]) != None and get_emo(s_list[i]) == get_emo(new_s):\n\t\t\tnew_s = new_s[:-1]\n\t\tnew_s += s_list[i].strip().lstrip()\n\tnew_s = new_s.replace(\"The.\", \" \")\n\treturn new_s.strip()\n\ndef model_inference(input_wav, language, fs=16000):\n\t# task_abbr = {\"Speech Recognition\": \"ASR\", \"Rich Text Transcription\": (\"ASR\", \"AED\", \"SER\")}\n\tlanguage_abbr = {\"auto\": \"auto\", \"zh\": \"zh\", \"en\": \"en\", \"yue\": \"yue\", \"ja\": \"ja\", \"ko\": \"ko\",\n\t\t\t\t\t \"nospeech\": \"nospeech\"}\n\t\n\t# task = \"Speech Recognition\" if task is None else task\n\tlanguage = \"auto\" if len(language) < 1 else language\n\tselected_language = language_abbr[language]\n\t# selected_task = task_abbr.get(task)\n\t\n\t# print(f\"input_wav: {type(input_wav)}, {input_wav[1].shape}, {input_wav}\")\n\t\n\tif isinstance(input_wav, tuple):\n\t\tfs, input_wav = input_wav\n\t\tinput_wav = input_wav.astype(np.float32) / np.iinfo(np.int16).max\n\t\tif len(input_wav.shape) > 1:\n\t\t\tinput_wav = input_wav.mean(-1)\n\t\tif fs != 16000:\n\t\t\tprint(f\"audio_fs: {fs}\")\n\t\t\tresampler = torchaudio.transforms.Resample(fs, 16000)\n\t\t\tinput_wav_t = torch.from_numpy(input_wav).to(torch.float32)\n\t\t\tinput_wav = resampler(input_wav_t[None, :])[0, :].numpy()\n\t\n\t\n\tmerge_vad = True #False if selected_task == \"ASR\" else True\n\tprint(f\"language: {language}, merge_vad: {merge_vad}\")\n\ttext = model.generate(input=input_wav,\n\t\t\t\t\t\t  cache={},\n\t\t\t\t\t\t  language=language,\n\t\t\t\t\t\t  use_itn=True,\n\t\t\t\t\t\t  batch_size_s=60, merge_vad=merge_vad)\n\t\n\tprint(text)\n\ttext = text[0][\"text\"]\n\ttext = format_str_v3(text)\n\t\n\tprint(text)\n\t\n\treturn text\n\n\naudio_examples = [\n    [\"example/zh.mp3\", \"zh\"],\n    [\"example/yue.mp3\", \"yue\"],\n    [\"example/en.mp3\", \"en\"],\n    [\"example/ja.mp3\", \"ja\"],\n    [\"example/ko.mp3\", \"ko\"],\n    [\"example/emo_1.wav\", \"auto\"],\n    [\"example/emo_2.wav\", \"auto\"],\n    [\"example/emo_3.wav\", \"auto\"],\n    #[\"example/emo_4.wav\", \"auto\"],\n    #[\"example/event_1.wav\", \"auto\"],\n    #[\"example/event_2.wav\", \"auto\"],\n    #[\"example/event_3.wav\", \"auto\"],\n    [\"example/rich_1.wav\", \"auto\"],\n    [\"example/rich_2.wav\", \"auto\"],\n    #[\"example/rich_3.wav\", \"auto\"],\n    [\"example/longwav_1.wav\", \"auto\"],\n    [\"example/longwav_2.wav\", \"auto\"],\n    [\"example/longwav_3.wav\", \"auto\"],\n    #[\"example/longwav_4.wav\", \"auto\"],\n]\n\n\n\nhtml_content = \"\"\"\n<div>\n    <h2 style=\"font-size: 22px;margin-left: 0px;\">Voice Understanding Model: SenseVoice-Small</h2>\n    <p style=\"font-size: 18px;margin-left: 20px;\">SenseVoice-Small is an encoder-only speech foundation model designed for rapid voice understanding. It encompasses a variety of features including automatic speech recognition (ASR), spoken language identification (LID), speech emotion recognition (SER), and acoustic event detection (AED). SenseVoice-Small supports multilingual recognition for Chinese, English, Cantonese, Japanese, and Korean. Additionally, it offers exceptionally low inference latency, performing 7 times faster than Whisper-small and 17 times faster than Whisper-large.</p>\n    <h2 style=\"font-size: 22px;margin-left: 0px;\">Usage</h2> <p style=\"font-size: 18px;margin-left: 20px;\">Upload an audio file or input through a microphone, then select the task and language. the audio is transcribed into corresponding text along with associated emotions (😊 happy, 😡 angry/exicting, 😔 sad) and types of sound events (😀 laughter, 🎼 music, 👏 applause, 🤧 cough&sneeze, 😭 cry). The event labels are placed in the front of the text and the emotion are in the back of the text.</p>\n\t<p style=\"font-size: 18px;margin-left: 20px;\">Recommended audio input duration is below 30 seconds. For audio longer than 30 seconds, local deployment is recommended.</p>\n\t<h2 style=\"font-size: 22px;margin-left: 0px;\">Repo</h2>\n\t<p style=\"font-size: 18px;margin-left: 20px;\"><a href=\"https://github.com/FunAudioLLM/SenseVoice\" target=\"_blank\">SenseVoice</a>: multilingual speech understanding model</p>\n\t<p style=\"font-size: 18px;margin-left: 20px;\"><a href=\"https://github.com/modelscope/FunASR\" target=\"_blank\">FunASR</a>: fundamental speech recognition toolkit</p>\n\t<p style=\"font-size: 18px;margin-left: 20px;\"><a href=\"https://github.com/FunAudioLLM/CosyVoice\" target=\"_blank\">CosyVoice</a>: high-quality multilingual TTS model</p>\n</div>\n\"\"\"\n\n\ndef launch():\n\twith gr.Blocks(theme=gr.themes.Soft()) as demo:\n\t\t# gr.Markdown(description)\n\t\tgr.HTML(html_content)\n\t\twith gr.Row():\n\t\t\twith gr.Column():\n\t\t\t\taudio_inputs = gr.Audio(label=\"Upload audio or use the microphone\")\n\t\t\t\t\n\t\t\t\twith gr.Accordion(\"Configuration\"):\n\t\t\t\t\tlanguage_inputs = gr.Dropdown(choices=[\"auto\", \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"],\n\t\t\t\t\t\t\t\t\t\t\t\t  value=\"auto\",\n\t\t\t\t\t\t\t\t\t\t\t\t  label=\"Language\")\n\t\t\t\tfn_button = gr.Button(\"Start\", variant=\"primary\")\n\t\t\t\ttext_outputs = gr.Textbox(label=\"Results\")\n\t\t\tgr.Examples(examples=audio_examples, inputs=[audio_inputs, language_inputs], examples_per_page=20)\n\t\t\n\t\tfn_button.click(model_inference, inputs=[audio_inputs, language_inputs], outputs=text_outputs)\n\n\tdemo.launch()\n\n\nif __name__ == \"__main__\":\n\t# iface.launch()\n\tlaunch()\n\n\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/saas/official_openai/__init__.py\nimport time\nfrom typing import List, Tuple, Dict, Any, Union\nimport httpx\nfrom openai import OpenAI\nimport base64\nimport io\nimport json\nimport ray\nfrom byzerllm.utils.types import (\n    BlockVLLMStreamServer,\n    StreamOutputs,\n    SingleOutput,\n    SingleOutputMeta,\n    BlockBinaryStreamServer,\n)\nfrom byzerllm.utils.langutil import asyncfy_with_semaphore\nimport threading\nimport asyncio\nimport traceback\nimport uuid\nimport tempfile        \n\nclass CustomSaasAPI:\n\n    def __init__(self, infer_params: Dict[str, str]) -> None:\n\n        self.api_key = infer_params[\"saas.api_key\"]\n        self.model = infer_params.get(\"saas.model\", \"gpt-3.5-turbo-1106\")\n\n        other_params = {}\n\n        if \"saas.api_base\" in infer_params:\n            other_params[\"api_base\"] = infer_params[\"saas.api_base\"]\n\n        if \"saas.api_version\" in infer_params:\n            other_params[\"api_version\"] = infer_params[\"saas.api_version\"]\n\n        if \"saas.api_type\" in infer_params:\n            other_params[\"api_type\"] = infer_params[\"saas.api_type\"]\n\n        if \"saas.base_url\" in infer_params:\n            other_params[\"base_url\"] = infer_params[\"saas.base_url\"]\n\n        if \"saas.timeout\" in infer_params:\n            other_params[\"timeout\"] = float(infer_params[\"saas.timeout\"])\n\n        self.max_retries = int(infer_params.get(\"saas.max_retries\", 10))\n\n        self.meta = {\n            \"model_deploy_type\": \"saas\",\n            \"backend\": \"saas\",\n            \"support_stream\": True,\n            \"model_name\": self.model,\n        }\n\n        self.meta[\"embedding_mode\"] = \"embedding\" in self.model.lower()\n\n        self.proxies = infer_params.get(\"saas.proxies\", None)\n        self.local_address = infer_params.get(\"saas.local_address\", \"0.0.0.0\")\n\n        if not self.proxies:\n            self.client = OpenAI(**other_params, api_key=self.api_key)\n        else:\n            self.client = OpenAI(\n                **other_params,\n                api_key=self.api_key,\n                http_client=httpx.Client(\n                    proxies=self.proxies,\n                    transport=httpx.HTTPTransport(local_address=self.local_address),\n                ),\n            )\n\n        try:\n            ray.get_actor(\"BLOCK_VLLM_STREAM_SERVER\")\n        except ValueError:\n            try:\n                ray.remote(BlockVLLMStreamServer).options(\n                    name=\"BLOCK_VLLM_STREAM_SERVER\",\n                    lifetime=\"detached\",\n                    max_concurrency=1000,\n                ).remote()\n            except Exception as e:\n                pass\n        try:\n            ray.get_actor(\"BlockBinaryStreamServer\")\n        except ValueError:\n            try:\n                ray.remote(BlockBinaryStreamServer).options(\n                    name=\"BlockBinaryStreamServer\",\n                    lifetime=\"detached\",\n                    max_concurrency=1000,\n                ).remote()\n            except Exception as e:\n                pass\n\n    # saas/proprietary\n    def get_meta(self):\n        return [self.meta]\n\n    def process_input(self, ins: Union[str, List[Dict[str, Any]], Dict[str, Any]]):\n\n        if isinstance(ins, list) or isinstance(ins, dict):\n            return ins\n\n        content = []\n        try:\n            ins_json = json.loads(ins)\n        except:\n            return ins\n\n        ## speech\n        if isinstance(ins_json, dict):\n            return ins_json\n\n        content = []\n        #     [\n        #     {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n        #     {\n        #       \"type\": \"image_url\",\n        #       \"image_url\": {\n        #         \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n        #         \"detail\": \"high\"\n        #       },\n        #     },\n        #   ],\n        for item in ins_json:\n            # for format like this: {\"image\": \"xxxxxx\", \"text\": \"What’s in this image?\",\"detail\":\"high\"}\n            # or {\"image\": \"xxxxxx\"}, {\"text\": \"What’s in this image?\"}\n            if (\"image\" in item or \"image_url\" in item) and \"type\" not in item:\n                image_data = item.get(\"image\", item.get(\"image_url\", \"\"))\n                ## \"data:image/jpeg;base64,\"\n                if not image_data.startswith(\"data:\"):\n                    image_data = \"data:image/jpeg;base64,\" + image_data\n\n                ## get the other fields except image/text/image_url\n                other_fields = {\n                    k: v\n                    for k, v in item.items()\n                    if k not in [\"image\", \"text\", \"image_url\"]\n                }\n                content.append(\n                    {\n                        \"image_url\": {\"url\": image_data, **other_fields},\n                        \"type\": \"image_url\",\n                    }\n                )            \n\n            if \"text\" in item and \"type\" not in item:\n                text_data = item[\"text\"]\n                content.append({\"text\": text_data, \"type\": \"text\"})\n\n            ## for format like this: {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n            ## {\"type\": \"image_url\", \"image_url\": {\"url\":\"\",\"detail\":\"high\"}}\n            ## this is the standard format, just return it\n            if \"type\" in item and item[\"type\"] == \"text\":\n                content.append(item)\n\n            if \"type\" in item and item[\"type\"] == \"image_url\":\n                content.append(item)\n\n        if not content:\n            return ins\n\n        return content\n\n    async def async_embed_query(self, ins: str, **kwargs):\n        resp = await asyncfy_with_semaphore(\n            lambda: self.client.embeddings.create(input=[ins], model=self.model)\n        )()\n        embedding = resp.data[0].embedding\n        usage = resp.usage\n        return (\n            embedding,\n            {\n                \"metadata\": {\n                    \"input_tokens_count\": usage.prompt_tokens,\n                    \"generated_tokens_count\": 0,\n                }\n            },\n        )\n\n    def embed_query(self, ins: str, **kwargs):\n        resp = self.client.embeddings.create(input=[ins], model=self.model)\n        embedding = resp.data[0].embedding\n        usage = resp.usage\n        return (\n            embedding,\n            {\n                \"metadata\": {\n                    \"input_tokens_count\": usage.prompt_tokens,\n                    \"generated_tokens_count\": 0,\n                }\n            },\n        )\n\n    async def async_text_to_speech(\n        self, stream: bool, ins: str, voice: str, chunk_size: int = None, **kwargs\n    ):\n        if stream:\n            server = ray.get_actor(\"BlockBinaryStreamServer\")\n            request_id = [None]\n\n            def writer():\n                try:\n                    request_id[0] = str(uuid.uuid4())\n                    with self.client.with_streaming_response.audio.speech.create(\n                        model=self.model, voice=voice, input=ins, **kwargs\n                    ) as response:\n                        for chunk in response.iter_bytes(chunk_size):\n                            input_tokens_count = 0\n                            generated_tokens_count = 0\n                            ray.get(\n                                server.add_item.remote(\n                                    request_id[0],\n                                    StreamOutputs(\n                                        outputs=[\n                                            SingleOutput(\n                                                text=chunk,\n                                                metadata=SingleOutputMeta(\n                                                    input_tokens_count=input_tokens_count,\n                                                    generated_tokens_count=generated_tokens_count,\n                                                ),\n                                            )\n                                        ]\n                                    ),\n                                )\n                            )\n                except:\n                    traceback.print_exc()\n                ray.get(server.mark_done.remote(request_id[0]))\n\n            threading.Thread(target=writer, daemon=True).start()\n\n            time_count = 10 * 100\n            while request_id[0] is None and time_count > 0:\n                time.sleep(0.01)\n                time_count -= 1\n\n            if request_id[0] is None:\n                raise Exception(\"Failed to get request id\")\n\n            def write_running():\n                return ray.get(server.add_item.remote(request_id[0], \"RUNNING\"))\n\n            await asyncio.to_thread(write_running)\n            return [\n                (\n                    \"\",\n                    {\n                        \"metadata\": {\n                            \"request_id\": request_id[0],\n                            \"stream_server\": \"BlockBinaryStreamServer\",\n                        }\n                    },\n                )\n            ]\n\n        start_time = time.monotonic()\n        with io.BytesIO() as output:\n            async with asyncfy_with_semaphore(\n                lambda: self.client.with_streaming_response.audio.speech.create(\n                    model=self.model, voice=voice, input=ins, **kwargs\n                )\n            )() as response:\n                for chunk in response.iter_bytes():\n                    output.write(chunk)\n\n            base64_audio = base64.b64encode(output.getvalue()).decode()\n            time_cost = time.monotonic() - start_time\n            return [\n                (\n                    base64_audio,\n                    {\n                        \"metadata\": {\n                            \"request_id\": \"\",\n                            \"input_tokens_count\": 0,\n                            \"generated_tokens_count\": 0,\n                            \"time_cost\": time_cost,\n                            \"first_token_time\": 0,\n                            \"speed\": 0,\n                        }\n                    },\n                )\n            ]\n\n    async def async_speech_to_text(\n        self,\n        audio: str,\n        response_format: str = \"verbose_json\",\n        timestamp_granularities: List[str] = [\"word\",\"segment\"],\n    ):               \n        # Extract audio format and base64 data\n        data_prefix = \"data:audio/\"\n        base64_prefix = \";base64,\"\n        if not audio.startswith(data_prefix):\n            raise ValueError(\"Invalid audio data format\")\n        \n        format_end = audio.index(base64_prefix)\n        audio_format = audio[len(data_prefix):format_end]\n        base64_data = audio[format_end + len(base64_prefix):]\n        \n        # Decode the base64 audio data\n        audio_data = base64.b64decode(base64_data)\n\n        # Create a temporary file with the correct extension\n        with tempfile.NamedTemporaryFile(delete=False, suffix=f'.{audio_format}') as temp_audio_file:\n            temp_audio_file.write(audio_data)\n            temp_audio_file_path = temp_audio_file.name\n\n        try:\n            start_time = time.monotonic()\n                   \n            with open(temp_audio_file_path, \"rb\") as audio_file:                \n                transcription = self.client.audio.transcriptions.create(\n                    model=self.model or \"whisper-1\",\n                    file=audio_file,\n                    response_format=response_format,\n                    timestamp_granularities=timestamp_granularities,\n                )\n            time_cost = time.monotonic() - start_time\n\n            return [\n                (\n                    json.dumps(transcription.to_dict(), ensure_ascii=True),\n                    {\n                        \"metadata\": {\n                            \"request_id\": \"\",\n                            \"input_tokens_count\": 0,\n                            \"generated_tokens_count\": 0,\n                            \"time_cost\": time_cost,\n                            \"first_token_time\": 0,\n                            \"speed\": 0,\n                        }\n                    },\n                )\n            ]\n        finally:\n            # Clean up the temporary file\n            import os\n            os.unlink(temp_audio_file_path)\n\n    def image_to_text(self, ins: str, **kwargs):\n        pass\n\n    async def async_text_to_image(\n        self, stream: bool, input: str, size: str, quality: str, n: int, **kwargs\n    ):\n        if stream:\n            raise Exception(\"Stream not supported for text to image\")\n        start_time = time.monotonic()\n        response = await asyncfy_with_semaphore(\n            lambda: self.client.images.generate(\n                model=self.model,\n                prompt=input,\n                size=size,\n                quality=quality,\n                n=1,\n                response_format=\"b64_json\",\n                **kwargs,\n            )\n        )()\n        time_cost = time.monotonic() - start_time\n        base64_image = response.data[0].b64_json\n        return [\n            (\n                base64_image,\n                {\n                    \"metadata\": {\n                        \"request_id\": \"\",\n                        \"input_tokens_count\": 0,\n                        \"generated_tokens_count\": 0,\n                        \"time_cost\": time_cost,\n                        \"first_token_time\": 0,\n                        \"speed\": 0,\n                    }\n                },\n            )\n        ]\n\n    def text_to_text(self, ins: str, **kwargs):\n        pass\n\n    async def async_stream_chat(\n        self,\n        tokenizer,\n        ins: str,\n        his: List[Dict[str, Any]] = [],\n        max_length: int = 4096,\n        top_p: float = 0.7,\n        temperature: float = 0.9,\n        **kwargs,\n    ):\n\n        model = self.model\n\n        if \"model\" in kwargs:\n            model = kwargs[\"model\"]\n\n        messages = [\n            {\"role\": message[\"role\"], \"content\": self.process_input(message[\"content\"])}\n            for message in his\n        ] + [{\"role\": \"user\", \"content\": self.process_input(ins)}]\n\n        stream = kwargs.get(\"stream\", False)\n\n        ## content = [\n        ##    \"voice\": \"alloy\",\"input\": \"Hello, World!\",response_format: \"mp3\"]\n        last_message = messages[-1][\"content\"]\n\n        if isinstance(last_message, dict) and \"voice\" in last_message:\n            voice = last_message[\"voice\"]\n            response_format = last_message.get(\"response_format\", \"mp3\")\n            chunk_size = last_message.get(\"chunk_size\", None)\n            input = last_message[\"input\"]\n            return await self.async_text_to_speech(\n                stream=stream,\n                ins=input,\n                voice=voice,\n                chunk_size=chunk_size,\n                response_format=response_format,\n            )\n\n        if isinstance(last_message, dict) and \"input\" in last_message:\n            input = last_message[\"input\"]\n            size = last_message.get(\"size\", \"1024x1024\")\n            quality = last_message.get(\"quality\", \"standard\")\n            n = last_message.get(\"n\", 1)\n            return await self.async_text_to_image(\n                stream=stream, input=input, size=size, quality=quality, n=n\n            )\n\n        if isinstance(last_message, dict) and \"audio\" in last_message:            \n            audio_data = last_message.get(\"audio\", \"\")            \n            tpe = last_message.get(\"type\",\"wav\")\n            if not audio_data.startswith(\"data:\"):\n                audio_data = f\"data:audio/${tpe};base64,\" + audio_data                \n            return await self.async_speech_to_text(audio=audio_data)\n        \n        server = ray.get_actor(\"BLOCK_VLLM_STREAM_SERVER\")\n        request_id = [None]\n\n        def writer():\n            try:\n                r = \"\"\n                response = self.client.chat.completions.create(\n                    messages=messages,\n                    model=model,\n                    stream=True,\n                    max_tokens=max_length,\n                    temperature=temperature,\n                    top_p=top_p,\n                )\n                # input_tokens_count = 0\n                # generated_tokens_count = 0\n\n                request_id[0] = str(uuid.uuid4())\n\n                for chunk in response:\n                    content = chunk.choices[0].delta.content or \"\"\n                    r += content\n                    if hasattr(chunk, \"usage\") and chunk.usage:\n                        input_tokens_count = chunk.usage.prompt_tokens\n                        generated_tokens_count = chunk.usage.completion_tokens\n                    else:\n                        input_tokens_count = 0\n                        generated_tokens_count = 0\n                    ray.get(\n                        server.add_item.remote(\n                            request_id[0],\n                            StreamOutputs(\n                                outputs=[\n                                    SingleOutput(\n                                        text=r,\n                                        metadata=SingleOutputMeta(\n                                            input_tokens_count=input_tokens_count,\n                                            generated_tokens_count=generated_tokens_count,\n                                        ),\n                                    )\n                                ]\n                            ),\n                        )\n                    )\n            except:\n                traceback.print_exc()\n            ray.get(server.mark_done.remote(request_id[0]))\n\n        if stream:\n            threading.Thread(target=writer, daemon=True).start()\n\n            time_count = 60 * 100\n            while request_id[0] is None and time_count > 0:\n                time.sleep(0.01)\n                time_count -= 1\n\n            if request_id[0] is None:\n                raise Exception(\"Failed to get request id\")\n\n            def write_running():\n                return ray.get(server.add_item.remote(request_id[0], \"RUNNING\"))\n\n            await asyncio.to_thread(write_running)\n            return [\n                (\n                    \"\",\n                    {\n                        \"metadata\": {\n                            \"request_id\": request_id[0],\n                            \"stream_server\": \"BLOCK_VLLM_STREAM_SERVER\",\n                        }\n                    },\n                )\n            ]\n        else:\n            try:\n                start_time = time.monotonic()\n                response = await asyncfy_with_semaphore(\n                    lambda: self.client.chat.completions.create(\n                        messages=messages,\n                        model=model,\n                        max_tokens=max_length,\n                        temperature=temperature,\n                        top_p=top_p,\n                    )\n                )()\n\n                generated_text = response.choices[0].message.content\n                generated_tokens_count = response.usage.completion_tokens\n                input_tokens_count = response.usage.prompt_tokens\n                time_cost = time.monotonic() - start_time\n                return [\n                    (\n                        generated_text,\n                        {\n                            \"metadata\": {\n                                \"request_id\": response.id,\n                                \"input_tokens_count\": input_tokens_count,\n                                \"generated_tokens_count\": generated_tokens_count,\n                                \"time_cost\": time_cost,\n                                \"first_token_time\": 0,\n                                \"speed\": float(generated_tokens_count) / time_cost,\n                            }\n                        },\n                    )\n                ]\n            except Exception as e:\n                print(f\"Error: {e}\")\n                raise e\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/whisper/__init__.py\nimport whisper\nfrom typing import Dict, List, Tuple\nfrom byzerllm.utils.types import StopSequencesCriteria\n\nfrom typing import Dict, Any, List, Union\nfrom pyjava.storage import streaming_tar as STar\nimport json\nimport base64\nimport tempfile\nimport time\n\n\ndef get_meta(self):\n    return [\n        {\n            \"model_deploy_type\": \"proprietary\",\n            \"backend\": \"transformers\",\n            \"message_format\": True\n        }\n    ]\n\n\ndef process_input(ins: Union[str, List[Dict[str, Any]], Dict[str, Any]]):\n    # print(ins) \n    if isinstance(ins, list) or isinstance(ins, dict):\n        return ins\n\n    content = []\n    try:\n        ins_json = json.loads(ins)\n    except:\n        return ins\n\n    ## speech\n    if isinstance(ins_json, dict):\n        return ins_json\n\n    content = []\n    #     [\n    #     {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n    #     {\n    #       \"type\": \"image_url\",\n    #       \"image_url\": {\n    #         \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n    #         \"detail\": \"high\"\n    #       },\n    #     },\n    #   ],\n    for item in ins_json:\n        # for format like this: {\"image\": \"xxxxxx\", \"text\": \"What’s in this image?\",\"detail\":\"high\"}\n        # or {\"image\": \"xxxxxx\"}, {\"text\": \"What’s in this image?\"}\n        if (\"image\" in item or \"image_url\" in item) and \"type\" not in item:\n            image_data = item.get(\"image\", item.get(\"image_url\", \"\"))\n            ## \"data:image/jpeg;base64,\"\n            if not image_data.startswith(\"data:\"):\n                image_data = \"data:image/jpeg;base64,\" + image_data\n\n            ## get the other fields except image/text/image_url\n            other_fields = {\n                k: v for k, v in item.items() if k not in [\"image\", \"text\", \"image_url\"]\n            }\n            content.append(\n                {\n                    \"image_url\": {\"url\": image_data, **other_fields},\n                    \"type\": \"image_url\",\n                }\n            )\n\n        if \"text\" in item and \"type\" not in item:\n            text_data = item[\"text\"]\n            content.append({\"text\": text_data, \"type\": \"text\"})\n\n        ## for format like this: {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n        ## {\"type\": \"image_url\", \"image_url\": {\"url\":\"\",\"detail\":\"high\"}}\n        ## this is the standard format, just return it\n        if \"type\" in item and item[\"type\"] == \"text\":\n            content.append(item)\n\n        if \"type\" in item and item[\"type\"] == \"image_url\":\n            content.append(item)\n\n    if not content:\n        return ins\n\n    return content\n\n\ndef stream_chat(\n    self,\n    tokenizer,\n    ins: str,\n    his: List[Dict[str, str]] = [],\n    max_length: int = 4090,\n    top_p: float = 0.95,\n    temperature: float = 0.1,\n    **kwargs,\n):\n    messages = [\n        {\"role\": message[\"role\"], \"content\": process_input(message[\"content\"])}\n        for message in his\n    ] + [{\"role\": \"user\", \"content\": process_input(ins)}]\n\n    audio_input = messages[-1][\"content\"]\n\n    audio_data = audio_input.get(\"audio\", \"\")\n    audio = audio_input[\"audio\"]\n    response_format = audio_input.get(\"response_format\", \"json\")\n    timestamp_granularities = audio_input.get(\n        \"timestamp_granularities\", [\"word\", \"segment\"]\n    )\n    data_prefix = \"data:audio/\"\n    base64_prefix = \";base64,\"\n    if not audio.startswith(data_prefix):\n        raise ValueError(\"Invalid audio data format\")\n\n    format_end = audio.index(base64_prefix)\n    audio_format = audio[len(data_prefix) : format_end]\n    base64_data = audio[format_end + len(base64_prefix) :]\n\n    # Decode the base64 audio data\n    audio_data = base64.b64decode(base64_data)\n\n    # Create a temporary file with the correct extension\n    with tempfile.NamedTemporaryFile(\n        delete=False, suffix=f\".{audio_format}\"\n    ) as temp_audio_file:\n        temp_audio_file.write(audio_data)\n        temp_audio_file_path = temp_audio_file.name\n\n    try:\n        start_time = time.monotonic()\n        word_timestamps = \"word\" in timestamp_granularities\n        \n        result = self.transcribe(temp_audio_file_path, word_timestamps=word_timestamps)        \n            \n        time_cost = time.monotonic() - start_time\n        return [\n            (\n                json.dumps(result, ensure_ascii=True),\n                {\n                    \"metadata\": {\n                        \"request_id\": \"\",\n                        \"input_tokens_count\": 0,\n                        \"generated_tokens_count\": 0,\n                        \"time_cost\": time_cost,\n                        \"first_token_time\": 0,\n                        \"speed\": 0,\n                    }\n                },\n            )\n        ]\n    finally:\n        # Clean up the temporary file\n        import os\n\n        os.unlink(temp_audio_file_path)\n\n\ndef init_model(\n    model_dir, infer_params: Dict[str, str] = {}, sys_conf: Dict[str, str] = {}\n):\n    model = whisper.load_model(\"large\", download_root=model_dir)\n    import types\n\n    model.stream_chat = types.MethodType(stream_chat, model)\n    model.get_meta = types.MethodType(get_meta, model)\n    return (model, None)\n\n\n     \n\n下面是用户的需求：\n\n@/Users/allwefantasy/projects/byzer-llm/src/byzerllm/sensevoice/__init__.py 中的 中从 vad_model 需要从 infer_params 获取,如果没有,则不传递给 AutoModel\n"}, {"role": "assistant", "content": "To implement this change, we need to modify the `init_model` function in the `/Users/allwefantasy/projects/byzer-llm/src/byzerllm/sensevoice/__init__.py` file. Here's the *SEARCH/REPLACE block* to make this modification:\n\n```python\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/sensevoice/__init__.py\n<<<<<<< SEARCH\ndef init_model(model_dir, infer_params: Dict[str, str] = {}, sys_conf: Dict[str, str] = {}):\n    model = AutoModel(model=model_dir, vad_model=model_dir)\n    import types\n=======\ndef init_model(model_dir, infer_params: Dict[str, str] = {}, sys_conf: Dict[str, str] = {}):\n    model_kwargs = {\"model\": model_dir}\n    \n    if \"vad_model\" in infer_params:\n        model_kwargs[\"vad_model\"] = infer_params[\"vad_model\"]\n    \n    model = AutoModel(**model_kwargs)\n    import types\n>>>>>>> REPLACE\n```\n\nThis change does the following:\n\n1. We create a `model_kwargs` dictionary to hold the arguments for `AutoModel`.\n2. We always include the `model` parameter with the value of `model_dir`.\n3. We check if `vad_model` is in `infer_params`. If it is, we add it to `model_kwargs`.\n4. Finally, we create the `AutoModel` instance by unpacking `model_kwargs`.\n\nThis implementation ensures that the `vad_model` is only passed to `AutoModel` if it's provided in `infer_params`, meeting the user's requirement."}], "model": "human", "yaml_file": "actions/019_chat_action.yml"}
{"instruction": "@/Users/allwefantasy/projects/byzer-llm/src/byzerllm/sensevoice/__init__.py 中的 中从 vad_model 需要从 infer_params 获取,如果没有,则不传递给 AutoModel", "conversations": [{"role": "user", "content": "\n如果你需要生成代码，对于每个需要更改的文件,你需要按 *SEARCH/REPLACE block* 的格式进行生成。\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The opening fence and code language, eg: ```python\n2. The file path alone on a line, starting with \"##File:\" and verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n*SEARCH/REPLACE* blocks will replace *all* matching occurrences.\nInclude enough lines to make the SEARCH blocks unique.\n\nInclude *ALL* the code being searched and replaced!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n下面我们来看一个例子：\n\n当前项目目录结构：\n1. 项目根目录： /tmp/projects/mathweb\n2. 项目子目录/文件列表(类似tree 命令输出)\nflask/\n    app.py\n    templates/\n        index.html\n    static/\n        style.css\n\n用户需求： Change get_factorial() to use math.factorial\n\n回答： To make this change we need to modify `/tmp/projects/mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n用户需求： Refactor hello() into its own file.\n\n回答：To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/hello.py\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/main.py\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n现在让我们开始一个新的任务:\n\n当前项目目录结构：\n1. 项目根目录： /Users/allwefantasy/projects/byzer-llm\n2. 项目子目录/文件列表(类似tree 命令输出)：\nsetup.py\ntests/\n    test_emb_rerank.py\n    test_deepseek_code.py\n    test_prompt.py\n    test_model_meta.py\n    __init__.py\n    test_transformers_deploy.py\n    test_data_analysis.py\n    test_yi.py\n    test_deploy.py\n    functions_suite.py\n    test_template.py\n    auto/\n        test_backend_llama_cpp.py\n        __init__.py\nsetup-machine/\n    convert_safetensor.py\nnotebooks/\n    test.py\nsrc/\n    byzerllm/\n        lang.py\n        store.py\n        version.py\n        log.py\n        __init__.py\n        types.py\n        easy_byzerllm_command.py\n        byzerllm_command.py\n        command_args.py\n        bark/\n            generation.py\n            __init__.py\n            model.py\n            api.py\n            model_fine.py\n            bark_voice.py\n        moss/\n            finetune_moss.py\n            __init__.py\n            moss_inference.py\n            models/\n                configuration_moss.py\n                quantization.py\n                custom_autotune.py\n                __init__.py\n                modeling_moss.py\n                tokenization_moss.py\n        llama2/\n            __init__.py\n        records/\n            __init__.py\n        visualglm/\n            __init__.py\n        auto/\n            __init__.py\n            backend_llama_cpp.py\n            backend_vllm.py\n            backend_ds.py\n        alpha_moss/\n            __init__.py\n        processor/\n            __init__.py\n        sensevoice/\n            __init__.py\n        qwen/\n            __init__.py\n        utils/\n            json_repaire.py\n            object_store_ref_util.py\n            __init__.py\n            types.py\n            tokenizer.py\n            openai_utils.py\n            ray_utils.py\n            text_generator.py\n            emb.py\n            connect_ray.py\n            testing.py\n            langutil.py\n            nontext.py\n            fulltune/\n                deepspeed_trainner.py\n                __init__.py\n                launch.py\n                trainner.py\n                base_model/\n                    __init__.py\n                    configuration_baichuan.py\n                    modeling_baichuan.py\n                pretrain/\n                    __init__.py\n                    convert_to_transformers.py\n            metrics/\n                __init__.py\n            config/\n                __init__.py\n            retrieval/\n                rest.py\n                __init__.py\n                udf.py\n            sft/\n                argument.py\n                collator.py\n                qlora.py\n                __init__.py\n                model.py\n                dataset.py\n                loss.py\n                merge_lora.py\n                trainer.py\n            client/\n                message_utils.py\n                math_utils.py\n                byzerllm_client.py\n                parallel_utils.py\n                __init__.py\n                types.py\n                code_utils.py\n                img_utils.py\n                entrypoints/\n                    __init__.py\n                    openai/\n                        protocol.py\n                        __init__.py\n                        serve.py\n                        serving_chat.py\n                        api_server.py\n                        serving_engine.py\n                        serving_completion.py\n                        tool.py\n        chatglm2/\n            __init__.py\n        baichuan/\n            __init__.py\n        m3e/\n            __init__.py\n        stable_diffusion/\n            config.py\n            __init__.py\n            logger.py\n            shared.py\n            model.py\n            utils.py\n            images.py\n            diffusion/\n                __init__.py\n                embeddings.py\n                utils.py\n                networks/\n                    lyco.py\n                    __init__.py\n                upscalers/\n                    __init__.py\n                    multidiffusion.py\n                    samplers.py\n                piplines/\n                    __init__.py\n                    lpw.py\n                    diffusers.py\n            lib/\n                __init__.py\n                diffusers/\n                    __init__.py\n                    scheduler.py\n            api/\n                __init__.py\n                plugin.py\n                models/\n                    diffusion.py\n                    __init__.py\n                diffusion/\n                    __init__.py\n                    pipelines/\n                        __init__.py\n                        diffusers.py\n                events/\n                    generation.py\n                    __init__.py\n        bge/\n            __init__.py\n        bge_rerank/\n            __init__.py\n        apps/\n            client.py\n            qa_strategy.py\n            __init__.py\n            builder.py\n            vector_db.py\n            utils.py\n            qa.py\n            byzer_storage/\n                memory_model_based.py\n                command.py\n                env.py\n                train_pt.py\n                train_sft.py\n                __init__.py\n                train_dpo.py\n                simple_api.py\n                generate_sft_data.py\n            byzer_sql/\n                __init__.py\n            llama_index/\n                byzerai_vectordb.py\n                byzerai.py\n                byzerai_embedding.py\n                __init__.py\n                collection_manager.py\n                byzerai_index_store.py\n                byzerai_docstore.py\n                simple_retrieval.py\n                byzerai_kvstore.py\n            agent/\n                conversable_agent.py\n                registry.py\n                groupchat.py\n                user_proxy_agent.py\n                __init__.py\n                agent.py\n                extensions/\n                    simple_retrieval_client.py\n                    planner.py\n                    python_codesandbox_agent.py\n                    preview_file_agent.py\n                    llama_index_retrieval_agent.py\n                    visualization_agent.py\n                    assistant_agent.py\n                    load_data_agent.py\n                    retrieval_agent.py\n                    output_agent.py\n                    __init__.py\n                    byzer_engine_agent.py\n                    subquestion_agent.py\n                    sql_reviewer_agent.py\n                    spark_sql_agent.py\n                    rhetorical_agent.py\n                    data_analysis.py\n                    data_analysis_pipeline_agent.py\n                    common_agent.py\n                    query_rewrite/\n                        time.py\n                        rhetorical.py\n                        __init__.py\n                        condition.py\n                        context.py\n                store/\n                    memory_store.py\n                    __init__.py\n                    stores.py\n        saas/\n            __init__.py\n            chatglm/\n                __init__.py\n            sparkdesk/\n                __init__.py\n            gemini/\n                __init__.py\n            azure_openai/\n                __init__.py\n            azure/\n                __init__.py\n            qianfan/\n                __init__.py\n            claude/\n                __init__.py\n            qianwen/\n                __init__.py\n            volcengine/\n                __init__.py\n            minimax/\n                __init__.py\n            baichuan/\n                __init__.py\n            qianwen_vl/\n                __init__.py\n            official_openai/\n                __init__.py\n            zhipu/\n                __init__.py\n            aws_bedrock/\n                __init__.py\n            openai/\n                __init__.py\n        falcon/\n            __init__.py\n        chatglm6b/\n            trainer_seq2seq.py\n            arguments.py\n            __init__.py\n            finetune.py\n            tunning/\n                __init__.py\n                infer.py\n                finetune.py\n                utils/\n                    config.py\n                    __init__.py\n                    other.py\n                    common.py\n                    seq2seq.py\n        starcode/\n            __init__.py\n        qwen_vl_chat/\n            __init__.py\n        zephyr/\n            __init__.py\n        dolly/\n            generate.py\n            __init__.py\n            consts.py\n            dolly_inference.py\n            trainer.py\n        llama/\n            __init__.py\n        whisper/\n            whisper_inference.py\n            __init__.py\n下面是一些文件路径以及每个文件对应的源码：\n\n##File: /Users/allwefantasy/projects/SenseVoice/webui.py\n# coding=utf-8\n\nimport os\nimport librosa\nimport base64\nimport io\nimport gradio as gr\nimport re\n\nimport numpy as np\nimport torch\nimport torchaudio\n\n\nfrom funasr import AutoModel\n\nmodel = \"iic/SenseVoiceSmall\"\nmodel_kwargs = {\n    \"model\": model,\n    \"trust_remote_code\": True,\n}\n\n# Check if vad_model is provided in infer_params\nif \"vad_model\" in infer_params:\n    model_kwargs[\"vad_model\"] = infer_params[\"vad_model\"]\n    model_kwargs[\"vad_kwargs\"] = {\"max_single_segment_time\": 30000}\n\nmodel = AutoModel(**model_kwargs)\n\nimport re\n\nemo_dict = {\n\t\"<|HAPPY|>\": \"😊\",\n\t\"<|SAD|>\": \"😔\",\n\t\"<|ANGRY|>\": \"😡\",\n\t\"<|NEUTRAL|>\": \"\",\n\t\"<|FEARFUL|>\": \"😰\",\n\t\"<|DISGUSTED|>\": \"🤢\",\n\t\"<|SURPRISED|>\": \"😮\",\n}\n\nevent_dict = {\n\t\"<|BGM|>\": \"🎼\",\n\t\"<|Speech|>\": \"\",\n\t\"<|Applause|>\": \"👏\",\n\t\"<|Laughter|>\": \"😀\",\n\t\"<|Cry|>\": \"😭\",\n\t\"<|Sneeze|>\": \"🤧\",\n\t\"<|Breath|>\": \"\",\n\t\"<|Cough|>\": \"🤧\",\n}\n\nemoji_dict = {\n\t\"<|nospeech|><|Event_UNK|>\": \"❓\",\n\t\"<|zh|>\": \"\",\n\t\"<|en|>\": \"\",\n\t\"<|yue|>\": \"\",\n\t\"<|ja|>\": \"\",\n\t\"<|ko|>\": \"\",\n\t\"<|nospeech|>\": \"\",\n\t\"<|HAPPY|>\": \"😊\",\n\t\"<|SAD|>\": \"😔\",\n\t\"<|ANGRY|>\": \"😡\",\n\t\"<|NEUTRAL|>\": \"\",\n\t\"<|BGM|>\": \"🎼\",\n\t\"<|Speech|>\": \"\",\n\t\"<|Applause|>\": \"👏\",\n\t\"<|Laughter|>\": \"😀\",\n\t\"<|FEARFUL|>\": \"😰\",\n\t\"<|DISGUSTED|>\": \"🤢\",\n\t\"<|SURPRISED|>\": \"😮\",\n\t\"<|Cry|>\": \"😭\",\n\t\"<|EMO_UNKNOWN|>\": \"\",\n\t\"<|Sneeze|>\": \"🤧\",\n\t\"<|Breath|>\": \"\",\n\t\"<|Cough|>\": \"😷\",\n\t\"<|Sing|>\": \"\",\n\t\"<|Speech_Noise|>\": \"\",\n\t\"<|withitn|>\": \"\",\n\t\"<|woitn|>\": \"\",\n\t\"<|GBG|>\": \"\",\n\t\"<|Event_UNK|>\": \"\",\n}\n\nlang_dict =  {\n    \"<|zh|>\": \"<|lang|>\",\n    \"<|en|>\": \"<|lang|>\",\n    \"<|yue|>\": \"<|lang|>\",\n    \"<|ja|>\": \"<|lang|>\",\n    \"<|ko|>\": \"<|lang|>\",\n    \"<|nospeech|>\": \"<|lang|>\",\n}\n\nemo_set = {\"😊\", \"😔\", \"😡\", \"😰\", \"🤢\", \"😮\"}\nevent_set = {\"🎼\", \"👏\", \"😀\", \"😭\", \"🤧\", \"😷\",}\n\ndef format_str(s):\n\tfor sptk in emoji_dict:\n\t\ts = s.replace(sptk, emoji_dict[sptk])\n\treturn s\n\n\ndef format_str_v2(s):\n\tsptk_dict = {}\n\tfor sptk in emoji_dict:\n\t\tsptk_dict[sptk] = s.count(sptk)\n\t\ts = s.replace(sptk, \"\")\n\temo = \"<|NEUTRAL|>\"\n\tfor e in emo_dict:\n\t\tif sptk_dict[e] > sptk_dict[emo]:\n\t\t\temo = e\n\tfor e in event_dict:\n\t\tif sptk_dict[e] > 0:\n\t\t\ts = event_dict[e] + s\n\ts = s + emo_dict[emo]\n\n\tfor emoji in emo_set.union(event_set):\n\t\ts = s.replace(\" \" + emoji, emoji)\n\t\ts = s.replace(emoji + \" \", emoji)\n\treturn s.strip()\n\ndef format_str_v3(s):\n\tdef get_emo(s):\n\t\treturn s[-1] if s[-1] in emo_set else None\n\tdef get_event(s):\n\t\treturn s[0] if s[0] in event_set else None\n\n\ts = s.replace(\"<|nospeech|><|Event_UNK|>\", \"❓\")\n\tfor lang in lang_dict:\n\t\ts = s.replace(lang, \"<|lang|>\")\n\ts_list = [format_str_v2(s_i).strip(\" \") for s_i in s.split(\"<|lang|>\")]\n\tnew_s = \" \" + s_list[0]\n\tcur_ent_event = get_event(new_s)\n\tfor i in range(1, len(s_list)):\n\t\tif len(s_list[i]) == 0:\n\t\t\tcontinue\n\t\tif get_event(s_list[i]) == cur_ent_event and get_event(s_list[i]) != None:\n\t\t\ts_list[i] = s_list[i][1:]\n\t\t#else:\n\t\tcur_ent_event = get_event(s_list[i])\n\t\tif get_emo(s_list[i]) != None and get_emo(s_list[i]) == get_emo(new_s):\n\t\t\tnew_s = new_s[:-1]\n\t\tnew_s += s_list[i].strip().lstrip()\n\tnew_s = new_s.replace(\"The.\", \" \")\n\treturn new_s.strip()\n\ndef model_inference(input_wav, language, fs=16000):\n\t# task_abbr = {\"Speech Recognition\": \"ASR\", \"Rich Text Transcription\": (\"ASR\", \"AED\", \"SER\")}\n\tlanguage_abbr = {\"auto\": \"auto\", \"zh\": \"zh\", \"en\": \"en\", \"yue\": \"yue\", \"ja\": \"ja\", \"ko\": \"ko\",\n\t\t\t\t\t \"nospeech\": \"nospeech\"}\n\t\n\t# task = \"Speech Recognition\" if task is None else task\n\tlanguage = \"auto\" if len(language) < 1 else language\n\tselected_language = language_abbr[language]\n\t# selected_task = task_abbr.get(task)\n\t\n\t# print(f\"input_wav: {type(input_wav)}, {input_wav[1].shape}, {input_wav}\")\n\t\n\tif isinstance(input_wav, tuple):\n\t\tfs, input_wav = input_wav\n\t\tinput_wav = input_wav.astype(np.float32) / np.iinfo(np.int16).max\n\t\tif len(input_wav.shape) > 1:\n\t\t\tinput_wav = input_wav.mean(-1)\n\t\tif fs != 16000:\n\t\t\tprint(f\"audio_fs: {fs}\")\n\t\t\tresampler = torchaudio.transforms.Resample(fs, 16000)\n\t\t\tinput_wav_t = torch.from_numpy(input_wav).to(torch.float32)\n\t\t\tinput_wav = resampler(input_wav_t[None, :])[0, :].numpy()\n\t\n\t\n\tmerge_vad = True #False if selected_task == \"ASR\" else True\n\tprint(f\"language: {language}, merge_vad: {merge_vad}\")\n\ttext = model.generate(input=input_wav,\n\t\t\t\t\t\t  cache={},\n\t\t\t\t\t\t  language=language,\n\t\t\t\t\t\t  use_itn=True,\n\t\t\t\t\t\t  batch_size_s=60, merge_vad=merge_vad)\n\t\n\tprint(text)\n\ttext = text[0][\"text\"]\n\ttext = format_str_v3(text)\n\t\n\tprint(text)\n\t\n\treturn text\n\n\naudio_examples = [\n    [\"example/zh.mp3\", \"zh\"],\n    [\"example/yue.mp3\", \"yue\"],\n    [\"example/en.mp3\", \"en\"],\n    [\"example/ja.mp3\", \"ja\"],\n    [\"example/ko.mp3\", \"ko\"],\n    [\"example/emo_1.wav\", \"auto\"],\n    [\"example/emo_2.wav\", \"auto\"],\n    [\"example/emo_3.wav\", \"auto\"],\n    #[\"example/emo_4.wav\", \"auto\"],\n    #[\"example/event_1.wav\", \"auto\"],\n    #[\"example/event_2.wav\", \"auto\"],\n    #[\"example/event_3.wav\", \"auto\"],\n    [\"example/rich_1.wav\", \"auto\"],\n    [\"example/rich_2.wav\", \"auto\"],\n    #[\"example/rich_3.wav\", \"auto\"],\n    [\"example/longwav_1.wav\", \"auto\"],\n    [\"example/longwav_2.wav\", \"auto\"],\n    [\"example/longwav_3.wav\", \"auto\"],\n    #[\"example/longwav_4.wav\", \"auto\"],\n]\n\n\n\nhtml_content = \"\"\"\n<div>\n    <h2 style=\"font-size: 22px;margin-left: 0px;\">Voice Understanding Model: SenseVoice-Small</h2>\n    <p style=\"font-size: 18px;margin-left: 20px;\">SenseVoice-Small is an encoder-only speech foundation model designed for rapid voice understanding. It encompasses a variety of features including automatic speech recognition (ASR), spoken language identification (LID), speech emotion recognition (SER), and acoustic event detection (AED). SenseVoice-Small supports multilingual recognition for Chinese, English, Cantonese, Japanese, and Korean. Additionally, it offers exceptionally low inference latency, performing 7 times faster than Whisper-small and 17 times faster than Whisper-large.</p>\n    <h2 style=\"font-size: 22px;margin-left: 0px;\">Usage</h2> <p style=\"font-size: 18px;margin-left: 20px;\">Upload an audio file or input through a microphone, then select the task and language. the audio is transcribed into corresponding text along with associated emotions (😊 happy, 😡 angry/exicting, 😔 sad) and types of sound events (😀 laughter, 🎼 music, 👏 applause, 🤧 cough&sneeze, 😭 cry). The event labels are placed in the front of the text and the emotion are in the back of the text.</p>\n\t<p style=\"font-size: 18px;margin-left: 20px;\">Recommended audio input duration is below 30 seconds. For audio longer than 30 seconds, local deployment is recommended.</p>\n\t<h2 style=\"font-size: 22px;margin-left: 0px;\">Repo</h2>\n\t<p style=\"font-size: 18px;margin-left: 20px;\"><a href=\"https://github.com/FunAudioLLM/SenseVoice\" target=\"_blank\">SenseVoice</a>: multilingual speech understanding model</p>\n\t<p style=\"font-size: 18px;margin-left: 20px;\"><a href=\"https://github.com/modelscope/FunASR\" target=\"_blank\">FunASR</a>: fundamental speech recognition toolkit</p>\n\t<p style=\"font-size: 18px;margin-left: 20px;\"><a href=\"https://github.com/FunAudioLLM/CosyVoice\" target=\"_blank\">CosyVoice</a>: high-quality multilingual TTS model</p>\n</div>\n\"\"\"\n\n\ndef launch():\n\twith gr.Blocks(theme=gr.themes.Soft()) as demo:\n\t\t# gr.Markdown(description)\n\t\tgr.HTML(html_content)\n\t\twith gr.Row():\n\t\t\twith gr.Column():\n\t\t\t\taudio_inputs = gr.Audio(label=\"Upload audio or use the microphone\")\n\t\t\t\t\n\t\t\t\twith gr.Accordion(\"Configuration\"):\n\t\t\t\t\tlanguage_inputs = gr.Dropdown(choices=[\"auto\", \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"],\n\t\t\t\t\t\t\t\t\t\t\t\t  value=\"auto\",\n\t\t\t\t\t\t\t\t\t\t\t\t  label=\"Language\")\n\t\t\t\tfn_button = gr.Button(\"Start\", variant=\"primary\")\n\t\t\t\ttext_outputs = gr.Textbox(label=\"Results\")\n\t\t\tgr.Examples(examples=audio_examples, inputs=[audio_inputs, language_inputs], examples_per_page=20)\n\t\t\n\t\tfn_button.click(model_inference, inputs=[audio_inputs, language_inputs], outputs=text_outputs)\n\n\tdemo.launch()\n\n\nif __name__ == \"__main__\":\n\t# iface.launch()\n\tlaunch()\n\n\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/saas/official_openai/__init__.py\nimport time\nfrom typing import List, Tuple, Dict, Any, Union\nimport httpx\nfrom openai import OpenAI\nimport base64\nimport io\nimport json\nimport ray\nfrom byzerllm.utils.types import (\n    BlockVLLMStreamServer,\n    StreamOutputs,\n    SingleOutput,\n    SingleOutputMeta,\n    BlockBinaryStreamServer,\n)\nfrom byzerllm.utils.langutil import asyncfy_with_semaphore\nimport threading\nimport asyncio\nimport traceback\nimport uuid\nimport tempfile        \n\nclass CustomSaasAPI:\n\n    def __init__(self, infer_params: Dict[str, str]) -> None:\n\n        self.api_key = infer_params[\"saas.api_key\"]\n        self.model = infer_params.get(\"saas.model\", \"gpt-3.5-turbo-1106\")\n\n        other_params = {}\n\n        if \"saas.api_base\" in infer_params:\n            other_params[\"api_base\"] = infer_params[\"saas.api_base\"]\n\n        if \"saas.api_version\" in infer_params:\n            other_params[\"api_version\"] = infer_params[\"saas.api_version\"]\n\n        if \"saas.api_type\" in infer_params:\n            other_params[\"api_type\"] = infer_params[\"saas.api_type\"]\n\n        if \"saas.base_url\" in infer_params:\n            other_params[\"base_url\"] = infer_params[\"saas.base_url\"]\n\n        if \"saas.timeout\" in infer_params:\n            other_params[\"timeout\"] = float(infer_params[\"saas.timeout\"])\n\n        self.max_retries = int(infer_params.get(\"saas.max_retries\", 10))\n\n        self.meta = {\n            \"model_deploy_type\": \"saas\",\n            \"backend\": \"saas\",\n            \"support_stream\": True,\n            \"model_name\": self.model,\n        }\n\n        self.meta[\"embedding_mode\"] = \"embedding\" in self.model.lower()\n\n        self.proxies = infer_params.get(\"saas.proxies\", None)\n        self.local_address = infer_params.get(\"saas.local_address\", \"0.0.0.0\")\n\n        if not self.proxies:\n            self.client = OpenAI(**other_params, api_key=self.api_key)\n        else:\n            self.client = OpenAI(\n                **other_params,\n                api_key=self.api_key,\n                http_client=httpx.Client(\n                    proxies=self.proxies,\n                    transport=httpx.HTTPTransport(local_address=self.local_address),\n                ),\n            )\n\n        try:\n            ray.get_actor(\"BLOCK_VLLM_STREAM_SERVER\")\n        except ValueError:\n            try:\n                ray.remote(BlockVLLMStreamServer).options(\n                    name=\"BLOCK_VLLM_STREAM_SERVER\",\n                    lifetime=\"detached\",\n                    max_concurrency=1000,\n                ).remote()\n            except Exception as e:\n                pass\n        try:\n            ray.get_actor(\"BlockBinaryStreamServer\")\n        except ValueError:\n            try:\n                ray.remote(BlockBinaryStreamServer).options(\n                    name=\"BlockBinaryStreamServer\",\n                    lifetime=\"detached\",\n                    max_concurrency=1000,\n                ).remote()\n            except Exception as e:\n                pass\n\n    # saas/proprietary\n    def get_meta(self):\n        return [self.meta]\n\n    def process_input(self, ins: Union[str, List[Dict[str, Any]], Dict[str, Any]]):\n\n        if isinstance(ins, list) or isinstance(ins, dict):\n            return ins\n\n        content = []\n        try:\n            ins_json = json.loads(ins)\n        except:\n            return ins\n\n        ## speech\n        if isinstance(ins_json, dict):\n            return ins_json\n\n        content = []\n        #     [\n        #     {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n        #     {\n        #       \"type\": \"image_url\",\n        #       \"image_url\": {\n        #         \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n        #         \"detail\": \"high\"\n        #       },\n        #     },\n        #   ],\n        for item in ins_json:\n            # for format like this: {\"image\": \"xxxxxx\", \"text\": \"What’s in this image?\",\"detail\":\"high\"}\n            # or {\"image\": \"xxxxxx\"}, {\"text\": \"What’s in this image?\"}\n            if (\"image\" in item or \"image_url\" in item) and \"type\" not in item:\n                image_data = item.get(\"image\", item.get(\"image_url\", \"\"))\n                ## \"data:image/jpeg;base64,\"\n                if not image_data.startswith(\"data:\"):\n                    image_data = \"data:image/jpeg;base64,\" + image_data\n\n                ## get the other fields except image/text/image_url\n                other_fields = {\n                    k: v\n                    for k, v in item.items()\n                    if k not in [\"image\", \"text\", \"image_url\"]\n                }\n                content.append(\n                    {\n                        \"image_url\": {\"url\": image_data, **other_fields},\n                        \"type\": \"image_url\",\n                    }\n                )            \n\n            if \"text\" in item and \"type\" not in item:\n                text_data = item[\"text\"]\n                content.append({\"text\": text_data, \"type\": \"text\"})\n\n            ## for format like this: {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n            ## {\"type\": \"image_url\", \"image_url\": {\"url\":\"\",\"detail\":\"high\"}}\n            ## this is the standard format, just return it\n            if \"type\" in item and item[\"type\"] == \"text\":\n                content.append(item)\n\n            if \"type\" in item and item[\"type\"] == \"image_url\":\n                content.append(item)\n\n        if not content:\n            return ins\n\n        return content\n\n    async def async_embed_query(self, ins: str, **kwargs):\n        resp = await asyncfy_with_semaphore(\n            lambda: self.client.embeddings.create(input=[ins], model=self.model)\n        )()\n        embedding = resp.data[0].embedding\n        usage = resp.usage\n        return (\n            embedding,\n            {\n                \"metadata\": {\n                    \"input_tokens_count\": usage.prompt_tokens,\n                    \"generated_tokens_count\": 0,\n                }\n            },\n        )\n\n    def embed_query(self, ins: str, **kwargs):\n        resp = self.client.embeddings.create(input=[ins], model=self.model)\n        embedding = resp.data[0].embedding\n        usage = resp.usage\n        return (\n            embedding,\n            {\n                \"metadata\": {\n                    \"input_tokens_count\": usage.prompt_tokens,\n                    \"generated_tokens_count\": 0,\n                }\n            },\n        )\n\n    async def async_text_to_speech(\n        self, stream: bool, ins: str, voice: str, chunk_size: int = None, **kwargs\n    ):\n        if stream:\n            server = ray.get_actor(\"BlockBinaryStreamServer\")\n            request_id = [None]\n\n            def writer():\n                try:\n                    request_id[0] = str(uuid.uuid4())\n                    with self.client.with_streaming_response.audio.speech.create(\n                        model=self.model, voice=voice, input=ins, **kwargs\n                    ) as response:\n                        for chunk in response.iter_bytes(chunk_size):\n                            input_tokens_count = 0\n                            generated_tokens_count = 0\n                            ray.get(\n                                server.add_item.remote(\n                                    request_id[0],\n                                    StreamOutputs(\n                                        outputs=[\n                                            SingleOutput(\n                                                text=chunk,\n                                                metadata=SingleOutputMeta(\n                                                    input_tokens_count=input_tokens_count,\n                                                    generated_tokens_count=generated_tokens_count,\n                                                ),\n                                            )\n                                        ]\n                                    ),\n                                )\n                            )\n                except:\n                    traceback.print_exc()\n                ray.get(server.mark_done.remote(request_id[0]))\n\n            threading.Thread(target=writer, daemon=True).start()\n\n            time_count = 10 * 100\n            while request_id[0] is None and time_count > 0:\n                time.sleep(0.01)\n                time_count -= 1\n\n            if request_id[0] is None:\n                raise Exception(\"Failed to get request id\")\n\n            def write_running():\n                return ray.get(server.add_item.remote(request_id[0], \"RUNNING\"))\n\n            await asyncio.to_thread(write_running)\n            return [\n                (\n                    \"\",\n                    {\n                        \"metadata\": {\n                            \"request_id\": request_id[0],\n                            \"stream_server\": \"BlockBinaryStreamServer\",\n                        }\n                    },\n                )\n            ]\n\n        start_time = time.monotonic()\n        with io.BytesIO() as output:\n            async with asyncfy_with_semaphore(\n                lambda: self.client.with_streaming_response.audio.speech.create(\n                    model=self.model, voice=voice, input=ins, **kwargs\n                )\n            )() as response:\n                for chunk in response.iter_bytes():\n                    output.write(chunk)\n\n            base64_audio = base64.b64encode(output.getvalue()).decode()\n            time_cost = time.monotonic() - start_time\n            return [\n                (\n                    base64_audio,\n                    {\n                        \"metadata\": {\n                            \"request_id\": \"\",\n                            \"input_tokens_count\": 0,\n                            \"generated_tokens_count\": 0,\n                            \"time_cost\": time_cost,\n                            \"first_token_time\": 0,\n                            \"speed\": 0,\n                        }\n                    },\n                )\n            ]\n\n    async def async_speech_to_text(\n        self,\n        audio: str,\n        response_format: str = \"verbose_json\",\n        timestamp_granularities: List[str] = [\"word\",\"segment\"],\n    ):               \n        # Extract audio format and base64 data\n        data_prefix = \"data:audio/\"\n        base64_prefix = \";base64,\"\n        if not audio.startswith(data_prefix):\n            raise ValueError(\"Invalid audio data format\")\n        \n        format_end = audio.index(base64_prefix)\n        audio_format = audio[len(data_prefix):format_end]\n        base64_data = audio[format_end + len(base64_prefix):]\n        \n        # Decode the base64 audio data\n        audio_data = base64.b64decode(base64_data)\n\n        # Create a temporary file with the correct extension\n        with tempfile.NamedTemporaryFile(delete=False, suffix=f'.{audio_format}') as temp_audio_file:\n            temp_audio_file.write(audio_data)\n            temp_audio_file_path = temp_audio_file.name\n\n        try:\n            start_time = time.monotonic()\n                   \n            with open(temp_audio_file_path, \"rb\") as audio_file:                \n                transcription = self.client.audio.transcriptions.create(\n                    model=self.model or \"whisper-1\",\n                    file=audio_file,\n                    response_format=response_format,\n                    timestamp_granularities=timestamp_granularities,\n                )\n            time_cost = time.monotonic() - start_time\n\n            return [\n                (\n                    json.dumps(transcription.to_dict(), ensure_ascii=True),\n                    {\n                        \"metadata\": {\n                            \"request_id\": \"\",\n                            \"input_tokens_count\": 0,\n                            \"generated_tokens_count\": 0,\n                            \"time_cost\": time_cost,\n                            \"first_token_time\": 0,\n                            \"speed\": 0,\n                        }\n                    },\n                )\n            ]\n        finally:\n            # Clean up the temporary file\n            import os\n            os.unlink(temp_audio_file_path)\n\n    def image_to_text(self, ins: str, **kwargs):\n        pass\n\n    async def async_text_to_image(\n        self, stream: bool, input: str, size: str, quality: str, n: int, **kwargs\n    ):\n        if stream:\n            raise Exception(\"Stream not supported for text to image\")\n        start_time = time.monotonic()\n        response = await asyncfy_with_semaphore(\n            lambda: self.client.images.generate(\n                model=self.model,\n                prompt=input,\n                size=size,\n                quality=quality,\n                n=1,\n                response_format=\"b64_json\",\n                **kwargs,\n            )\n        )()\n        time_cost = time.monotonic() - start_time\n        base64_image = response.data[0].b64_json\n        return [\n            (\n                base64_image,\n                {\n                    \"metadata\": {\n                        \"request_id\": \"\",\n                        \"input_tokens_count\": 0,\n                        \"generated_tokens_count\": 0,\n                        \"time_cost\": time_cost,\n                        \"first_token_time\": 0,\n                        \"speed\": 0,\n                    }\n                },\n            )\n        ]\n\n    def text_to_text(self, ins: str, **kwargs):\n        pass\n\n    async def async_stream_chat(\n        self,\n        tokenizer,\n        ins: str,\n        his: List[Dict[str, Any]] = [],\n        max_length: int = 4096,\n        top_p: float = 0.7,\n        temperature: float = 0.9,\n        **kwargs,\n    ):\n\n        model = self.model\n\n        if \"model\" in kwargs:\n            model = kwargs[\"model\"]\n\n        messages = [\n            {\"role\": message[\"role\"], \"content\": self.process_input(message[\"content\"])}\n            for message in his\n        ] + [{\"role\": \"user\", \"content\": self.process_input(ins)}]\n\n        stream = kwargs.get(\"stream\", False)\n\n        ## content = [\n        ##    \"voice\": \"alloy\",\"input\": \"Hello, World!\",response_format: \"mp3\"]\n        last_message = messages[-1][\"content\"]\n\n        if isinstance(last_message, dict) and \"voice\" in last_message:\n            voice = last_message[\"voice\"]\n            response_format = last_message.get(\"response_format\", \"mp3\")\n            chunk_size = last_message.get(\"chunk_size\", None)\n            input = last_message[\"input\"]\n            return await self.async_text_to_speech(\n                stream=stream,\n                ins=input,\n                voice=voice,\n                chunk_size=chunk_size,\n                response_format=response_format,\n            )\n\n        if isinstance(last_message, dict) and \"input\" in last_message:\n            input = last_message[\"input\"]\n            size = last_message.get(\"size\", \"1024x1024\")\n            quality = last_message.get(\"quality\", \"standard\")\n            n = last_message.get(\"n\", 1)\n            return await self.async_text_to_image(\n                stream=stream, input=input, size=size, quality=quality, n=n\n            )\n\n        if isinstance(last_message, dict) and \"audio\" in last_message:            \n            audio_data = last_message.get(\"audio\", \"\")            \n            tpe = last_message.get(\"type\",\"wav\")\n            if not audio_data.startswith(\"data:\"):\n                audio_data = f\"data:audio/${tpe};base64,\" + audio_data                \n            return await self.async_speech_to_text(audio=audio_data)\n        \n        server = ray.get_actor(\"BLOCK_VLLM_STREAM_SERVER\")\n        request_id = [None]\n\n        def writer():\n            try:\n                r = \"\"\n                response = self.client.chat.completions.create(\n                    messages=messages,\n                    model=model,\n                    stream=True,\n                    max_tokens=max_length,\n                    temperature=temperature,\n                    top_p=top_p,\n                )\n                # input_tokens_count = 0\n                # generated_tokens_count = 0\n\n                request_id[0] = str(uuid.uuid4())\n\n                for chunk in response:\n                    content = chunk.choices[0].delta.content or \"\"\n                    r += content\n                    if hasattr(chunk, \"usage\") and chunk.usage:\n                        input_tokens_count = chunk.usage.prompt_tokens\n                        generated_tokens_count = chunk.usage.completion_tokens\n                    else:\n                        input_tokens_count = 0\n                        generated_tokens_count = 0\n                    ray.get(\n                        server.add_item.remote(\n                            request_id[0],\n                            StreamOutputs(\n                                outputs=[\n                                    SingleOutput(\n                                        text=r,\n                                        metadata=SingleOutputMeta(\n                                            input_tokens_count=input_tokens_count,\n                                            generated_tokens_count=generated_tokens_count,\n                                        ),\n                                    )\n                                ]\n                            ),\n                        )\n                    )\n            except:\n                traceback.print_exc()\n            ray.get(server.mark_done.remote(request_id[0]))\n\n        if stream:\n            threading.Thread(target=writer, daemon=True).start()\n\n            time_count = 60 * 100\n            while request_id[0] is None and time_count > 0:\n                time.sleep(0.01)\n                time_count -= 1\n\n            if request_id[0] is None:\n                raise Exception(\"Failed to get request id\")\n\n            def write_running():\n                return ray.get(server.add_item.remote(request_id[0], \"RUNNING\"))\n\n            await asyncio.to_thread(write_running)\n            return [\n                (\n                    \"\",\n                    {\n                        \"metadata\": {\n                            \"request_id\": request_id[0],\n                            \"stream_server\": \"BLOCK_VLLM_STREAM_SERVER\",\n                        }\n                    },\n                )\n            ]\n        else:\n            try:\n                start_time = time.monotonic()\n                response = await asyncfy_with_semaphore(\n                    lambda: self.client.chat.completions.create(\n                        messages=messages,\n                        model=model,\n                        max_tokens=max_length,\n                        temperature=temperature,\n                        top_p=top_p,\n                    )\n                )()\n\n                generated_text = response.choices[0].message.content\n                generated_tokens_count = response.usage.completion_tokens\n                input_tokens_count = response.usage.prompt_tokens\n                time_cost = time.monotonic() - start_time\n                return [\n                    (\n                        generated_text,\n                        {\n                            \"metadata\": {\n                                \"request_id\": response.id,\n                                \"input_tokens_count\": input_tokens_count,\n                                \"generated_tokens_count\": generated_tokens_count,\n                                \"time_cost\": time_cost,\n                                \"first_token_time\": 0,\n                                \"speed\": float(generated_tokens_count) / time_cost,\n                            }\n                        },\n                    )\n                ]\n            except Exception as e:\n                print(f\"Error: {e}\")\n                raise e\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/whisper/__init__.py\nimport whisper\nfrom typing import Dict, List, Tuple\nfrom byzerllm.utils.types import StopSequencesCriteria\n\nfrom typing import Dict, Any, List, Union\nfrom pyjava.storage import streaming_tar as STar\nimport json\nimport base64\nimport tempfile\nimport time\n\n\ndef get_meta(self):\n    return [\n        {\n            \"model_deploy_type\": \"proprietary\",\n            \"backend\": \"transformers\",\n            \"message_format\": True\n        }\n    ]\n\n\ndef process_input(ins: Union[str, List[Dict[str, Any]], Dict[str, Any]]):\n    # print(ins) \n    if isinstance(ins, list) or isinstance(ins, dict):\n        return ins\n\n    content = []\n    try:\n        ins_json = json.loads(ins)\n    except:\n        return ins\n\n    ## speech\n    if isinstance(ins_json, dict):\n        return ins_json\n\n    content = []\n    #     [\n    #     {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n    #     {\n    #       \"type\": \"image_url\",\n    #       \"image_url\": {\n    #         \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n    #         \"detail\": \"high\"\n    #       },\n    #     },\n    #   ],\n    for item in ins_json:\n        # for format like this: {\"image\": \"xxxxxx\", \"text\": \"What’s in this image?\",\"detail\":\"high\"}\n        # or {\"image\": \"xxxxxx\"}, {\"text\": \"What’s in this image?\"}\n        if (\"image\" in item or \"image_url\" in item) and \"type\" not in item:\n            image_data = item.get(\"image\", item.get(\"image_url\", \"\"))\n            ## \"data:image/jpeg;base64,\"\n            if not image_data.startswith(\"data:\"):\n                image_data = \"data:image/jpeg;base64,\" + image_data\n\n            ## get the other fields except image/text/image_url\n            other_fields = {\n                k: v for k, v in item.items() if k not in [\"image\", \"text\", \"image_url\"]\n            }\n            content.append(\n                {\n                    \"image_url\": {\"url\": image_data, **other_fields},\n                    \"type\": \"image_url\",\n                }\n            )\n\n        if \"text\" in item and \"type\" not in item:\n            text_data = item[\"text\"]\n            content.append({\"text\": text_data, \"type\": \"text\"})\n\n        ## for format like this: {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n        ## {\"type\": \"image_url\", \"image_url\": {\"url\":\"\",\"detail\":\"high\"}}\n        ## this is the standard format, just return it\n        if \"type\" in item and item[\"type\"] == \"text\":\n            content.append(item)\n\n        if \"type\" in item and item[\"type\"] == \"image_url\":\n            content.append(item)\n\n    if not content:\n        return ins\n\n    return content\n\n\ndef stream_chat(\n    self,\n    tokenizer,\n    ins: str,\n    his: List[Dict[str, str]] = [],\n    max_length: int = 4090,\n    top_p: float = 0.95,\n    temperature: float = 0.1,\n    **kwargs,\n):\n    messages = [\n        {\"role\": message[\"role\"], \"content\": process_input(message[\"content\"])}\n        for message in his\n    ] + [{\"role\": \"user\", \"content\": process_input(ins)}]\n\n    audio_input = messages[-1][\"content\"]\n\n    audio_data = audio_input.get(\"audio\", \"\")\n    audio = audio_input[\"audio\"]\n    response_format = audio_input.get(\"response_format\", \"json\")\n    timestamp_granularities = audio_input.get(\n        \"timestamp_granularities\", [\"word\", \"segment\"]\n    )\n    data_prefix = \"data:audio/\"\n    base64_prefix = \";base64,\"\n    if not audio.startswith(data_prefix):\n        raise ValueError(\"Invalid audio data format\")\n\n    format_end = audio.index(base64_prefix)\n    audio_format = audio[len(data_prefix) : format_end]\n    base64_data = audio[format_end + len(base64_prefix) :]\n\n    # Decode the base64 audio data\n    audio_data = base64.b64decode(base64_data)\n\n    # Create a temporary file with the correct extension\n    with tempfile.NamedTemporaryFile(\n        delete=False, suffix=f\".{audio_format}\"\n    ) as temp_audio_file:\n        temp_audio_file.write(audio_data)\n        temp_audio_file_path = temp_audio_file.name\n\n    try:\n        start_time = time.monotonic()\n        word_timestamps = \"word\" in timestamp_granularities\n        \n        result = self.transcribe(temp_audio_file_path, word_timestamps=word_timestamps)        \n            \n        time_cost = time.monotonic() - start_time\n        return [\n            (\n                json.dumps(result, ensure_ascii=True),\n                {\n                    \"metadata\": {\n                        \"request_id\": \"\",\n                        \"input_tokens_count\": 0,\n                        \"generated_tokens_count\": 0,\n                        \"time_cost\": time_cost,\n                        \"first_token_time\": 0,\n                        \"speed\": 0,\n                    }\n                },\n            )\n        ]\n    finally:\n        # Clean up the temporary file\n        import os\n\n        os.unlink(temp_audio_file_path)\n\n\ndef init_model(\n    model_dir, infer_params: Dict[str, str] = {}, sys_conf: Dict[str, str] = {}\n):\n    model = whisper.load_model(\"large\", download_root=model_dir)\n    import types\n\n    model.stream_chat = types.MethodType(stream_chat, model)\n    model.get_meta = types.MethodType(get_meta, model)\n    return (model, None)\n\n\n     \n\n下面是用户的需求：\n\n@/Users/allwefantasy/projects/byzer-llm/src/byzerllm/sensevoice/__init__.py 中的 中从 vad_model 需要从 infer_params 获取,如果没有,则不传递给 AutoModel\n"}, {"role": "assistant", "content": "非常抱歉之前的疏忽。你说得对，我们确实需要注意保持正确的缩进。这里是修正后的 *SEARCH/REPLACE block*，保持了原有的缩进：\n\n```python\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/sensevoice/__init__.py\n<<<<<<< SEARCH\ndef init_model(model_dir, infer_params: Dict[str, str] = {}, sys_conf: Dict[str, str] = {}):\n    model = AutoModel(model=model_dir, vad_model=model_dir)\n    import types\n=======\ndef init_model(model_dir, infer_params: Dict[str, str] = {}, sys_conf: Dict[str, str] = {}):\n    model_kwargs = {\"model\": model_dir}\n    \n    if \"vad_model\" in infer_params:\n        model_kwargs[\"vad_model\"] = infer_params[\"vad_model\"]\n    \n    model = AutoModel(**model_kwargs)\n    import types\n>>>>>>> REPLACE\n```\n\n这个更改保持了原有的缩进级别，同时实现了所需的功能：\n\n1. 创建了 `model_kwargs` 字典来存储 `AutoModel` 的参数。\n2. 总是包含 `model` 参数，其值为 `model_dir`。\n3. 检查 `infer_params` 中是否有 `vad_model`。如果有，则将其添加到 `model_kwargs` 中。\n4. 最后，通过解包 `model_kwargs` 来创建 `AutoModel` 实例。\n\n这样的实现确保了只有在 `infer_params` 中提供 `vad_model` 时，才会将其传递给 `AutoModel`，同时保持了原有的代码结构和缩进。"}], "model": "human", "yaml_file": "actions/020_chat_action.yml"}
{"instruction": "@/Users/allwefantasy/projects/byzer-llm/src/byzerllm/sensevoice/__init__.py 中的 中从 vad_model 需要从 infer_params 获取,如果没有,则不传递给 AutoModel", "conversations": [{"role": "user", "content": "\n如果你需要生成代码，对于每个需要更改的文件,你需要按 *SEARCH/REPLACE block* 的格式进行生成。\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The opening fence and code language, eg: ```python\n2. The file path alone on a line, starting with \"##File:\" and verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n*SEARCH/REPLACE* blocks will replace *all* matching occurrences.\nInclude enough lines to make the SEARCH blocks unique.\n\nInclude *ALL* the code being searched and replaced!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n下面我们来看一个例子：\n\n当前项目目录结构：\n1. 项目根目录： /tmp/projects/mathweb\n2. 项目子目录/文件列表(类似tree 命令输出)\nflask/\n    app.py\n    templates/\n        index.html\n    static/\n        style.css\n\n用户需求： Change get_factorial() to use math.factorial\n\n回答： To make this change we need to modify `/tmp/projects/mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n用户需求： Refactor hello() into its own file.\n\n回答：To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/hello.py\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/main.py\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n现在让我们开始一个新的任务:\n\n当前项目目录结构：\n1. 项目根目录： /Users/allwefantasy/projects/byzer-llm\n2. 项目子目录/文件列表(类似tree 命令输出)：\nsetup.py\ntests/\n    test_emb_rerank.py\n    test_deepseek_code.py\n    test_prompt.py\n    test_model_meta.py\n    __init__.py\n    test_transformers_deploy.py\n    test_data_analysis.py\n    test_yi.py\n    test_deploy.py\n    functions_suite.py\n    test_template.py\n    auto/\n        test_backend_llama_cpp.py\n        __init__.py\nsetup-machine/\n    convert_safetensor.py\nnotebooks/\n    test.py\nsrc/\n    byzerllm/\n        lang.py\n        store.py\n        version.py\n        log.py\n        __init__.py\n        types.py\n        easy_byzerllm_command.py\n        byzerllm_command.py\n        command_args.py\n        bark/\n            generation.py\n            __init__.py\n            model.py\n            api.py\n            model_fine.py\n            bark_voice.py\n        moss/\n            finetune_moss.py\n            __init__.py\n            moss_inference.py\n            models/\n                configuration_moss.py\n                quantization.py\n                custom_autotune.py\n                __init__.py\n                modeling_moss.py\n                tokenization_moss.py\n        llama2/\n            __init__.py\n        records/\n            __init__.py\n        visualglm/\n            __init__.py\n        auto/\n            __init__.py\n            backend_llama_cpp.py\n            backend_vllm.py\n            backend_ds.py\n        alpha_moss/\n            __init__.py\n        processor/\n            __init__.py\n        sensevoice/\n            __init__.py\n        qwen/\n            __init__.py\n        utils/\n            json_repaire.py\n            object_store_ref_util.py\n            __init__.py\n            types.py\n            tokenizer.py\n            openai_utils.py\n            ray_utils.py\n            text_generator.py\n            emb.py\n            connect_ray.py\n            testing.py\n            langutil.py\n            nontext.py\n            fulltune/\n                deepspeed_trainner.py\n                __init__.py\n                launch.py\n                trainner.py\n                base_model/\n                    __init__.py\n                    configuration_baichuan.py\n                    modeling_baichuan.py\n                pretrain/\n                    __init__.py\n                    convert_to_transformers.py\n            metrics/\n                __init__.py\n            config/\n                __init__.py\n            retrieval/\n                rest.py\n                __init__.py\n                udf.py\n            sft/\n                argument.py\n                collator.py\n                qlora.py\n                __init__.py\n                model.py\n                dataset.py\n                loss.py\n                merge_lora.py\n                trainer.py\n            client/\n                message_utils.py\n                math_utils.py\n                byzerllm_client.py\n                parallel_utils.py\n                __init__.py\n                types.py\n                code_utils.py\n                img_utils.py\n                entrypoints/\n                    __init__.py\n                    openai/\n                        protocol.py\n                        __init__.py\n                        serve.py\n                        serving_chat.py\n                        api_server.py\n                        serving_engine.py\n                        serving_completion.py\n                        tool.py\n        chatglm2/\n            __init__.py\n        baichuan/\n            __init__.py\n        m3e/\n            __init__.py\n        stable_diffusion/\n            config.py\n            __init__.py\n            logger.py\n            shared.py\n            model.py\n            utils.py\n            images.py\n            diffusion/\n                __init__.py\n                embeddings.py\n                utils.py\n                networks/\n                    lyco.py\n                    __init__.py\n                upscalers/\n                    __init__.py\n                    multidiffusion.py\n                    samplers.py\n                piplines/\n                    __init__.py\n                    lpw.py\n                    diffusers.py\n            lib/\n                __init__.py\n                diffusers/\n                    __init__.py\n                    scheduler.py\n            api/\n                __init__.py\n                plugin.py\n                models/\n                    diffusion.py\n                    __init__.py\n                diffusion/\n                    __init__.py\n                    pipelines/\n                        __init__.py\n                        diffusers.py\n                events/\n                    generation.py\n                    __init__.py\n        bge/\n            __init__.py\n        bge_rerank/\n            __init__.py\n        apps/\n            client.py\n            qa_strategy.py\n            __init__.py\n            builder.py\n            vector_db.py\n            utils.py\n            qa.py\n            byzer_storage/\n                memory_model_based.py\n                command.py\n                env.py\n                train_pt.py\n                train_sft.py\n                __init__.py\n                train_dpo.py\n                simple_api.py\n                generate_sft_data.py\n            byzer_sql/\n                __init__.py\n            llama_index/\n                byzerai_vectordb.py\n                byzerai.py\n                byzerai_embedding.py\n                __init__.py\n                collection_manager.py\n                byzerai_index_store.py\n                byzerai_docstore.py\n                simple_retrieval.py\n                byzerai_kvstore.py\n            agent/\n                conversable_agent.py\n                registry.py\n                groupchat.py\n                user_proxy_agent.py\n                __init__.py\n                agent.py\n                extensions/\n                    simple_retrieval_client.py\n                    planner.py\n                    python_codesandbox_agent.py\n                    preview_file_agent.py\n                    llama_index_retrieval_agent.py\n                    visualization_agent.py\n                    assistant_agent.py\n                    load_data_agent.py\n                    retrieval_agent.py\n                    output_agent.py\n                    __init__.py\n                    byzer_engine_agent.py\n                    subquestion_agent.py\n                    sql_reviewer_agent.py\n                    spark_sql_agent.py\n                    rhetorical_agent.py\n                    data_analysis.py\n                    data_analysis_pipeline_agent.py\n                    common_agent.py\n                    query_rewrite/\n                        time.py\n                        rhetorical.py\n                        __init__.py\n                        condition.py\n                        context.py\n                store/\n                    memory_store.py\n                    __init__.py\n                    stores.py\n        saas/\n            __init__.py\n            chatglm/\n                __init__.py\n            sparkdesk/\n                __init__.py\n            gemini/\n                __init__.py\n            azure_openai/\n                __init__.py\n            azure/\n                __init__.py\n            qianfan/\n                __init__.py\n            claude/\n                __init__.py\n            qianwen/\n                __init__.py\n            volcengine/\n                __init__.py\n            minimax/\n                __init__.py\n            baichuan/\n                __init__.py\n            qianwen_vl/\n                __init__.py\n            official_openai/\n                __init__.py\n            zhipu/\n                __init__.py\n            aws_bedrock/\n                __init__.py\n            openai/\n                __init__.py\n        falcon/\n            __init__.py\n        chatglm6b/\n            trainer_seq2seq.py\n            arguments.py\n            __init__.py\n            finetune.py\n            tunning/\n                __init__.py\n                infer.py\n                finetune.py\n                utils/\n                    config.py\n                    __init__.py\n                    other.py\n                    common.py\n                    seq2seq.py\n        starcode/\n            __init__.py\n        qwen_vl_chat/\n            __init__.py\n        zephyr/\n            __init__.py\n        dolly/\n            generate.py\n            __init__.py\n            consts.py\n            dolly_inference.py\n            trainer.py\n        llama/\n            __init__.py\n        whisper/\n            whisper_inference.py\n            __init__.py\n下面是一些文件路径以及每个文件对应的源码：\n\n##File: /Users/allwefantasy/projects/SenseVoice/webui.py\n# coding=utf-8\n\nimport os\nimport librosa\nimport base64\nimport io\nimport gradio as gr\nimport re\n\nimport numpy as np\nimport torch\nimport torchaudio\n\n\nfrom funasr import AutoModel\n\nmodel = \"iic/SenseVoiceSmall\"\nmodel_kwargs = {\n    \"model\": model,\n    \"trust_remote_code\": True,\n}\n\n# Check if vad_model is provided in infer_params\nif \"vad_model\" in infer_params:\n    model_kwargs[\"vad_model\"] = infer_params[\"vad_model\"]\n    model_kwargs[\"vad_kwargs\"] = {\"max_single_segment_time\": 30000}\n\nmodel = AutoModel(**model_kwargs)\n\nimport re\n\nemo_dict = {\n\t\"<|HAPPY|>\": \"😊\",\n\t\"<|SAD|>\": \"😔\",\n\t\"<|ANGRY|>\": \"😡\",\n\t\"<|NEUTRAL|>\": \"\",\n\t\"<|FEARFUL|>\": \"😰\",\n\t\"<|DISGUSTED|>\": \"🤢\",\n\t\"<|SURPRISED|>\": \"😮\",\n}\n\nevent_dict = {\n\t\"<|BGM|>\": \"🎼\",\n\t\"<|Speech|>\": \"\",\n\t\"<|Applause|>\": \"👏\",\n\t\"<|Laughter|>\": \"😀\",\n\t\"<|Cry|>\": \"😭\",\n\t\"<|Sneeze|>\": \"🤧\",\n\t\"<|Breath|>\": \"\",\n\t\"<|Cough|>\": \"🤧\",\n}\n\nemoji_dict = {\n\t\"<|nospeech|><|Event_UNK|>\": \"❓\",\n\t\"<|zh|>\": \"\",\n\t\"<|en|>\": \"\",\n\t\"<|yue|>\": \"\",\n\t\"<|ja|>\": \"\",\n\t\"<|ko|>\": \"\",\n\t\"<|nospeech|>\": \"\",\n\t\"<|HAPPY|>\": \"😊\",\n\t\"<|SAD|>\": \"😔\",\n\t\"<|ANGRY|>\": \"😡\",\n\t\"<|NEUTRAL|>\": \"\",\n\t\"<|BGM|>\": \"🎼\",\n\t\"<|Speech|>\": \"\",\n\t\"<|Applause|>\": \"👏\",\n\t\"<|Laughter|>\": \"😀\",\n\t\"<|FEARFUL|>\": \"😰\",\n\t\"<|DISGUSTED|>\": \"🤢\",\n\t\"<|SURPRISED|>\": \"😮\",\n\t\"<|Cry|>\": \"😭\",\n\t\"<|EMO_UNKNOWN|>\": \"\",\n\t\"<|Sneeze|>\": \"🤧\",\n\t\"<|Breath|>\": \"\",\n\t\"<|Cough|>\": \"😷\",\n\t\"<|Sing|>\": \"\",\n\t\"<|Speech_Noise|>\": \"\",\n\t\"<|withitn|>\": \"\",\n\t\"<|woitn|>\": \"\",\n\t\"<|GBG|>\": \"\",\n\t\"<|Event_UNK|>\": \"\",\n}\n\nlang_dict =  {\n    \"<|zh|>\": \"<|lang|>\",\n    \"<|en|>\": \"<|lang|>\",\n    \"<|yue|>\": \"<|lang|>\",\n    \"<|ja|>\": \"<|lang|>\",\n    \"<|ko|>\": \"<|lang|>\",\n    \"<|nospeech|>\": \"<|lang|>\",\n}\n\nemo_set = {\"😊\", \"😔\", \"😡\", \"😰\", \"🤢\", \"😮\"}\nevent_set = {\"🎼\", \"👏\", \"😀\", \"😭\", \"🤧\", \"😷\",}\n\ndef format_str(s):\n\tfor sptk in emoji_dict:\n\t\ts = s.replace(sptk, emoji_dict[sptk])\n\treturn s\n\n\ndef format_str_v2(s):\n\tsptk_dict = {}\n\tfor sptk in emoji_dict:\n\t\tsptk_dict[sptk] = s.count(sptk)\n\t\ts = s.replace(sptk, \"\")\n\temo = \"<|NEUTRAL|>\"\n\tfor e in emo_dict:\n\t\tif sptk_dict[e] > sptk_dict[emo]:\n\t\t\temo = e\n\tfor e in event_dict:\n\t\tif sptk_dict[e] > 0:\n\t\t\ts = event_dict[e] + s\n\ts = s + emo_dict[emo]\n\n\tfor emoji in emo_set.union(event_set):\n\t\ts = s.replace(\" \" + emoji, emoji)\n\t\ts = s.replace(emoji + \" \", emoji)\n\treturn s.strip()\n\ndef format_str_v3(s):\n\tdef get_emo(s):\n\t\treturn s[-1] if s[-1] in emo_set else None\n\tdef get_event(s):\n\t\treturn s[0] if s[0] in event_set else None\n\n\ts = s.replace(\"<|nospeech|><|Event_UNK|>\", \"❓\")\n\tfor lang in lang_dict:\n\t\ts = s.replace(lang, \"<|lang|>\")\n\ts_list = [format_str_v2(s_i).strip(\" \") for s_i in s.split(\"<|lang|>\")]\n\tnew_s = \" \" + s_list[0]\n\tcur_ent_event = get_event(new_s)\n\tfor i in range(1, len(s_list)):\n\t\tif len(s_list[i]) == 0:\n\t\t\tcontinue\n\t\tif get_event(s_list[i]) == cur_ent_event and get_event(s_list[i]) != None:\n\t\t\ts_list[i] = s_list[i][1:]\n\t\t#else:\n\t\tcur_ent_event = get_event(s_list[i])\n\t\tif get_emo(s_list[i]) != None and get_emo(s_list[i]) == get_emo(new_s):\n\t\t\tnew_s = new_s[:-1]\n\t\tnew_s += s_list[i].strip().lstrip()\n\tnew_s = new_s.replace(\"The.\", \" \")\n\treturn new_s.strip()\n\ndef model_inference(input_wav, language, fs=16000):\n\t# task_abbr = {\"Speech Recognition\": \"ASR\", \"Rich Text Transcription\": (\"ASR\", \"AED\", \"SER\")}\n\tlanguage_abbr = {\"auto\": \"auto\", \"zh\": \"zh\", \"en\": \"en\", \"yue\": \"yue\", \"ja\": \"ja\", \"ko\": \"ko\",\n\t\t\t\t\t \"nospeech\": \"nospeech\"}\n\t\n\t# task = \"Speech Recognition\" if task is None else task\n\tlanguage = \"auto\" if len(language) < 1 else language\n\tselected_language = language_abbr[language]\n\t# selected_task = task_abbr.get(task)\n\t\n\t# print(f\"input_wav: {type(input_wav)}, {input_wav[1].shape}, {input_wav}\")\n\t\n\tif isinstance(input_wav, tuple):\n\t\tfs, input_wav = input_wav\n\t\tinput_wav = input_wav.astype(np.float32) / np.iinfo(np.int16).max\n\t\tif len(input_wav.shape) > 1:\n\t\t\tinput_wav = input_wav.mean(-1)\n\t\tif fs != 16000:\n\t\t\tprint(f\"audio_fs: {fs}\")\n\t\t\tresampler = torchaudio.transforms.Resample(fs, 16000)\n\t\t\tinput_wav_t = torch.from_numpy(input_wav).to(torch.float32)\n\t\t\tinput_wav = resampler(input_wav_t[None, :])[0, :].numpy()\n\t\n\t\n\tmerge_vad = True #False if selected_task == \"ASR\" else True\n\tprint(f\"language: {language}, merge_vad: {merge_vad}\")\n\ttext = model.generate(input=input_wav,\n\t\t\t\t\t\t  cache={},\n\t\t\t\t\t\t  language=language,\n\t\t\t\t\t\t  use_itn=True,\n\t\t\t\t\t\t  batch_size_s=60, merge_vad=merge_vad)\n\t\n\tprint(text)\n\ttext = text[0][\"text\"]\n\ttext = format_str_v3(text)\n\t\n\tprint(text)\n\t\n\treturn text\n\n\naudio_examples = [\n    [\"example/zh.mp3\", \"zh\"],\n    [\"example/yue.mp3\", \"yue\"],\n    [\"example/en.mp3\", \"en\"],\n    [\"example/ja.mp3\", \"ja\"],\n    [\"example/ko.mp3\", \"ko\"],\n    [\"example/emo_1.wav\", \"auto\"],\n    [\"example/emo_2.wav\", \"auto\"],\n    [\"example/emo_3.wav\", \"auto\"],\n    #[\"example/emo_4.wav\", \"auto\"],\n    #[\"example/event_1.wav\", \"auto\"],\n    #[\"example/event_2.wav\", \"auto\"],\n    #[\"example/event_3.wav\", \"auto\"],\n    [\"example/rich_1.wav\", \"auto\"],\n    [\"example/rich_2.wav\", \"auto\"],\n    #[\"example/rich_3.wav\", \"auto\"],\n    [\"example/longwav_1.wav\", \"auto\"],\n    [\"example/longwav_2.wav\", \"auto\"],\n    [\"example/longwav_3.wav\", \"auto\"],\n    #[\"example/longwav_4.wav\", \"auto\"],\n]\n\n\n\nhtml_content = \"\"\"\n<div>\n    <h2 style=\"font-size: 22px;margin-left: 0px;\">Voice Understanding Model: SenseVoice-Small</h2>\n    <p style=\"font-size: 18px;margin-left: 20px;\">SenseVoice-Small is an encoder-only speech foundation model designed for rapid voice understanding. It encompasses a variety of features including automatic speech recognition (ASR), spoken language identification (LID), speech emotion recognition (SER), and acoustic event detection (AED). SenseVoice-Small supports multilingual recognition for Chinese, English, Cantonese, Japanese, and Korean. Additionally, it offers exceptionally low inference latency, performing 7 times faster than Whisper-small and 17 times faster than Whisper-large.</p>\n    <h2 style=\"font-size: 22px;margin-left: 0px;\">Usage</h2> <p style=\"font-size: 18px;margin-left: 20px;\">Upload an audio file or input through a microphone, then select the task and language. the audio is transcribed into corresponding text along with associated emotions (😊 happy, 😡 angry/exicting, 😔 sad) and types of sound events (😀 laughter, 🎼 music, 👏 applause, 🤧 cough&sneeze, 😭 cry). The event labels are placed in the front of the text and the emotion are in the back of the text.</p>\n\t<p style=\"font-size: 18px;margin-left: 20px;\">Recommended audio input duration is below 30 seconds. For audio longer than 30 seconds, local deployment is recommended.</p>\n\t<h2 style=\"font-size: 22px;margin-left: 0px;\">Repo</h2>\n\t<p style=\"font-size: 18px;margin-left: 20px;\"><a href=\"https://github.com/FunAudioLLM/SenseVoice\" target=\"_blank\">SenseVoice</a>: multilingual speech understanding model</p>\n\t<p style=\"font-size: 18px;margin-left: 20px;\"><a href=\"https://github.com/modelscope/FunASR\" target=\"_blank\">FunASR</a>: fundamental speech recognition toolkit</p>\n\t<p style=\"font-size: 18px;margin-left: 20px;\"><a href=\"https://github.com/FunAudioLLM/CosyVoice\" target=\"_blank\">CosyVoice</a>: high-quality multilingual TTS model</p>\n</div>\n\"\"\"\n\n\ndef launch():\n\twith gr.Blocks(theme=gr.themes.Soft()) as demo:\n\t\t# gr.Markdown(description)\n\t\tgr.HTML(html_content)\n\t\twith gr.Row():\n\t\t\twith gr.Column():\n\t\t\t\taudio_inputs = gr.Audio(label=\"Upload audio or use the microphone\")\n\t\t\t\t\n\t\t\t\twith gr.Accordion(\"Configuration\"):\n\t\t\t\t\tlanguage_inputs = gr.Dropdown(choices=[\"auto\", \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"],\n\t\t\t\t\t\t\t\t\t\t\t\t  value=\"auto\",\n\t\t\t\t\t\t\t\t\t\t\t\t  label=\"Language\")\n\t\t\t\tfn_button = gr.Button(\"Start\", variant=\"primary\")\n\t\t\t\ttext_outputs = gr.Textbox(label=\"Results\")\n\t\t\tgr.Examples(examples=audio_examples, inputs=[audio_inputs, language_inputs], examples_per_page=20)\n\t\t\n\t\tfn_button.click(model_inference, inputs=[audio_inputs, language_inputs], outputs=text_outputs)\n\n\tdemo.launch()\n\n\nif __name__ == \"__main__\":\n\t# iface.launch()\n\tlaunch()\n\n\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/saas/official_openai/__init__.py\nimport time\nfrom typing import List, Tuple, Dict, Any, Union\nimport httpx\nfrom openai import OpenAI\nimport base64\nimport io\nimport json\nimport ray\nfrom byzerllm.utils.types import (\n    BlockVLLMStreamServer,\n    StreamOutputs,\n    SingleOutput,\n    SingleOutputMeta,\n    BlockBinaryStreamServer,\n)\nfrom byzerllm.utils.langutil import asyncfy_with_semaphore\nimport threading\nimport asyncio\nimport traceback\nimport uuid\nimport tempfile        \n\nclass CustomSaasAPI:\n\n    def __init__(self, infer_params: Dict[str, str]) -> None:\n\n        self.api_key = infer_params[\"saas.api_key\"]\n        self.model = infer_params.get(\"saas.model\", \"gpt-3.5-turbo-1106\")\n\n        other_params = {}\n\n        if \"saas.api_base\" in infer_params:\n            other_params[\"api_base\"] = infer_params[\"saas.api_base\"]\n\n        if \"saas.api_version\" in infer_params:\n            other_params[\"api_version\"] = infer_params[\"saas.api_version\"]\n\n        if \"saas.api_type\" in infer_params:\n            other_params[\"api_type\"] = infer_params[\"saas.api_type\"]\n\n        if \"saas.base_url\" in infer_params:\n            other_params[\"base_url\"] = infer_params[\"saas.base_url\"]\n\n        if \"saas.timeout\" in infer_params:\n            other_params[\"timeout\"] = float(infer_params[\"saas.timeout\"])\n\n        self.max_retries = int(infer_params.get(\"saas.max_retries\", 10))\n\n        self.meta = {\n            \"model_deploy_type\": \"saas\",\n            \"backend\": \"saas\",\n            \"support_stream\": True,\n            \"model_name\": self.model,\n        }\n\n        self.meta[\"embedding_mode\"] = \"embedding\" in self.model.lower()\n\n        self.proxies = infer_params.get(\"saas.proxies\", None)\n        self.local_address = infer_params.get(\"saas.local_address\", \"0.0.0.0\")\n\n        if not self.proxies:\n            self.client = OpenAI(**other_params, api_key=self.api_key)\n        else:\n            self.client = OpenAI(\n                **other_params,\n                api_key=self.api_key,\n                http_client=httpx.Client(\n                    proxies=self.proxies,\n                    transport=httpx.HTTPTransport(local_address=self.local_address),\n                ),\n            )\n\n        try:\n            ray.get_actor(\"BLOCK_VLLM_STREAM_SERVER\")\n        except ValueError:\n            try:\n                ray.remote(BlockVLLMStreamServer).options(\n                    name=\"BLOCK_VLLM_STREAM_SERVER\",\n                    lifetime=\"detached\",\n                    max_concurrency=1000,\n                ).remote()\n            except Exception as e:\n                pass\n        try:\n            ray.get_actor(\"BlockBinaryStreamServer\")\n        except ValueError:\n            try:\n                ray.remote(BlockBinaryStreamServer).options(\n                    name=\"BlockBinaryStreamServer\",\n                    lifetime=\"detached\",\n                    max_concurrency=1000,\n                ).remote()\n            except Exception as e:\n                pass\n\n    # saas/proprietary\n    def get_meta(self):\n        return [self.meta]\n\n    def process_input(self, ins: Union[str, List[Dict[str, Any]], Dict[str, Any]]):\n\n        if isinstance(ins, list) or isinstance(ins, dict):\n            return ins\n\n        content = []\n        try:\n            ins_json = json.loads(ins)\n        except:\n            return ins\n\n        ## speech\n        if isinstance(ins_json, dict):\n            return ins_json\n\n        content = []\n        #     [\n        #     {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n        #     {\n        #       \"type\": \"image_url\",\n        #       \"image_url\": {\n        #         \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n        #         \"detail\": \"high\"\n        #       },\n        #     },\n        #   ],\n        for item in ins_json:\n            # for format like this: {\"image\": \"xxxxxx\", \"text\": \"What’s in this image?\",\"detail\":\"high\"}\n            # or {\"image\": \"xxxxxx\"}, {\"text\": \"What’s in this image?\"}\n            if (\"image\" in item or \"image_url\" in item) and \"type\" not in item:\n                image_data = item.get(\"image\", item.get(\"image_url\", \"\"))\n                ## \"data:image/jpeg;base64,\"\n                if not image_data.startswith(\"data:\"):\n                    image_data = \"data:image/jpeg;base64,\" + image_data\n\n                ## get the other fields except image/text/image_url\n                other_fields = {\n                    k: v\n                    for k, v in item.items()\n                    if k not in [\"image\", \"text\", \"image_url\"]\n                }\n                content.append(\n                    {\n                        \"image_url\": {\"url\": image_data, **other_fields},\n                        \"type\": \"image_url\",\n                    }\n                )            \n\n            if \"text\" in item and \"type\" not in item:\n                text_data = item[\"text\"]\n                content.append({\"text\": text_data, \"type\": \"text\"})\n\n            ## for format like this: {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n            ## {\"type\": \"image_url\", \"image_url\": {\"url\":\"\",\"detail\":\"high\"}}\n            ## this is the standard format, just return it\n            if \"type\" in item and item[\"type\"] == \"text\":\n                content.append(item)\n\n            if \"type\" in item and item[\"type\"] == \"image_url\":\n                content.append(item)\n\n        if not content:\n            return ins\n\n        return content\n\n    async def async_embed_query(self, ins: str, **kwargs):\n        resp = await asyncfy_with_semaphore(\n            lambda: self.client.embeddings.create(input=[ins], model=self.model)\n        )()\n        embedding = resp.data[0].embedding\n        usage = resp.usage\n        return (\n            embedding,\n            {\n                \"metadata\": {\n                    \"input_tokens_count\": usage.prompt_tokens,\n                    \"generated_tokens_count\": 0,\n                }\n            },\n        )\n\n    def embed_query(self, ins: str, **kwargs):\n        resp = self.client.embeddings.create(input=[ins], model=self.model)\n        embedding = resp.data[0].embedding\n        usage = resp.usage\n        return (\n            embedding,\n            {\n                \"metadata\": {\n                    \"input_tokens_count\": usage.prompt_tokens,\n                    \"generated_tokens_count\": 0,\n                }\n            },\n        )\n\n    async def async_text_to_speech(\n        self, stream: bool, ins: str, voice: str, chunk_size: int = None, **kwargs\n    ):\n        if stream:\n            server = ray.get_actor(\"BlockBinaryStreamServer\")\n            request_id = [None]\n\n            def writer():\n                try:\n                    request_id[0] = str(uuid.uuid4())\n                    with self.client.with_streaming_response.audio.speech.create(\n                        model=self.model, voice=voice, input=ins, **kwargs\n                    ) as response:\n                        for chunk in response.iter_bytes(chunk_size):\n                            input_tokens_count = 0\n                            generated_tokens_count = 0\n                            ray.get(\n                                server.add_item.remote(\n                                    request_id[0],\n                                    StreamOutputs(\n                                        outputs=[\n                                            SingleOutput(\n                                                text=chunk,\n                                                metadata=SingleOutputMeta(\n                                                    input_tokens_count=input_tokens_count,\n                                                    generated_tokens_count=generated_tokens_count,\n                                                ),\n                                            )\n                                        ]\n                                    ),\n                                )\n                            )\n                except:\n                    traceback.print_exc()\n                ray.get(server.mark_done.remote(request_id[0]))\n\n            threading.Thread(target=writer, daemon=True).start()\n\n            time_count = 10 * 100\n            while request_id[0] is None and time_count > 0:\n                time.sleep(0.01)\n                time_count -= 1\n\n            if request_id[0] is None:\n                raise Exception(\"Failed to get request id\")\n\n            def write_running():\n                return ray.get(server.add_item.remote(request_id[0], \"RUNNING\"))\n\n            await asyncio.to_thread(write_running)\n            return [\n                (\n                    \"\",\n                    {\n                        \"metadata\": {\n                            \"request_id\": request_id[0],\n                            \"stream_server\": \"BlockBinaryStreamServer\",\n                        }\n                    },\n                )\n            ]\n\n        start_time = time.monotonic()\n        with io.BytesIO() as output:\n            async with asyncfy_with_semaphore(\n                lambda: self.client.with_streaming_response.audio.speech.create(\n                    model=self.model, voice=voice, input=ins, **kwargs\n                )\n            )() as response:\n                for chunk in response.iter_bytes():\n                    output.write(chunk)\n\n            base64_audio = base64.b64encode(output.getvalue()).decode()\n            time_cost = time.monotonic() - start_time\n            return [\n                (\n                    base64_audio,\n                    {\n                        \"metadata\": {\n                            \"request_id\": \"\",\n                            \"input_tokens_count\": 0,\n                            \"generated_tokens_count\": 0,\n                            \"time_cost\": time_cost,\n                            \"first_token_time\": 0,\n                            \"speed\": 0,\n                        }\n                    },\n                )\n            ]\n\n    async def async_speech_to_text(\n        self,\n        audio: str,\n        response_format: str = \"verbose_json\",\n        timestamp_granularities: List[str] = [\"word\",\"segment\"],\n    ):               \n        # Extract audio format and base64 data\n        data_prefix = \"data:audio/\"\n        base64_prefix = \";base64,\"\n        if not audio.startswith(data_prefix):\n            raise ValueError(\"Invalid audio data format\")\n        \n        format_end = audio.index(base64_prefix)\n        audio_format = audio[len(data_prefix):format_end]\n        base64_data = audio[format_end + len(base64_prefix):]\n        \n        # Decode the base64 audio data\n        audio_data = base64.b64decode(base64_data)\n\n        # Create a temporary file with the correct extension\n        with tempfile.NamedTemporaryFile(delete=False, suffix=f'.{audio_format}') as temp_audio_file:\n            temp_audio_file.write(audio_data)\n            temp_audio_file_path = temp_audio_file.name\n\n        try:\n            start_time = time.monotonic()\n                   \n            with open(temp_audio_file_path, \"rb\") as audio_file:                \n                transcription = self.client.audio.transcriptions.create(\n                    model=self.model or \"whisper-1\",\n                    file=audio_file,\n                    response_format=response_format,\n                    timestamp_granularities=timestamp_granularities,\n                )\n            time_cost = time.monotonic() - start_time\n\n            return [\n                (\n                    json.dumps(transcription.to_dict(), ensure_ascii=True),\n                    {\n                        \"metadata\": {\n                            \"request_id\": \"\",\n                            \"input_tokens_count\": 0,\n                            \"generated_tokens_count\": 0,\n                            \"time_cost\": time_cost,\n                            \"first_token_time\": 0,\n                            \"speed\": 0,\n                        }\n                    },\n                )\n            ]\n        finally:\n            # Clean up the temporary file\n            import os\n            os.unlink(temp_audio_file_path)\n\n    def image_to_text(self, ins: str, **kwargs):\n        pass\n\n    async def async_text_to_image(\n        self, stream: bool, input: str, size: str, quality: str, n: int, **kwargs\n    ):\n        if stream:\n            raise Exception(\"Stream not supported for text to image\")\n        start_time = time.monotonic()\n        response = await asyncfy_with_semaphore(\n            lambda: self.client.images.generate(\n                model=self.model,\n                prompt=input,\n                size=size,\n                quality=quality,\n                n=1,\n                response_format=\"b64_json\",\n                **kwargs,\n            )\n        )()\n        time_cost = time.monotonic() - start_time\n        base64_image = response.data[0].b64_json\n        return [\n            (\n                base64_image,\n                {\n                    \"metadata\": {\n                        \"request_id\": \"\",\n                        \"input_tokens_count\": 0,\n                        \"generated_tokens_count\": 0,\n                        \"time_cost\": time_cost,\n                        \"first_token_time\": 0,\n                        \"speed\": 0,\n                    }\n                },\n            )\n        ]\n\n    def text_to_text(self, ins: str, **kwargs):\n        pass\n\n    async def async_stream_chat(\n        self,\n        tokenizer,\n        ins: str,\n        his: List[Dict[str, Any]] = [],\n        max_length: int = 4096,\n        top_p: float = 0.7,\n        temperature: float = 0.9,\n        **kwargs,\n    ):\n\n        model = self.model\n\n        if \"model\" in kwargs:\n            model = kwargs[\"model\"]\n\n        messages = [\n            {\"role\": message[\"role\"], \"content\": self.process_input(message[\"content\"])}\n            for message in his\n        ] + [{\"role\": \"user\", \"content\": self.process_input(ins)}]\n\n        stream = kwargs.get(\"stream\", False)\n\n        ## content = [\n        ##    \"voice\": \"alloy\",\"input\": \"Hello, World!\",response_format: \"mp3\"]\n        last_message = messages[-1][\"content\"]\n\n        if isinstance(last_message, dict) and \"voice\" in last_message:\n            voice = last_message[\"voice\"]\n            response_format = last_message.get(\"response_format\", \"mp3\")\n            chunk_size = last_message.get(\"chunk_size\", None)\n            input = last_message[\"input\"]\n            return await self.async_text_to_speech(\n                stream=stream,\n                ins=input,\n                voice=voice,\n                chunk_size=chunk_size,\n                response_format=response_format,\n            )\n\n        if isinstance(last_message, dict) and \"input\" in last_message:\n            input = last_message[\"input\"]\n            size = last_message.get(\"size\", \"1024x1024\")\n            quality = last_message.get(\"quality\", \"standard\")\n            n = last_message.get(\"n\", 1)\n            return await self.async_text_to_image(\n                stream=stream, input=input, size=size, quality=quality, n=n\n            )\n\n        if isinstance(last_message, dict) and \"audio\" in last_message:            \n            audio_data = last_message.get(\"audio\", \"\")            \n            tpe = last_message.get(\"type\",\"wav\")\n            if not audio_data.startswith(\"data:\"):\n                audio_data = f\"data:audio/${tpe};base64,\" + audio_data                \n            return await self.async_speech_to_text(audio=audio_data)\n        \n        server = ray.get_actor(\"BLOCK_VLLM_STREAM_SERVER\")\n        request_id = [None]\n\n        def writer():\n            try:\n                r = \"\"\n                response = self.client.chat.completions.create(\n                    messages=messages,\n                    model=model,\n                    stream=True,\n                    max_tokens=max_length,\n                    temperature=temperature,\n                    top_p=top_p,\n                )\n                # input_tokens_count = 0\n                # generated_tokens_count = 0\n\n                request_id[0] = str(uuid.uuid4())\n\n                for chunk in response:\n                    content = chunk.choices[0].delta.content or \"\"\n                    r += content\n                    if hasattr(chunk, \"usage\") and chunk.usage:\n                        input_tokens_count = chunk.usage.prompt_tokens\n                        generated_tokens_count = chunk.usage.completion_tokens\n                    else:\n                        input_tokens_count = 0\n                        generated_tokens_count = 0\n                    ray.get(\n                        server.add_item.remote(\n                            request_id[0],\n                            StreamOutputs(\n                                outputs=[\n                                    SingleOutput(\n                                        text=r,\n                                        metadata=SingleOutputMeta(\n                                            input_tokens_count=input_tokens_count,\n                                            generated_tokens_count=generated_tokens_count,\n                                        ),\n                                    )\n                                ]\n                            ),\n                        )\n                    )\n            except:\n                traceback.print_exc()\n            ray.get(server.mark_done.remote(request_id[0]))\n\n        if stream:\n            threading.Thread(target=writer, daemon=True).start()\n\n            time_count = 60 * 100\n            while request_id[0] is None and time_count > 0:\n                time.sleep(0.01)\n                time_count -= 1\n\n            if request_id[0] is None:\n                raise Exception(\"Failed to get request id\")\n\n            def write_running():\n                return ray.get(server.add_item.remote(request_id[0], \"RUNNING\"))\n\n            await asyncio.to_thread(write_running)\n            return [\n                (\n                    \"\",\n                    {\n                        \"metadata\": {\n                            \"request_id\": request_id[0],\n                            \"stream_server\": \"BLOCK_VLLM_STREAM_SERVER\",\n                        }\n                    },\n                )\n            ]\n        else:\n            try:\n                start_time = time.monotonic()\n                response = await asyncfy_with_semaphore(\n                    lambda: self.client.chat.completions.create(\n                        messages=messages,\n                        model=model,\n                        max_tokens=max_length,\n                        temperature=temperature,\n                        top_p=top_p,\n                    )\n                )()\n\n                generated_text = response.choices[0].message.content\n                generated_tokens_count = response.usage.completion_tokens\n                input_tokens_count = response.usage.prompt_tokens\n                time_cost = time.monotonic() - start_time\n                return [\n                    (\n                        generated_text,\n                        {\n                            \"metadata\": {\n                                \"request_id\": response.id,\n                                \"input_tokens_count\": input_tokens_count,\n                                \"generated_tokens_count\": generated_tokens_count,\n                                \"time_cost\": time_cost,\n                                \"first_token_time\": 0,\n                                \"speed\": float(generated_tokens_count) / time_cost,\n                            }\n                        },\n                    )\n                ]\n            except Exception as e:\n                print(f\"Error: {e}\")\n                raise e\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/whisper/__init__.py\nimport whisper\nfrom typing import Dict, List, Tuple\nfrom byzerllm.utils.types import StopSequencesCriteria\n\nfrom typing import Dict, Any, List, Union\nfrom pyjava.storage import streaming_tar as STar\nimport json\nimport base64\nimport tempfile\nimport time\n\n\ndef get_meta(self):\n    return [\n        {\n            \"model_deploy_type\": \"proprietary\",\n            \"backend\": \"transformers\",\n            \"message_format\": True\n        }\n    ]\n\n\ndef process_input(ins: Union[str, List[Dict[str, Any]], Dict[str, Any]]):\n    # print(ins) \n    if isinstance(ins, list) or isinstance(ins, dict):\n        return ins\n\n    content = []\n    try:\n        ins_json = json.loads(ins)\n    except:\n        return ins\n\n    ## speech\n    if isinstance(ins_json, dict):\n        return ins_json\n\n    content = []\n    #     [\n    #     {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n    #     {\n    #       \"type\": \"image_url\",\n    #       \"image_url\": {\n    #         \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n    #         \"detail\": \"high\"\n    #       },\n    #     },\n    #   ],\n    for item in ins_json:\n        # for format like this: {\"image\": \"xxxxxx\", \"text\": \"What’s in this image?\",\"detail\":\"high\"}\n        # or {\"image\": \"xxxxxx\"}, {\"text\": \"What’s in this image?\"}\n        if (\"image\" in item or \"image_url\" in item) and \"type\" not in item:\n            image_data = item.get(\"image\", item.get(\"image_url\", \"\"))\n            ## \"data:image/jpeg;base64,\"\n            if not image_data.startswith(\"data:\"):\n                image_data = \"data:image/jpeg;base64,\" + image_data\n\n            ## get the other fields except image/text/image_url\n            other_fields = {\n                k: v for k, v in item.items() if k not in [\"image\", \"text\", \"image_url\"]\n            }\n            content.append(\n                {\n                    \"image_url\": {\"url\": image_data, **other_fields},\n                    \"type\": \"image_url\",\n                }\n            )\n\n        if \"text\" in item and \"type\" not in item:\n            text_data = item[\"text\"]\n            content.append({\"text\": text_data, \"type\": \"text\"})\n\n        ## for format like this: {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n        ## {\"type\": \"image_url\", \"image_url\": {\"url\":\"\",\"detail\":\"high\"}}\n        ## this is the standard format, just return it\n        if \"type\" in item and item[\"type\"] == \"text\":\n            content.append(item)\n\n        if \"type\" in item and item[\"type\"] == \"image_url\":\n            content.append(item)\n\n    if not content:\n        return ins\n\n    return content\n\n\ndef stream_chat(\n    self,\n    tokenizer,\n    ins: str,\n    his: List[Dict[str, str]] = [],\n    max_length: int = 4090,\n    top_p: float = 0.95,\n    temperature: float = 0.1,\n    **kwargs,\n):\n    messages = [\n        {\"role\": message[\"role\"], \"content\": process_input(message[\"content\"])}\n        for message in his\n    ] + [{\"role\": \"user\", \"content\": process_input(ins)}]\n\n    audio_input = messages[-1][\"content\"]\n\n    audio_data = audio_input.get(\"audio\", \"\")\n    audio = audio_input[\"audio\"]\n    response_format = audio_input.get(\"response_format\", \"json\")\n    timestamp_granularities = audio_input.get(\n        \"timestamp_granularities\", [\"word\", \"segment\"]\n    )\n    data_prefix = \"data:audio/\"\n    base64_prefix = \";base64,\"\n    if not audio.startswith(data_prefix):\n        raise ValueError(\"Invalid audio data format\")\n\n    format_end = audio.index(base64_prefix)\n    audio_format = audio[len(data_prefix) : format_end]\n    base64_data = audio[format_end + len(base64_prefix) :]\n\n    # Decode the base64 audio data\n    audio_data = base64.b64decode(base64_data)\n\n    # Create a temporary file with the correct extension\n    with tempfile.NamedTemporaryFile(\n        delete=False, suffix=f\".{audio_format}\"\n    ) as temp_audio_file:\n        temp_audio_file.write(audio_data)\n        temp_audio_file_path = temp_audio_file.name\n\n    try:\n        start_time = time.monotonic()\n        word_timestamps = \"word\" in timestamp_granularities\n        \n        result = self.transcribe(temp_audio_file_path, word_timestamps=word_timestamps)        \n            \n        time_cost = time.monotonic() - start_time\n        return [\n            (\n                json.dumps(result, ensure_ascii=True),\n                {\n                    \"metadata\": {\n                        \"request_id\": \"\",\n                        \"input_tokens_count\": 0,\n                        \"generated_tokens_count\": 0,\n                        \"time_cost\": time_cost,\n                        \"first_token_time\": 0,\n                        \"speed\": 0,\n                    }\n                },\n            )\n        ]\n    finally:\n        # Clean up the temporary file\n        import os\n\n        os.unlink(temp_audio_file_path)\n\n\ndef init_model(\n    model_dir, infer_params: Dict[str, str] = {}, sys_conf: Dict[str, str] = {}\n):\n    model = whisper.load_model(\"large\", download_root=model_dir)\n    import types\n\n    model.stream_chat = types.MethodType(stream_chat, model)\n    model.get_meta = types.MethodType(get_meta, model)\n    return (model, None)\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/sensevoice/__init__.py\nimport json\nimport base64\nimport tempfile\nimport time\nfrom typing import Dict, List, Union, Any\nfrom funasr import AutoModel\n\nemo_dict = {\n    \"<|HAPPY|>\": \"😊\",\n    \"<|SAD|>\": \"😔\",\n    \"<|ANGRY|>\": \"😡\",\n    \"<|NEUTRAL|>\": \"\",\n    \"<|FEARFUL|>\": \"😰\",\n    \"<|DISGUSTED|>\": \"🤢\",\n    \"<|SURPRISED|>\": \"😮\",\n}\n\nevent_dict = {\n    \"<|BGM|>\": \"🎼\",\n    \"<|Speech|>\": \"\",\n    \"<|Applause|>\": \"👏\",\n    \"<|Laughter|>\": \"��\",\n    \"<|Cry|>\": \"😭\",\n    \"<|Sneeze|>\": \"🤧\",\n    \"<|Breath|>\": \"\",\n    \"<|Cough|>\": \"🤧\",\n}\n\nemoji_dict = {\n    \"<|nospeech|><|Event_UNK|>\": \"❓\",\n    \"<|zh|>\": \"\",\n    \"<|en|>\": \"\",\n    \"<|yue|>\": \"\",\n    \"<|ja|>\": \"\",\n    \"<|ko|>\": \"\",\n    \"<|nospeech|>\": \"\",\n    \"<|HAPPY|>\": \"😊\",\n    \"<|SAD|>\": \"😔\",\n    \"<|ANGRY|>\": \"😡\",\n    \"<|NEUTRAL|>\": \"\",\n    \"<|BGM|>\": \"🎼\",\n    \"<|Speech|>\": \"\",\n    \"<|Applause|>\": \"👏\",\n    \"<|Laughter|>\": \"😀\",\n    \"<|FEARFUL|>\": \"😰\",\n    \"<|DISGUSTED|>\": \"🤢\",\n    \"<|SURPRISED|>\": \"😮\",\n    \"<|Cry|>\": \"😭\",\n    \"<|EMO_UNKNOWN|>\": \"\",\n    \"<|Sneeze|>\": \"🤧\",\n    \"<|Breath|>\": \"\",\n    \"<|Cough|>\": \"😷\",\n    \"<|Sing|>\": \"\",\n    \"<|Speech_Noise|>\": \"\",\n    \"<|withitn|>\": \"\",\n    \"<|woitn|>\": \"\",\n    \"<|GBG|>\": \"\",\n    \"<|Event_UNK|>\": \"\",\n}\n\nlang_dict = {\n    \"<|zh|>\": \"<|lang|>\",\n    \"<|en|>\": \"<|lang|>\",\n    \"<|yue|>\": \"<|lang|>\",\n    \"<|ja|>\": \"<|lang|>\",\n    \"<|ko|>\": \"<|lang|>\",\n    \"<|nospeech|>\": \"<|lang|>\",\n}\n\nemo_set = {\"😊\", \"😔\", \"😡\", \"😰\", \"🤢\", \"😮\"}\nevent_set = {\"🎼\", \"👏\", \"😀\", \"😭\", \"🤧\", \"😷\"}\n\n\ndef format_str(s):\n    for sptk in emoji_dict:\n        s = s.replace(sptk, emoji_dict[sptk])\n    return s\n\n\ndef format_str_v2(s):\n    sptk_dict = {sptk: s.count(sptk) for sptk in emoji_dict}\n    s = \"\".join(c for c in s if c not in emoji_dict.values())\n    emo = max(emo_dict, key=lambda e: sptk_dict[e])\n    event = \"\".join(event_dict[e] for e in event_dict if sptk_dict[e] > 0)\n    s = event + s + emo_dict[emo]\n    for emoji in emo_set.union(event_set):\n        s = s.replace(\" \" + emoji, emoji).replace(emoji + \" \", emoji)\n    return s.strip()\n\n\ndef format_str_v3(s):\n    def get_emo(s):\n        return s[-1] if s[-1] in emo_set else None\n\n    def get_event(s):\n        return s[0] if s[0] in event_set else None\n\n    s = s.replace(\"<|nospeech|><|Event_UNK|>\", \"❓\")\n    for lang in lang_dict:\n        s = s.replace(lang, \"<|lang|>\")\n    s_list = [format_str_v2(s_i).strip() for s_i in s.split(\"<|lang|>\")]\n    new_s = \" \" + s_list[0]\n    cur_ent_event = get_event(new_s)\n    for i in range(1, len(s_list)):\n        if len(s_list[i]) == 0:\n            continue\n        if get_event(s_list[i]) == cur_ent_event and get_event(s_list[i]) is not None:\n            s_list[i] = s_list[i][1:]\n        cur_ent_event = get_event(s_list[i])\n        if get_emo(s_list[i]) is not None and get_emo(s_list[i]) == get_emo(new_s):\n            new_s = new_s[:-1]\n        new_s += s_list[i].strip()\n    new_s = new_s.replace(\"The.\", \" \")\n    return new_s.strip()\n\n\ndef get_meta(self):\n    return [\n        {\n            \"model_deploy_type\": \"proprietary\",\n            \"backend\": \"funasr\",\n            \"message_format\": True,\n        }\n    ]\n\n\ndef process_input(ins: Union[str, List[Dict[str, Any]], Dict[str, Any]]):\n    if isinstance(ins, list) or isinstance(ins, dict):\n        return ins\n\n    try:\n        ins_json = json.loads(ins)\n    except:\n        return ins\n\n    if isinstance(ins_json, dict):\n        return ins_json\n\n    content = []\n    for item in ins_json:\n        if \"audio\" in item and \"type\" not in item:\n            audio_data = item.get(\"audio\", \"\")\n            if not audio_data.startswith(\"data:\"):\n                audio_data = f\"data:audio/wav;base64,{audio_data}\"\n            content.append({\"audio\": audio_data, \"type\": \"audio\"})\n\n        if \"text\" in item and \"type\" not in item:\n            content.append({\"text\": item[\"text\"], \"type\": \"text\"})\n\n        if \"type\" in item:\n            content.append(item)\n\n    return content if content else ins\n\n\ndef stream_chat(\n    self,\n    tokenizer,\n    ins: str,\n    his: List[Dict[str, str]] = [],\n    max_length: int = 4090,\n    top_p: float = 0.95,\n    temperature: float = 0.1,\n    **kwargs,\n):\n    messages = [\n        {\"role\": message[\"role\"], \"content\": process_input(message[\"content\"])}\n        for message in his\n    ] + [{\"role\": \"user\", \"content\": process_input(ins)}]\n\n    audio_input = messages[-1][\"content\"]\n\n    audio = audio_input[\"audio\"]\n    data_prefix = \"data:audio/\"\n    base64_prefix = \";base64,\"\n    if not audio.startswith(data_prefix):\n        raise ValueError(\"Invalid audio data format\")\n\n    format_end = audio.index(base64_prefix)\n    audio_format = audio[len(data_prefix) : format_end]\n    base64_data = audio[format_end + len(base64_prefix) :]\n\n    audio_data = base64.b64decode(base64_data)\n\n    with tempfile.NamedTemporaryFile(\n        delete=False, suffix=f\".{audio_format}\"\n    ) as temp_audio_file:\n        temp_audio_file.write(audio_data)\n        temp_audio_file_path = temp_audio_file.name\n\n    try:\n        start_time = time.monotonic()\n\n        result = self.generate(input=temp_audio_file_path, cache={})\n\n        time_cost = time.monotonic() - start_time\n\n        formatted_result = format_str_v3(result[0][\"text\"])\n\n        return [\n            (\n                json.dumps({\"text\": formatted_result}, ensure_ascii=False),\n                {\n                    \"metadata\": {\n                        \"request_id\": \"\",\n                        \"input_tokens_count\": 0,\n                        \"generated_tokens_count\": 0,\n                        \"time_cost\": time_cost,\n                        \"first_token_time\": 0,\n                        \"speed\": 0,\n                    }\n                },\n            )\n        ]\n    finally:\n        import os\n\n        os.unlink(temp_audio_file_path)\n\n\ndef init_model(\n    model_dir, infer_params: Dict[str, str] = {}, sys_conf: Dict[str, str] = {}\n):\n    model = AutoModel(\n        model=model_dir,\n        vad_model=\"iic/speech_fsmn_vad_zh-cn-16k-common-pytorch\",\n        vad_kwargs={\"max_single_segment_time\": 30000},\n        trust_remote_code=True,\n    )\n\n    import types\n\n    model.stream_chat = types.MethodType(stream_chat, model)\n    model.get_meta = types.MethodType(get_meta, model)\n\n    return (model, None)\n\n\n# Helper functions from web_ui.py\n\n\n     \n\n下面是用户的需求：\n\n@/Users/allwefantasy/projects/byzer-llm/src/byzerllm/sensevoice/__init__.py 中的 中从 vad_model 需要从 infer_params 获取,如果没有,则不传递给 AutoModel\n"}, {"role": "assistant", "content": "To implement this change, we need to modify the `init_model` function in the `/Users/allwefantasy/projects/byzer-llm/src/byzerllm/sensevoice/__init__.py` file. Here's the *SEARCH/REPLACE* block to make this change:\n\n```python\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/sensevoice/__init__.py\n<<<<<<< SEARCH\ndef init_model(\n    model_dir, infer_params: Dict[str, str] = {}, sys_conf: Dict[str, str] = {}\n):\n    model = AutoModel(\n        model=model_dir,\n        vad_model=\"iic/speech_fsmn_vad_zh-cn-16k-common-pytorch\",\n        vad_kwargs={\"max_single_segment_time\": 30000},\n        trust_remote_code=True,\n    )\n=======\ndef init_model(\n    model_dir, infer_params: Dict[str, str] = {}, sys_conf: Dict[str, str] = {}\n):\n    model_kwargs = {\n        \"model\": model_dir,\n        \"trust_remote_code\": True,\n    }\n\n    if \"vad_model\" in infer_params:\n        model_kwargs[\"vad_model\"] = infer_params[\"vad_model\"]\n        model_kwargs[\"vad_kwargs\"] = {\"max_single_segment_time\": 30000}\n\n    model = AutoModel(**model_kwargs)\n>>>>>>> REPLACE\n```\n\nThis change does the following:\n\n1. We create a `model_kwargs` dictionary with the basic parameters that are always needed.\n2. We check if `vad_model` is present in `infer_params`. If it is, we add it to `model_kwargs` along with the `vad_kwargs`.\n3. We then use `**model_kwargs` to pass these arguments to `AutoModel`, which allows for flexible parameter passing.\n\nThis implementation ensures that the `vad_model` is only passed to `AutoModel` if it's provided in `infer_params`. If it's not provided, the `AutoModel` will be initialized without the `vad_model` parameter."}], "model": "human", "yaml_file": "actions/021_chat_action.yml"}
{"instruction": "在  notebooks 目录里写个运行 @/Users/allwefantasy/projects/byzer-llm/src/byzerllm/sensevoice/__init__.py 测试类,注意是简单的main函数,不是单元测试.参考webui 里的测试文件", "conversations": [{"role": "user", "content": "\n如果你需要生成代码，对于每个需要更改的文件,你需要按 *SEARCH/REPLACE block* 的格式进行生成。\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The opening fence and code language, eg: ```python\n2. The file path alone on a line, starting with \"##File:\" and verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n*SEARCH/REPLACE* blocks will replace *all* matching occurrences.\nInclude enough lines to make the SEARCH blocks unique.\n\nInclude *ALL* the code being searched and replaced!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n下面我们来看一个例子：\n\n当前项目目录结构：\n1. 项目根目录： /tmp/projects/mathweb\n2. 项目子目录/文件列表(类似tree 命令输出)\nflask/\n    app.py\n    templates/\n        index.html\n    static/\n        style.css\n\n用户需求： Change get_factorial() to use math.factorial\n\n回答： To make this change we need to modify `/tmp/projects/mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n用户需求： Refactor hello() into its own file.\n\n回答：To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/hello.py\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/main.py\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n现在让我们开始一个新的任务:\n\n当前项目目录结构：\n1. 项目根目录： /Users/allwefantasy/projects/byzer-llm\n2. 项目子目录/文件列表(类似tree 命令输出)：\nsetup.py\ntests/\n    test_emb_rerank.py\n    test_deepseek_code.py\n    test_prompt.py\n    test_model_meta.py\n    __init__.py\n    test_transformers_deploy.py\n    test_data_analysis.py\n    test_yi.py\n    test_deploy.py\n    functions_suite.py\n    test_template.py\n    auto/\n        test_backend_llama_cpp.py\n        __init__.py\nsetup-machine/\n    convert_safetensor.py\nnotebooks/\n    test.py\nsrc/\n    byzerllm/\n        lang.py\n        store.py\n        version.py\n        log.py\n        __init__.py\n        types.py\n        easy_byzerllm_command.py\n        byzerllm_command.py\n        command_args.py\n        bark/\n            generation.py\n            __init__.py\n            model.py\n            api.py\n            model_fine.py\n            bark_voice.py\n        moss/\n            finetune_moss.py\n            __init__.py\n            moss_inference.py\n            models/\n                configuration_moss.py\n                quantization.py\n                custom_autotune.py\n                __init__.py\n                modeling_moss.py\n                tokenization_moss.py\n        llama2/\n            __init__.py\n        records/\n            __init__.py\n        visualglm/\n            __init__.py\n        auto/\n            __init__.py\n            backend_llama_cpp.py\n            backend_vllm.py\n            backend_ds.py\n        alpha_moss/\n            __init__.py\n        processor/\n            __init__.py\n        sensevoice/\n            __init__.py\n        qwen/\n            __init__.py\n        utils/\n            json_repaire.py\n            object_store_ref_util.py\n            __init__.py\n            types.py\n            tokenizer.py\n            openai_utils.py\n            ray_utils.py\n            text_generator.py\n            emb.py\n            connect_ray.py\n            testing.py\n            langutil.py\n            nontext.py\n            fulltune/\n                deepspeed_trainner.py\n                __init__.py\n                launch.py\n                trainner.py\n                base_model/\n                    __init__.py\n                    configuration_baichuan.py\n                    modeling_baichuan.py\n                pretrain/\n                    __init__.py\n                    convert_to_transformers.py\n            metrics/\n                __init__.py\n            config/\n                __init__.py\n            retrieval/\n                rest.py\n                __init__.py\n                udf.py\n            sft/\n                argument.py\n                collator.py\n                qlora.py\n                __init__.py\n                model.py\n                dataset.py\n                loss.py\n                merge_lora.py\n                trainer.py\n            client/\n                message_utils.py\n                math_utils.py\n                byzerllm_client.py\n                parallel_utils.py\n                __init__.py\n                types.py\n                code_utils.py\n                img_utils.py\n                entrypoints/\n                    __init__.py\n                    openai/\n                        protocol.py\n                        __init__.py\n                        serve.py\n                        serving_chat.py\n                        api_server.py\n                        serving_engine.py\n                        serving_completion.py\n                        tool.py\n        chatglm2/\n            __init__.py\n        baichuan/\n            __init__.py\n        m3e/\n            __init__.py\n        stable_diffusion/\n            config.py\n            __init__.py\n            logger.py\n            shared.py\n            model.py\n            utils.py\n            images.py\n            diffusion/\n                __init__.py\n                embeddings.py\n                utils.py\n                networks/\n                    lyco.py\n                    __init__.py\n                upscalers/\n                    __init__.py\n                    multidiffusion.py\n                    samplers.py\n                piplines/\n                    __init__.py\n                    lpw.py\n                    diffusers.py\n            lib/\n                __init__.py\n                diffusers/\n                    __init__.py\n                    scheduler.py\n            api/\n                __init__.py\n                plugin.py\n                models/\n                    diffusion.py\n                    __init__.py\n                diffusion/\n                    __init__.py\n                    pipelines/\n                        __init__.py\n                        diffusers.py\n                events/\n                    generation.py\n                    __init__.py\n        bge/\n            __init__.py\n        bge_rerank/\n            __init__.py\n        apps/\n            client.py\n            qa_strategy.py\n            __init__.py\n            builder.py\n            vector_db.py\n            utils.py\n            qa.py\n            byzer_storage/\n                memory_model_based.py\n                command.py\n                env.py\n                train_pt.py\n                train_sft.py\n                __init__.py\n                train_dpo.py\n                simple_api.py\n                generate_sft_data.py\n            byzer_sql/\n                __init__.py\n            llama_index/\n                byzerai_vectordb.py\n                byzerai.py\n                byzerai_embedding.py\n                __init__.py\n                collection_manager.py\n                byzerai_index_store.py\n                byzerai_docstore.py\n                simple_retrieval.py\n                byzerai_kvstore.py\n            agent/\n                conversable_agent.py\n                registry.py\n                groupchat.py\n                user_proxy_agent.py\n                __init__.py\n                agent.py\n                extensions/\n                    simple_retrieval_client.py\n                    planner.py\n                    python_codesandbox_agent.py\n                    preview_file_agent.py\n                    llama_index_retrieval_agent.py\n                    visualization_agent.py\n                    assistant_agent.py\n                    load_data_agent.py\n                    retrieval_agent.py\n                    output_agent.py\n                    __init__.py\n                    byzer_engine_agent.py\n                    subquestion_agent.py\n                    sql_reviewer_agent.py\n                    spark_sql_agent.py\n                    rhetorical_agent.py\n                    data_analysis.py\n                    data_analysis_pipeline_agent.py\n                    common_agent.py\n                    query_rewrite/\n                        time.py\n                        rhetorical.py\n                        __init__.py\n                        condition.py\n                        context.py\n                store/\n                    memory_store.py\n                    __init__.py\n                    stores.py\n        saas/\n            __init__.py\n            chatglm/\n                __init__.py\n            sparkdesk/\n                __init__.py\n            gemini/\n                __init__.py\n            azure_openai/\n                __init__.py\n            azure/\n                __init__.py\n            qianfan/\n                __init__.py\n            claude/\n                __init__.py\n            qianwen/\n                __init__.py\n            volcengine/\n                __init__.py\n            minimax/\n                __init__.py\n            baichuan/\n                __init__.py\n            qianwen_vl/\n                __init__.py\n            official_openai/\n                __init__.py\n            zhipu/\n                __init__.py\n            aws_bedrock/\n                __init__.py\n            openai/\n                __init__.py\n        falcon/\n            __init__.py\n        chatglm6b/\n            trainer_seq2seq.py\n            arguments.py\n            __init__.py\n            finetune.py\n            tunning/\n                __init__.py\n                infer.py\n                finetune.py\n                utils/\n                    config.py\n                    __init__.py\n                    other.py\n                    common.py\n                    seq2seq.py\n        starcode/\n            __init__.py\n        qwen_vl_chat/\n            __init__.py\n        zephyr/\n            __init__.py\n        dolly/\n            generate.py\n            __init__.py\n            consts.py\n            dolly_inference.py\n            trainer.py\n        llama/\n            __init__.py\n        whisper/\n            whisper_inference.py\n            __init__.py\n下面是一些文件路径以及每个文件对应的源码：\n\n##File: /Users/allwefantasy/projects/SenseVoice/webui.py\n# coding=utf-8\n\nimport os\nimport librosa\nimport base64\nimport io\nimport gradio as gr\nimport re\n\nimport numpy as np\nimport torch\nimport torchaudio\n\n\nfrom funasr import AutoModel\n\nmodel = \"iic/SenseVoiceSmall\"\nmodel_kwargs = {\n    \"model\": model,\n    \"trust_remote_code\": True,\n}\n\n# Check if vad_model is provided in infer_params\nif \"vad_model\" in infer_params:\n    model_kwargs[\"vad_model\"] = infer_params[\"vad_model\"]\n    model_kwargs[\"vad_kwargs\"] = {\"max_single_segment_time\": 30000}\n\nmodel = AutoModel(**model_kwargs)\n\nimport re\n\nemo_dict = {\n\t\"<|HAPPY|>\": \"😊\",\n\t\"<|SAD|>\": \"😔\",\n\t\"<|ANGRY|>\": \"😡\",\n\t\"<|NEUTRAL|>\": \"\",\n\t\"<|FEARFUL|>\": \"😰\",\n\t\"<|DISGUSTED|>\": \"🤢\",\n\t\"<|SURPRISED|>\": \"😮\",\n}\n\nevent_dict = {\n\t\"<|BGM|>\": \"🎼\",\n\t\"<|Speech|>\": \"\",\n\t\"<|Applause|>\": \"👏\",\n\t\"<|Laughter|>\": \"😀\",\n\t\"<|Cry|>\": \"😭\",\n\t\"<|Sneeze|>\": \"🤧\",\n\t\"<|Breath|>\": \"\",\n\t\"<|Cough|>\": \"🤧\",\n}\n\nemoji_dict = {\n\t\"<|nospeech|><|Event_UNK|>\": \"❓\",\n\t\"<|zh|>\": \"\",\n\t\"<|en|>\": \"\",\n\t\"<|yue|>\": \"\",\n\t\"<|ja|>\": \"\",\n\t\"<|ko|>\": \"\",\n\t\"<|nospeech|>\": \"\",\n\t\"<|HAPPY|>\": \"😊\",\n\t\"<|SAD|>\": \"😔\",\n\t\"<|ANGRY|>\": \"😡\",\n\t\"<|NEUTRAL|>\": \"\",\n\t\"<|BGM|>\": \"🎼\",\n\t\"<|Speech|>\": \"\",\n\t\"<|Applause|>\": \"👏\",\n\t\"<|Laughter|>\": \"😀\",\n\t\"<|FEARFUL|>\": \"😰\",\n\t\"<|DISGUSTED|>\": \"🤢\",\n\t\"<|SURPRISED|>\": \"😮\",\n\t\"<|Cry|>\": \"😭\",\n\t\"<|EMO_UNKNOWN|>\": \"\",\n\t\"<|Sneeze|>\": \"🤧\",\n\t\"<|Breath|>\": \"\",\n\t\"<|Cough|>\": \"😷\",\n\t\"<|Sing|>\": \"\",\n\t\"<|Speech_Noise|>\": \"\",\n\t\"<|withitn|>\": \"\",\n\t\"<|woitn|>\": \"\",\n\t\"<|GBG|>\": \"\",\n\t\"<|Event_UNK|>\": \"\",\n}\n\nlang_dict =  {\n    \"<|zh|>\": \"<|lang|>\",\n    \"<|en|>\": \"<|lang|>\",\n    \"<|yue|>\": \"<|lang|>\",\n    \"<|ja|>\": \"<|lang|>\",\n    \"<|ko|>\": \"<|lang|>\",\n    \"<|nospeech|>\": \"<|lang|>\",\n}\n\nemo_set = {\"😊\", \"😔\", \"😡\", \"😰\", \"🤢\", \"😮\"}\nevent_set = {\"🎼\", \"👏\", \"😀\", \"😭\", \"🤧\", \"😷\",}\n\ndef format_str(s):\n\tfor sptk in emoji_dict:\n\t\ts = s.replace(sptk, emoji_dict[sptk])\n\treturn s\n\n\ndef format_str_v2(s):\n\tsptk_dict = {}\n\tfor sptk in emoji_dict:\n\t\tsptk_dict[sptk] = s.count(sptk)\n\t\ts = s.replace(sptk, \"\")\n\temo = \"<|NEUTRAL|>\"\n\tfor e in emo_dict:\n\t\tif sptk_dict[e] > sptk_dict[emo]:\n\t\t\temo = e\n\tfor e in event_dict:\n\t\tif sptk_dict[e] > 0:\n\t\t\ts = event_dict[e] + s\n\ts = s + emo_dict[emo]\n\n\tfor emoji in emo_set.union(event_set):\n\t\ts = s.replace(\" \" + emoji, emoji)\n\t\ts = s.replace(emoji + \" \", emoji)\n\treturn s.strip()\n\ndef format_str_v3(s):\n\tdef get_emo(s):\n\t\treturn s[-1] if s[-1] in emo_set else None\n\tdef get_event(s):\n\t\treturn s[0] if s[0] in event_set else None\n\n\ts = s.replace(\"<|nospeech|><|Event_UNK|>\", \"❓\")\n\tfor lang in lang_dict:\n\t\ts = s.replace(lang, \"<|lang|>\")\n\ts_list = [format_str_v2(s_i).strip(\" \") for s_i in s.split(\"<|lang|>\")]\n\tnew_s = \" \" + s_list[0]\n\tcur_ent_event = get_event(new_s)\n\tfor i in range(1, len(s_list)):\n\t\tif len(s_list[i]) == 0:\n\t\t\tcontinue\n\t\tif get_event(s_list[i]) == cur_ent_event and get_event(s_list[i]) != None:\n\t\t\ts_list[i] = s_list[i][1:]\n\t\t#else:\n\t\tcur_ent_event = get_event(s_list[i])\n\t\tif get_emo(s_list[i]) != None and get_emo(s_list[i]) == get_emo(new_s):\n\t\t\tnew_s = new_s[:-1]\n\t\tnew_s += s_list[i].strip().lstrip()\n\tnew_s = new_s.replace(\"The.\", \" \")\n\treturn new_s.strip()\n\ndef model_inference(input_wav, language, fs=16000):\n\t# task_abbr = {\"Speech Recognition\": \"ASR\", \"Rich Text Transcription\": (\"ASR\", \"AED\", \"SER\")}\n\tlanguage_abbr = {\"auto\": \"auto\", \"zh\": \"zh\", \"en\": \"en\", \"yue\": \"yue\", \"ja\": \"ja\", \"ko\": \"ko\",\n\t\t\t\t\t \"nospeech\": \"nospeech\"}\n\t\n\t# task = \"Speech Recognition\" if task is None else task\n\tlanguage = \"auto\" if len(language) < 1 else language\n\tselected_language = language_abbr[language]\n\t# selected_task = task_abbr.get(task)\n\t\n\t# print(f\"input_wav: {type(input_wav)}, {input_wav[1].shape}, {input_wav}\")\n\t\n\tif isinstance(input_wav, tuple):\n\t\tfs, input_wav = input_wav\n\t\tinput_wav = input_wav.astype(np.float32) / np.iinfo(np.int16).max\n\t\tif len(input_wav.shape) > 1:\n\t\t\tinput_wav = input_wav.mean(-1)\n\t\tif fs != 16000:\n\t\t\tprint(f\"audio_fs: {fs}\")\n\t\t\tresampler = torchaudio.transforms.Resample(fs, 16000)\n\t\t\tinput_wav_t = torch.from_numpy(input_wav).to(torch.float32)\n\t\t\tinput_wav = resampler(input_wav_t[None, :])[0, :].numpy()\n\t\n\t\n\tmerge_vad = True #False if selected_task == \"ASR\" else True\n\tprint(f\"language: {language}, merge_vad: {merge_vad}\")\n\ttext = model.generate(input=input_wav,\n\t\t\t\t\t\t  cache={},\n\t\t\t\t\t\t  language=language,\n\t\t\t\t\t\t  use_itn=True,\n\t\t\t\t\t\t  batch_size_s=60, merge_vad=merge_vad)\n\t\n\tprint(text)\n\ttext = text[0][\"text\"]\n\ttext = format_str_v3(text)\n\t\n\tprint(text)\n\t\n\treturn text\n\n\naudio_examples = [\n    [\"example/zh.mp3\", \"zh\"],\n    [\"example/yue.mp3\", \"yue\"],\n    [\"example/en.mp3\", \"en\"],\n    [\"example/ja.mp3\", \"ja\"],\n    [\"example/ko.mp3\", \"ko\"],\n    [\"example/emo_1.wav\", \"auto\"],\n    [\"example/emo_2.wav\", \"auto\"],\n    [\"example/emo_3.wav\", \"auto\"],\n    #[\"example/emo_4.wav\", \"auto\"],\n    #[\"example/event_1.wav\", \"auto\"],\n    #[\"example/event_2.wav\", \"auto\"],\n    #[\"example/event_3.wav\", \"auto\"],\n    [\"example/rich_1.wav\", \"auto\"],\n    [\"example/rich_2.wav\", \"auto\"],\n    #[\"example/rich_3.wav\", \"auto\"],\n    [\"example/longwav_1.wav\", \"auto\"],\n    [\"example/longwav_2.wav\", \"auto\"],\n    [\"example/longwav_3.wav\", \"auto\"],\n    #[\"example/longwav_4.wav\", \"auto\"],\n]\n\n\n\nhtml_content = \"\"\"\n<div>\n    <h2 style=\"font-size: 22px;margin-left: 0px;\">Voice Understanding Model: SenseVoice-Small</h2>\n    <p style=\"font-size: 18px;margin-left: 20px;\">SenseVoice-Small is an encoder-only speech foundation model designed for rapid voice understanding. It encompasses a variety of features including automatic speech recognition (ASR), spoken language identification (LID), speech emotion recognition (SER), and acoustic event detection (AED). SenseVoice-Small supports multilingual recognition for Chinese, English, Cantonese, Japanese, and Korean. Additionally, it offers exceptionally low inference latency, performing 7 times faster than Whisper-small and 17 times faster than Whisper-large.</p>\n    <h2 style=\"font-size: 22px;margin-left: 0px;\">Usage</h2> <p style=\"font-size: 18px;margin-left: 20px;\">Upload an audio file or input through a microphone, then select the task and language. the audio is transcribed into corresponding text along with associated emotions (😊 happy, 😡 angry/exicting, 😔 sad) and types of sound events (😀 laughter, 🎼 music, 👏 applause, 🤧 cough&sneeze, 😭 cry). The event labels are placed in the front of the text and the emotion are in the back of the text.</p>\n\t<p style=\"font-size: 18px;margin-left: 20px;\">Recommended audio input duration is below 30 seconds. For audio longer than 30 seconds, local deployment is recommended.</p>\n\t<h2 style=\"font-size: 22px;margin-left: 0px;\">Repo</h2>\n\t<p style=\"font-size: 18px;margin-left: 20px;\"><a href=\"https://github.com/FunAudioLLM/SenseVoice\" target=\"_blank\">SenseVoice</a>: multilingual speech understanding model</p>\n\t<p style=\"font-size: 18px;margin-left: 20px;\"><a href=\"https://github.com/modelscope/FunASR\" target=\"_blank\">FunASR</a>: fundamental speech recognition toolkit</p>\n\t<p style=\"font-size: 18px;margin-left: 20px;\"><a href=\"https://github.com/FunAudioLLM/CosyVoice\" target=\"_blank\">CosyVoice</a>: high-quality multilingual TTS model</p>\n</div>\n\"\"\"\n\n\ndef launch():\n\twith gr.Blocks(theme=gr.themes.Soft()) as demo:\n\t\t# gr.Markdown(description)\n\t\tgr.HTML(html_content)\n\t\twith gr.Row():\n\t\t\twith gr.Column():\n\t\t\t\taudio_inputs = gr.Audio(label=\"Upload audio or use the microphone\")\n\t\t\t\t\n\t\t\t\twith gr.Accordion(\"Configuration\"):\n\t\t\t\t\tlanguage_inputs = gr.Dropdown(choices=[\"auto\", \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"],\n\t\t\t\t\t\t\t\t\t\t\t\t  value=\"auto\",\n\t\t\t\t\t\t\t\t\t\t\t\t  label=\"Language\")\n\t\t\t\tfn_button = gr.Button(\"Start\", variant=\"primary\")\n\t\t\t\ttext_outputs = gr.Textbox(label=\"Results\")\n\t\t\tgr.Examples(examples=audio_examples, inputs=[audio_inputs, language_inputs], examples_per_page=20)\n\t\t\n\t\tfn_button.click(model_inference, inputs=[audio_inputs, language_inputs], outputs=text_outputs)\n\n\tdemo.launch()\n\n\nif __name__ == \"__main__\":\n\t# iface.launch()\n\tlaunch()\n\n\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/saas/official_openai/__init__.py\nimport time\nfrom typing import List, Tuple, Dict, Any, Union\nimport httpx\nfrom openai import OpenAI\nimport base64\nimport io\nimport json\nimport ray\nfrom byzerllm.utils.types import (\n    BlockVLLMStreamServer,\n    StreamOutputs,\n    SingleOutput,\n    SingleOutputMeta,\n    BlockBinaryStreamServer,\n)\nfrom byzerllm.utils.langutil import asyncfy_with_semaphore\nimport threading\nimport asyncio\nimport traceback\nimport uuid\nimport tempfile        \n\nclass CustomSaasAPI:\n\n    def __init__(self, infer_params: Dict[str, str]) -> None:\n\n        self.api_key = infer_params[\"saas.api_key\"]\n        self.model = infer_params.get(\"saas.model\", \"gpt-3.5-turbo-1106\")\n\n        other_params = {}\n\n        if \"saas.api_base\" in infer_params:\n            other_params[\"api_base\"] = infer_params[\"saas.api_base\"]\n\n        if \"saas.api_version\" in infer_params:\n            other_params[\"api_version\"] = infer_params[\"saas.api_version\"]\n\n        if \"saas.api_type\" in infer_params:\n            other_params[\"api_type\"] = infer_params[\"saas.api_type\"]\n\n        if \"saas.base_url\" in infer_params:\n            other_params[\"base_url\"] = infer_params[\"saas.base_url\"]\n\n        if \"saas.timeout\" in infer_params:\n            other_params[\"timeout\"] = float(infer_params[\"saas.timeout\"])\n\n        self.max_retries = int(infer_params.get(\"saas.max_retries\", 10))\n\n        self.meta = {\n            \"model_deploy_type\": \"saas\",\n            \"backend\": \"saas\",\n            \"support_stream\": True,\n            \"model_name\": self.model,\n        }\n\n        self.meta[\"embedding_mode\"] = \"embedding\" in self.model.lower()\n\n        self.proxies = infer_params.get(\"saas.proxies\", None)\n        self.local_address = infer_params.get(\"saas.local_address\", \"0.0.0.0\")\n\n        if not self.proxies:\n            self.client = OpenAI(**other_params, api_key=self.api_key)\n        else:\n            self.client = OpenAI(\n                **other_params,\n                api_key=self.api_key,\n                http_client=httpx.Client(\n                    proxies=self.proxies,\n                    transport=httpx.HTTPTransport(local_address=self.local_address),\n                ),\n            )\n\n        try:\n            ray.get_actor(\"BLOCK_VLLM_STREAM_SERVER\")\n        except ValueError:\n            try:\n                ray.remote(BlockVLLMStreamServer).options(\n                    name=\"BLOCK_VLLM_STREAM_SERVER\",\n                    lifetime=\"detached\",\n                    max_concurrency=1000,\n                ).remote()\n            except Exception as e:\n                pass\n        try:\n            ray.get_actor(\"BlockBinaryStreamServer\")\n        except ValueError:\n            try:\n                ray.remote(BlockBinaryStreamServer).options(\n                    name=\"BlockBinaryStreamServer\",\n                    lifetime=\"detached\",\n                    max_concurrency=1000,\n                ).remote()\n            except Exception as e:\n                pass\n\n    # saas/proprietary\n    def get_meta(self):\n        return [self.meta]\n\n    def process_input(self, ins: Union[str, List[Dict[str, Any]], Dict[str, Any]]):\n\n        if isinstance(ins, list) or isinstance(ins, dict):\n            return ins\n\n        content = []\n        try:\n            ins_json = json.loads(ins)\n        except:\n            return ins\n\n        ## speech\n        if isinstance(ins_json, dict):\n            return ins_json\n\n        content = []\n        #     [\n        #     {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n        #     {\n        #       \"type\": \"image_url\",\n        #       \"image_url\": {\n        #         \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n        #         \"detail\": \"high\"\n        #       },\n        #     },\n        #   ],\n        for item in ins_json:\n            # for format like this: {\"image\": \"xxxxxx\", \"text\": \"What’s in this image?\",\"detail\":\"high\"}\n            # or {\"image\": \"xxxxxx\"}, {\"text\": \"What’s in this image?\"}\n            if (\"image\" in item or \"image_url\" in item) and \"type\" not in item:\n                image_data = item.get(\"image\", item.get(\"image_url\", \"\"))\n                ## \"data:image/jpeg;base64,\"\n                if not image_data.startswith(\"data:\"):\n                    image_data = \"data:image/jpeg;base64,\" + image_data\n\n                ## get the other fields except image/text/image_url\n                other_fields = {\n                    k: v\n                    for k, v in item.items()\n                    if k not in [\"image\", \"text\", \"image_url\"]\n                }\n                content.append(\n                    {\n                        \"image_url\": {\"url\": image_data, **other_fields},\n                        \"type\": \"image_url\",\n                    }\n                )            \n\n            if \"text\" in item and \"type\" not in item:\n                text_data = item[\"text\"]\n                content.append({\"text\": text_data, \"type\": \"text\"})\n\n            ## for format like this: {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n            ## {\"type\": \"image_url\", \"image_url\": {\"url\":\"\",\"detail\":\"high\"}}\n            ## this is the standard format, just return it\n            if \"type\" in item and item[\"type\"] == \"text\":\n                content.append(item)\n\n            if \"type\" in item and item[\"type\"] == \"image_url\":\n                content.append(item)\n\n        if not content:\n            return ins\n\n        return content\n\n    async def async_embed_query(self, ins: str, **kwargs):\n        resp = await asyncfy_with_semaphore(\n            lambda: self.client.embeddings.create(input=[ins], model=self.model)\n        )()\n        embedding = resp.data[0].embedding\n        usage = resp.usage\n        return (\n            embedding,\n            {\n                \"metadata\": {\n                    \"input_tokens_count\": usage.prompt_tokens,\n                    \"generated_tokens_count\": 0,\n                }\n            },\n        )\n\n    def embed_query(self, ins: str, **kwargs):\n        resp = self.client.embeddings.create(input=[ins], model=self.model)\n        embedding = resp.data[0].embedding\n        usage = resp.usage\n        return (\n            embedding,\n            {\n                \"metadata\": {\n                    \"input_tokens_count\": usage.prompt_tokens,\n                    \"generated_tokens_count\": 0,\n                }\n            },\n        )\n\n    async def async_text_to_speech(\n        self, stream: bool, ins: str, voice: str, chunk_size: int = None, **kwargs\n    ):\n        if stream:\n            server = ray.get_actor(\"BlockBinaryStreamServer\")\n            request_id = [None]\n\n            def writer():\n                try:\n                    request_id[0] = str(uuid.uuid4())\n                    with self.client.with_streaming_response.audio.speech.create(\n                        model=self.model, voice=voice, input=ins, **kwargs\n                    ) as response:\n                        for chunk in response.iter_bytes(chunk_size):\n                            input_tokens_count = 0\n                            generated_tokens_count = 0\n                            ray.get(\n                                server.add_item.remote(\n                                    request_id[0],\n                                    StreamOutputs(\n                                        outputs=[\n                                            SingleOutput(\n                                                text=chunk,\n                                                metadata=SingleOutputMeta(\n                                                    input_tokens_count=input_tokens_count,\n                                                    generated_tokens_count=generated_tokens_count,\n                                                ),\n                                            )\n                                        ]\n                                    ),\n                                )\n                            )\n                except:\n                    traceback.print_exc()\n                ray.get(server.mark_done.remote(request_id[0]))\n\n            threading.Thread(target=writer, daemon=True).start()\n\n            time_count = 10 * 100\n            while request_id[0] is None and time_count > 0:\n                time.sleep(0.01)\n                time_count -= 1\n\n            if request_id[0] is None:\n                raise Exception(\"Failed to get request id\")\n\n            def write_running():\n                return ray.get(server.add_item.remote(request_id[0], \"RUNNING\"))\n\n            await asyncio.to_thread(write_running)\n            return [\n                (\n                    \"\",\n                    {\n                        \"metadata\": {\n                            \"request_id\": request_id[0],\n                            \"stream_server\": \"BlockBinaryStreamServer\",\n                        }\n                    },\n                )\n            ]\n\n        start_time = time.monotonic()\n        with io.BytesIO() as output:\n            async with asyncfy_with_semaphore(\n                lambda: self.client.with_streaming_response.audio.speech.create(\n                    model=self.model, voice=voice, input=ins, **kwargs\n                )\n            )() as response:\n                for chunk in response.iter_bytes():\n                    output.write(chunk)\n\n            base64_audio = base64.b64encode(output.getvalue()).decode()\n            time_cost = time.monotonic() - start_time\n            return [\n                (\n                    base64_audio,\n                    {\n                        \"metadata\": {\n                            \"request_id\": \"\",\n                            \"input_tokens_count\": 0,\n                            \"generated_tokens_count\": 0,\n                            \"time_cost\": time_cost,\n                            \"first_token_time\": 0,\n                            \"speed\": 0,\n                        }\n                    },\n                )\n            ]\n\n    async def async_speech_to_text(\n        self,\n        audio: str,\n        response_format: str = \"verbose_json\",\n        timestamp_granularities: List[str] = [\"word\",\"segment\"],\n    ):               \n        # Extract audio format and base64 data\n        data_prefix = \"data:audio/\"\n        base64_prefix = \";base64,\"\n        if not audio.startswith(data_prefix):\n            raise ValueError(\"Invalid audio data format\")\n        \n        format_end = audio.index(base64_prefix)\n        audio_format = audio[len(data_prefix):format_end]\n        base64_data = audio[format_end + len(base64_prefix):]\n        \n        # Decode the base64 audio data\n        audio_data = base64.b64decode(base64_data)\n\n        # Create a temporary file with the correct extension\n        with tempfile.NamedTemporaryFile(delete=False, suffix=f'.{audio_format}') as temp_audio_file:\n            temp_audio_file.write(audio_data)\n            temp_audio_file_path = temp_audio_file.name\n\n        try:\n            start_time = time.monotonic()\n                   \n            with open(temp_audio_file_path, \"rb\") as audio_file:                \n                transcription = self.client.audio.transcriptions.create(\n                    model=self.model or \"whisper-1\",\n                    file=audio_file,\n                    response_format=response_format,\n                    timestamp_granularities=timestamp_granularities,\n                )\n            time_cost = time.monotonic() - start_time\n\n            return [\n                (\n                    json.dumps(transcription.to_dict(), ensure_ascii=True),\n                    {\n                        \"metadata\": {\n                            \"request_id\": \"\",\n                            \"input_tokens_count\": 0,\n                            \"generated_tokens_count\": 0,\n                            \"time_cost\": time_cost,\n                            \"first_token_time\": 0,\n                            \"speed\": 0,\n                        }\n                    },\n                )\n            ]\n        finally:\n            # Clean up the temporary file\n            import os\n            os.unlink(temp_audio_file_path)\n\n    def image_to_text(self, ins: str, **kwargs):\n        pass\n\n    async def async_text_to_image(\n        self, stream: bool, input: str, size: str, quality: str, n: int, **kwargs\n    ):\n        if stream:\n            raise Exception(\"Stream not supported for text to image\")\n        start_time = time.monotonic()\n        response = await asyncfy_with_semaphore(\n            lambda: self.client.images.generate(\n                model=self.model,\n                prompt=input,\n                size=size,\n                quality=quality,\n                n=1,\n                response_format=\"b64_json\",\n                **kwargs,\n            )\n        )()\n        time_cost = time.monotonic() - start_time\n        base64_image = response.data[0].b64_json\n        return [\n            (\n                base64_image,\n                {\n                    \"metadata\": {\n                        \"request_id\": \"\",\n                        \"input_tokens_count\": 0,\n                        \"generated_tokens_count\": 0,\n                        \"time_cost\": time_cost,\n                        \"first_token_time\": 0,\n                        \"speed\": 0,\n                    }\n                },\n            )\n        ]\n\n    def text_to_text(self, ins: str, **kwargs):\n        pass\n\n    async def async_stream_chat(\n        self,\n        tokenizer,\n        ins: str,\n        his: List[Dict[str, Any]] = [],\n        max_length: int = 4096,\n        top_p: float = 0.7,\n        temperature: float = 0.9,\n        **kwargs,\n    ):\n\n        model = self.model\n\n        if \"model\" in kwargs:\n            model = kwargs[\"model\"]\n\n        messages = [\n            {\"role\": message[\"role\"], \"content\": self.process_input(message[\"content\"])}\n            for message in his\n        ] + [{\"role\": \"user\", \"content\": self.process_input(ins)}]\n\n        stream = kwargs.get(\"stream\", False)\n\n        ## content = [\n        ##    \"voice\": \"alloy\",\"input\": \"Hello, World!\",response_format: \"mp3\"]\n        last_message = messages[-1][\"content\"]\n\n        if isinstance(last_message, dict) and \"voice\" in last_message:\n            voice = last_message[\"voice\"]\n            response_format = last_message.get(\"response_format\", \"mp3\")\n            chunk_size = last_message.get(\"chunk_size\", None)\n            input = last_message[\"input\"]\n            return await self.async_text_to_speech(\n                stream=stream,\n                ins=input,\n                voice=voice,\n                chunk_size=chunk_size,\n                response_format=response_format,\n            )\n\n        if isinstance(last_message, dict) and \"input\" in last_message:\n            input = last_message[\"input\"]\n            size = last_message.get(\"size\", \"1024x1024\")\n            quality = last_message.get(\"quality\", \"standard\")\n            n = last_message.get(\"n\", 1)\n            return await self.async_text_to_image(\n                stream=stream, input=input, size=size, quality=quality, n=n\n            )\n\n        if isinstance(last_message, dict) and \"audio\" in last_message:            \n            audio_data = last_message.get(\"audio\", \"\")            \n            tpe = last_message.get(\"type\",\"wav\")\n            if not audio_data.startswith(\"data:\"):\n                audio_data = f\"data:audio/${tpe};base64,\" + audio_data                \n            return await self.async_speech_to_text(audio=audio_data)\n        \n        server = ray.get_actor(\"BLOCK_VLLM_STREAM_SERVER\")\n        request_id = [None]\n\n        def writer():\n            try:\n                r = \"\"\n                response = self.client.chat.completions.create(\n                    messages=messages,\n                    model=model,\n                    stream=True,\n                    max_tokens=max_length,\n                    temperature=temperature,\n                    top_p=top_p,\n                )\n                # input_tokens_count = 0\n                # generated_tokens_count = 0\n\n                request_id[0] = str(uuid.uuid4())\n\n                for chunk in response:\n                    content = chunk.choices[0].delta.content or \"\"\n                    r += content\n                    if hasattr(chunk, \"usage\") and chunk.usage:\n                        input_tokens_count = chunk.usage.prompt_tokens\n                        generated_tokens_count = chunk.usage.completion_tokens\n                    else:\n                        input_tokens_count = 0\n                        generated_tokens_count = 0\n                    ray.get(\n                        server.add_item.remote(\n                            request_id[0],\n                            StreamOutputs(\n                                outputs=[\n                                    SingleOutput(\n                                        text=r,\n                                        metadata=SingleOutputMeta(\n                                            input_tokens_count=input_tokens_count,\n                                            generated_tokens_count=generated_tokens_count,\n                                        ),\n                                    )\n                                ]\n                            ),\n                        )\n                    )\n            except:\n                traceback.print_exc()\n            ray.get(server.mark_done.remote(request_id[0]))\n\n        if stream:\n            threading.Thread(target=writer, daemon=True).start()\n\n            time_count = 60 * 100\n            while request_id[0] is None and time_count > 0:\n                time.sleep(0.01)\n                time_count -= 1\n\n            if request_id[0] is None:\n                raise Exception(\"Failed to get request id\")\n\n            def write_running():\n                return ray.get(server.add_item.remote(request_id[0], \"RUNNING\"))\n\n            await asyncio.to_thread(write_running)\n            return [\n                (\n                    \"\",\n                    {\n                        \"metadata\": {\n                            \"request_id\": request_id[0],\n                            \"stream_server\": \"BLOCK_VLLM_STREAM_SERVER\",\n                        }\n                    },\n                )\n            ]\n        else:\n            try:\n                start_time = time.monotonic()\n                response = await asyncfy_with_semaphore(\n                    lambda: self.client.chat.completions.create(\n                        messages=messages,\n                        model=model,\n                        max_tokens=max_length,\n                        temperature=temperature,\n                        top_p=top_p,\n                    )\n                )()\n\n                generated_text = response.choices[0].message.content\n                generated_tokens_count = response.usage.completion_tokens\n                input_tokens_count = response.usage.prompt_tokens\n                time_cost = time.monotonic() - start_time\n                return [\n                    (\n                        generated_text,\n                        {\n                            \"metadata\": {\n                                \"request_id\": response.id,\n                                \"input_tokens_count\": input_tokens_count,\n                                \"generated_tokens_count\": generated_tokens_count,\n                                \"time_cost\": time_cost,\n                                \"first_token_time\": 0,\n                                \"speed\": float(generated_tokens_count) / time_cost,\n                            }\n                        },\n                    )\n                ]\n            except Exception as e:\n                print(f\"Error: {e}\")\n                raise e\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/whisper/__init__.py\nimport whisper\nfrom typing import Dict, List, Tuple\nfrom byzerllm.utils.types import StopSequencesCriteria\n\nfrom typing import Dict, Any, List, Union\nfrom pyjava.storage import streaming_tar as STar\nimport json\nimport base64\nimport tempfile\nimport time\n\n\ndef get_meta(self):\n    return [\n        {\n            \"model_deploy_type\": \"proprietary\",\n            \"backend\": \"transformers\",\n            \"message_format\": True\n        }\n    ]\n\n\ndef process_input(ins: Union[str, List[Dict[str, Any]], Dict[str, Any]]):\n    # print(ins) \n    if isinstance(ins, list) or isinstance(ins, dict):\n        return ins\n\n    content = []\n    try:\n        ins_json = json.loads(ins)\n    except:\n        return ins\n\n    ## speech\n    if isinstance(ins_json, dict):\n        return ins_json\n\n    content = []\n    #     [\n    #     {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n    #     {\n    #       \"type\": \"image_url\",\n    #       \"image_url\": {\n    #         \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n    #         \"detail\": \"high\"\n    #       },\n    #     },\n    #   ],\n    for item in ins_json:\n        # for format like this: {\"image\": \"xxxxxx\", \"text\": \"What’s in this image?\",\"detail\":\"high\"}\n        # or {\"image\": \"xxxxxx\"}, {\"text\": \"What’s in this image?\"}\n        if (\"image\" in item or \"image_url\" in item) and \"type\" not in item:\n            image_data = item.get(\"image\", item.get(\"image_url\", \"\"))\n            ## \"data:image/jpeg;base64,\"\n            if not image_data.startswith(\"data:\"):\n                image_data = \"data:image/jpeg;base64,\" + image_data\n\n            ## get the other fields except image/text/image_url\n            other_fields = {\n                k: v for k, v in item.items() if k not in [\"image\", \"text\", \"image_url\"]\n            }\n            content.append(\n                {\n                    \"image_url\": {\"url\": image_data, **other_fields},\n                    \"type\": \"image_url\",\n                }\n            )\n\n        if \"text\" in item and \"type\" not in item:\n            text_data = item[\"text\"]\n            content.append({\"text\": text_data, \"type\": \"text\"})\n\n        ## for format like this: {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n        ## {\"type\": \"image_url\", \"image_url\": {\"url\":\"\",\"detail\":\"high\"}}\n        ## this is the standard format, just return it\n        if \"type\" in item and item[\"type\"] == \"text\":\n            content.append(item)\n\n        if \"type\" in item and item[\"type\"] == \"image_url\":\n            content.append(item)\n\n    if not content:\n        return ins\n\n    return content\n\n\ndef stream_chat(\n    self,\n    tokenizer,\n    ins: str,\n    his: List[Dict[str, str]] = [],\n    max_length: int = 4090,\n    top_p: float = 0.95,\n    temperature: float = 0.1,\n    **kwargs,\n):\n    messages = [\n        {\"role\": message[\"role\"], \"content\": process_input(message[\"content\"])}\n        for message in his\n    ] + [{\"role\": \"user\", \"content\": process_input(ins)}]\n\n    audio_input = messages[-1][\"content\"]\n\n    audio_data = audio_input.get(\"audio\", \"\")\n    audio = audio_input[\"audio\"]\n    response_format = audio_input.get(\"response_format\", \"json\")\n    timestamp_granularities = audio_input.get(\n        \"timestamp_granularities\", [\"word\", \"segment\"]\n    )\n    data_prefix = \"data:audio/\"\n    base64_prefix = \";base64,\"\n    if not audio.startswith(data_prefix):\n        raise ValueError(\"Invalid audio data format\")\n\n    format_end = audio.index(base64_prefix)\n    audio_format = audio[len(data_prefix) : format_end]\n    base64_data = audio[format_end + len(base64_prefix) :]\n\n    # Decode the base64 audio data\n    audio_data = base64.b64decode(base64_data)\n\n    # Create a temporary file with the correct extension\n    with tempfile.NamedTemporaryFile(\n        delete=False, suffix=f\".{audio_format}\"\n    ) as temp_audio_file:\n        temp_audio_file.write(audio_data)\n        temp_audio_file_path = temp_audio_file.name\n\n    try:\n        start_time = time.monotonic()\n        word_timestamps = \"word\" in timestamp_granularities\n        \n        result = self.transcribe(temp_audio_file_path, word_timestamps=word_timestamps)        \n            \n        time_cost = time.monotonic() - start_time\n        return [\n            (\n                json.dumps(result, ensure_ascii=True),\n                {\n                    \"metadata\": {\n                        \"request_id\": \"\",\n                        \"input_tokens_count\": 0,\n                        \"generated_tokens_count\": 0,\n                        \"time_cost\": time_cost,\n                        \"first_token_time\": 0,\n                        \"speed\": 0,\n                    }\n                },\n            )\n        ]\n    finally:\n        # Clean up the temporary file\n        import os\n\n        os.unlink(temp_audio_file_path)\n\n\ndef init_model(\n    model_dir, infer_params: Dict[str, str] = {}, sys_conf: Dict[str, str] = {}\n):\n    model = whisper.load_model(\"large\", download_root=model_dir)\n    import types\n\n    model.stream_chat = types.MethodType(stream_chat, model)\n    model.get_meta = types.MethodType(get_meta, model)\n    return (model, None)\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/sensevoice/__init__.py\nimport json\nimport base64\nimport tempfile\nimport time\nfrom typing import Dict, List, Union, Any\nfrom funasr import AutoModel\n\nemo_dict = {\n    \"<|HAPPY|>\": \"😊\",\n    \"<|SAD|>\": \"😔\",\n    \"<|ANGRY|>\": \"😡\",\n    \"<|NEUTRAL|>\": \"\",\n    \"<|FEARFUL|>\": \"😰\",\n    \"<|DISGUSTED|>\": \"🤢\",\n    \"<|SURPRISED|>\": \"😮\",\n}\n\nevent_dict = {\n    \"<|BGM|>\": \"🎼\",\n    \"<|Speech|>\": \"\",\n    \"<|Applause|>\": \"👏\",\n    \"<|Laughter|>\": \"��\",\n    \"<|Cry|>\": \"😭\",\n    \"<|Sneeze|>\": \"🤧\",\n    \"<|Breath|>\": \"\",\n    \"<|Cough|>\": \"🤧\",\n}\n\nemoji_dict = {\n    \"<|nospeech|><|Event_UNK|>\": \"❓\",\n    \"<|zh|>\": \"\",\n    \"<|en|>\": \"\",\n    \"<|yue|>\": \"\",\n    \"<|ja|>\": \"\",\n    \"<|ko|>\": \"\",\n    \"<|nospeech|>\": \"\",\n    \"<|HAPPY|>\": \"😊\",\n    \"<|SAD|>\": \"😔\",\n    \"<|ANGRY|>\": \"😡\",\n    \"<|NEUTRAL|>\": \"\",\n    \"<|BGM|>\": \"🎼\",\n    \"<|Speech|>\": \"\",\n    \"<|Applause|>\": \"👏\",\n    \"<|Laughter|>\": \"😀\",\n    \"<|FEARFUL|>\": \"😰\",\n    \"<|DISGUSTED|>\": \"🤢\",\n    \"<|SURPRISED|>\": \"😮\",\n    \"<|Cry|>\": \"😭\",\n    \"<|EMO_UNKNOWN|>\": \"\",\n    \"<|Sneeze|>\": \"🤧\",\n    \"<|Breath|>\": \"\",\n    \"<|Cough|>\": \"😷\",\n    \"<|Sing|>\": \"\",\n    \"<|Speech_Noise|>\": \"\",\n    \"<|withitn|>\": \"\",\n    \"<|woitn|>\": \"\",\n    \"<|GBG|>\": \"\",\n    \"<|Event_UNK|>\": \"\",\n}\n\nlang_dict = {\n    \"<|zh|>\": \"<|lang|>\",\n    \"<|en|>\": \"<|lang|>\",\n    \"<|yue|>\": \"<|lang|>\",\n    \"<|ja|>\": \"<|lang|>\",\n    \"<|ko|>\": \"<|lang|>\",\n    \"<|nospeech|>\": \"<|lang|>\",\n}\n\nemo_set = {\"😊\", \"😔\", \"😡\", \"😰\", \"🤢\", \"😮\"}\nevent_set = {\"🎼\", \"👏\", \"😀\", \"😭\", \"🤧\", \"😷\"}\n\n\ndef format_str(s):\n    for sptk in emoji_dict:\n        s = s.replace(sptk, emoji_dict[sptk])\n    return s\n\n\ndef format_str_v2(s):\n    sptk_dict = {sptk: s.count(sptk) for sptk in emoji_dict}\n    s = \"\".join(c for c in s if c not in emoji_dict.values())\n    emo = max(emo_dict, key=lambda e: sptk_dict[e])\n    event = \"\".join(event_dict[e] for e in event_dict if sptk_dict[e] > 0)\n    s = event + s + emo_dict[emo]\n    for emoji in emo_set.union(event_set):\n        s = s.replace(\" \" + emoji, emoji).replace(emoji + \" \", emoji)\n    return s.strip()\n\n\ndef format_str_v3(s):\n    def get_emo(s):\n        return s[-1] if s[-1] in emo_set else None\n\n    def get_event(s):\n        return s[0] if s[0] in event_set else None\n\n    s = s.replace(\"<|nospeech|><|Event_UNK|>\", \"❓\")\n    for lang in lang_dict:\n        s = s.replace(lang, \"<|lang|>\")\n    s_list = [format_str_v2(s_i).strip() for s_i in s.split(\"<|lang|>\")]\n    new_s = \" \" + s_list[0]\n    cur_ent_event = get_event(new_s)\n    for i in range(1, len(s_list)):\n        if len(s_list[i]) == 0:\n            continue\n        if get_event(s_list[i]) == cur_ent_event and get_event(s_list[i]) is not None:\n            s_list[i] = s_list[i][1:]\n        cur_ent_event = get_event(s_list[i])\n        if get_emo(s_list[i]) is not None and get_emo(s_list[i]) == get_emo(new_s):\n            new_s = new_s[:-1]\n        new_s += s_list[i].strip()\n    new_s = new_s.replace(\"The.\", \" \")\n    return new_s.strip()\n\n\ndef get_meta(self):\n    return [\n        {\n            \"model_deploy_type\": \"proprietary\",\n            \"backend\": \"funasr\",\n            \"message_format\": True,\n        }\n    ]\n\n\ndef process_input(ins: Union[str, List[Dict[str, Any]], Dict[str, Any]]):\n    if isinstance(ins, list) or isinstance(ins, dict):\n        return ins\n\n    try:\n        ins_json = json.loads(ins)\n    except:\n        return ins\n\n    if isinstance(ins_json, dict):\n        return ins_json\n\n    content = []\n    for item in ins_json:\n        if \"audio\" in item and \"type\" not in item:\n            audio_data = item.get(\"audio\", \"\")\n            if not audio_data.startswith(\"data:\"):\n                audio_data = f\"data:audio/wav;base64,{audio_data}\"\n            content.append({\"audio\": audio_data, \"type\": \"audio\"})\n\n        if \"text\" in item and \"type\" not in item:\n            content.append({\"text\": item[\"text\"], \"type\": \"text\"})\n\n        if \"type\" in item:\n            content.append(item)\n\n    return content if content else ins\n\n\ndef stream_chat(\n    self,\n    tokenizer,\n    ins: str,\n    his: List[Dict[str, str]] = [],\n    max_length: int = 4090,\n    top_p: float = 0.95,\n    temperature: float = 0.1,\n    **kwargs,\n):\n    messages = [\n        {\"role\": message[\"role\"], \"content\": process_input(message[\"content\"])}\n        for message in his\n    ] + [{\"role\": \"user\", \"content\": process_input(ins)}]\n\n    audio_input = messages[-1][\"content\"]\n\n    audio = audio_input[\"audio\"]\n    data_prefix = \"data:audio/\"\n    base64_prefix = \";base64,\"\n    if not audio.startswith(data_prefix):\n        raise ValueError(\"Invalid audio data format\")\n\n    format_end = audio.index(base64_prefix)\n    audio_format = audio[len(data_prefix) : format_end]\n    base64_data = audio[format_end + len(base64_prefix) :]\n\n    audio_data = base64.b64decode(base64_data)\n\n    with tempfile.NamedTemporaryFile(\n        delete=False, suffix=f\".{audio_format}\"\n    ) as temp_audio_file:\n        temp_audio_file.write(audio_data)\n        temp_audio_file_path = temp_audio_file.name\n\n    try:\n        start_time = time.monotonic()\n\n        result = self.generate(input=temp_audio_file_path, cache={})\n\n        time_cost = time.monotonic() - start_time\n\n        formatted_result = format_str_v3(result[0][\"text\"])\n\n        return [\n            (\n                json.dumps({\"text\": formatted_result}, ensure_ascii=False),\n                {\n                    \"metadata\": {\n                        \"request_id\": \"\",\n                        \"input_tokens_count\": 0,\n                        \"generated_tokens_count\": 0,\n                        \"time_cost\": time_cost,\n                        \"first_token_time\": 0,\n                        \"speed\": 0,\n                    }\n                },\n            )\n        ]\n    finally:\n        import os\n\n        os.unlink(temp_audio_file_path)\n\n\ndef init_model(\n    model_dir, infer_params: Dict[str, str] = {}, sys_conf: Dict[str, str] = {}\n):\n    model_kwargs = {\n        \"model\": model_dir,\n        \"trust_remote_code\": True,\n    }\n\n    if \"vad_model\" in infer_params:\n        model_kwargs[\"vad_model\"] = infer_params[\"vad_model\"]\n        model_kwargs[\"vad_kwargs\"] = {\"max_single_segment_time\": 30000}\n\n    model = AutoModel(**model_kwargs)\n\n    import types\n\n    model.stream_chat = types.MethodType(stream_chat, model)\n    model.get_meta = types.MethodType(get_meta, model)\n\n    return (model, None)\n\n\n# Helper functions from web_ui.py\n\n\n     \n\n下面是用户的需求：\n\n在  notebooks 目录里写个运行 @/Users/allwefantasy/projects/byzer-llm/src/byzerllm/sensevoice/__init__.py 测试类,注意是简单的main函数,不是单元测试.参考webui 里的测试文件\n"}, {"role": "assistant", "content": "好的，我会在 notebooks 目录下创建一个简单的测试脚本来运行 SenseVoice 模型。这个脚本将包含一个 main 函数，用于测试 SenseVoice 的功能。我们将参考 webui.py 中的一些代码来构建这个测试。\n\n这里是相应的 SEARCH/REPLACE 块：\n\n```python\n##File: /Users/allwefantasy/projects/byzer-llm/notebooks/test_sensevoice.py\n<<<<<<< SEARCH\n=======\nimport sys\nimport os\n\n# Add the project root to the Python path\nsys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n\nfrom src.byzerllm.sensevoice import init_model, format_str_v3\nimport base64\nimport json\n\ndef main():\n    # Initialize the model\n    model_dir = \"path/to/your/model/directory\"  # Replace with actual path\n    model, _ = init_model(model_dir)\n\n    # Prepare test audio\n    with open(\"path/to/test/audio.wav\", \"rb\") as audio_file:  # Replace with actual path\n        audio_data = audio_file.read()\n        base64_audio = base64.b64encode(audio_data).decode('utf-8')\n\n    # Prepare input in the expected format\n    test_input = json.dumps([{\n        \"role\": \"user\",\n        \"content\": [{\n            \"type\": \"audio\",\n            \"audio\": f\"data:audio/wav;base64,{base64_audio}\"\n        }]\n    }])\n\n    # Call stream_chat\n    result = model.stream_chat(None, test_input)\n\n    # Process and print the result\n    if result and len(result) > 0:\n        output, metadata = result[0]\n        parsed_output = json.loads(output)\n        formatted_text = parsed_output.get('text', '')\n        print(\"Transcription:\", formatted_text)\n        print(\"Metadata:\", metadata)\n    else:\n        print(\"No result returned from the model.\")\n\nif __name__ == \"__main__\":\n    main()\n>>>>>>> REPLACE\n```\n\n这个测试脚本做了以下几件事：\n\n1. 导入必要的模块和函数。\n2. 初始化 SenseVoice 模型。\n3. 读取一个测试音频文件并将其转换为 base64 编码。\n4. 准备输入数据，格式与 SenseVoice 期望的一致。\n5. 调用 `stream_chat` 方法进行语音识别。\n6. 处理并打印结果。\n\n请注意，你需要替换以下内容：\n\n- `\"path/to/your/model/directory\"`: 替换为实际的模型目录路径。\n- `\"path/to/test/audio.wav\"`: 替换为实际的测试音频文件路径。\n\n这个脚本提供了一个简单的方式来测试 SenseVoice 模型的功能。你可以根据需要进一步修改或扩展这个测试。"}], "model": "human", "yaml_file": "actions/022_chat_action.yml"}
{"instruction": "@test_sensevoice.py 中测试数据改成 /Users/allwefantasy/models/SenseVoiceSmall/example/zh.mp3 模型路径改成 /Users/allwefantasy/models/SenseVoiceSmall", "conversations": [{"role": "user", "content": "\n如果你需要生成代码，对于每个需要更改的文件,你需要按 *SEARCH/REPLACE block* 的格式进行生成。\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The opening fence and code language, eg: ```python\n2. The file path alone on a line, starting with \"##File:\" and verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n*SEARCH/REPLACE* blocks will replace *all* matching occurrences.\nInclude enough lines to make the SEARCH blocks unique.\n\nInclude *ALL* the code being searched and replaced!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n下面我们来看一个例子：\n\n当前项目目录结构：\n1. 项目根目录： /tmp/projects/mathweb\n2. 项目子目录/文件列表(类似tree 命令输出)\nflask/\n    app.py\n    templates/\n        index.html\n    static/\n        style.css\n\n用户需求： Change get_factorial() to use math.factorial\n\n回答： To make this change we need to modify `/tmp/projects/mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n用户需求： Refactor hello() into its own file.\n\n回答：To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/hello.py\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/main.py\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n现在让我们开始一个新的任务:\n\n当前项目目录结构：\n1. 项目根目录： /Users/allwefantasy/projects/byzer-llm\n2. 项目子目录/文件列表(类似tree 命令输出)：\nsetup.py\ntests/\n    test_emb_rerank.py\n    test_deepseek_code.py\n    test_prompt.py\n    test_model_meta.py\n    __init__.py\n    test_transformers_deploy.py\n    test_data_analysis.py\n    test_yi.py\n    test_deploy.py\n    functions_suite.py\n    test_template.py\n    auto/\n        test_backend_llama_cpp.py\n        __init__.py\nsetup-machine/\n    convert_safetensor.py\nnotebooks/\n    test.py\n    test_sensevoice.py\nsrc/\n    byzerllm/\n        lang.py\n        store.py\n        version.py\n        log.py\n        __init__.py\n        types.py\n        easy_byzerllm_command.py\n        byzerllm_command.py\n        command_args.py\n        bark/\n            generation.py\n            __init__.py\n            model.py\n            api.py\n            model_fine.py\n            bark_voice.py\n        moss/\n            finetune_moss.py\n            __init__.py\n            moss_inference.py\n            models/\n                configuration_moss.py\n                quantization.py\n                custom_autotune.py\n                __init__.py\n                modeling_moss.py\n                tokenization_moss.py\n        llama2/\n            __init__.py\n        records/\n            __init__.py\n        visualglm/\n            __init__.py\n        auto/\n            __init__.py\n            backend_llama_cpp.py\n            backend_vllm.py\n            backend_ds.py\n        alpha_moss/\n            __init__.py\n        processor/\n            __init__.py\n        sensevoice/\n            __init__.py\n        qwen/\n            __init__.py\n        utils/\n            json_repaire.py\n            object_store_ref_util.py\n            __init__.py\n            types.py\n            tokenizer.py\n            openai_utils.py\n            ray_utils.py\n            text_generator.py\n            emb.py\n            connect_ray.py\n            testing.py\n            langutil.py\n            nontext.py\n            fulltune/\n                deepspeed_trainner.py\n                __init__.py\n                launch.py\n                trainner.py\n                base_model/\n                    __init__.py\n                    configuration_baichuan.py\n                    modeling_baichuan.py\n                pretrain/\n                    __init__.py\n                    convert_to_transformers.py\n            metrics/\n                __init__.py\n            config/\n                __init__.py\n            retrieval/\n                rest.py\n                __init__.py\n                udf.py\n            sft/\n                argument.py\n                collator.py\n                qlora.py\n                __init__.py\n                model.py\n                dataset.py\n                loss.py\n                merge_lora.py\n                trainer.py\n            client/\n                message_utils.py\n                math_utils.py\n                byzerllm_client.py\n                parallel_utils.py\n                __init__.py\n                types.py\n                code_utils.py\n                img_utils.py\n                entrypoints/\n                    __init__.py\n                    openai/\n                        protocol.py\n                        __init__.py\n                        serve.py\n                        serving_chat.py\n                        api_server.py\n                        serving_engine.py\n                        serving_completion.py\n                        tool.py\n        chatglm2/\n            __init__.py\n        baichuan/\n            __init__.py\n        m3e/\n            __init__.py\n        stable_diffusion/\n            config.py\n            __init__.py\n            logger.py\n            shared.py\n            model.py\n            utils.py\n            images.py\n            diffusion/\n                __init__.py\n                embeddings.py\n                utils.py\n                networks/\n                    lyco.py\n                    __init__.py\n                upscalers/\n                    __init__.py\n                    multidiffusion.py\n                    samplers.py\n                piplines/\n                    __init__.py\n                    lpw.py\n                    diffusers.py\n            lib/\n                __init__.py\n                diffusers/\n                    __init__.py\n                    scheduler.py\n            api/\n                __init__.py\n                plugin.py\n                models/\n                    diffusion.py\n                    __init__.py\n                diffusion/\n                    __init__.py\n                    pipelines/\n                        __init__.py\n                        diffusers.py\n                events/\n                    generation.py\n                    __init__.py\n        bge/\n            __init__.py\n        bge_rerank/\n            __init__.py\n        apps/\n            client.py\n            qa_strategy.py\n            __init__.py\n            builder.py\n            vector_db.py\n            utils.py\n            qa.py\n            byzer_storage/\n                memory_model_based.py\n                command.py\n                env.py\n                train_pt.py\n                train_sft.py\n                __init__.py\n                train_dpo.py\n                simple_api.py\n                generate_sft_data.py\n            byzer_sql/\n                __init__.py\n            llama_index/\n                byzerai_vectordb.py\n                byzerai.py\n                byzerai_embedding.py\n                __init__.py\n                collection_manager.py\n                byzerai_index_store.py\n                byzerai_docstore.py\n                simple_retrieval.py\n                byzerai_kvstore.py\n            agent/\n                conversable_agent.py\n                registry.py\n                groupchat.py\n                user_proxy_agent.py\n                __init__.py\n                agent.py\n                extensions/\n                    simple_retrieval_client.py\n                    planner.py\n                    python_codesandbox_agent.py\n                    preview_file_agent.py\n                    llama_index_retrieval_agent.py\n                    visualization_agent.py\n                    assistant_agent.py\n                    load_data_agent.py\n                    retrieval_agent.py\n                    output_agent.py\n                    __init__.py\n                    byzer_engine_agent.py\n                    subquestion_agent.py\n                    sql_reviewer_agent.py\n                    spark_sql_agent.py\n                    rhetorical_agent.py\n                    data_analysis.py\n                    data_analysis_pipeline_agent.py\n                    common_agent.py\n                    query_rewrite/\n                        time.py\n                        rhetorical.py\n                        __init__.py\n                        condition.py\n                        context.py\n                store/\n                    memory_store.py\n                    __init__.py\n                    stores.py\n        saas/\n            __init__.py\n            chatglm/\n                __init__.py\n            sparkdesk/\n                __init__.py\n            gemini/\n                __init__.py\n            azure_openai/\n                __init__.py\n            azure/\n                __init__.py\n            qianfan/\n                __init__.py\n            claude/\n                __init__.py\n            qianwen/\n                __init__.py\n            volcengine/\n                __init__.py\n            minimax/\n                __init__.py\n            baichuan/\n                __init__.py\n            qianwen_vl/\n                __init__.py\n            official_openai/\n                __init__.py\n            zhipu/\n                __init__.py\n            aws_bedrock/\n                __init__.py\n            openai/\n                __init__.py\n        falcon/\n            __init__.py\n        chatglm6b/\n            trainer_seq2seq.py\n            arguments.py\n            __init__.py\n            finetune.py\n            tunning/\n                __init__.py\n                infer.py\n                finetune.py\n                utils/\n                    config.py\n                    __init__.py\n                    other.py\n                    common.py\n                    seq2seq.py\n        starcode/\n            __init__.py\n        qwen_vl_chat/\n            __init__.py\n        zephyr/\n            __init__.py\n        dolly/\n            generate.py\n            __init__.py\n            consts.py\n            dolly_inference.py\n            trainer.py\n        llama/\n            __init__.py\n        whisper/\n            whisper_inference.py\n            __init__.py\n下面是一些文件路径以及每个文件对应的源码：\n\n##File: /Users/allwefantasy/projects/SenseVoice/webui.py\n# coding=utf-8\n\nimport os\nimport librosa\nimport base64\nimport io\nimport gradio as gr\nimport re\n\nimport numpy as np\nimport torch\nimport torchaudio\n\n\nfrom funasr import AutoModel\n\nmodel = \"iic/SenseVoiceSmall\"\nmodel_kwargs = {\n    \"model\": model,\n    \"trust_remote_code\": True,\n}\n\n# Check if vad_model is provided in infer_params\nif \"vad_model\" in infer_params:\n    model_kwargs[\"vad_model\"] = infer_params[\"vad_model\"]\n    model_kwargs[\"vad_kwargs\"] = {\"max_single_segment_time\": 30000}\n\nmodel = AutoModel(**model_kwargs)\n\nimport re\n\nemo_dict = {\n\t\"<|HAPPY|>\": \"😊\",\n\t\"<|SAD|>\": \"😔\",\n\t\"<|ANGRY|>\": \"😡\",\n\t\"<|NEUTRAL|>\": \"\",\n\t\"<|FEARFUL|>\": \"😰\",\n\t\"<|DISGUSTED|>\": \"🤢\",\n\t\"<|SURPRISED|>\": \"😮\",\n}\n\nevent_dict = {\n\t\"<|BGM|>\": \"🎼\",\n\t\"<|Speech|>\": \"\",\n\t\"<|Applause|>\": \"👏\",\n\t\"<|Laughter|>\": \"😀\",\n\t\"<|Cry|>\": \"😭\",\n\t\"<|Sneeze|>\": \"🤧\",\n\t\"<|Breath|>\": \"\",\n\t\"<|Cough|>\": \"🤧\",\n}\n\nemoji_dict = {\n\t\"<|nospeech|><|Event_UNK|>\": \"❓\",\n\t\"<|zh|>\": \"\",\n\t\"<|en|>\": \"\",\n\t\"<|yue|>\": \"\",\n\t\"<|ja|>\": \"\",\n\t\"<|ko|>\": \"\",\n\t\"<|nospeech|>\": \"\",\n\t\"<|HAPPY|>\": \"😊\",\n\t\"<|SAD|>\": \"😔\",\n\t\"<|ANGRY|>\": \"😡\",\n\t\"<|NEUTRAL|>\": \"\",\n\t\"<|BGM|>\": \"🎼\",\n\t\"<|Speech|>\": \"\",\n\t\"<|Applause|>\": \"👏\",\n\t\"<|Laughter|>\": \"😀\",\n\t\"<|FEARFUL|>\": \"😰\",\n\t\"<|DISGUSTED|>\": \"🤢\",\n\t\"<|SURPRISED|>\": \"😮\",\n\t\"<|Cry|>\": \"😭\",\n\t\"<|EMO_UNKNOWN|>\": \"\",\n\t\"<|Sneeze|>\": \"🤧\",\n\t\"<|Breath|>\": \"\",\n\t\"<|Cough|>\": \"😷\",\n\t\"<|Sing|>\": \"\",\n\t\"<|Speech_Noise|>\": \"\",\n\t\"<|withitn|>\": \"\",\n\t\"<|woitn|>\": \"\",\n\t\"<|GBG|>\": \"\",\n\t\"<|Event_UNK|>\": \"\",\n}\n\nlang_dict =  {\n    \"<|zh|>\": \"<|lang|>\",\n    \"<|en|>\": \"<|lang|>\",\n    \"<|yue|>\": \"<|lang|>\",\n    \"<|ja|>\": \"<|lang|>\",\n    \"<|ko|>\": \"<|lang|>\",\n    \"<|nospeech|>\": \"<|lang|>\",\n}\n\nemo_set = {\"😊\", \"😔\", \"😡\", \"😰\", \"🤢\", \"😮\"}\nevent_set = {\"🎼\", \"👏\", \"😀\", \"😭\", \"🤧\", \"😷\",}\n\ndef format_str(s):\n\tfor sptk in emoji_dict:\n\t\ts = s.replace(sptk, emoji_dict[sptk])\n\treturn s\n\n\ndef format_str_v2(s):\n\tsptk_dict = {}\n\tfor sptk in emoji_dict:\n\t\tsptk_dict[sptk] = s.count(sptk)\n\t\ts = s.replace(sptk, \"\")\n\temo = \"<|NEUTRAL|>\"\n\tfor e in emo_dict:\n\t\tif sptk_dict[e] > sptk_dict[emo]:\n\t\t\temo = e\n\tfor e in event_dict:\n\t\tif sptk_dict[e] > 0:\n\t\t\ts = event_dict[e] + s\n\ts = s + emo_dict[emo]\n\n\tfor emoji in emo_set.union(event_set):\n\t\ts = s.replace(\" \" + emoji, emoji)\n\t\ts = s.replace(emoji + \" \", emoji)\n\treturn s.strip()\n\ndef format_str_v3(s):\n\tdef get_emo(s):\n\t\treturn s[-1] if s[-1] in emo_set else None\n\tdef get_event(s):\n\t\treturn s[0] if s[0] in event_set else None\n\n\ts = s.replace(\"<|nospeech|><|Event_UNK|>\", \"❓\")\n\tfor lang in lang_dict:\n\t\ts = s.replace(lang, \"<|lang|>\")\n\ts_list = [format_str_v2(s_i).strip(\" \") for s_i in s.split(\"<|lang|>\")]\n\tnew_s = \" \" + s_list[0]\n\tcur_ent_event = get_event(new_s)\n\tfor i in range(1, len(s_list)):\n\t\tif len(s_list[i]) == 0:\n\t\t\tcontinue\n\t\tif get_event(s_list[i]) == cur_ent_event and get_event(s_list[i]) != None:\n\t\t\ts_list[i] = s_list[i][1:]\n\t\t#else:\n\t\tcur_ent_event = get_event(s_list[i])\n\t\tif get_emo(s_list[i]) != None and get_emo(s_list[i]) == get_emo(new_s):\n\t\t\tnew_s = new_s[:-1]\n\t\tnew_s += s_list[i].strip().lstrip()\n\tnew_s = new_s.replace(\"The.\", \" \")\n\treturn new_s.strip()\n\ndef model_inference(input_wav, language, fs=16000):\n\t# task_abbr = {\"Speech Recognition\": \"ASR\", \"Rich Text Transcription\": (\"ASR\", \"AED\", \"SER\")}\n\tlanguage_abbr = {\"auto\": \"auto\", \"zh\": \"zh\", \"en\": \"en\", \"yue\": \"yue\", \"ja\": \"ja\", \"ko\": \"ko\",\n\t\t\t\t\t \"nospeech\": \"nospeech\"}\n\t\n\t# task = \"Speech Recognition\" if task is None else task\n\tlanguage = \"auto\" if len(language) < 1 else language\n\tselected_language = language_abbr[language]\n\t# selected_task = task_abbr.get(task)\n\t\n\t# print(f\"input_wav: {type(input_wav)}, {input_wav[1].shape}, {input_wav}\")\n\t\n\tif isinstance(input_wav, tuple):\n\t\tfs, input_wav = input_wav\n\t\tinput_wav = input_wav.astype(np.float32) / np.iinfo(np.int16).max\n\t\tif len(input_wav.shape) > 1:\n\t\t\tinput_wav = input_wav.mean(-1)\n\t\tif fs != 16000:\n\t\t\tprint(f\"audio_fs: {fs}\")\n\t\t\tresampler = torchaudio.transforms.Resample(fs, 16000)\n\t\t\tinput_wav_t = torch.from_numpy(input_wav).to(torch.float32)\n\t\t\tinput_wav = resampler(input_wav_t[None, :])[0, :].numpy()\n\t\n\t\n\tmerge_vad = True #False if selected_task == \"ASR\" else True\n\tprint(f\"language: {language}, merge_vad: {merge_vad}\")\n\ttext = model.generate(input=input_wav,\n\t\t\t\t\t\t  cache={},\n\t\t\t\t\t\t  language=language,\n\t\t\t\t\t\t  use_itn=True,\n\t\t\t\t\t\t  batch_size_s=60, merge_vad=merge_vad)\n\t\n\tprint(text)\n\ttext = text[0][\"text\"]\n\ttext = format_str_v3(text)\n\t\n\tprint(text)\n\t\n\treturn text\n\n\naudio_examples = [\n    [\"example/zh.mp3\", \"zh\"],\n    [\"example/yue.mp3\", \"yue\"],\n    [\"example/en.mp3\", \"en\"],\n    [\"example/ja.mp3\", \"ja\"],\n    [\"example/ko.mp3\", \"ko\"],\n    [\"example/emo_1.wav\", \"auto\"],\n    [\"example/emo_2.wav\", \"auto\"],\n    [\"example/emo_3.wav\", \"auto\"],\n    #[\"example/emo_4.wav\", \"auto\"],\n    #[\"example/event_1.wav\", \"auto\"],\n    #[\"example/event_2.wav\", \"auto\"],\n    #[\"example/event_3.wav\", \"auto\"],\n    [\"example/rich_1.wav\", \"auto\"],\n    [\"example/rich_2.wav\", \"auto\"],\n    #[\"example/rich_3.wav\", \"auto\"],\n    [\"example/longwav_1.wav\", \"auto\"],\n    [\"example/longwav_2.wav\", \"auto\"],\n    [\"example/longwav_3.wav\", \"auto\"],\n    #[\"example/longwav_4.wav\", \"auto\"],\n]\n\n\n\nhtml_content = \"\"\"\n<div>\n    <h2 style=\"font-size: 22px;margin-left: 0px;\">Voice Understanding Model: SenseVoice-Small</h2>\n    <p style=\"font-size: 18px;margin-left: 20px;\">SenseVoice-Small is an encoder-only speech foundation model designed for rapid voice understanding. It encompasses a variety of features including automatic speech recognition (ASR), spoken language identification (LID), speech emotion recognition (SER), and acoustic event detection (AED). SenseVoice-Small supports multilingual recognition for Chinese, English, Cantonese, Japanese, and Korean. Additionally, it offers exceptionally low inference latency, performing 7 times faster than Whisper-small and 17 times faster than Whisper-large.</p>\n    <h2 style=\"font-size: 22px;margin-left: 0px;\">Usage</h2> <p style=\"font-size: 18px;margin-left: 20px;\">Upload an audio file or input through a microphone, then select the task and language. the audio is transcribed into corresponding text along with associated emotions (😊 happy, 😡 angry/exicting, 😔 sad) and types of sound events (😀 laughter, 🎼 music, 👏 applause, 🤧 cough&sneeze, 😭 cry). The event labels are placed in the front of the text and the emotion are in the back of the text.</p>\n\t<p style=\"font-size: 18px;margin-left: 20px;\">Recommended audio input duration is below 30 seconds. For audio longer than 30 seconds, local deployment is recommended.</p>\n\t<h2 style=\"font-size: 22px;margin-left: 0px;\">Repo</h2>\n\t<p style=\"font-size: 18px;margin-left: 20px;\"><a href=\"https://github.com/FunAudioLLM/SenseVoice\" target=\"_blank\">SenseVoice</a>: multilingual speech understanding model</p>\n\t<p style=\"font-size: 18px;margin-left: 20px;\"><a href=\"https://github.com/modelscope/FunASR\" target=\"_blank\">FunASR</a>: fundamental speech recognition toolkit</p>\n\t<p style=\"font-size: 18px;margin-left: 20px;\"><a href=\"https://github.com/FunAudioLLM/CosyVoice\" target=\"_blank\">CosyVoice</a>: high-quality multilingual TTS model</p>\n</div>\n\"\"\"\n\n\ndef launch():\n\twith gr.Blocks(theme=gr.themes.Soft()) as demo:\n\t\t# gr.Markdown(description)\n\t\tgr.HTML(html_content)\n\t\twith gr.Row():\n\t\t\twith gr.Column():\n\t\t\t\taudio_inputs = gr.Audio(label=\"Upload audio or use the microphone\")\n\t\t\t\t\n\t\t\t\twith gr.Accordion(\"Configuration\"):\n\t\t\t\t\tlanguage_inputs = gr.Dropdown(choices=[\"auto\", \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"],\n\t\t\t\t\t\t\t\t\t\t\t\t  value=\"auto\",\n\t\t\t\t\t\t\t\t\t\t\t\t  label=\"Language\")\n\t\t\t\tfn_button = gr.Button(\"Start\", variant=\"primary\")\n\t\t\t\ttext_outputs = gr.Textbox(label=\"Results\")\n\t\t\tgr.Examples(examples=audio_examples, inputs=[audio_inputs, language_inputs], examples_per_page=20)\n\t\t\n\t\tfn_button.click(model_inference, inputs=[audio_inputs, language_inputs], outputs=text_outputs)\n\n\tdemo.launch()\n\n\nif __name__ == \"__main__\":\n\t# iface.launch()\n\tlaunch()\n\n\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/saas/official_openai/__init__.py\nimport time\nfrom typing import List, Tuple, Dict, Any, Union\nimport httpx\nfrom openai import OpenAI\nimport base64\nimport io\nimport json\nimport ray\nfrom byzerllm.utils.types import (\n    BlockVLLMStreamServer,\n    StreamOutputs,\n    SingleOutput,\n    SingleOutputMeta,\n    BlockBinaryStreamServer,\n)\nfrom byzerllm.utils.langutil import asyncfy_with_semaphore\nimport threading\nimport asyncio\nimport traceback\nimport uuid\nimport tempfile        \n\nclass CustomSaasAPI:\n\n    def __init__(self, infer_params: Dict[str, str]) -> None:\n\n        self.api_key = infer_params[\"saas.api_key\"]\n        self.model = infer_params.get(\"saas.model\", \"gpt-3.5-turbo-1106\")\n\n        other_params = {}\n\n        if \"saas.api_base\" in infer_params:\n            other_params[\"api_base\"] = infer_params[\"saas.api_base\"]\n\n        if \"saas.api_version\" in infer_params:\n            other_params[\"api_version\"] = infer_params[\"saas.api_version\"]\n\n        if \"saas.api_type\" in infer_params:\n            other_params[\"api_type\"] = infer_params[\"saas.api_type\"]\n\n        if \"saas.base_url\" in infer_params:\n            other_params[\"base_url\"] = infer_params[\"saas.base_url\"]\n\n        if \"saas.timeout\" in infer_params:\n            other_params[\"timeout\"] = float(infer_params[\"saas.timeout\"])\n\n        self.max_retries = int(infer_params.get(\"saas.max_retries\", 10))\n\n        self.meta = {\n            \"model_deploy_type\": \"saas\",\n            \"backend\": \"saas\",\n            \"support_stream\": True,\n            \"model_name\": self.model,\n        }\n\n        self.meta[\"embedding_mode\"] = \"embedding\" in self.model.lower()\n\n        self.proxies = infer_params.get(\"saas.proxies\", None)\n        self.local_address = infer_params.get(\"saas.local_address\", \"0.0.0.0\")\n\n        if not self.proxies:\n            self.client = OpenAI(**other_params, api_key=self.api_key)\n        else:\n            self.client = OpenAI(\n                **other_params,\n                api_key=self.api_key,\n                http_client=httpx.Client(\n                    proxies=self.proxies,\n                    transport=httpx.HTTPTransport(local_address=self.local_address),\n                ),\n            )\n\n        try:\n            ray.get_actor(\"BLOCK_VLLM_STREAM_SERVER\")\n        except ValueError:\n            try:\n                ray.remote(BlockVLLMStreamServer).options(\n                    name=\"BLOCK_VLLM_STREAM_SERVER\",\n                    lifetime=\"detached\",\n                    max_concurrency=1000,\n                ).remote()\n            except Exception as e:\n                pass\n        try:\n            ray.get_actor(\"BlockBinaryStreamServer\")\n        except ValueError:\n            try:\n                ray.remote(BlockBinaryStreamServer).options(\n                    name=\"BlockBinaryStreamServer\",\n                    lifetime=\"detached\",\n                    max_concurrency=1000,\n                ).remote()\n            except Exception as e:\n                pass\n\n    # saas/proprietary\n    def get_meta(self):\n        return [self.meta]\n\n    def process_input(self, ins: Union[str, List[Dict[str, Any]], Dict[str, Any]]):\n\n        if isinstance(ins, list) or isinstance(ins, dict):\n            return ins\n\n        content = []\n        try:\n            ins_json = json.loads(ins)\n        except:\n            return ins\n\n        ## speech\n        if isinstance(ins_json, dict):\n            return ins_json\n\n        content = []\n        #     [\n        #     {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n        #     {\n        #       \"type\": \"image_url\",\n        #       \"image_url\": {\n        #         \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n        #         \"detail\": \"high\"\n        #       },\n        #     },\n        #   ],\n        for item in ins_json:\n            # for format like this: {\"image\": \"xxxxxx\", \"text\": \"What’s in this image?\",\"detail\":\"high\"}\n            # or {\"image\": \"xxxxxx\"}, {\"text\": \"What’s in this image?\"}\n            if (\"image\" in item or \"image_url\" in item) and \"type\" not in item:\n                image_data = item.get(\"image\", item.get(\"image_url\", \"\"))\n                ## \"data:image/jpeg;base64,\"\n                if not image_data.startswith(\"data:\"):\n                    image_data = \"data:image/jpeg;base64,\" + image_data\n\n                ## get the other fields except image/text/image_url\n                other_fields = {\n                    k: v\n                    for k, v in item.items()\n                    if k not in [\"image\", \"text\", \"image_url\"]\n                }\n                content.append(\n                    {\n                        \"image_url\": {\"url\": image_data, **other_fields},\n                        \"type\": \"image_url\",\n                    }\n                )            \n\n            if \"text\" in item and \"type\" not in item:\n                text_data = item[\"text\"]\n                content.append({\"text\": text_data, \"type\": \"text\"})\n\n            ## for format like this: {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n            ## {\"type\": \"image_url\", \"image_url\": {\"url\":\"\",\"detail\":\"high\"}}\n            ## this is the standard format, just return it\n            if \"type\" in item and item[\"type\"] == \"text\":\n                content.append(item)\n\n            if \"type\" in item and item[\"type\"] == \"image_url\":\n                content.append(item)\n\n        if not content:\n            return ins\n\n        return content\n\n    async def async_embed_query(self, ins: str, **kwargs):\n        resp = await asyncfy_with_semaphore(\n            lambda: self.client.embeddings.create(input=[ins], model=self.model)\n        )()\n        embedding = resp.data[0].embedding\n        usage = resp.usage\n        return (\n            embedding,\n            {\n                \"metadata\": {\n                    \"input_tokens_count\": usage.prompt_tokens,\n                    \"generated_tokens_count\": 0,\n                }\n            },\n        )\n\n    def embed_query(self, ins: str, **kwargs):\n        resp = self.client.embeddings.create(input=[ins], model=self.model)\n        embedding = resp.data[0].embedding\n        usage = resp.usage\n        return (\n            embedding,\n            {\n                \"metadata\": {\n                    \"input_tokens_count\": usage.prompt_tokens,\n                    \"generated_tokens_count\": 0,\n                }\n            },\n        )\n\n    async def async_text_to_speech(\n        self, stream: bool, ins: str, voice: str, chunk_size: int = None, **kwargs\n    ):\n        if stream:\n            server = ray.get_actor(\"BlockBinaryStreamServer\")\n            request_id = [None]\n\n            def writer():\n                try:\n                    request_id[0] = str(uuid.uuid4())\n                    with self.client.with_streaming_response.audio.speech.create(\n                        model=self.model, voice=voice, input=ins, **kwargs\n                    ) as response:\n                        for chunk in response.iter_bytes(chunk_size):\n                            input_tokens_count = 0\n                            generated_tokens_count = 0\n                            ray.get(\n                                server.add_item.remote(\n                                    request_id[0],\n                                    StreamOutputs(\n                                        outputs=[\n                                            SingleOutput(\n                                                text=chunk,\n                                                metadata=SingleOutputMeta(\n                                                    input_tokens_count=input_tokens_count,\n                                                    generated_tokens_count=generated_tokens_count,\n                                                ),\n                                            )\n                                        ]\n                                    ),\n                                )\n                            )\n                except:\n                    traceback.print_exc()\n                ray.get(server.mark_done.remote(request_id[0]))\n\n            threading.Thread(target=writer, daemon=True).start()\n\n            time_count = 10 * 100\n            while request_id[0] is None and time_count > 0:\n                time.sleep(0.01)\n                time_count -= 1\n\n            if request_id[0] is None:\n                raise Exception(\"Failed to get request id\")\n\n            def write_running():\n                return ray.get(server.add_item.remote(request_id[0], \"RUNNING\"))\n\n            await asyncio.to_thread(write_running)\n            return [\n                (\n                    \"\",\n                    {\n                        \"metadata\": {\n                            \"request_id\": request_id[0],\n                            \"stream_server\": \"BlockBinaryStreamServer\",\n                        }\n                    },\n                )\n            ]\n\n        start_time = time.monotonic()\n        with io.BytesIO() as output:\n            async with asyncfy_with_semaphore(\n                lambda: self.client.with_streaming_response.audio.speech.create(\n                    model=self.model, voice=voice, input=ins, **kwargs\n                )\n            )() as response:\n                for chunk in response.iter_bytes():\n                    output.write(chunk)\n\n            base64_audio = base64.b64encode(output.getvalue()).decode()\n            time_cost = time.monotonic() - start_time\n            return [\n                (\n                    base64_audio,\n                    {\n                        \"metadata\": {\n                            \"request_id\": \"\",\n                            \"input_tokens_count\": 0,\n                            \"generated_tokens_count\": 0,\n                            \"time_cost\": time_cost,\n                            \"first_token_time\": 0,\n                            \"speed\": 0,\n                        }\n                    },\n                )\n            ]\n\n    async def async_speech_to_text(\n        self,\n        audio: str,\n        response_format: str = \"verbose_json\",\n        timestamp_granularities: List[str] = [\"word\",\"segment\"],\n    ):               \n        # Extract audio format and base64 data\n        data_prefix = \"data:audio/\"\n        base64_prefix = \";base64,\"\n        if not audio.startswith(data_prefix):\n            raise ValueError(\"Invalid audio data format\")\n        \n        format_end = audio.index(base64_prefix)\n        audio_format = audio[len(data_prefix):format_end]\n        base64_data = audio[format_end + len(base64_prefix):]\n        \n        # Decode the base64 audio data\n        audio_data = base64.b64decode(base64_data)\n\n        # Create a temporary file with the correct extension\n        with tempfile.NamedTemporaryFile(delete=False, suffix=f'.{audio_format}') as temp_audio_file:\n            temp_audio_file.write(audio_data)\n            temp_audio_file_path = temp_audio_file.name\n\n        try:\n            start_time = time.monotonic()\n                   \n            with open(temp_audio_file_path, \"rb\") as audio_file:                \n                transcription = self.client.audio.transcriptions.create(\n                    model=self.model or \"whisper-1\",\n                    file=audio_file,\n                    response_format=response_format,\n                    timestamp_granularities=timestamp_granularities,\n                )\n            time_cost = time.monotonic() - start_time\n\n            return [\n                (\n                    json.dumps(transcription.to_dict(), ensure_ascii=True),\n                    {\n                        \"metadata\": {\n                            \"request_id\": \"\",\n                            \"input_tokens_count\": 0,\n                            \"generated_tokens_count\": 0,\n                            \"time_cost\": time_cost,\n                            \"first_token_time\": 0,\n                            \"speed\": 0,\n                        }\n                    },\n                )\n            ]\n        finally:\n            # Clean up the temporary file\n            import os\n            os.unlink(temp_audio_file_path)\n\n    def image_to_text(self, ins: str, **kwargs):\n        pass\n\n    async def async_text_to_image(\n        self, stream: bool, input: str, size: str, quality: str, n: int, **kwargs\n    ):\n        if stream:\n            raise Exception(\"Stream not supported for text to image\")\n        start_time = time.monotonic()\n        response = await asyncfy_with_semaphore(\n            lambda: self.client.images.generate(\n                model=self.model,\n                prompt=input,\n                size=size,\n                quality=quality,\n                n=1,\n                response_format=\"b64_json\",\n                **kwargs,\n            )\n        )()\n        time_cost = time.monotonic() - start_time\n        base64_image = response.data[0].b64_json\n        return [\n            (\n                base64_image,\n                {\n                    \"metadata\": {\n                        \"request_id\": \"\",\n                        \"input_tokens_count\": 0,\n                        \"generated_tokens_count\": 0,\n                        \"time_cost\": time_cost,\n                        \"first_token_time\": 0,\n                        \"speed\": 0,\n                    }\n                },\n            )\n        ]\n\n    def text_to_text(self, ins: str, **kwargs):\n        pass\n\n    async def async_stream_chat(\n        self,\n        tokenizer,\n        ins: str,\n        his: List[Dict[str, Any]] = [],\n        max_length: int = 4096,\n        top_p: float = 0.7,\n        temperature: float = 0.9,\n        **kwargs,\n    ):\n\n        model = self.model\n\n        if \"model\" in kwargs:\n            model = kwargs[\"model\"]\n\n        messages = [\n            {\"role\": message[\"role\"], \"content\": self.process_input(message[\"content\"])}\n            for message in his\n        ] + [{\"role\": \"user\", \"content\": self.process_input(ins)}]\n\n        stream = kwargs.get(\"stream\", False)\n\n        ## content = [\n        ##    \"voice\": \"alloy\",\"input\": \"Hello, World!\",response_format: \"mp3\"]\n        last_message = messages[-1][\"content\"]\n\n        if isinstance(last_message, dict) and \"voice\" in last_message:\n            voice = last_message[\"voice\"]\n            response_format = last_message.get(\"response_format\", \"mp3\")\n            chunk_size = last_message.get(\"chunk_size\", None)\n            input = last_message[\"input\"]\n            return await self.async_text_to_speech(\n                stream=stream,\n                ins=input,\n                voice=voice,\n                chunk_size=chunk_size,\n                response_format=response_format,\n            )\n\n        if isinstance(last_message, dict) and \"input\" in last_message:\n            input = last_message[\"input\"]\n            size = last_message.get(\"size\", \"1024x1024\")\n            quality = last_message.get(\"quality\", \"standard\")\n            n = last_message.get(\"n\", 1)\n            return await self.async_text_to_image(\n                stream=stream, input=input, size=size, quality=quality, n=n\n            )\n\n        if isinstance(last_message, dict) and \"audio\" in last_message:            \n            audio_data = last_message.get(\"audio\", \"\")            \n            tpe = last_message.get(\"type\",\"wav\")\n            if not audio_data.startswith(\"data:\"):\n                audio_data = f\"data:audio/${tpe};base64,\" + audio_data                \n            return await self.async_speech_to_text(audio=audio_data)\n        \n        server = ray.get_actor(\"BLOCK_VLLM_STREAM_SERVER\")\n        request_id = [None]\n\n        def writer():\n            try:\n                r = \"\"\n                response = self.client.chat.completions.create(\n                    messages=messages,\n                    model=model,\n                    stream=True,\n                    max_tokens=max_length,\n                    temperature=temperature,\n                    top_p=top_p,\n                )\n                # input_tokens_count = 0\n                # generated_tokens_count = 0\n\n                request_id[0] = str(uuid.uuid4())\n\n                for chunk in response:\n                    content = chunk.choices[0].delta.content or \"\"\n                    r += content\n                    if hasattr(chunk, \"usage\") and chunk.usage:\n                        input_tokens_count = chunk.usage.prompt_tokens\n                        generated_tokens_count = chunk.usage.completion_tokens\n                    else:\n                        input_tokens_count = 0\n                        generated_tokens_count = 0\n                    ray.get(\n                        server.add_item.remote(\n                            request_id[0],\n                            StreamOutputs(\n                                outputs=[\n                                    SingleOutput(\n                                        text=r,\n                                        metadata=SingleOutputMeta(\n                                            input_tokens_count=input_tokens_count,\n                                            generated_tokens_count=generated_tokens_count,\n                                        ),\n                                    )\n                                ]\n                            ),\n                        )\n                    )\n            except:\n                traceback.print_exc()\n            ray.get(server.mark_done.remote(request_id[0]))\n\n        if stream:\n            threading.Thread(target=writer, daemon=True).start()\n\n            time_count = 60 * 100\n            while request_id[0] is None and time_count > 0:\n                time.sleep(0.01)\n                time_count -= 1\n\n            if request_id[0] is None:\n                raise Exception(\"Failed to get request id\")\n\n            def write_running():\n                return ray.get(server.add_item.remote(request_id[0], \"RUNNING\"))\n\n            await asyncio.to_thread(write_running)\n            return [\n                (\n                    \"\",\n                    {\n                        \"metadata\": {\n                            \"request_id\": request_id[0],\n                            \"stream_server\": \"BLOCK_VLLM_STREAM_SERVER\",\n                        }\n                    },\n                )\n            ]\n        else:\n            try:\n                start_time = time.monotonic()\n                response = await asyncfy_with_semaphore(\n                    lambda: self.client.chat.completions.create(\n                        messages=messages,\n                        model=model,\n                        max_tokens=max_length,\n                        temperature=temperature,\n                        top_p=top_p,\n                    )\n                )()\n\n                generated_text = response.choices[0].message.content\n                generated_tokens_count = response.usage.completion_tokens\n                input_tokens_count = response.usage.prompt_tokens\n                time_cost = time.monotonic() - start_time\n                return [\n                    (\n                        generated_text,\n                        {\n                            \"metadata\": {\n                                \"request_id\": response.id,\n                                \"input_tokens_count\": input_tokens_count,\n                                \"generated_tokens_count\": generated_tokens_count,\n                                \"time_cost\": time_cost,\n                                \"first_token_time\": 0,\n                                \"speed\": float(generated_tokens_count) / time_cost,\n                            }\n                        },\n                    )\n                ]\n            except Exception as e:\n                print(f\"Error: {e}\")\n                raise e\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/whisper/__init__.py\nimport whisper\nfrom typing import Dict, List, Tuple\nfrom byzerllm.utils.types import StopSequencesCriteria\n\nfrom typing import Dict, Any, List, Union\nfrom pyjava.storage import streaming_tar as STar\nimport json\nimport base64\nimport tempfile\nimport time\n\n\ndef get_meta(self):\n    return [\n        {\n            \"model_deploy_type\": \"proprietary\",\n            \"backend\": \"transformers\",\n            \"message_format\": True\n        }\n    ]\n\n\ndef process_input(ins: Union[str, List[Dict[str, Any]], Dict[str, Any]]):\n    # print(ins) \n    if isinstance(ins, list) or isinstance(ins, dict):\n        return ins\n\n    content = []\n    try:\n        ins_json = json.loads(ins)\n    except:\n        return ins\n\n    ## speech\n    if isinstance(ins_json, dict):\n        return ins_json\n\n    content = []\n    #     [\n    #     {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n    #     {\n    #       \"type\": \"image_url\",\n    #       \"image_url\": {\n    #         \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n    #         \"detail\": \"high\"\n    #       },\n    #     },\n    #   ],\n    for item in ins_json:\n        # for format like this: {\"image\": \"xxxxxx\", \"text\": \"What’s in this image?\",\"detail\":\"high\"}\n        # or {\"image\": \"xxxxxx\"}, {\"text\": \"What’s in this image?\"}\n        if (\"image\" in item or \"image_url\" in item) and \"type\" not in item:\n            image_data = item.get(\"image\", item.get(\"image_url\", \"\"))\n            ## \"data:image/jpeg;base64,\"\n            if not image_data.startswith(\"data:\"):\n                image_data = \"data:image/jpeg;base64,\" + image_data\n\n            ## get the other fields except image/text/image_url\n            other_fields = {\n                k: v for k, v in item.items() if k not in [\"image\", \"text\", \"image_url\"]\n            }\n            content.append(\n                {\n                    \"image_url\": {\"url\": image_data, **other_fields},\n                    \"type\": \"image_url\",\n                }\n           \n\n下面是用户的需求：\n\n@test_sensevoice.py 中测试数据改成 /Users/allwefantasy/models/SenseVoiceSmall/example/zh.mp3 模型路径改成 /Users/allwefantasy/models/SenseVoiceSmall\n"}, {"role": "assistant", "content": "为了满足用户的需求，我们需要修改 `/Users/allwefantasy/projects/byzer-llm/tests/test_sensevoice.py` 文件中的测试数据路径和模型路径。以下是相应的 *SEARCH/REPLACE* 块：\n\n```python\n##File: /Users/allwefantasy/projects/byzer-llm/tests/test_sensevoice.py\n<<<<<<< SEARCH\naudio_path = \"example/zh.mp3\"\nmodel_path = \"iic/SenseVoiceSmall\"\n=======\naudio_path = \"/Users/allwefantasy/models/SenseVoiceSmall/example/zh.mp3\"\nmodel_path = \"/Users/allwefantasy/models/SenseVoiceSmall\"\n>>>>>>> REPLACE\n```\n\n这个 *SEARCH/REPLACE* 块将确保测试数据路径和模型路径被正确更新。"}], "model": "deepseek_chat", "yaml_file": "actions/024_chat_action.yml"}
{"instruction": "@test_sensevoice.py 中测试数据改成 /Users/allwefantasy/models/SenseVoiceSmall/example/zh.mp3 模型路径改成 /Users/allwefantasy/models/SenseVoiceSmall", "conversations": [{"role": "user", "content": "\n如果你需要生成代码，对于每个需要更改的文件,你需要按 *SEARCH/REPLACE block* 的格式进行生成。\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The opening fence and code language, eg: ```python\n2. The file path alone on a line, starting with \"##File:\" and verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n*SEARCH/REPLACE* blocks will replace *all* matching occurrences.\nInclude enough lines to make the SEARCH blocks unique.\n\nInclude *ALL* the code being searched and replaced!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n下面我们来看一个例子：\n\n当前项目目录结构：\n1. 项目根目录： /tmp/projects/mathweb\n2. 项目子目录/文件列表(类似tree 命令输出)\nflask/\n    app.py\n    templates/\n        index.html\n    static/\n        style.css\n\n用户需求： Change get_factorial() to use math.factorial\n\n回答： To make this change we need to modify `/tmp/projects/mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n用户需求： Refactor hello() into its own file.\n\n回答：To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/hello.py\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/main.py\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n现在让我们开始一个新的任务:\n\n当前项目目录结构：\n1. 项目根目录： /Users/allwefantasy/projects/byzer-llm\n2. 项目子目录/文件列表(类似tree 命令输出)：\nsetup.py\ntests/\n    test_emb_rerank.py\n    test_deepseek_code.py\n    test_prompt.py\n    test_model_meta.py\n    __init__.py\n    test_transformers_deploy.py\n    test_data_analysis.py\n    test_yi.py\n    test_deploy.py\n    functions_suite.py\n    test_template.py\n    auto/\n        test_backend_llama_cpp.py\n        __init__.py\nsetup-machine/\n    convert_safetensor.py\nnotebooks/\n    test.py\n    test_sensevoice.py\nsrc/\n    byzerllm/\n        lang.py\n        store.py\n        version.py\n        log.py\n        __init__.py\n        types.py\n        easy_byzerllm_command.py\n        byzerllm_command.py\n        command_args.py\n        bark/\n            generation.py\n            __init__.py\n            model.py\n            api.py\n            model_fine.py\n            bark_voice.py\n        moss/\n            finetune_moss.py\n            __init__.py\n            moss_inference.py\n            models/\n                configuration_moss.py\n                quantization.py\n                custom_autotune.py\n                __init__.py\n                modeling_moss.py\n                tokenization_moss.py\n        llama2/\n            __init__.py\n        records/\n            __init__.py\n        visualglm/\n            __init__.py\n        auto/\n            __init__.py\n            backend_llama_cpp.py\n            backend_vllm.py\n            backend_ds.py\n        alpha_moss/\n            __init__.py\n        processor/\n            __init__.py\n        sensevoice/\n            __init__.py\n        qwen/\n            __init__.py\n        utils/\n            json_repaire.py\n            object_store_ref_util.py\n            __init__.py\n            types.py\n            tokenizer.py\n            openai_utils.py\n            ray_utils.py\n            text_generator.py\n            emb.py\n            connect_ray.py\n            testing.py\n            langutil.py\n            nontext.py\n            fulltune/\n                deepspeed_trainner.py\n                __init__.py\n                launch.py\n                trainner.py\n                base_model/\n                    __init__.py\n                    configuration_baichuan.py\n                    modeling_baichuan.py\n                pretrain/\n                    __init__.py\n                    convert_to_transformers.py\n            metrics/\n                __init__.py\n            config/\n                __init__.py\n            retrieval/\n                rest.py\n                __init__.py\n                udf.py\n            sft/\n                argument.py\n                collator.py\n                qlora.py\n                __init__.py\n                model.py\n                dataset.py\n                loss.py\n                merge_lora.py\n                trainer.py\n            client/\n                message_utils.py\n                math_utils.py\n                byzerllm_client.py\n                parallel_utils.py\n                __init__.py\n                types.py\n                code_utils.py\n                img_utils.py\n                entrypoints/\n                    __init__.py\n                    openai/\n                        protocol.py\n                        __init__.py\n                        serve.py\n                        serving_chat.py\n                        api_server.py\n                        serving_engine.py\n                        serving_completion.py\n                        tool.py\n        chatglm2/\n            __init__.py\n        baichuan/\n            __init__.py\n        m3e/\n            __init__.py\n        stable_diffusion/\n            config.py\n            __init__.py\n            logger.py\n            shared.py\n            model.py\n            utils.py\n            images.py\n            diffusion/\n                __init__.py\n                embeddings.py\n                utils.py\n                networks/\n                    lyco.py\n                    __init__.py\n                upscalers/\n                    __init__.py\n                    multidiffusion.py\n                    samplers.py\n                piplines/\n                    __init__.py\n                    lpw.py\n                    diffusers.py\n            lib/\n                __init__.py\n                diffusers/\n                    __init__.py\n                    scheduler.py\n            api/\n                __init__.py\n                plugin.py\n                models/\n                    diffusion.py\n                    __init__.py\n                diffusion/\n                    __init__.py\n                    pipelines/\n                        __init__.py\n                        diffusers.py\n                events/\n                    generation.py\n                    __init__.py\n        bge/\n            __init__.py\n        bge_rerank/\n            __init__.py\n        apps/\n            client.py\n            qa_strategy.py\n            __init__.py\n            builder.py\n            vector_db.py\n            utils.py\n            qa.py\n            byzer_storage/\n                memory_model_based.py\n                command.py\n                env.py\n                train_pt.py\n                train_sft.py\n                __init__.py\n                train_dpo.py\n                simple_api.py\n                generate_sft_data.py\n            byzer_sql/\n                __init__.py\n            llama_index/\n                byzerai_vectordb.py\n                byzerai.py\n                byzerai_embedding.py\n                __init__.py\n                collection_manager.py\n                byzerai_index_store.py\n                byzerai_docstore.py\n                simple_retrieval.py\n                byzerai_kvstore.py\n            agent/\n                conversable_agent.py\n                registry.py\n                groupchat.py\n                user_proxy_agent.py\n                __init__.py\n                agent.py\n                extensions/\n                    simple_retrieval_client.py\n                    planner.py\n                    python_codesandbox_agent.py\n                    preview_file_agent.py\n                    llama_index_retrieval_agent.py\n                    visualization_agent.py\n                    assistant_agent.py\n                    load_data_agent.py\n                    retrieval_agent.py\n                    output_agent.py\n                    __init__.py\n                    byzer_engine_agent.py\n                    subquestion_agent.py\n                    sql_reviewer_agent.py\n                    spark_sql_agent.py\n                    rhetorical_agent.py\n                    data_analysis.py\n                    data_analysis_pipeline_agent.py\n                    common_agent.py\n                    query_rewrite/\n                        time.py\n                        rhetorical.py\n                        __init__.py\n                        condition.py\n                        context.py\n                store/\n                    memory_store.py\n                    __init__.py\n                    stores.py\n        saas/\n            __init__.py\n            chatglm/\n                __init__.py\n            sparkdesk/\n                __init__.py\n            gemini/\n                __init__.py\n            azure_openai/\n                __init__.py\n            azure/\n                __init__.py\n            qianfan/\n                __init__.py\n            claude/\n                __init__.py\n            qianwen/\n                __init__.py\n            volcengine/\n                __init__.py\n            minimax/\n                __init__.py\n            baichuan/\n                __init__.py\n            qianwen_vl/\n                __init__.py\n            official_openai/\n                __init__.py\n            zhipu/\n                __init__.py\n            aws_bedrock/\n                __init__.py\n            openai/\n                __init__.py\n        falcon/\n            __init__.py\n        chatglm6b/\n            trainer_seq2seq.py\n            arguments.py\n            __init__.py\n            finetune.py\n            tunning/\n                __init__.py\n                infer.py\n                finetune.py\n                utils/\n                    config.py\n                    __init__.py\n                    other.py\n                    common.py\n                    seq2seq.py\n        starcode/\n            __init__.py\n        qwen_vl_chat/\n            __init__.py\n        zephyr/\n            __init__.py\n        dolly/\n            generate.py\n            __init__.py\n            consts.py\n            dolly_inference.py\n            trainer.py\n        llama/\n            __init__.py\n        whisper/\n            whisper_inference.py\n            __init__.py\n下面是一些文件路径以及每个文件对应的源码：\n\n##File: /Users/allwefantasy/projects/SenseVoice/webui.py\n# coding=utf-8\n\nimport os\nimport librosa\nimport base64\nimport io\nimport gradio as gr\nimport re\n\nimport numpy as np\nimport torch\nimport torchaudio\n\n\nfrom funasr import AutoModel\n\nmodel = \"iic/SenseVoiceSmall\"\nmodel_kwargs = {\n    \"model\": model,\n    \"trust_remote_code\": True,\n}\n\n# Check if vad_model is provided in infer_params\nif \"vad_model\" in infer_params:\n    model_kwargs[\"vad_model\"] = infer_params[\"vad_model\"]\n    model_kwargs[\"vad_kwargs\"] = {\"max_single_segment_time\": 30000}\n\nmodel = AutoModel(**model_kwargs)\n\nimport re\n\nemo_dict = {\n\t\"<|HAPPY|>\": \"😊\",\n\t\"<|SAD|>\": \"😔\",\n\t\"<|ANGRY|>\": \"😡\",\n\t\"<|NEUTRAL|>\": \"\",\n\t\"<|FEARFUL|>\": \"😰\",\n\t\"<|DISGUSTED|>\": \"🤢\",\n\t\"<|SURPRISED|>\": \"😮\",\n}\n\nevent_dict = {\n\t\"<|BGM|>\": \"🎼\",\n\t\"<|Speech|>\": \"\",\n\t\"<|Applause|>\": \"👏\",\n\t\"<|Laughter|>\": \"😀\",\n\t\"<|Cry|>\": \"😭\",\n\t\"<|Sneeze|>\": \"🤧\",\n\t\"<|Breath|>\": \"\",\n\t\"<|Cough|>\": \"🤧\",\n}\n\nemoji_dict = {\n\t\"<|nospeech|><|Event_UNK|>\": \"❓\",\n\t\"<|zh|>\": \"\",\n\t\"<|en|>\": \"\",\n\t\"<|yue|>\": \"\",\n\t\"<|ja|>\": \"\",\n\t\"<|ko|>\": \"\",\n\t\"<|nospeech|>\": \"\",\n\t\"<|HAPPY|>\": \"😊\",\n\t\"<|SAD|>\": \"😔\",\n\t\"<|ANGRY|>\": \"😡\",\n\t\"<|NEUTRAL|>\": \"\",\n\t\"<|BGM|>\": \"🎼\",\n\t\"<|Speech|>\": \"\",\n\t\"<|Applause|>\": \"👏\",\n\t\"<|Laughter|>\": \"😀\",\n\t\"<|FEARFUL|>\": \"😰\",\n\t\"<|DISGUSTED|>\": \"🤢\",\n\t\"<|SURPRISED|>\": \"😮\",\n\t\"<|Cry|>\": \"😭\",\n\t\"<|EMO_UNKNOWN|>\": \"\",\n\t\"<|Sneeze|>\": \"🤧\",\n\t\"<|Breath|>\": \"\",\n\t\"<|Cough|>\": \"😷\",\n\t\"<|Sing|>\": \"\",\n\t\"<|Speech_Noise|>\": \"\",\n\t\"<|withitn|>\": \"\",\n\t\"<|woitn|>\": \"\",\n\t\"<|GBG|>\": \"\",\n\t\"<|Event_UNK|>\": \"\",\n}\n\nlang_dict =  {\n    \"<|zh|>\": \"<|lang|>\",\n    \"<|en|>\": \"<|lang|>\",\n    \"<|yue|>\": \"<|lang|>\",\n    \"<|ja|>\": \"<|lang|>\",\n    \"<|ko|>\": \"<|lang|>\",\n    \"<|nospeech|>\": \"<|lang|>\",\n}\n\nemo_set = {\"😊\", \"😔\", \"😡\", \"😰\", \"🤢\", \"😮\"}\nevent_set = {\"🎼\", \"👏\", \"😀\", \"😭\", \"🤧\", \"😷\",}\n\ndef format_str(s):\n\tfor sptk in emoji_dict:\n\t\ts = s.replace(sptk, emoji_dict[sptk])\n\treturn s\n\n\ndef format_str_v2(s):\n\tsptk_dict = {}\n\tfor sptk in emoji_dict:\n\t\tsptk_dict[sptk] = s.count(sptk)\n\t\ts = s.replace(sptk, \"\")\n\temo = \"<|NEUTRAL|>\"\n\tfor e in emo_dict:\n\t\tif sptk_dict[e] > sptk_dict[emo]:\n\t\t\temo = e\n\tfor e in event_dict:\n\t\tif sptk_dict[e] > 0:\n\t\t\ts = event_dict[e] + s\n\ts = s + emo_dict[emo]\n\n\tfor emoji in emo_set.union(event_set):\n\t\ts = s.replace(\" \" + emoji, emoji)\n\t\ts = s.replace(emoji + \" \", emoji)\n\treturn s.strip()\n\ndef format_str_v3(s):\n\tdef get_emo(s):\n\t\treturn s[-1] if s[-1] in emo_set else None\n\tdef get_event(s):\n\t\treturn s[0] if s[0] in event_set else None\n\n\ts = s.replace(\"<|nospeech|><|Event_UNK|>\", \"❓\")\n\tfor lang in lang_dict:\n\t\ts = s.replace(lang, \"<|lang|>\")\n\ts_list = [format_str_v2(s_i).strip(\" \") for s_i in s.split(\"<|lang|>\")]\n\tnew_s = \" \" + s_list[0]\n\tcur_ent_event = get_event(new_s)\n\tfor i in range(1, len(s_list)):\n\t\tif len(s_list[i]) == 0:\n\t\t\tcontinue\n\t\tif get_event(s_list[i]) == cur_ent_event and get_event(s_list[i]) != None:\n\t\t\ts_list[i] = s_list[i][1:]\n\t\t#else:\n\t\tcur_ent_event = get_event(s_list[i])\n\t\tif get_emo(s_list[i]) != None and get_emo(s_list[i]) == get_emo(new_s):\n\t\t\tnew_s = new_s[:-1]\n\t\tnew_s += s_list[i].strip().lstrip()\n\tnew_s = new_s.replace(\"The.\", \" \")\n\treturn new_s.strip()\n\ndef model_inference(input_wav, language, fs=16000):\n\t# task_abbr = {\"Speech Recognition\": \"ASR\", \"Rich Text Transcription\": (\"ASR\", \"AED\", \"SER\")}\n\tlanguage_abbr = {\"auto\": \"auto\", \"zh\": \"zh\", \"en\": \"en\", \"yue\": \"yue\", \"ja\": \"ja\", \"ko\": \"ko\",\n\t\t\t\t\t \"nospeech\": \"nospeech\"}\n\t\n\t# task = \"Speech Recognition\" if task is None else task\n\tlanguage = \"auto\" if len(language) < 1 else language\n\tselected_language = language_abbr[language]\n\t# selected_task = task_abbr.get(task)\n\t\n\t# print(f\"input_wav: {type(input_wav)}, {input_wav[1].shape}, {input_wav}\")\n\t\n\tif isinstance(input_wav, tuple):\n\t\tfs, input_wav = input_wav\n\t\tinput_wav = input_wav.astype(np.float32) / np.iinfo(np.int16).max\n\t\tif len(input_wav.shape) > 1:\n\t\t\tinput_wav = input_wav.mean(-1)\n\t\tif fs != 16000:\n\t\t\tprint(f\"audio_fs: {fs}\")\n\t\t\tresampler = torchaudio.transforms.Resample(fs, 16000)\n\t\t\tinput_wav_t = torch.from_numpy(input_wav).to(torch.float32)\n\t\t\tinput_wav = resampler(input_wav_t[None, :])[0, :].numpy()\n\t\n\t\n\tmerge_vad = True #False if selected_task == \"ASR\" else True\n\tprint(f\"language: {language}, merge_vad: {merge_vad}\")\n\ttext = model.generate(input=input_wav,\n\t\t\t\t\t\t  cache={},\n\t\t\t\t\t\t  language=language,\n\t\t\t\t\t\t  use_itn=True,\n\t\t\t\t\t\t  batch_size_s=60, merge_vad=merge_vad)\n\t\n\tprint(text)\n\ttext = text[0][\"text\"]\n\ttext = format_str_v3(text)\n\t\n\tprint(text)\n\t\n\treturn text\n\n\naudio_examples = [\n    [\"example/zh.mp3\", \"zh\"],\n    [\"example/yue.mp3\", \"yue\"],\n    [\"example/en.mp3\", \"en\"],\n    [\"example/ja.mp3\", \"ja\"],\n    [\"example/ko.mp3\", \"ko\"],\n    [\"example/emo_1.wav\", \"auto\"],\n    [\"example/emo_2.wav\", \"auto\"],\n    [\"example/emo_3.wav\", \"auto\"],\n    #[\"example/emo_4.wav\", \"auto\"],\n    #[\"example/event_1.wav\", \"auto\"],\n    #[\"example/event_2.wav\", \"auto\"],\n    #[\"example/event_3.wav\", \"auto\"],\n    [\"example/rich_1.wav\", \"auto\"],\n    [\"example/rich_2.wav\", \"auto\"],\n    #[\"example/rich_3.wav\", \"auto\"],\n    [\"example/longwav_1.wav\", \"auto\"],\n    [\"example/longwav_2.wav\", \"auto\"],\n    [\"example/longwav_3.wav\", \"auto\"],\n    #[\"example/longwav_4.wav\", \"auto\"],\n]\n\n\n\nhtml_content = \"\"\"\n<div>\n    <h2 style=\"font-size: 22px;margin-left: 0px;\">Voice Understanding Model: SenseVoice-Small</h2>\n    <p style=\"font-size: 18px;margin-left: 20px;\">SenseVoice-Small is an encoder-only speech foundation model designed for rapid voice understanding. It encompasses a variety of features including automatic speech recognition (ASR), spoken language identification (LID), speech emotion recognition (SER), and acoustic event detection (AED). SenseVoice-Small supports multilingual recognition for Chinese, English, Cantonese, Japanese, and Korean. Additionally, it offers exceptionally low inference latency, performing 7 times faster than Whisper-small and 17 times faster than Whisper-large.</p>\n    <h2 style=\"font-size: 22px;margin-left: 0px;\">Usage</h2> <p style=\"font-size: 18px;margin-left: 20px;\">Upload an audio file or input through a microphone, then select the task and language. the audio is transcribed into corresponding text along with associated emotions (😊 happy, 😡 angry/exicting, 😔 sad) and types of sound events (😀 laughter, 🎼 music, 👏 applause, 🤧 cough&sneeze, 😭 cry). The event labels are placed in the front of the text and the emotion are in the back of the text.</p>\n\t<p style=\"font-size: 18px;margin-left: 20px;\">Recommended audio input duration is below 30 seconds. For audio longer than 30 seconds, local deployment is recommended.</p>\n\t<h2 style=\"font-size: 22px;margin-left: 0px;\">Repo</h2>\n\t<p style=\"font-size: 18px;margin-left: 20px;\"><a href=\"https://github.com/FunAudioLLM/SenseVoice\" target=\"_blank\">SenseVoice</a>: multilingual speech understanding model</p>\n\t<p style=\"font-size: 18px;margin-left: 20px;\"><a href=\"https://github.com/modelscope/FunASR\" target=\"_blank\">FunASR</a>: fundamental speech recognition toolkit</p>\n\t<p style=\"font-size: 18px;margin-left: 20px;\"><a href=\"https://github.com/FunAudioLLM/CosyVoice\" target=\"_blank\">CosyVoice</a>: high-quality multilingual TTS model</p>\n</div>\n\"\"\"\n\n\ndef launch():\n\twith gr.Blocks(theme=gr.themes.Soft()) as demo:\n\t\t# gr.Markdown(description)\n\t\tgr.HTML(html_content)\n\t\twith gr.Row():\n\t\t\twith gr.Column():\n\t\t\t\taudio_inputs = gr.Audio(label=\"Upload audio or use the microphone\")\n\t\t\t\t\n\t\t\t\twith gr.Accordion(\"Configuration\"):\n\t\t\t\t\tlanguage_inputs = gr.Dropdown(choices=[\"auto\", \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"],\n\t\t\t\t\t\t\t\t\t\t\t\t  value=\"auto\",\n\t\t\t\t\t\t\t\t\t\t\t\t  label=\"Language\")\n\t\t\t\tfn_button = gr.Button(\"Start\", variant=\"primary\")\n\t\t\t\ttext_outputs = gr.Textbox(label=\"Results\")\n\t\t\tgr.Examples(examples=audio_examples, inputs=[audio_inputs, language_inputs], examples_per_page=20)\n\t\t\n\t\tfn_button.click(model_inference, inputs=[audio_inputs, language_inputs], outputs=text_outputs)\n\n\tdemo.launch()\n\n\nif __name__ == \"__main__\":\n\t# iface.launch()\n\tlaunch()\n\n\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/saas/official_openai/__init__.py\nimport time\nfrom typing import List, Tuple, Dict, Any, Union\nimport httpx\nfrom openai import OpenAI\nimport base64\nimport io\nimport json\nimport ray\nfrom byzerllm.utils.types import (\n    BlockVLLMStreamServer,\n    StreamOutputs,\n    SingleOutput,\n    SingleOutputMeta,\n    BlockBinaryStreamServer,\n)\nfrom byzerllm.utils.langutil import asyncfy_with_semaphore\nimport threading\nimport asyncio\nimport traceback\nimport uuid\nimport tempfile        \n\nclass CustomSaasAPI:\n\n    def __init__(self, infer_params: Dict[str, str]) -> None:\n\n        self.api_key = infer_params[\"saas.api_key\"]\n        self.model = infer_params.get(\"saas.model\", \"gpt-3.5-turbo-1106\")\n\n        other_params = {}\n\n        if \"saas.api_base\" in infer_params:\n            other_params[\"api_base\"] = infer_params[\"saas.api_base\"]\n\n        if \"saas.api_version\" in infer_params:\n            other_params[\"api_version\"] = infer_params[\"saas.api_version\"]\n\n        if \"saas.api_type\" in infer_params:\n            other_params[\"api_type\"] = infer_params[\"saas.api_type\"]\n\n        if \"saas.base_url\" in infer_params:\n            other_params[\"base_url\"] = infer_params[\"saas.base_url\"]\n\n        if \"saas.timeout\" in infer_params:\n            other_params[\"timeout\"] = float(infer_params[\"saas.timeout\"])\n\n        self.max_retries = int(infer_params.get(\"saas.max_retries\", 10))\n\n        self.meta = {\n            \"model_deploy_type\": \"saas\",\n            \"backend\": \"saas\",\n            \"support_stream\": True,\n            \"model_name\": self.model,\n        }\n\n        self.meta[\"embedding_mode\"] = \"embedding\" in self.model.lower()\n\n        self.proxies = infer_params.get(\"saas.proxies\", None)\n        self.local_address = infer_params.get(\"saas.local_address\", \"0.0.0.0\")\n\n        if not self.proxies:\n            self.client = OpenAI(**other_params, api_key=self.api_key)\n        else:\n            self.client = OpenAI(\n                **other_params,\n                api_key=self.api_key,\n                http_client=httpx.Client(\n                    proxies=self.proxies,\n                    transport=httpx.HTTPTransport(local_address=self.local_address),\n                ),\n            )\n\n        try:\n            ray.get_actor(\"BLOCK_VLLM_STREAM_SERVER\")\n        except ValueError:\n            try:\n                ray.remote(BlockVLLMStreamServer).options(\n                    name=\"BLOCK_VLLM_STREAM_SERVER\",\n                    lifetime=\"detached\",\n                    max_concurrency=1000,\n                ).remote()\n            except Exception as e:\n                pass\n        try:\n            ray.get_actor(\"BlockBinaryStreamServer\")\n        except ValueError:\n            try:\n                ray.remote(BlockBinaryStreamServer).options(\n                    name=\"BlockBinaryStreamServer\",\n                    lifetime=\"detached\",\n                    max_concurrency=1000,\n                ).remote()\n            except Exception as e:\n                pass\n\n    # saas/proprietary\n    def get_meta(self):\n        return [self.meta]\n\n    def process_input(self, ins: Union[str, List[Dict[str, Any]], Dict[str, Any]]):\n\n        if isinstance(ins, list) or isinstance(ins, dict):\n            return ins\n\n        content = []\n        try:\n            ins_json = json.loads(ins)\n        except:\n            return ins\n\n        ## speech\n        if isinstance(ins_json, dict):\n            return ins_json\n\n        content = []\n        #     [\n        #     {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n        #     {\n        #       \"type\": \"image_url\",\n        #       \"image_url\": {\n        #         \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n        #         \"detail\": \"high\"\n        #       },\n        #     },\n        #   ],\n        for item in ins_json:\n            # for format like this: {\"image\": \"xxxxxx\", \"text\": \"What’s in this image?\",\"detail\":\"high\"}\n            # or {\"image\": \"xxxxxx\"}, {\"text\": \"What’s in this image?\"}\n            if (\"image\" in item or \"image_url\" in item) and \"type\" not in item:\n                image_data = item.get(\"image\", item.get(\"image_url\", \"\"))\n                ## \"data:image/jpeg;base64,\"\n                if not image_data.startswith(\"data:\"):\n                    image_data = \"data:image/jpeg;base64,\" + image_data\n\n                ## get the other fields except image/text/image_url\n                other_fields = {\n                    k: v\n                    for k, v in item.items()\n                    if k not in [\"image\", \"text\", \"image_url\"]\n                }\n                content.append(\n                    {\n                        \"image_url\": {\"url\": image_data, **other_fields},\n                        \"type\": \"image_url\",\n                    }\n                )            \n\n            if \"text\" in item and \"type\" not in item:\n                text_data = item[\"text\"]\n                content.append({\"text\": text_data, \"type\": \"text\"})\n\n            ## for format like this: {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n            ## {\"type\": \"image_url\", \"image_url\": {\"url\":\"\",\"detail\":\"high\"}}\n            ## this is the standard format, just return it\n            if \"type\" in item and item[\"type\"] == \"text\":\n                content.append(item)\n\n            if \"type\" in item and item[\"type\"] == \"image_url\":\n                content.append(item)\n\n        if not content:\n            return ins\n\n        return content\n\n    async def async_embed_query(self, ins: str, **kwargs):\n        resp = await asyncfy_with_semaphore(\n            lambda: self.client.embeddings.create(input=[ins], model=self.model)\n        )()\n        embedding = resp.data[0].embedding\n        usage = resp.usage\n        return (\n            embedding,\n            {\n                \"metadata\": {\n                    \"input_tokens_count\": usage.prompt_tokens,\n                    \"generated_tokens_count\": 0,\n                }\n            },\n        )\n\n    def embed_query(self, ins: str, **kwargs):\n        resp = self.client.embeddings.create(input=[ins], model=self.model)\n        embedding = resp.data[0].embedding\n        usage = resp.usage\n        return (\n            embedding,\n            {\n                \"metadata\": {\n                    \"input_tokens_count\": usage.prompt_tokens,\n                    \"generated_tokens_count\": 0,\n                }\n            },\n        )\n\n    async def async_text_to_speech(\n        self, stream: bool, ins: str, voice: str, chunk_size: int = None, **kwargs\n    ):\n        if stream:\n            server = ray.get_actor(\"BlockBinaryStreamServer\")\n            request_id = [None]\n\n            def writer():\n                try:\n                    request_id[0] = str(uuid.uuid4())\n                    with self.client.with_streaming_response.audio.speech.create(\n                        model=self.model, voice=voice, input=ins, **kwargs\n                    ) as response:\n                        for chunk in response.iter_bytes(chunk_size):\n                            input_tokens_count = 0\n                            generated_tokens_count = 0\n                            ray.get(\n                                server.add_item.remote(\n                                    request_id[0],\n                                    StreamOutputs(\n                                        outputs=[\n                                            SingleOutput(\n                                                text=chunk,\n                                                metadata=SingleOutputMeta(\n                                                    input_tokens_count=input_tokens_count,\n                                                    generated_tokens_count=generated_tokens_count,\n                                                ),\n                                            )\n                                        ]\n                                    ),\n                                )\n                            )\n                except:\n                    traceback.print_exc()\n                ray.get(server.mark_done.remote(request_id[0]))\n\n            threading.Thread(target=writer, daemon=True).start()\n\n            time_count = 10 * 100\n            while request_id[0] is None and time_count > 0:\n                time.sleep(0.01)\n                time_count -= 1\n\n            if request_id[0] is None:\n                raise Exception(\"Failed to get request id\")\n\n            def write_running():\n                return ray.get(server.add_item.remote(request_id[0], \"RUNNING\"))\n\n            await asyncio.to_thread(write_running)\n            return [\n                (\n                    \"\",\n                    {\n                        \"metadata\": {\n                            \"request_id\": request_id[0],\n                            \"stream_server\": \"BlockBinaryStreamServer\",\n                        }\n                    },\n                )\n            ]\n\n        start_time = time.monotonic()\n        with io.BytesIO() as output:\n            async with asyncfy_with_semaphore(\n                lambda: self.client.with_streaming_response.audio.speech.create(\n                    model=self.model, voice=voice, input=ins, **kwargs\n                )\n            )() as response:\n                for chunk in response.iter_bytes():\n                    output.write(chunk)\n\n            base64_audio = base64.b64encode(output.getvalue()).decode()\n            time_cost = time.monotonic() - start_time\n            return [\n                (\n                    base64_audio,\n                    {\n                        \"metadata\": {\n                            \"request_id\": \"\",\n                            \"input_tokens_count\": 0,\n                            \"generated_tokens_count\": 0,\n                            \"time_cost\": time_cost,\n                            \"first_token_time\": 0,\n                            \"speed\": 0,\n                        }\n                    },\n                )\n            ]\n\n    async def async_speech_to_text(\n        self,\n        audio: str,\n        response_format: str = \"verbose_json\",\n        timestamp_granularities: List[str] = [\"word\",\"segment\"],\n    ):               \n        # Extract audio format and base64 data\n        data_prefix = \"data:audio/\"\n        base64_prefix = \";base64,\"\n        if not audio.startswith(data_prefix):\n            raise ValueError(\"Invalid audio data format\")\n        \n        format_end = audio.index(base64_prefix)\n        audio_format = audio[len(data_prefix):format_end]\n        base64_data = audio[format_end + len(base64_prefix):]\n        \n        # Decode the base64 audio data\n        audio_data = base64.b64decode(base64_data)\n\n        # Create a temporary file with the correct extension\n        with tempfile.NamedTemporaryFile(delete=False, suffix=f'.{audio_format}') as temp_audio_file:\n            temp_audio_file.write(audio_data)\n            temp_audio_file_path = temp_audio_file.name\n\n        try:\n            start_time = time.monotonic()\n                   \n            with open(temp_audio_file_path, \"rb\") as audio_file:                \n                transcription = self.client.audio.transcriptions.create(\n                    model=self.model or \"whisper-1\",\n                    file=audio_file,\n                    response_format=response_format,\n                    timestamp_granularities=timestamp_granularities,\n                )\n            time_cost = time.monotonic() - start_time\n\n            return [\n                (\n                    json.dumps(transcription.to_dict(), ensure_ascii=True),\n                    {\n                        \"metadata\": {\n                            \"request_id\": \"\",\n                            \"input_tokens_count\": 0,\n                            \"generated_tokens_count\": 0,\n                            \"time_cost\": time_cost,\n                            \"first_token_time\": 0,\n                            \"speed\": 0,\n                        }\n                    },\n                )\n            ]\n        finally:\n            # Clean up the temporary file\n            import os\n            os.unlink(temp_audio_file_path)\n\n    def image_to_text(self, ins: str, **kwargs):\n        pass\n\n    async def async_text_to_image(\n        self, stream: bool, input: str, size: str, quality: str, n: int, **kwargs\n    ):\n        if stream:\n            raise Exception(\"Stream not supported for text to image\")\n        start_time = time.monotonic()\n        response = await asyncfy_with_semaphore(\n            lambda: self.client.images.generate(\n                model=self.model,\n                prompt=input,\n                size=size,\n                quality=quality,\n                n=1,\n                response_format=\"b64_json\",\n                **kwargs,\n            )\n        )()\n        time_cost = time.monotonic() - start_time\n        base64_image = response.data[0].b64_json\n        return [\n            (\n                base64_image,\n                {\n                    \"metadata\": {\n                        \"request_id\": \"\",\n                        \"input_tokens_count\": 0,\n                        \"generated_tokens_count\": 0,\n                        \"time_cost\": time_cost,\n                        \"first_token_time\": 0,\n                        \"speed\": 0,\n                    }\n                },\n            )\n        ]\n\n    def text_to_text(self, ins: str, **kwargs):\n        pass\n\n    async def async_stream_chat(\n        self,\n        tokenizer,\n        ins: str,\n        his: List[Dict[str, Any]] = [],\n        max_length: int = 4096,\n        top_p: float = 0.7,\n        temperature: float = 0.9,\n        **kwargs,\n    ):\n\n        model = self.model\n\n        if \"model\" in kwargs:\n            model = kwargs[\"model\"]\n\n        messages = [\n            {\"role\": message[\"role\"], \"content\": self.process_input(message[\"content\"])}\n            for message in his\n        ] + [{\"role\": \"user\", \"content\": self.process_input(ins)}]\n\n        stream = kwargs.get(\"stream\", False)\n\n        ## content = [\n        ##    \"voice\": \"alloy\",\"input\": \"Hello, World!\",response_format: \"mp3\"]\n        last_message = messages[-1][\"content\"]\n\n        if isinstance(last_message, dict) and \"voice\" in last_message:\n            voice = last_message[\"voice\"]\n            response_format = last_message.get(\"response_format\", \"mp3\")\n            chunk_size = last_message.get(\"chunk_size\", None)\n            input = last_message[\"input\"]\n            return await self.async_text_to_speech(\n                stream=stream,\n                ins=input,\n                voice=voice,\n                chunk_size=chunk_size,\n                response_format=response_format,\n            )\n\n        if isinstance(last_message, dict) and \"input\" in last_message:\n            input = last_message[\"input\"]\n            size = last_message.get(\"size\", \"1024x1024\")\n            quality = last_message.get(\"quality\", \"standard\")\n            n = last_message.get(\"n\", 1)\n            return await self.async_text_to_image(\n                stream=stream, input=input, size=size, quality=quality, n=n\n            )\n\n        if isinstance(last_message, dict) and \"audio\" in last_message:            \n            audio_data = last_message.get(\"audio\", \"\")            \n            tpe = last_message.get(\"type\",\"wav\")\n            if not audio_data.startswith(\"data:\"):\n                audio_data = f\"data:audio/${tpe};base64,\" + audio_data                \n            return await self.async_speech_to_text(audio=audio_data)\n        \n        server = ray.get_actor(\"BLOCK_VLLM_STREAM_SERVER\")\n        request_id = [None]\n\n        def writer():\n            try:\n                r = \"\"\n                response = self.client.chat.completions.create(\n                    messages=messages,\n                    model=model,\n                    stream=True,\n                    max_tokens=max_length,\n                    temperature=temperature,\n                    top_p=top_p,\n                )\n                # input_tokens_count = 0\n                # generated_tokens_count = 0\n\n                request_id[0] = str(uuid.uuid4())\n\n                for chunk in response:\n                    content = chunk.choices[0].delta.content or \"\"\n                    r += content\n                    if hasattr(chunk, \"usage\") and chunk.usage:\n                        input_tokens_count = chunk.usage.prompt_tokens\n                        generated_tokens_count = chunk.usage.completion_tokens\n                    else:\n                        input_tokens_count = 0\n                        generated_tokens_count = 0\n                    ray.get(\n                        server.add_item.remote(\n                            request_id[0],\n                            StreamOutputs(\n                                outputs=[\n                                    SingleOutput(\n                                        text=r,\n                                        metadata=SingleOutputMeta(\n                                            input_tokens_count=input_tokens_count,\n                                            generated_tokens_count=generated_tokens_count,\n                                        ),\n                                    )\n                                ]\n                            ),\n                        )\n                    )\n            except:\n                traceback.print_exc()\n            ray.get(server.mark_done.remote(request_id[0]))\n\n        if stream:\n            threading.Thread(target=writer, daemon=True).start()\n\n            time_count = 60 * 100\n            while request_id[0] is None and time_count > 0:\n                time.sleep(0.01)\n                time_count -= 1\n\n            if request_id[0] is None:\n                raise Exception(\"Failed to get request id\")\n\n            def write_running():\n                return ray.get(server.add_item.remote(request_id[0], \"RUNNING\"))\n\n            await asyncio.to_thread(write_running)\n            return [\n                (\n                    \"\",\n                    {\n                        \"metadata\": {\n                            \"request_id\": request_id[0],\n                            \"stream_server\": \"BLOCK_VLLM_STREAM_SERVER\",\n                        }\n                    },\n                )\n            ]\n        else:\n            try:\n                start_time = time.monotonic()\n                response = await asyncfy_with_semaphore(\n                    lambda: self.client.chat.completions.create(\n                        messages=messages,\n                        model=model,\n                        max_tokens=max_length,\n                        temperature=temperature,\n                        top_p=top_p,\n                    )\n                )()\n\n                generated_text = response.choices[0].message.content\n                generated_tokens_count = response.usage.completion_tokens\n                input_tokens_count = response.usage.prompt_tokens\n                time_cost = time.monotonic() - start_time\n                return [\n                    (\n                        generated_text,\n                        {\n                            \"metadata\": {\n                                \"request_id\": response.id,\n                                \"input_tokens_count\": input_tokens_count,\n                                \"generated_tokens_count\": generated_tokens_count,\n                                \"time_cost\": time_cost,\n                                \"first_token_time\": 0,\n                                \"speed\": float(generated_tokens_count) / time_cost,\n                            }\n                        },\n                    )\n                ]\n            except Exception as e:\n                print(f\"Error: {e}\")\n                raise e\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/whisper/__init__.py\nimport whisper\nfrom typing import Dict, List, Tuple\nfrom byzerllm.utils.types import StopSequencesCriteria\n\nfrom typing import Dict, Any, List, Union\nfrom pyjava.storage import streaming_tar as STar\nimport json\nimport base64\nimport tempfile\nimport time\n\n\ndef get_meta(self):\n    return [\n        {\n            \"model_deploy_type\": \"proprietary\",\n            \"backend\": \"transformers\",\n            \"message_format\": True\n        }\n    ]\n\n\ndef process_input(ins: Union[str, List[Dict[str, Any]], Dict[str, Any]]):\n    # print(ins) \n    if isinstance(ins, list) or isinstance(ins, dict):\n        return ins\n\n    content = []\n    try:\n        ins_json = json.loads(ins)\n    except:\n        return ins\n\n    ## speech\n    if isinstance(ins_json, dict):\n        return ins_json\n\n    content = []\n    #     [\n    #     {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n    #     {\n    #       \"type\": \"image_url\",\n    #       \"image_url\": {\n    #         \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n    #         \"detail\": \"high\"\n    #       },\n    #     },\n    #   ],\n    for item in ins_json:\n        # for format like this: {\"image\": \"xxxxxx\", \"text\": \"What’s in this image?\",\"detail\":\"high\"}\n        # or {\"image\": \"xxxxxx\"}, {\"text\": \"What’s in this image?\"}\n        if (\"image\" in item or \"image_url\" in item) and \"type\" not in item:\n            image_data = item.get(\"image\", item.get(\"image_url\", \"\"))\n            ## \"data:image/jpeg;base64,\"\n            if not image_data.startswith(\"data:\"):\n                image_data = \"data:image/jpeg;base64,\" + image_data\n\n            ## get the other fields except image/text/image_url\n            other_fields = {\n                k: v for k, v in item.items() if k not in [\"image\", \"text\", \"image_url\"]\n            }\n            content.append(\n                {\n                    \"image_url\": {\"url\": image_data, **other_fields},\n                    \"type\": \"image_url\",\n                }\n           \n\n下面是用户的需求：\n\n@test_sensevoice.py 中测试数据改成 /Users/allwefantasy/models/SenseVoiceSmall/example/zh.mp3 模型路径改成 /Users/allwefantasy/models/SenseVoiceSmall\n"}, {"role": "assistant", "content": "To fulfill the user's requirements, we need to modify the test file `test_sensevoice.py` to update the test data path and the model path. Here are the *SEARCH/REPLACE* blocks to achieve this:\n\n```python\n##File: /Users/allwefantasy/projects/byzer-llm/tests/test_sensevoice.py\n<<<<<<< SEARCH\naudio_path = \"example/zh.mp3\"\nmodel_path = \"iic/SenseVoiceSmall\"\n=======\naudio_path = \"/Users/allwefantasy/models/SenseVoiceSmall/example/zh.mp3\"\nmodel_path = \"/Users/allwefantasy/models/SenseVoiceSmall\"\n>>>>>>> REPLACE\n```\n\nThis block ensures that the audio path and model path are updated to the specified directories."}], "model": "deepseek_chat", "yaml_file": "actions/025_chat_action.yml"}
{"instruction": "@test_sensevoice.py 中测试数据改成 /Users/allwefantasy/models/SenseVoiceSmall/example/zh.mp3 模型路径改成 /Users/allwefantasy/models/SenseVoiceSmall", "conversations": [{"role": "user", "content": "\n如果你需要生成代码，对于每个需要更改的文件,你需要按 *SEARCH/REPLACE block* 的格式进行生成。\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The opening fence and code language, eg: ```python\n2. The file path alone on a line, starting with \"##File:\" and verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n*SEARCH/REPLACE* blocks will replace *all* matching occurrences.\nInclude enough lines to make the SEARCH blocks unique.\n\nInclude *ALL* the code being searched and replaced!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n下面我们来看一个例子：\n\n当前项目目录结构：\n1. 项目根目录： /tmp/projects/mathweb\n2. 项目子目录/文件列表(类似tree 命令输出)\nflask/\n    app.py\n    templates/\n        index.html\n    static/\n        style.css\n\n用户需求： Change get_factorial() to use math.factorial\n\n回答： To make this change we need to modify `/tmp/projects/mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n用户需求： Refactor hello() into its own file.\n\n回答：To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/hello.py\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/main.py\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n现在让我们开始一个新的任务:\n\n当前项目目录结构：\n1. 项目根目录： /Users/allwefantasy/projects/byzer-llm\n2. 项目子目录/文件列表(类似tree 命令输出)：\nsetup.py\ntests/\n    test_emb_rerank.py\n    test_deepseek_code.py\n    test_prompt.py\n    test_model_meta.py\n    __init__.py\n    test_transformers_deploy.py\n    test_data_analysis.py\n    test_yi.py\n    test_deploy.py\n    functions_suite.py\n    test_template.py\n    auto/\n        test_backend_llama_cpp.py\n        __init__.py\nsetup-machine/\n    convert_safetensor.py\nnotebooks/\n    test.py\n    test_sensevoice.py\nsrc/\n    byzerllm/\n        lang.py\n        store.py\n        version.py\n        log.py\n        __init__.py\n        types.py\n        easy_byzerllm_command.py\n        byzerllm_command.py\n        command_args.py\n        bark/\n            generation.py\n            __init__.py\n            model.py\n            api.py\n            model_fine.py\n            bark_voice.py\n        moss/\n            finetune_moss.py\n            __init__.py\n            moss_inference.py\n            models/\n                configuration_moss.py\n                quantization.py\n                custom_autotune.py\n                __init__.py\n                modeling_moss.py\n                tokenization_moss.py\n        llama2/\n            __init__.py\n        records/\n            __init__.py\n        visualglm/\n            __init__.py\n        auto/\n            __init__.py\n            backend_llama_cpp.py\n            backend_vllm.py\n            backend_ds.py\n        alpha_moss/\n            __init__.py\n        processor/\n            __init__.py\n        sensevoice/\n            __init__.py\n        qwen/\n            __init__.py\n        utils/\n            json_repaire.py\n            object_store_ref_util.py\n            __init__.py\n            types.py\n            tokenizer.py\n            openai_utils.py\n            ray_utils.py\n            text_generator.py\n            emb.py\n            connect_ray.py\n            testing.py\n            langutil.py\n            nontext.py\n            fulltune/\n                deepspeed_trainner.py\n                __init__.py\n                launch.py\n                trainner.py\n                base_model/\n                    __init__.py\n                    configuration_baichuan.py\n                    modeling_baichuan.py\n                pretrain/\n                    __init__.py\n                    convert_to_transformers.py\n            metrics/\n                __init__.py\n            config/\n                __init__.py\n            retrieval/\n                rest.py\n                __init__.py\n                udf.py\n            sft/\n                argument.py\n                collator.py\n                qlora.py\n                __init__.py\n                model.py\n                dataset.py\n                loss.py\n                merge_lora.py\n                trainer.py\n            client/\n                message_utils.py\n                math_utils.py\n                byzerllm_client.py\n                parallel_utils.py\n                __init__.py\n                types.py\n                code_utils.py\n                img_utils.py\n                entrypoints/\n                    __init__.py\n                    openai/\n                        protocol.py\n                        __init__.py\n                        serve.py\n                        serving_chat.py\n                        api_server.py\n                        serving_engine.py\n                        serving_completion.py\n                        tool.py\n        chatglm2/\n            __init__.py\n        baichuan/\n            __init__.py\n        m3e/\n            __init__.py\n        stable_diffusion/\n            config.py\n            __init__.py\n            logger.py\n            shared.py\n            model.py\n            utils.py\n            images.py\n            diffusion/\n                __init__.py\n                embeddings.py\n                utils.py\n                networks/\n                    lyco.py\n                    __init__.py\n                upscalers/\n                    __init__.py\n                    multidiffusion.py\n                    samplers.py\n                piplines/\n                    __init__.py\n                    lpw.py\n                    diffusers.py\n            lib/\n                __init__.py\n                diffusers/\n                    __init__.py\n                    scheduler.py\n            api/\n                __init__.py\n                plugin.py\n                models/\n                    diffusion.py\n                    __init__.py\n                diffusion/\n                    __init__.py\n                    pipelines/\n                        __init__.py\n                        diffusers.py\n                events/\n                    generation.py\n                    __init__.py\n        bge/\n            __init__.py\n        bge_rerank/\n            __init__.py\n        apps/\n            client.py\n            qa_strategy.py\n            __init__.py\n            builder.py\n            vector_db.py\n            utils.py\n            qa.py\n            byzer_storage/\n                memory_model_based.py\n                command.py\n                env.py\n                train_pt.py\n                train_sft.py\n                __init__.py\n                train_dpo.py\n                simple_api.py\n                generate_sft_data.py\n            byzer_sql/\n                __init__.py\n            llama_index/\n                byzerai_vectordb.py\n                byzerai.py\n                byzerai_embedding.py\n                __init__.py\n                collection_manager.py\n                byzerai_index_store.py\n                byzerai_docstore.py\n                simple_retrieval.py\n                byzerai_kvstore.py\n            agent/\n                conversable_agent.py\n                registry.py\n                groupchat.py\n                user_proxy_agent.py\n                __init__.py\n                agent.py\n                extensions/\n                    simple_retrieval_client.py\n                    planner.py\n                    python_codesandbox_agent.py\n                    preview_file_agent.py\n                    llama_index_retrieval_agent.py\n                    visualization_agent.py\n                    assistant_agent.py\n                    load_data_agent.py\n                    retrieval_agent.py\n                    output_agent.py\n                    __init__.py\n                    byzer_engine_agent.py\n                    subquestion_agent.py\n                    sql_reviewer_agent.py\n                    spark_sql_agent.py\n                    rhetorical_agent.py\n                    data_analysis.py\n                    data_analysis_pipeline_agent.py\n                    common_agent.py\n                    query_rewrite/\n                        time.py\n                        rhetorical.py\n                        __init__.py\n                        condition.py\n                        context.py\n                store/\n                    memory_store.py\n                    __init__.py\n                    stores.py\n        saas/\n            __init__.py\n            chatglm/\n                __init__.py\n            sparkdesk/\n                __init__.py\n            gemini/\n                __init__.py\n            azure_openai/\n                __init__.py\n            azure/\n                __init__.py\n            qianfan/\n                __init__.py\n            claude/\n                __init__.py\n            qianwen/\n                __init__.py\n            volcengine/\n                __init__.py\n            minimax/\n                __init__.py\n            baichuan/\n                __init__.py\n            qianwen_vl/\n                __init__.py\n            official_openai/\n                __init__.py\n            zhipu/\n                __init__.py\n            aws_bedrock/\n                __init__.py\n            openai/\n                __init__.py\n        falcon/\n            __init__.py\n        chatglm6b/\n            trainer_seq2seq.py\n            arguments.py\n            __init__.py\n            finetune.py\n            tunning/\n                __init__.py\n                infer.py\n                finetune.py\n                utils/\n                    config.py\n                    __init__.py\n                    other.py\n                    common.py\n                    seq2seq.py\n        starcode/\n            __init__.py\n        qwen_vl_chat/\n            __init__.py\n        zephyr/\n            __init__.py\n        dolly/\n            generate.py\n            __init__.py\n            consts.py\n            dolly_inference.py\n            trainer.py\n        llama/\n            __init__.py\n        whisper/\n            whisper_inference.py\n            __init__.py\n下面是一些文件路径以及每个文件对应的源码：\n\n##File: /Users/allwefantasy/projects/SenseVoice/webui.py\n# coding=utf-8\n\nimport os\nimport librosa\nimport base64\nimport io\nimport gradio as gr\nimport re\n\nimport numpy as np\nimport torch\nimport torchaudio\n\n\nfrom funasr import AutoModel\n\nmodel = \"iic/SenseVoiceSmall\"\nmodel_kwargs = {\n    \"model\": model,\n    \"trust_remote_code\": True,\n}\n\n# Check if vad_model is provided in infer_params\nif \"vad_model\" in infer_params:\n    model_kwargs[\"vad_model\"] = infer_params[\"vad_model\"]\n    model_kwargs[\"vad_kwargs\"] = {\"max_single_segment_time\": 30000}\n\nmodel = AutoModel(**model_kwargs)\n\nimport re\n\nemo_dict = {\n\t\"<|HAPPY|>\": \"😊\",\n\t\"<|SAD|>\": \"😔\",\n\t\"<|ANGRY|>\": \"😡\",\n\t\"<|NEUTRAL|>\": \"\",\n\t\"<|FEARFUL|>\": \"😰\",\n\t\"<|DISGUSTED|>\": \"🤢\",\n\t\"<|SURPRISED|>\": \"😮\",\n}\n\nevent_dict = {\n\t\"<|BGM|>\": \"🎼\",\n\t\"<|Speech|>\": \"\",\n\t\"<|Applause|>\": \"👏\",\n\t\"<|Laughter|>\": \"😀\",\n\t\"<|Cry|>\": \"😭\",\n\t\"<|Sneeze|>\": \"🤧\",\n\t\"<|Breath|>\": \"\",\n\t\"<|Cough|>\": \"🤧\",\n}\n\nemoji_dict = {\n\t\"<|nospeech|><|Event_UNK|>\": \"❓\",\n\t\"<|zh|>\": \"\",\n\t\"<|en|>\": \"\",\n\t\"<|yue|>\": \"\",\n\t\"<|ja|>\": \"\",\n\t\"<|ko|>\": \"\",\n\t\"<|nospeech|>\": \"\",\n\t\"<|HAPPY|>\": \"😊\",\n\t\"<|SAD|>\": \"😔\",\n\t\"<|ANGRY|>\": \"😡\",\n\t\"<|NEUTRAL|>\": \"\",\n\t\"<|BGM|>\": \"🎼\",\n\t\"<|Speech|>\": \"\",\n\t\"<|Applause|>\": \"👏\",\n\t\"<|Laughter|>\": \"😀\",\n\t\"<|FEARFUL|>\": \"😰\",\n\t\"<|DISGUSTED|>\": \"🤢\",\n\t\"<|SURPRISED|>\": \"😮\",\n\t\"<|Cry|>\": \"😭\",\n\t\"<|EMO_UNKNOWN|>\": \"\",\n\t\"<|Sneeze|>\": \"🤧\",\n\t\"<|Breath|>\": \"\",\n\t\"<|Cough|>\": \"😷\",\n\t\"<|Sing|>\": \"\",\n\t\"<|Speech_Noise|>\": \"\",\n\t\"<|withitn|>\": \"\",\n\t\"<|woitn|>\": \"\",\n\t\"<|GBG|>\": \"\",\n\t\"<|Event_UNK|>\": \"\",\n}\n\nlang_dict =  {\n    \"<|zh|>\": \"<|lang|>\",\n    \"<|en|>\": \"<|lang|>\",\n    \"<|yue|>\": \"<|lang|>\",\n    \"<|ja|>\": \"<|lang|>\",\n    \"<|ko|>\": \"<|lang|>\",\n    \"<|nospeech|>\": \"<|lang|>\",\n}\n\nemo_set = {\"😊\", \"😔\", \"😡\", \"😰\", \"🤢\", \"😮\"}\nevent_set = {\"🎼\", \"👏\", \"😀\", \"😭\", \"🤧\", \"😷\",}\n\ndef format_str(s):\n\tfor sptk in emoji_dict:\n\t\ts = s.replace(sptk, emoji_dict[sptk])\n\treturn s\n\n\ndef format_str_v2(s):\n\tsptk_dict = {}\n\tfor sptk in emoji_dict:\n\t\tsptk_dict[sptk] = s.count(sptk)\n\t\ts = s.replace(sptk, \"\")\n\temo = \"<|NEUTRAL|>\"\n\tfor e in emo_dict:\n\t\tif sptk_dict[e] > sptk_dict[emo]:\n\t\t\temo = e\n\tfor e in event_dict:\n\t\tif sptk_dict[e] > 0:\n\t\t\ts = event_dict[e] + s\n\ts = s + emo_dict[emo]\n\n\tfor emoji in emo_set.union(event_set):\n\t\ts = s.replace(\" \" + emoji, emoji)\n\t\ts = s.replace(emoji + \" \", emoji)\n\treturn s.strip()\n\ndef format_str_v3(s):\n\tdef get_emo(s):\n\t\treturn s[-1] if s[-1] in emo_set else None\n\tdef get_event(s):\n\t\treturn s[0] if s[0] in event_set else None\n\n\ts = s.replace(\"<|nospeech|><|Event_UNK|>\", \"❓\")\n\tfor lang in lang_dict:\n\t\ts = s.replace(lang, \"<|lang|>\")\n\ts_list = [format_str_v2(s_i).strip(\" \") for s_i in s.split(\"<|lang|>\")]\n\tnew_s = \" \" + s_list[0]\n\tcur_ent_event = get_event(new_s)\n\tfor i in range(1, len(s_list)):\n\t\tif len(s_list[i]) == 0:\n\t\t\tcontinue\n\t\tif get_event(s_list[i]) == cur_ent_event and get_event(s_list[i]) != None:\n\t\t\ts_list[i] = s_list[i][1:]\n\t\t#else:\n\t\tcur_ent_event = get_event(s_list[i])\n\t\tif get_emo(s_list[i]) != None and get_emo(s_list[i]) == get_emo(new_s):\n\t\t\tnew_s = new_s[:-1]\n\t\tnew_s += s_list[i].strip().lstrip()\n\tnew_s = new_s.replace(\"The.\", \" \")\n\treturn new_s.strip()\n\ndef model_inference(input_wav, language, fs=16000):\n\t# task_abbr = {\"Speech Recognition\": \"ASR\", \"Rich Text Transcription\": (\"ASR\", \"AED\", \"SER\")}\n\tlanguage_abbr = {\"auto\": \"auto\", \"zh\": \"zh\", \"en\": \"en\", \"yue\": \"yue\", \"ja\": \"ja\", \"ko\": \"ko\",\n\t\t\t\t\t \"nospeech\": \"nospeech\"}\n\t\n\t# task = \"Speech Recognition\" if task is None else task\n\tlanguage = \"auto\" if len(language) < 1 else language\n\tselected_language = language_abbr[language]\n\t# selected_task = task_abbr.get(task)\n\t\n\t# print(f\"input_wav: {type(input_wav)}, {input_wav[1].shape}, {input_wav}\")\n\t\n\tif isinstance(input_wav, tuple):\n\t\tfs, input_wav = input_wav\n\t\tinput_wav = input_wav.astype(np.float32) / np.iinfo(np.int16).max\n\t\tif len(input_wav.shape) > 1:\n\t\t\tinput_wav = input_wav.mean(-1)\n\t\tif fs != 16000:\n\t\t\tprint(f\"audio_fs: {fs}\")\n\t\t\tresampler = torchaudio.transforms.Resample(fs, 16000)\n\t\t\tinput_wav_t = torch.from_numpy(input_wav).to(torch.float32)\n\t\t\tinput_wav = resampler(input_wav_t[None, :])[0, :].numpy()\n\t\n\t\n\tmerge_vad = True #False if selected_task == \"ASR\" else True\n\tprint(f\"language: {language}, merge_vad: {merge_vad}\")\n\ttext = model.generate(input=input_wav,\n\t\t\t\t\t\t  cache={},\n\t\t\t\t\t\t  language=language,\n\t\t\t\t\t\t  use_itn=True,\n\t\t\t\t\t\t  batch_size_s=60, merge_vad=merge_vad)\n\t\n\tprint(text)\n\ttext = text[0][\"text\"]\n\ttext = format_str_v3(text)\n\t\n\tprint(text)\n\t\n\treturn text\n\n\naudio_examples = [\n    [\"example/zh.mp3\", \"zh\"],\n    [\"example/yue.mp3\", \"yue\"],\n    [\"example/en.mp3\", \"en\"],\n    [\"example/ja.mp3\", \"ja\"],\n    [\"example/ko.mp3\", \"ko\"],\n    [\"example/emo_1.wav\", \"auto\"],\n    [\"example/emo_2.wav\", \"auto\"],\n    [\"example/emo_3.wav\", \"auto\"],\n    #[\"example/emo_4.wav\", \"auto\"],\n    #[\"example/event_1.wav\", \"auto\"],\n    #[\"example/event_2.wav\", \"auto\"],\n    #[\"example/event_3.wav\", \"auto\"],\n    [\"example/rich_1.wav\", \"auto\"],\n    [\"example/rich_2.wav\", \"auto\"],\n    #[\"example/rich_3.wav\", \"auto\"],\n    [\"example/longwav_1.wav\", \"auto\"],\n    [\"example/longwav_2.wav\", \"auto\"],\n    [\"example/longwav_3.wav\", \"auto\"],\n    #[\"example/longwav_4.wav\", \"auto\"],\n]\n\n\n\nhtml_content = \"\"\"\n<div>\n    <h2 style=\"font-size: 22px;margin-left: 0px;\">Voice Understanding Model: SenseVoice-Small</h2>\n    <p style=\"font-size: 18px;margin-left: 20px;\">SenseVoice-Small is an encoder-only speech foundation model designed for rapid voice understanding. It encompasses a variety of features including automatic speech recognition (ASR), spoken language identification (LID), speech emotion recognition (SER), and acoustic event detection (AED). SenseVoice-Small supports multilingual recognition for Chinese, English, Cantonese, Japanese, and Korean. Additionally, it offers exceptionally low inference latency, performing 7 times faster than Whisper-small and 17 times faster than Whisper-large.</p>\n    <h2 style=\"font-size: 22px;margin-left: 0px;\">Usage</h2> <p style=\"font-size: 18px;margin-left: 20px;\">Upload an audio file or input through a microphone, then select the task and language. the audio is transcribed into corresponding text along with associated emotions (😊 happy, 😡 angry/exicting, 😔 sad) and types of sound events (😀 laughter, 🎼 music, 👏 applause, 🤧 cough&sneeze, 😭 cry). The event labels are placed in the front of the text and the emotion are in the back of the text.</p>\n\t<p style=\"font-size: 18px;margin-left: 20px;\">Recommended audio input duration is below 30 seconds. For audio longer than 30 seconds, local deployment is recommended.</p>\n\t<h2 style=\"font-size: 22px;margin-left: 0px;\">Repo</h2>\n\t<p style=\"font-size: 18px;margin-left: 20px;\"><a href=\"https://github.com/FunAudioLLM/SenseVoice\" target=\"_blank\">SenseVoice</a>: multilingual speech understanding model</p>\n\t<p style=\"font-size: 18px;margin-left: 20px;\"><a href=\"https://github.com/modelscope/FunASR\" target=\"_blank\">FunASR</a>: fundamental speech recognition toolkit</p>\n\t<p style=\"font-size: 18px;margin-left: 20px;\"><a href=\"https://github.com/FunAudioLLM/CosyVoice\" target=\"_blank\">CosyVoice</a>: high-quality multilingual TTS model</p>\n</div>\n\"\"\"\n\n\ndef launch():\n\twith gr.Blocks(theme=gr.themes.Soft()) as demo:\n\t\t# gr.Markdown(description)\n\t\tgr.HTML(html_content)\n\t\twith gr.Row():\n\t\t\twith gr.Column():\n\t\t\t\taudio_inputs = gr.Audio(label=\"Upload audio or use the microphone\")\n\t\t\t\t\n\t\t\t\twith gr.Accordion(\"Configuration\"):\n\t\t\t\t\tlanguage_inputs = gr.Dropdown(choices=[\"auto\", \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"],\n\t\t\t\t\t\t\t\t\t\t\t\t  value=\"auto\",\n\t\t\t\t\t\t\t\t\t\t\t\t  label=\"Language\")\n\t\t\t\tfn_button = gr.Button(\"Start\", variant=\"primary\")\n\t\t\t\ttext_outputs = gr.Textbox(label=\"Results\")\n\t\t\tgr.Examples(examples=audio_examples, inputs=[audio_inputs, language_inputs], examples_per_page=20)\n\t\t\n\t\tfn_button.click(model_inference, inputs=[audio_inputs, language_inputs], outputs=text_outputs)\n\n\tdemo.launch()\n\n\nif __name__ == \"__main__\":\n\t# iface.launch()\n\tlaunch()\n\n\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/saas/official_openai/__init__.py\nimport time\nfrom typing import List, Tuple, Dict, Any, Union\nimport httpx\nfrom openai import OpenAI\nimport base64\nimport io\nimport json\nimport ray\nfrom byzerllm.utils.types import (\n    BlockVLLMStreamServer,\n    StreamOutputs,\n    SingleOutput,\n    SingleOutputMeta,\n    BlockBinaryStreamServer,\n)\nfrom byzerllm.utils.langutil import asyncfy_with_semaphore\nimport threading\nimport asyncio\nimport traceback\nimport uuid\nimport tempfile        \n\nclass CustomSaasAPI:\n\n    def __init__(self, infer_params: Dict[str, str]) -> None:\n\n        self.api_key = infer_params[\"saas.api_key\"]\n        self.model = infer_params.get(\"saas.model\", \"gpt-3.5-turbo-1106\")\n\n        other_params = {}\n\n        if \"saas.api_base\" in infer_params:\n            other_params[\"api_base\"] = infer_params[\"saas.api_base\"]\n\n        if \"saas.api_version\" in infer_params:\n            other_params[\"api_version\"] = infer_params[\"saas.api_version\"]\n\n        if \"saas.api_type\" in infer_params:\n            other_params[\"api_type\"] = infer_params[\"saas.api_type\"]\n\n        if \"saas.base_url\" in infer_params:\n            other_params[\"base_url\"] = infer_params[\"saas.base_url\"]\n\n        if \"saas.timeout\" in infer_params:\n            other_params[\"timeout\"] = float(infer_params[\"saas.timeout\"])\n\n        self.max_retries = int(infer_params.get(\"saas.max_retries\", 10))\n\n        self.meta = {\n            \"model_deploy_type\": \"saas\",\n            \"backend\": \"saas\",\n            \"support_stream\": True,\n            \"model_name\": self.model,\n        }\n\n        self.meta[\"embedding_mode\"] = \"embedding\" in self.model.lower()\n\n        self.proxies = infer_params.get(\"saas.proxies\", None)\n        self.local_address = infer_params.get(\"saas.local_address\", \"0.0.0.0\")\n\n        if not self.proxies:\n            self.client = OpenAI(**other_params, api_key=self.api_key)\n        else:\n            self.client = OpenAI(\n                **other_params,\n                api_key=self.api_key,\n                http_client=httpx.Client(\n                    proxies=self.proxies,\n                    transport=httpx.HTTPTransport(local_address=self.local_address),\n                ),\n            )\n\n        try:\n            ray.get_actor(\"BLOCK_VLLM_STREAM_SERVER\")\n        except ValueError:\n            try:\n                ray.remote(BlockVLLMStreamServer).options(\n                    name=\"BLOCK_VLLM_STREAM_SERVER\",\n                    lifetime=\"detached\",\n                    max_concurrency=1000,\n                ).remote()\n            except Exception as e:\n                pass\n        try:\n            ray.get_actor(\"BlockBinaryStreamServer\")\n        except ValueError:\n            try:\n                ray.remote(BlockBinaryStreamServer).options(\n                    name=\"BlockBinaryStreamServer\",\n                    lifetime=\"detached\",\n                    max_concurrency=1000,\n                ).remote()\n            except Exception as e:\n                pass\n\n    # saas/proprietary\n    def get_meta(self):\n        return [self.meta]\n\n    def process_input(self, ins: Union[str, List[Dict[str, Any]], Dict[str, Any]]):\n\n        if isinstance(ins, list) or isinstance(ins, dict):\n            return ins\n\n        content = []\n        try:\n            ins_json = json.loads(ins)\n        except:\n            return ins\n\n        ## speech\n        if isinstance(ins_json, dict):\n            return ins_json\n\n        content = []\n        #     [\n        #     {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n        #     {\n        #       \"type\": \"image_url\",\n        #       \"image_url\": {\n        #         \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n        #         \"detail\": \"high\"\n        #       },\n        #     },\n        #   ],\n        for item in ins_json:\n            # for format like this: {\"image\": \"xxxxxx\", \"text\": \"What’s in this image?\",\"detail\":\"high\"}\n            # or {\"image\": \"xxxxxx\"}, {\"text\": \"What’s in this image?\"}\n            if (\"image\" in item or \"image_url\" in item) and \"type\" not in item:\n                image_data = item.get(\"image\", item.get(\"image_url\", \"\"))\n                ## \"data:image/jpeg;base64,\"\n                if not image_data.startswith(\"data:\"):\n                    image_data = \"data:image/jpeg;base64,\" + image_data\n\n                ## get the other fields except image/text/image_url\n                other_fields = {\n                    k: v\n                    for k, v in item.items()\n                    if k not in [\"image\", \"text\", \"image_url\"]\n                }\n                content.append(\n                    {\n                        \"image_url\": {\"url\": image_data, **other_fields},\n                        \"type\": \"image_url\",\n                    }\n                )            \n\n            if \"text\" in item and \"type\" not in item:\n                text_data = item[\"text\"]\n                content.append({\"text\": text_data, \"type\": \"text\"})\n\n            ## for format like this: {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n            ## {\"type\": \"image_url\", \"image_url\": {\"url\":\"\",\"detail\":\"high\"}}\n            ## this is the standard format, just return it\n            if \"type\" in item and item[\"type\"] == \"text\":\n                content.append(item)\n\n            if \"type\" in item and item[\"type\"] == \"image_url\":\n                content.append(item)\n\n        if not content:\n            return ins\n\n        return content\n\n    async def async_embed_query(self, ins: str, **kwargs):\n        resp = await asyncfy_with_semaphore(\n            lambda: self.client.embeddings.create(input=[ins], model=self.model)\n        )()\n        embedding = resp.data[0].embedding\n        usage = resp.usage\n        return (\n            embedding,\n            {\n                \"metadata\": {\n                    \"input_tokens_count\": usage.prompt_tokens,\n                    \"generated_tokens_count\": 0,\n                }\n            },\n        )\n\n    def embed_query(self, ins: str, **kwargs):\n        resp = self.client.embeddings.create(input=[ins], model=self.model)\n        embedding = resp.data[0].embedding\n        usage = resp.usage\n        return (\n            embedding,\n            {\n                \"metadata\": {\n                    \"input_tokens_count\": usage.prompt_tokens,\n                    \"generated_tokens_count\": 0,\n                }\n            },\n        )\n\n    async def async_text_to_speech(\n        self, stream: bool, ins: str, voice: str, chunk_size: int = None, **kwargs\n    ):\n        if stream:\n            server = ray.get_actor(\"BlockBinaryStreamServer\")\n            request_id = [None]\n\n            def writer():\n                try:\n                    request_id[0] = str(uuid.uuid4())\n                    with self.client.with_streaming_response.audio.speech.create(\n                        model=self.model, voice=voice, input=ins, **kwargs\n                    ) as response:\n                        for chunk in response.iter_bytes(chunk_size):\n                            input_tokens_count = 0\n                            generated_tokens_count = 0\n                            ray.get(\n                                server.add_item.remote(\n                                    request_id[0],\n                                    StreamOutputs(\n                                        outputs=[\n                                            SingleOutput(\n                                                text=chunk,\n                                                metadata=SingleOutputMeta(\n                                                    input_tokens_count=input_tokens_count,\n                                                    generated_tokens_count=generated_tokens_count,\n                                                ),\n                                            )\n                                        ]\n                                    ),\n                                )\n                            )\n                except:\n                    traceback.print_exc()\n                ray.get(server.mark_done.remote(request_id[0]))\n\n            threading.Thread(target=writer, daemon=True).start()\n\n            time_count = 10 * 100\n            while request_id[0] is None and time_count > 0:\n                time.sleep(0.01)\n                time_count -= 1\n\n            if request_id[0] is None:\n                raise Exception(\"Failed to get request id\")\n\n            def write_running():\n                return ray.get(server.add_item.remote(request_id[0], \"RUNNING\"))\n\n            await asyncio.to_thread(write_running)\n            return [\n                (\n                    \"\",\n                    {\n                        \"metadata\": {\n                            \"request_id\": request_id[0],\n                            \"stream_server\": \"BlockBinaryStreamServer\",\n                        }\n                    },\n                )\n            ]\n\n        start_time = time.monotonic()\n        with io.BytesIO() as output:\n            async with asyncfy_with_semaphore(\n                lambda: self.client.with_streaming_response.audio.speech.create(\n                    model=self.model, voice=voice, input=ins, **kwargs\n                )\n            )() as response:\n                for chunk in response.iter_bytes():\n                    output.write(chunk)\n\n            base64_audio = base64.b64encode(output.getvalue()).decode()\n            time_cost = time.monotonic() - start_time\n            return [\n                (\n                    base64_audio,\n                    {\n                        \"metadata\": {\n                            \"request_id\": \"\",\n                            \"input_tokens_count\": 0,\n                            \"generated_tokens_count\": 0,\n                            \"time_cost\": time_cost,\n                            \"first_token_time\": 0,\n                            \"speed\": 0,\n                        }\n                    },\n                )\n            ]\n\n    async def async_speech_to_text(\n        self,\n        audio: str,\n        response_format: str = \"verbose_json\",\n        timestamp_granularities: List[str] = [\"word\",\"segment\"],\n    ):               \n        # Extract audio format and base64 data\n        data_prefix = \"data:audio/\"\n        base64_prefix = \";base64,\"\n        if not audio.startswith(data_prefix):\n            raise ValueError(\"Invalid audio data format\")\n        \n        format_end = audio.index(base64_prefix)\n        audio_format = audio[len(data_prefix):format_end]\n        base64_data = audio[format_end + len(base64_prefix):]\n        \n        # Decode the base64 audio data\n        audio_data = base64.b64decode(base64_data)\n\n        # Create a temporary file with the correct extension\n        with tempfile.NamedTemporaryFile(delete=False, suffix=f'.{audio_format}') as temp_audio_file:\n            temp_audio_file.write(audio_data)\n            temp_audio_file_path = temp_audio_file.name\n\n        try:\n            start_time = time.monotonic()\n                   \n            with open(temp_audio_file_path, \"rb\") as audio_file:                \n                transcription = self.client.audio.transcriptions.create(\n                    model=self.model or \"whisper-1\",\n                    file=audio_file,\n                    response_format=response_format,\n                    timestamp_granularities=timestamp_granularities,\n                )\n            time_cost = time.monotonic() - start_time\n\n            return [\n                (\n                    json.dumps(transcription.to_dict(), ensure_ascii=True),\n                    {\n                        \"metadata\": {\n                            \"request_id\": \"\",\n                            \"input_tokens_count\": 0,\n                            \"generated_tokens_count\": 0,\n                            \"time_cost\": time_cost,\n                            \"first_token_time\": 0,\n                            \"speed\": 0,\n                        }\n                    },\n                )\n            ]\n        finally:\n            # Clean up the temporary file\n            import os\n            os.unlink(temp_audio_file_path)\n\n    def image_to_text(self, ins: str, **kwargs):\n        pass\n\n    async def async_text_to_image(\n        self, stream: bool, input: str, size: str, quality: str, n: int, **kwargs\n    ):\n        if stream:\n            raise Exception(\"Stream not supported for text to image\")\n        start_time = time.monotonic()\n        response = await asyncfy_with_semaphore(\n            lambda: self.client.images.generate(\n                model=self.model,\n                prompt=input,\n                size=size,\n                quality=quality,\n                n=1,\n                response_format=\"b64_json\",\n                **kwargs,\n            )\n        )()\n        time_cost = time.monotonic() - start_time\n        base64_image = response.data[0].b64_json\n        return [\n            (\n                base64_image,\n                {\n                    \"metadata\": {\n                        \"request_id\": \"\",\n                        \"input_tokens_count\": 0,\n                        \"generated_tokens_count\": 0,\n                        \"time_cost\": time_cost,\n                        \"first_token_time\": 0,\n                        \"speed\": 0,\n                    }\n                },\n            )\n        ]\n\n    def text_to_text(self, ins: str, **kwargs):\n        pass\n\n    async def async_stream_chat(\n        self,\n        tokenizer,\n        ins: str,\n        his: List[Dict[str, Any]] = [],\n        max_length: int = 4096,\n        top_p: float = 0.7,\n        temperature: float = 0.9,\n        **kwargs,\n    ):\n\n        model = self.model\n\n        if \"model\" in kwargs:\n            model = kwargs[\"model\"]\n\n        messages = [\n            {\"role\": message[\"role\"], \"content\": self.process_input(message[\"content\"])}\n            for message in his\n        ] + [{\"role\": \"user\", \"content\": self.process_input(ins)}]\n\n        stream = kwargs.get(\"stream\", False)\n\n        ## content = [\n        ##    \"voice\": \"alloy\",\"input\": \"Hello, World!\",response_format: \"mp3\"]\n        last_message = messages[-1][\"content\"]\n\n        if isinstance(last_message, dict) and \"voice\" in last_message:\n            voice = last_message[\"voice\"]\n            response_format = last_message.get(\"response_format\", \"mp3\")\n            chunk_size = last_message.get(\"chunk_size\", None)\n            input = last_message[\"input\"]\n            return await self.async_text_to_speech(\n                stream=stream,\n                ins=input,\n                voice=voice,\n                chunk_size=chunk_size,\n                response_format=response_format,\n            )\n\n        if isinstance(last_message, dict) and \"input\" in last_message:\n            input = last_message[\"input\"]\n            size = last_message.get(\"size\", \"1024x1024\")\n            quality = last_message.get(\"quality\", \"standard\")\n            n = last_message.get(\"n\", 1)\n            return await self.async_text_to_image(\n                stream=stream, input=input, size=size, quality=quality, n=n\n            )\n\n        if isinstance(last_message, dict) and \"audio\" in last_message:            \n            audio_data = last_message.get(\"audio\", \"\")            \n            tpe = last_message.get(\"type\",\"wav\")\n            if not audio_data.startswith(\"data:\"):\n                audio_data = f\"data:audio/${tpe};base64,\" + audio_data                \n            return await self.async_speech_to_text(audio=audio_data)\n        \n        server = ray.get_actor(\"BLOCK_VLLM_STREAM_SERVER\")\n        request_id = [None]\n\n        def writer():\n            try:\n                r = \"\"\n                response = self.client.chat.completions.create(\n                    messages=messages,\n                    model=model,\n                    stream=True,\n                    max_tokens=max_length,\n                    temperature=temperature,\n                    top_p=top_p,\n                )\n                # input_tokens_count = 0\n                # generated_tokens_count = 0\n\n                request_id[0] = str(uuid.uuid4())\n\n                for chunk in response:\n                    content = chunk.choices[0].delta.content or \"\"\n                    r += content\n                    if hasattr(chunk, \"usage\") and chunk.usage:\n                        input_tokens_count = chunk.usage.prompt_tokens\n                        generated_tokens_count = chunk.usage.completion_tokens\n                    else:\n                        input_tokens_count = 0\n                        generated_tokens_count = 0\n                    ray.get(\n                        server.add_item.remote(\n                            request_id[0],\n                            StreamOutputs(\n                                outputs=[\n                                    SingleOutput(\n                                        text=r,\n                                        metadata=SingleOutputMeta(\n                                            input_tokens_count=input_tokens_count,\n                                            generated_tokens_count=generated_tokens_count,\n                                        ),\n                                    )\n                                ]\n                            ),\n                        )\n                    )\n            except:\n                traceback.print_exc()\n            ray.get(server.mark_done.remote(request_id[0]))\n\n        if stream:\n            threading.Thread(target=writer, daemon=True).start()\n\n            time_count = 60 * 100\n            while request_id[0] is None and time_count > 0:\n                time.sleep(0.01)\n                time_count -= 1\n\n            if request_id[0] is None:\n                raise Exception(\"Failed to get request id\")\n\n            def write_running():\n                return ray.get(server.add_item.remote(request_id[0], \"RUNNING\"))\n\n            await asyncio.to_thread(write_running)\n            return [\n                (\n                    \"\",\n                    {\n                        \"metadata\": {\n                            \"request_id\": request_id[0],\n                            \"stream_server\": \"BLOCK_VLLM_STREAM_SERVER\",\n                        }\n                    },\n                )\n            ]\n        else:\n            try:\n                start_time = time.monotonic()\n                response = await asyncfy_with_semaphore(\n                    lambda: self.client.chat.completions.create(\n                        messages=messages,\n                        model=model,\n                        max_tokens=max_length,\n                        temperature=temperature,\n                        top_p=top_p,\n                    )\n                )()\n\n                generated_text = response.choices[0].message.content\n                generated_tokens_count = response.usage.completion_tokens\n                input_tokens_count = response.usage.prompt_tokens\n                time_cost = time.monotonic() - start_time\n                return [\n                    (\n                        generated_text,\n                        {\n                            \"metadata\": {\n                                \"request_id\": response.id,\n                                \"input_tokens_count\": input_tokens_count,\n                                \"generated_tokens_count\": generated_tokens_count,\n                                \"time_cost\": time_cost,\n                                \"first_token_time\": 0,\n                                \"speed\": float(generated_tokens_count) / time_cost,\n                            }\n                        },\n                    )\n                ]\n            except Exception as e:\n                print(f\"Error: {e}\")\n                raise e\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/whisper/__init__.py\nimport whisper\nfrom typing import Dict, List, Tuple\nfrom byzerllm.utils.types import StopSequencesCriteria\n\nfrom typing import Dict, Any, List, Union\nfrom pyjava.storage import streaming_tar as STar\nimport json\nimport base64\nimport tempfile\nimport time\n\n\ndef get_meta(self):\n    return [\n        {\n            \"model_deploy_type\": \"proprietary\",\n            \"backend\": \"transformers\",\n            \"message_format\": True\n        }\n    ]\n\n\ndef process_input(ins: Union[str, List[Dict[str, Any]], Dict[str, Any]]):\n    # print(ins) \n    if isinstance(ins, list) or isinstance(ins, dict):\n        return ins\n\n    content = []\n    try:\n        ins_json = json.loads(ins)\n    except:\n        return ins\n\n    ## speech\n    if isinstance(ins_json, dict):\n        return ins_json\n\n    content = []\n    #     [\n    #     {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n    #     {\n    #       \"type\": \"image_url\",\n    #       \"image_url\": {\n    #         \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n    #         \"detail\": \"high\"\n    #       },\n    #     },\n    #   ],\n    for item in ins_json:\n        # for format like this: {\"image\": \"xxxxxx\", \"text\": \"What’s in this image?\",\"detail\":\"high\"}\n        # or {\"image\": \"xxxxxx\"}, {\"text\": \"What’s in this image?\"}\n        if (\"image\" in item or \"image_url\" in item) and \"type\" not in item:\n            image_data = item.get(\"image\", item.get(\"image_url\", \"\"))\n            ## \"data:image/jpeg;base64,\"\n            if not image_data.startswith(\"data:\"):\n                image_data = \"data:image/jpeg;base64,\" + image_data\n\n            ## get the other fields except image/text/image_url\n            other_fields = {\n                k: v for k, v in item.items() if k not in [\"image\", \"text\", \"image_url\"]\n            }\n            content.append(\n                {\n                    \"image_url\": {\"url\": image_data, **other_fields},\n                    \"type\": \"image_url\",\n                }\n            )\n\n        if \"text\" in item and \"type\" not in item:\n            text_data = item[\"text\"]\n            content.append({\"text\": text_data, \"type\": \"text\"})\n\n        ## for format like this: {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n        ## {\"type\": \"image_url\", \"image_url\": {\"url\":\"\",\"detail\":\"high\"}}\n        ## this is the standard format, just return it\n        if \"type\" in item and item[\"type\"] == \"text\":\n            content.append(item)\n\n        if \"type\" in item and item[\"type\"] == \"image_url\":\n            content.append(item)\n\n    if not content:\n        return ins\n\n    return content\n\n\ndef stream_chat(\n    self,\n    tokenizer,\n    ins: str,\n    his: List[Dict[str, str]] = [],\n    max_length: int = 4090,\n    top_p: float = 0.95,\n    temperature: float = 0.1,\n    **kwargs,\n):\n    messages = [\n        {\"role\": message[\"role\"], \"content\": process_input(message[\"content\"])}\n        for message in his\n    ] + [{\"role\": \"user\", \"content\": process_input(ins)}]\n\n    audio_input = messages[-1][\"content\"]\n\n    audio_data = audio_input.get(\"audio\", \"\")\n    audio = audio_input[\"audio\"]\n    response_format = audio_input.get(\"response_format\", \"json\")\n    timestamp_granularities = audio_input.get(\n        \"timestamp_granularities\", [\"word\", \"segment\"]\n    )\n    data_prefix = \"data:audio/\"\n    base64_prefix = \";base64,\"\n    if not audio.startswith(data_prefix):\n        raise ValueError(\"Invalid audio data format\")\n\n    format_end = audio.index(base64_prefix)\n    audio_format = audio[len(data_prefix) : format_end]\n    base64_data = audio[format_end + len(base64_prefix) :]\n\n    # Decode the base64 audio data\n    audio_data = base64.b64decode(base64_data)\n\n    # Create a temporary file with the correct extension\n    with tempfile.NamedTemporaryFile(\n        delete=False, suffix=f\".{audio_format}\"\n    ) as temp_audio_file:\n        temp_audio_file.write(audio_data)\n        temp_audio_file_path = temp_audio_file.name\n\n    try:\n        start_time = time.monotonic()\n        word_timestamps = \"word\" in timestamp_granularities\n        \n        result = self.transcribe(temp_audio_file_path, word_timestamps=word_timestamps)        \n            \n        time_cost = time.monotonic() - start_time\n        return [\n            (\n                json.dumps(result, ensure_ascii=True),\n                {\n                    \"metadata\": {\n                        \"request_id\": \"\",\n                        \"input_tokens_count\": 0,\n                        \"generated_tokens_count\": 0,\n                        \"time_cost\": time_cost,\n                        \"first_token_time\": 0,\n                        \"speed\": 0,\n                    }\n                },\n            )\n        ]\n    finally:\n        # Clean up the temporary file\n        import os\n\n        os.unlink(temp_audio_file_path)\n\n\ndef init_model(\n    model_dir, infer_params: Dict[str, str] = {}, sys_conf: Dict[str, str] = {}\n):\n    model = whisper.load_model(\"large\", download_root=model_dir)\n    import types\n\n    model.stream_chat = types.MethodType(stream_chat, model)\n    model.get_meta = types.MethodType(get_meta, model)\n    return (model, None)\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/sensevoice/__init__.py\nimport json\nimport base64\nimport tempfile\nimport time\nfrom typing import Dict, List, Union, Any\nfrom funasr import AutoModel\n\nemo_dict = {\n    \"<|HAPPY|>\": \"😊\",\n    \"<|SAD|>\": \"😔\",\n    \"<|ANGRY|>\": \"😡\",\n    \"<|NEUTRAL|>\": \"\",\n    \"<|FEARFUL|>\": \"😰\",\n    \"<|DISGUSTED|>\": \"🤢\",\n    \"<|SURPRISED|>\": \"😮\",\n}\n\nevent_dict = {\n    \"<|BGM|>\": \"🎼\",\n    \"<|Speech|>\": \"\",\n    \"<|Applause|>\": \"👏\",\n    \"<|Laughter|>\": \"��\",\n    \"<|Cry|>\": \"😭\",\n    \"<|Sneeze|>\": \"🤧\",\n    \"<|Breath|>\": \"\",\n    \"<|Cough|>\": \"🤧\",\n}\n\nemoji_dict = {\n    \"<|nospeech|><|Event_UNK|>\": \"❓\",\n    \"<|zh|>\": \"\",\n    \"<|en|>\": \"\",\n    \"<|yue|>\": \"\",\n    \"<|ja|>\": \"\",\n    \"<|ko|>\": \"\",\n    \"<|nospeech|>\": \"\",\n    \"<|HAPPY|>\": \"😊\",\n    \"<|SAD|>\": \"😔\",\n    \"<|ANGRY|>\": \"😡\",\n    \"<|NEUTRAL|>\": \"\",\n    \"<|BGM|>\": \"🎼\",\n    \"<|Speech|>\": \"\",\n    \"<|Applause|>\": \"👏\",\n    \"<|Laughter|>\": \"😀\",\n    \"<|FEARFUL|>\": \"😰\",\n    \"<|DISGUSTED|>\": \"🤢\",\n    \"<|SURPRISED|>\": \"😮\",\n    \"<|Cry|>\": \"😭\",\n    \"<|EMO_UNKNOWN|>\": \"\",\n    \"<|Sneeze|>\": \"🤧\",\n    \"<|Breath|>\": \"\",\n    \"<|Cough|>\": \"😷\",\n    \"<|Sing|>\": \"\",\n    \"<|Speech_Noise|>\": \"\",\n    \"<|withitn|>\": \"\",\n    \"<|woitn|>\": \"\",\n    \"<|GBG|>\": \"\",\n    \"<|Event_UNK|>\": \"\",\n}\n\nlang_dict = {\n    \"<|zh|>\": \"<|lang|>\",\n    \"<|en|>\": \"<|lang|>\",\n    \"<|yue|>\": \"<|lang|>\",\n    \"<|ja|>\": \"<|lang|>\",\n    \"<|ko|>\": \"<|lang|>\",\n    \"<|nospeech|>\": \"<|lang|>\",\n}\n\nemo_set = {\"😊\", \"😔\", \"😡\", \"😰\", \"🤢\", \"😮\"}\nevent_set = {\"🎼\", \"👏\", \"😀\", \"😭\", \"🤧\", \"😷\"}\n\n\ndef format_str(s):\n    for sptk in emoji_dict:\n        s = s.replace(sptk, emoji_dict[sptk])\n    return s\n\n\ndef format_str_v2(s):\n    sptk_dict = {sptk: s.count(sptk) for sptk in emoji_dict}\n    s = \"\".join(c for c in s if c not in emoji_dict.values())\n    emo = max(emo_dict, key=lambda e: sptk_dict[e])\n    event = \"\".join(event_dict[e] for e in event_dict if sptk_dict[e] > 0)\n    s = event + s + emo_dict[emo]\n    for emoji in emo_set.union(event_set):\n        s = s.replace(\" \" + emoji, emoji).replace(emoji + \" \", emoji)\n    return s.strip()\n\n\ndef format_str_v3(s):\n    def get_emo(s):\n        return s[-1] if s[-1] in emo_set else None\n\n    def get_event(s):\n        return s[0] if s[0] in event_set else None\n\n    s = s.replace(\"<|nospeech|><|Event_UNK|>\", \"❓\")\n    for lang in lang_dict:\n        s = s.replace(lang, \"<|lang|>\")\n    s_list = [format_str_v2(s_i).strip() for s_i in s.split(\"<|lang|>\")]\n    new_s = \" \" + s_list[0]\n    cur_ent_event = get_event(new_s)\n    for i in range(1, len(s_list)):\n        if len(s_list[i]) == 0:\n            continue\n        if get_event(s_list[i]) == cur_ent_event and get_event(s_list[i]) is not None:\n            s_list[i] = s_list[i][1:]\n        cur_ent_event = get_event(s_list[i])\n        if get_emo(s_list[i]) is not None and get_emo(s_list[i]) == get_emo(new_s):\n            new_s = new_s[:-1]\n        new_s += s_list[i].strip()\n    new_s = new_s.replace(\"The.\", \" \")\n    return new_s.strip()\n\n\ndef get_meta(self):\n    return [\n        {\n            \"model_deploy_type\": \"proprietary\",\n            \"backend\": \"funasr\",\n            \"message_format\": True,\n        }\n    ]\n\n\ndef process_input(ins: Union[str, List[Dict[str, Any]], Dict[str, Any]]):\n    if isinstance(ins, list) or isinstance(ins, dict):\n        return ins\n\n    try:\n        ins_json = json.loads(ins)\n    except:\n        return ins\n\n    if isinstance(ins_json, dict):\n        return ins_json\n\n    content = []\n    for item in ins_json:\n        if \"audio\" in item and \"type\" not in item:\n            audio_data = item.get(\"audio\", \"\")\n            if not audio_data.startswith(\"data:\"):\n                audio_data = f\"data:audio/wav;base64,{audio_data}\"\n            content.append({\"audio\": audio_data, \"type\": \"audio\"})\n\n        if \"text\" in item and \"type\" not in item:\n            content.append({\"text\": item[\"text\"], \"type\": \"text\"})\n\n        if \"type\" in item:\n            content.append(item)\n\n    return content if content else ins\n\n\ndef stream_chat(\n    self,\n    tokenizer,\n    ins: str,\n    his: List[Dict[str, str]] = [],\n    max_length: int = 4090,\n    top_p: float = 0.95,\n    temperature: float = 0.1,\n    **kwargs,\n):\n    messages = [\n        {\"role\": message[\"role\"], \"content\": process_input(message[\"content\"])}\n        for message in his\n    ] + [{\"role\": \"user\", \"content\": process_input(ins)}]\n\n    audio_input = messages[-1][\"content\"]\n\n    audio = audio_input[\"audio\"]\n    data_prefix = \"data:audio/\"\n    base64_prefix = \";base64,\"\n    if not audio.startswith(data_prefix):\n        raise ValueError(\"Invalid audio data format\")\n\n    format_end = audio.index(base64_prefix)\n    audio_format = audio[len(data_prefix) : format_end]\n    base64_data = audio[format_end + len(base64_prefix) :]\n\n    audio_data = base64.b64decode(base64_data)\n\n    with tempfile.NamedTemporaryFile(\n        delete=False, suffix=f\".{audio_format}\"\n    ) as temp_audio_file:\n        temp_audio_file.write(audio_data)\n        temp_audio_file_path = temp_audio_file.name\n\n    try:\n        start_time = time.monotonic()\n\n        result = self.generate(input=temp_audio_file_path, cache={})\n\n        time_cost = time.monotonic() - start_time\n\n        formatted_result = format_str_v3(result[0][\"text\"])\n\n        return [\n            (\n                json.dumps({\"text\": formatted_result}, ensure_ascii=False),\n                {\n                    \"metadata\": {\n                        \"request_id\": \"\",\n                        \"input_tokens_count\": 0,\n                        \"generated_tokens_count\": 0,\n                        \"time_cost\": time_cost,\n                        \"first_token_time\": 0,\n                        \"speed\": 0,\n                    }\n                },\n            )\n        ]\n    finally:\n        import os\n\n        os.unlink(temp_audio_file_path)\n\n\ndef init_model(\n    model_dir, infer_params: Dict[str, str] = {}, sys_conf: Dict[str, str] = {}\n):\n    model_kwargs = {\n        \"model\": model_dir,\n        \"trust_remote_code\": True,\n    }\n\n    if \"vad_model\" in infer_params:\n        model_kwargs[\"vad_model\"] = infer_params[\"vad_model\"]\n        model_kwargs[\"vad_kwargs\"] = {\"max_single_segment_time\": 30000}\n\n    model = AutoModel(**model_kwargs)\n\n    import types\n\n    model.stream_chat = types.MethodType(stream_chat, model)\n    model.get_meta = types.MethodType(get_meta, model)\n\n    return (model, None)\n\n\n# Helper functions from web_ui.py\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/notebooks/test_sensevoice.py\nimport sys\nimport os\n\n# Add the project root to the Python path\nsys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n\nfrom src.byzerllm.sensevoice import init_model, format_str_v3\nimport base64\nimport json\n\ndef main():\n    # Initialize the model\n    model_dir = \"path/to/your/model/directory\"  # Replace with actual path\n    model, _ = init_model(model_dir)\n\n    # Prepare test audio\n    with open(\"path/to/test/audio.wav\", \"rb\") as audio_file:  # Replace with actual path\n        audio_data = audio_file.read()\n        base64_audio = base64.b64encode(audio_data).decode('utf-8')\n\n    # Prepare input in the expected format\n    test_input = json.dumps([{\n        \"role\": \"user\",\n        \"content\": [{\n            \"type\": \"audio\",\n            \"audio\": f\"data:audio/wav;base64,{base64_audio}\"\n        }]\n    }])\n\n    # Call stream_chat\n    result = model.stream_chat(None, test_input)\n\n    # Process and print the result\n    if result and len(result) > 0:\n        output, metadata = result[0]\n        parsed_output = json.loads(output)\n        formatted_text = parsed_output.get('text', '')\n        print(\"Transcription:\", formatted_text)\n        print(\"Metadata:\", metadata)\n    else:\n        print(\"No result returned from the model.\")\n\nif __name__ == \"__main__\":\n    main()\n\n     \n\n下面是用户的需求：\n\n@test_sensevoice.py 中测试数据改成 /Users/allwefantasy/models/SenseVoiceSmall/example/zh.mp3 模型路径改成 /Users/allwefantasy/models/SenseVoiceSmall\n"}, {"role": "assistant", "content": "为了满足用户的需求，我们需要修改 `test_sensevoice.py` 文件中的测试音频路径和模型路径。以下是相应的 *SEARCH/REPLACE* 块：\n\n```python\n##File: /Users/allwefantasy/projects/byzer-llm/notebooks/test_sensevoice.py\n<<<<<<< SEARCH\n    model_dir = \"path/to/your/model/directory\"  # Replace with actual path\n    with open(\"path/to/test/audio.wav\", \"rb\") as audio_file:  # Replace with actual path\n=======\n    model_dir = \"/Users/allwefantasy/models/SenseVoiceSmall\"\n    with open(\"/Users/allwefantasy/models/SenseVoiceSmall/example/zh.mp3\", \"rb\") as audio_file:\n>>>>>>> REPLACE\n```\n\n这个 *SEARCH/REPLACE* 块会将 `test_sensevoice.py` 文件中的模型路径和测试音频路径分别替换为用户指定的路径。"}], "model": "deepseek_chat", "yaml_file": "actions/027_chat_action.yml"}
{"instruction": "@test_sensevoice.py 中测试数据改成 /Users/allwefantasy/models/SenseVoiceSmall/example/zh.mp3 模型路径改成 /Users/allwefantasy/models/SenseVoiceSmall", "conversations": [{"role": "user", "content": "\n如果你需要生成代码，对于每个需要更改的文件,你需要按 *SEARCH/REPLACE block* 的格式进行生成。\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The opening fence and code language, eg: ```python\n2. The file path alone on a line, starting with \"##File:\" and verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n*SEARCH/REPLACE* blocks will replace *all* matching occurrences.\nInclude enough lines to make the SEARCH blocks unique.\n\nInclude *ALL* the code being searched and replaced!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n下面我们来看一个例子：\n\n当前项目目录结构：\n1. 项目根目录： /tmp/projects/mathweb\n2. 项目子目录/文件列表(类似tree 命令输出)\nflask/\n    app.py\n    templates/\n        index.html\n    static/\n        style.css\n\n用户需求： Change get_factorial() to use math.factorial\n\n回答： To make this change we need to modify `/tmp/projects/mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n用户需求： Refactor hello() into its own file.\n\n回答：To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/hello.py\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/main.py\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n现在让我们开始一个新的任务:\n\n当前项目目录结构：\n1. 项目根目录： /Users/allwefantasy/projects/byzer-llm\n2. 项目子目录/文件列表(类似tree 命令输出)：\nsetup.py\ntests/\n    test_emb_rerank.py\n    test_deepseek_code.py\n    test_prompt.py\n    test_model_meta.py\n    __init__.py\n    test_transformers_deploy.py\n    test_data_analysis.py\n    test_yi.py\n    test_deploy.py\n    functions_suite.py\n    test_template.py\n    auto/\n        test_backend_llama_cpp.py\n        __init__.py\nsetup-machine/\n    convert_safetensor.py\nnotebooks/\n    test.py\n    test_sensevoice.py\nsrc/\n    byzerllm/\n        lang.py\n        store.py\n        version.py\n        log.py\n        __init__.py\n        types.py\n        easy_byzerllm_command.py\n        byzerllm_command.py\n        command_args.py\n        bark/\n            generation.py\n            __init__.py\n            model.py\n            api.py\n            model_fine.py\n            bark_voice.py\n        moss/\n            finetune_moss.py\n            __init__.py\n            moss_inference.py\n            models/\n                configuration_moss.py\n                quantization.py\n                custom_autotune.py\n                __init__.py\n                modeling_moss.py\n                tokenization_moss.py\n        llama2/\n            __init__.py\n        records/\n            __init__.py\n        visualglm/\n            __init__.py\n        auto/\n            __init__.py\n            backend_llama_cpp.py\n            backend_vllm.py\n            backend_ds.py\n        alpha_moss/\n            __init__.py\n        processor/\n            __init__.py\n        sensevoice/\n            __init__.py\n        qwen/\n            __init__.py\n        utils/\n            json_repaire.py\n            object_store_ref_util.py\n            __init__.py\n            types.py\n            tokenizer.py\n            openai_utils.py\n            ray_utils.py\n            text_generator.py\n            emb.py\n            connect_ray.py\n            testing.py\n            langutil.py\n            nontext.py\n            fulltune/\n                deepspeed_trainner.py\n                __init__.py\n                launch.py\n                trainner.py\n                base_model/\n                    __init__.py\n                    configuration_baichuan.py\n                    modeling_baichuan.py\n                pretrain/\n                    __init__.py\n                    convert_to_transformers.py\n            metrics/\n                __init__.py\n            config/\n                __init__.py\n            retrieval/\n                rest.py\n                __init__.py\n                udf.py\n            sft/\n                argument.py\n                collator.py\n                qlora.py\n                __init__.py\n                model.py\n                dataset.py\n                loss.py\n                merge_lora.py\n                trainer.py\n            client/\n                message_utils.py\n                math_utils.py\n                byzerllm_client.py\n                parallel_utils.py\n                __init__.py\n                types.py\n                code_utils.py\n                img_utils.py\n                entrypoints/\n                    __init__.py\n                    openai/\n                        protocol.py\n                        __init__.py\n                        serve.py\n                        serving_chat.py\n                        api_server.py\n                        serving_engine.py\n                        serving_completion.py\n                        tool.py\n        chatglm2/\n            __init__.py\n        baichuan/\n            __init__.py\n        m3e/\n            __init__.py\n        stable_diffusion/\n            config.py\n            __init__.py\n            logger.py\n            shared.py\n            model.py\n            utils.py\n            images.py\n            diffusion/\n                __init__.py\n                embeddings.py\n                utils.py\n                networks/\n                    lyco.py\n                    __init__.py\n                upscalers/\n                    __init__.py\n                    multidiffusion.py\n                    samplers.py\n                piplines/\n                    __init__.py\n                    lpw.py\n                    diffusers.py\n            lib/\n                __init__.py\n                diffusers/\n                    __init__.py\n                    scheduler.py\n            api/\n                __init__.py\n                plugin.py\n                models/\n                    diffusion.py\n                    __init__.py\n                diffusion/\n                    __init__.py\n                    pipelines/\n                        __init__.py\n                        diffusers.py\n                events/\n                    generation.py\n                    __init__.py\n        bge/\n            __init__.py\n        bge_rerank/\n            __init__.py\n        apps/\n            client.py\n            qa_strategy.py\n            __init__.py\n            builder.py\n            vector_db.py\n            utils.py\n            qa.py\n            byzer_storage/\n                memory_model_based.py\n                command.py\n                env.py\n                train_pt.py\n                train_sft.py\n                __init__.py\n                train_dpo.py\n                simple_api.py\n                generate_sft_data.py\n            byzer_sql/\n                __init__.py\n            llama_index/\n                byzerai_vectordb.py\n                byzerai.py\n                byzerai_embedding.py\n                __init__.py\n                collection_manager.py\n                byzerai_index_store.py\n                byzerai_docstore.py\n                simple_retrieval.py\n                byzerai_kvstore.py\n            agent/\n                conversable_agent.py\n                registry.py\n                groupchat.py\n                user_proxy_agent.py\n                __init__.py\n                agent.py\n                extensions/\n                    simple_retrieval_client.py\n                    planner.py\n                    python_codesandbox_agent.py\n                    preview_file_agent.py\n                    llama_index_retrieval_agent.py\n                    visualization_agent.py\n                    assistant_agent.py\n                    load_data_agent.py\n                    retrieval_agent.py\n                    output_agent.py\n                    __init__.py\n                    byzer_engine_agent.py\n                    subquestion_agent.py\n                    sql_reviewer_agent.py\n                    spark_sql_agent.py\n                    rhetorical_agent.py\n                    data_analysis.py\n                    data_analysis_pipeline_agent.py\n                    common_agent.py\n                    query_rewrite/\n                        time.py\n                        rhetorical.py\n                        __init__.py\n                        condition.py\n                        context.py\n                store/\n                    memory_store.py\n                    __init__.py\n                    stores.py\n        saas/\n            __init__.py\n            chatglm/\n                __init__.py\n            sparkdesk/\n                __init__.py\n            gemini/\n                __init__.py\n            azure_openai/\n                __init__.py\n            azure/\n                __init__.py\n            qianfan/\n                __init__.py\n            claude/\n                __init__.py\n            qianwen/\n                __init__.py\n            volcengine/\n                __init__.py\n            minimax/\n                __init__.py\n            baichuan/\n                __init__.py\n            qianwen_vl/\n                __init__.py\n            official_openai/\n                __init__.py\n            zhipu/\n                __init__.py\n            aws_bedrock/\n                __init__.py\n            openai/\n                __init__.py\n        falcon/\n            __init__.py\n        chatglm6b/\n            trainer_seq2seq.py\n            arguments.py\n            __init__.py\n            finetune.py\n            tunning/\n                __init__.py\n                infer.py\n                finetune.py\n                utils/\n                    config.py\n                    __init__.py\n                    other.py\n                    common.py\n                    seq2seq.py\n        starcode/\n            __init__.py\n        qwen_vl_chat/\n            __init__.py\n        zephyr/\n            __init__.py\n        dolly/\n            generate.py\n            __init__.py\n            consts.py\n            dolly_inference.py\n            trainer.py\n        llama/\n            __init__.py\n        whisper/\n            whisper_inference.py\n            __init__.py\n下面是一些文件路径以及每个文件对应的源码：\n\n##File: /Users/allwefantasy/projects/SenseVoice/webui.py\n# coding=utf-8\n\nimport os\nimport librosa\nimport base64\nimport io\nimport gradio as gr\nimport re\n\nimport numpy as np\nimport torch\nimport torchaudio\n\n\nfrom funasr import AutoModel\n\nmodel = \"iic/SenseVoiceSmall\"\nmodel_kwargs = {\n    \"model\": model,\n    \"trust_remote_code\": True,\n}\n\n# Check if vad_model is provided in infer_params\nif \"vad_model\" in infer_params:\n    model_kwargs[\"vad_model\"] = infer_params[\"vad_model\"]\n    model_kwargs[\"vad_kwargs\"] = {\"max_single_segment_time\": 30000}\n\nmodel = AutoModel(**model_kwargs)\n\nimport re\n\nemo_dict = {\n\t\"<|HAPPY|>\": \"😊\",\n\t\"<|SAD|>\": \"😔\",\n\t\"<|ANGRY|>\": \"😡\",\n\t\"<|NEUTRAL|>\": \"\",\n\t\"<|FEARFUL|>\": \"😰\",\n\t\"<|DISGUSTED|>\": \"🤢\",\n\t\"<|SURPRISED|>\": \"😮\",\n}\n\nevent_dict = {\n\t\"<|BGM|>\": \"🎼\",\n\t\"<|Speech|>\": \"\",\n\t\"<|Applause|>\": \"👏\",\n\t\"<|Laughter|>\": \"😀\",\n\t\"<|Cry|>\": \"😭\",\n\t\"<|Sneeze|>\": \"🤧\",\n\t\"<|Breath|>\": \"\",\n\t\"<|Cough|>\": \"🤧\",\n}\n\nemoji_dict = {\n\t\"<|nospeech|><|Event_UNK|>\": \"❓\",\n\t\"<|zh|>\": \"\",\n\t\"<|en|>\": \"\",\n\t\"<|yue|>\": \"\",\n\t\"<|ja|>\": \"\",\n\t\"<|ko|>\": \"\",\n\t\"<|nospeech|>\": \"\",\n\t\"<|HAPPY|>\": \"😊\",\n\t\"<|SAD|>\": \"😔\",\n\t\"<|ANGRY|>\": \"😡\",\n\t\"<|NEUTRAL|>\": \"\",\n\t\"<|BGM|>\": \"🎼\",\n\t\"<|Speech|>\": \"\",\n\t\"<|Applause|>\": \"👏\",\n\t\"<|Laughter|>\": \"😀\",\n\t\"<|FEARFUL|>\": \"😰\",\n\t\"<|DISGUSTED|>\": \"🤢\",\n\t\"<|SURPRISED|>\": \"😮\",\n\t\"<|Cry|>\": \"😭\",\n\t\"<|EMO_UNKNOWN|>\": \"\",\n\t\"<|Sneeze|>\": \"🤧\",\n\t\"<|Breath|>\": \"\",\n\t\"<|Cough|>\": \"😷\",\n\t\"<|Sing|>\": \"\",\n\t\"<|Speech_Noise|>\": \"\",\n\t\"<|withitn|>\": \"\",\n\t\"<|woitn|>\": \"\",\n\t\"<|GBG|>\": \"\",\n\t\"<|Event_UNK|>\": \"\",\n}\n\nlang_dict =  {\n    \"<|zh|>\": \"<|lang|>\",\n    \"<|en|>\": \"<|lang|>\",\n    \"<|yue|>\": \"<|lang|>\",\n    \"<|ja|>\": \"<|lang|>\",\n    \"<|ko|>\": \"<|lang|>\",\n    \"<|nospeech|>\": \"<|lang|>\",\n}\n\nemo_set = {\"😊\", \"😔\", \"😡\", \"😰\", \"🤢\", \"😮\"}\nevent_set = {\"🎼\", \"👏\", \"😀\", \"😭\", \"🤧\", \"😷\",}\n\ndef format_str(s):\n\tfor sptk in emoji_dict:\n\t\ts = s.replace(sptk, emoji_dict[sptk])\n\treturn s\n\n\ndef format_str_v2(s):\n\tsptk_dict = {}\n\tfor sptk in emoji_dict:\n\t\tsptk_dict[sptk] = s.count(sptk)\n\t\ts = s.replace(sptk, \"\")\n\temo = \"<|NEUTRAL|>\"\n\tfor e in emo_dict:\n\t\tif sptk_dict[e] > sptk_dict[emo]:\n\t\t\temo = e\n\tfor e in event_dict:\n\t\tif sptk_dict[e] > 0:\n\t\t\ts = event_dict[e] + s\n\ts = s + emo_dict[emo]\n\n\tfor emoji in emo_set.union(event_set):\n\t\ts = s.replace(\" \" + emoji, emoji)\n\t\ts = s.replace(emoji + \" \", emoji)\n\treturn s.strip()\n\ndef format_str_v3(s):\n\tdef get_emo(s):\n\t\treturn s[-1] if s[-1] in emo_set else None\n\tdef get_event(s):\n\t\treturn s[0] if s[0] in event_set else None\n\n\ts = s.replace(\"<|nospeech|><|Event_UNK|>\", \"❓\")\n\tfor lang in lang_dict:\n\t\ts = s.replace(lang, \"<|lang|>\")\n\ts_list = [format_str_v2(s_i).strip(\" \") for s_i in s.split(\"<|lang|>\")]\n\tnew_s = \" \" + s_list[0]\n\tcur_ent_event = get_event(new_s)\n\tfor i in range(1, len(s_list)):\n\t\tif len(s_list[i]) == 0:\n\t\t\tcontinue\n\t\tif get_event(s_list[i]) == cur_ent_event and get_event(s_list[i]) != None:\n\t\t\ts_list[i] = s_list[i][1:]\n\t\t#else:\n\t\tcur_ent_event = get_event(s_list[i])\n\t\tif get_emo(s_list[i]) != None and get_emo(s_list[i]) == get_emo(new_s):\n\t\t\tnew_s = new_s[:-1]\n\t\tnew_s += s_list[i].strip().lstrip()\n\tnew_s = new_s.replace(\"The.\", \" \")\n\treturn new_s.strip()\n\ndef model_inference(input_wav, language, fs=16000):\n\t# task_abbr = {\"Speech Recognition\": \"ASR\", \"Rich Text Transcription\": (\"ASR\", \"AED\", \"SER\")}\n\tlanguage_abbr = {\"auto\": \"auto\", \"zh\": \"zh\", \"en\": \"en\", \"yue\": \"yue\", \"ja\": \"ja\", \"ko\": \"ko\",\n\t\t\t\t\t \"nospeech\": \"nospeech\"}\n\t\n\t# task = \"Speech Recognition\" if task is None else task\n\tlanguage = \"auto\" if len(language) < 1 else language\n\tselected_language = language_abbr[language]\n\t# selected_task = task_abbr.get(task)\n\t\n\t# print(f\"input_wav: {type(input_wav)}, {input_wav[1].shape}, {input_wav}\")\n\t\n\tif isinstance(input_wav, tuple):\n\t\tfs, input_wav = input_wav\n\t\tinput_wav = input_wav.astype(np.float32) / np.iinfo(np.int16).max\n\t\tif len(input_wav.shape) > 1:\n\t\t\tinput_wav = input_wav.mean(-1)\n\t\tif fs != 16000:\n\t\t\tprint(f\"audio_fs: {fs}\")\n\t\t\tresampler = torchaudio.transforms.Resample(fs, 16000)\n\t\t\tinput_wav_t = torch.from_numpy(input_wav).to(torch.float32)\n\t\t\tinput_wav = resampler(input_wav_t[None, :])[0, :].numpy()\n\t\n\t\n\tmerge_vad = True #False if selected_task == \"ASR\" else True\n\tprint(f\"language: {language}, merge_vad: {merge_vad}\")\n\ttext = model.generate(input=input_wav,\n\t\t\t\t\t\t  cache={},\n\t\t\t\t\t\t  language=language,\n\t\t\t\t\t\t  use_itn=True,\n\t\t\t\t\t\t  batch_size_s=60, merge_vad=merge_vad)\n\t\n\tprint(text)\n\ttext = text[0][\"text\"]\n\ttext = format_str_v3(text)\n\t\n\tprint(text)\n\t\n\treturn text\n\n\naudio_examples = [\n    [\"example/zh.mp3\", \"zh\"],\n    [\"example/yue.mp3\", \"yue\"],\n    [\"example/en.mp3\", \"en\"],\n    [\"example/ja.mp3\", \"ja\"],\n    [\"example/ko.mp3\", \"ko\"],\n    [\"example/emo_1.wav\", \"auto\"],\n    [\"example/emo_2.wav\", \"auto\"],\n    [\"example/emo_3.wav\", \"auto\"],\n    #[\"example/emo_4.wav\", \"auto\"],\n    #[\"example/event_1.wav\", \"auto\"],\n    #[\"example/event_2.wav\", \"auto\"],\n    #[\"example/event_3.wav\", \"auto\"],\n    [\"example/rich_1.wav\", \"auto\"],\n    [\"example/rich_2.wav\", \"auto\"],\n    #[\"example/rich_3.wav\", \"auto\"],\n    [\"example/longwav_1.wav\", \"auto\"],\n    [\"example/longwav_2.wav\", \"auto\"],\n    [\"example/longwav_3.wav\", \"auto\"],\n    #[\"example/longwav_4.wav\", \"auto\"],\n]\n\n\n\nhtml_content = \"\"\"\n<div>\n    <h2 style=\"font-size: 22px;margin-left: 0px;\">Voice Understanding Model: SenseVoice-Small</h2>\n    <p style=\"font-size: 18px;margin-left: 20px;\">SenseVoice-Small is an encoder-only speech foundation model designed for rapid voice understanding. It encompasses a variety of features including automatic speech recognition (ASR), spoken language identification (LID), speech emotion recognition (SER), and acoustic event detection (AED). SenseVoice-Small supports multilingual recognition for Chinese, English, Cantonese, Japanese, and Korean. Additionally, it offers exceptionally low inference latency, performing 7 times faster than Whisper-small and 17 times faster than Whisper-large.</p>\n    <h2 style=\"font-size: 22px;margin-left: 0px;\">Usage</h2> <p style=\"font-size: 18px;margin-left: 20px;\">Upload an audio file or input through a microphone, then select the task and language. the audio is transcribed into corresponding text along with associated emotions (😊 happy, 😡 angry/exicting, 😔 sad) and types of sound events (😀 laughter, 🎼 music, 👏 applause, 🤧 cough&sneeze, 😭 cry). The event labels are placed in the front of the text and the emotion are in the back of the text.</p>\n\t<p style=\"font-size: 18px;margin-left: 20px;\">Recommended audio input duration is below 30 seconds. For audio longer than 30 seconds, local deployment is recommended.</p>\n\t<h2 style=\"font-size: 22px;margin-left: 0px;\">Repo</h2>\n\t<p style=\"font-size: 18px;margin-left: 20px;\"><a href=\"https://github.com/FunAudioLLM/SenseVoice\" target=\"_blank\">SenseVoice</a>: multilingual speech understanding model</p>\n\t<p style=\"font-size: 18px;margin-left: 20px;\"><a href=\"https://github.com/modelscope/FunASR\" target=\"_blank\">FunASR</a>: fundamental speech recognition toolkit</p>\n\t<p style=\"font-size: 18px;margin-left: 20px;\"><a href=\"https://github.com/FunAudioLLM/CosyVoice\" target=\"_blank\">CosyVoice</a>: high-quality multilingual TTS model</p>\n</div>\n\"\"\"\n\n\ndef launch():\n\twith gr.Blocks(theme=gr.themes.Soft()) as demo:\n\t\t# gr.Markdown(description)\n\t\tgr.HTML(html_content)\n\t\twith gr.Row():\n\t\t\twith gr.Column():\n\t\t\t\taudio_inputs = gr.Audio(label=\"Upload audio or use the microphone\")\n\t\t\t\t\n\t\t\t\twith gr.Accordion(\"Configuration\"):\n\t\t\t\t\tlanguage_inputs = gr.Dropdown(choices=[\"auto\", \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"],\n\t\t\t\t\t\t\t\t\t\t\t\t  value=\"auto\",\n\t\t\t\t\t\t\t\t\t\t\t\t  label=\"Language\")\n\t\t\t\tfn_button = gr.Button(\"Start\", variant=\"primary\")\n\t\t\t\ttext_outputs = gr.Textbox(label=\"Results\")\n\t\t\tgr.Examples(examples=audio_examples, inputs=[audio_inputs, language_inputs], examples_per_page=20)\n\t\t\n\t\tfn_button.click(model_inference, inputs=[audio_inputs, language_inputs], outputs=text_outputs)\n\n\tdemo.launch()\n\n\nif __name__ == \"__main__\":\n\t# iface.launch()\n\tlaunch()\n\n\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/saas/official_openai/__init__.py\nimport time\nfrom typing import List, Tuple, Dict, Any, Union\nimport httpx\nfrom openai import OpenAI\nimport base64\nimport io\nimport json\nimport ray\nfrom byzerllm.utils.types import (\n    BlockVLLMStreamServer,\n    StreamOutputs,\n    SingleOutput,\n    SingleOutputMeta,\n    BlockBinaryStreamServer,\n)\nfrom byzerllm.utils.langutil import asyncfy_with_semaphore\nimport threading\nimport asyncio\nimport traceback\nimport uuid\nimport tempfile        \n\nclass CustomSaasAPI:\n\n    def __init__(self, infer_params: Dict[str, str]) -> None:\n\n        self.api_key = infer_params[\"saas.api_key\"]\n        self.model = infer_params.get(\"saas.model\", \"gpt-3.5-turbo-1106\")\n\n        other_params = {}\n\n        if \"saas.api_base\" in infer_params:\n            other_params[\"api_base\"] = infer_params[\"saas.api_base\"]\n\n        if \"saas.api_version\" in infer_params:\n            other_params[\"api_version\"] = infer_params[\"saas.api_version\"]\n\n        if \"saas.api_type\" in infer_params:\n            other_params[\"api_type\"] = infer_params[\"saas.api_type\"]\n\n        if \"saas.base_url\" in infer_params:\n            other_params[\"base_url\"] = infer_params[\"saas.base_url\"]\n\n        if \"saas.timeout\" in infer_params:\n            other_params[\"timeout\"] = float(infer_params[\"saas.timeout\"])\n\n        self.max_retries = int(infer_params.get(\"saas.max_retries\", 10))\n\n        self.meta = {\n            \"model_deploy_type\": \"saas\",\n            \"backend\": \"saas\",\n            \"support_stream\": True,\n            \"model_name\": self.model,\n        }\n\n        self.meta[\"embedding_mode\"] = \"embedding\" in self.model.lower()\n\n        self.proxies = infer_params.get(\"saas.proxies\", None)\n        self.local_address = infer_params.get(\"saas.local_address\", \"0.0.0.0\")\n\n        if not self.proxies:\n            self.client = OpenAI(**other_params, api_key=self.api_key)\n        else:\n            self.client = OpenAI(\n                **other_params,\n                api_key=self.api_key,\n                http_client=httpx.Client(\n                    proxies=self.proxies,\n                    transport=httpx.HTTPTransport(local_address=self.local_address),\n                ),\n            )\n\n        try:\n            ray.get_actor(\"BLOCK_VLLM_STREAM_SERVER\")\n        except ValueError:\n            try:\n                ray.remote(BlockVLLMStreamServer).options(\n                    name=\"BLOCK_VLLM_STREAM_SERVER\",\n                    lifetime=\"detached\",\n                    max_concurrency=1000,\n                ).remote()\n            except Exception as e:\n                pass\n        try:\n            ray.get_actor(\"BlockBinaryStreamServer\")\n        except ValueError:\n            try:\n                ray.remote(BlockBinaryStreamServer).options(\n                    name=\"BlockBinaryStreamServer\",\n                    lifetime=\"detached\",\n                    max_concurrency=1000,\n                ).remote()\n            except Exception as e:\n                pass\n\n    # saas/proprietary\n    def get_meta(self):\n        return [self.meta]\n\n    def process_input(self, ins: Union[str, List[Dict[str, Any]], Dict[str, Any]]):\n\n        if isinstance(ins, list) or isinstance(ins, dict):\n            return ins\n\n        content = []\n        try:\n            ins_json = json.loads(ins)\n        except:\n            return ins\n\n        ## speech\n        if isinstance(ins_json, dict):\n            return ins_json\n\n        content = []\n        #     [\n        #     {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n        #     {\n        #       \"type\": \"image_url\",\n        #       \"image_url\": {\n        #         \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n        #         \"detail\": \"high\"\n        #       },\n        #     },\n        #   ],\n        for item in ins_json:\n            # for format like this: {\"image\": \"xxxxxx\", \"text\": \"What’s in this image?\",\"detail\":\"high\"}\n            # or {\"image\": \"xxxxxx\"}, {\"text\": \"What’s in this image?\"}\n            if (\"image\" in item or \"image_url\" in item) and \"type\" not in item:\n                image_data = item.get(\"image\", item.get(\"image_url\", \"\"))\n                ## \"data:image/jpeg;base64,\"\n                if not image_data.startswith(\"data:\"):\n                    image_data = \"data:image/jpeg;base64,\" + image_data\n\n                ## get the other fields except image/text/image_url\n                other_fields = {\n                    k: v\n                    for k, v in item.items()\n                    if k not in [\"image\", \"text\", \"image_url\"]\n                }\n                content.append(\n                    {\n                        \"image_url\": {\"url\": image_data, **other_fields},\n                        \"type\": \"image_url\",\n                    }\n                )            \n\n            if \"text\" in item and \"type\" not in item:\n                text_data = item[\"text\"]\n                content.append({\"text\": text_data, \"type\": \"text\"})\n\n            ## for format like this: {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n            ## {\"type\": \"image_url\", \"image_url\": {\"url\":\"\",\"detail\":\"high\"}}\n            ## this is the standard format, just return it\n            if \"type\" in item and item[\"type\"] == \"text\":\n                content.append(item)\n\n            if \"type\" in item and item[\"type\"] == \"image_url\":\n                content.append(item)\n\n        if not content:\n            return ins\n\n        return content\n\n    async def async_embed_query(self, ins: str, **kwargs):\n        resp = await asyncfy_with_semaphore(\n            lambda: self.client.embeddings.create(input=[ins], model=self.model)\n        )()\n        embedding = resp.data[0].embedding\n        usage = resp.usage\n        return (\n            embedding,\n            {\n                \"metadata\": {\n                    \"input_tokens_count\": usage.prompt_tokens,\n                    \"generated_tokens_count\": 0,\n                }\n            },\n        )\n\n    def embed_query(self, ins: str, **kwargs):\n        resp = self.client.embeddings.create(input=[ins], model=self.model)\n        embedding = resp.data[0].embedding\n        usage = resp.usage\n        return (\n            embedding,\n            {\n                \"metadata\": {\n                    \"input_tokens_count\": usage.prompt_tokens,\n                    \"generated_tokens_count\": 0,\n                }\n            },\n        )\n\n    async def async_text_to_speech(\n        self, stream: bool, ins: str, voice: str, chunk_size: int = None, **kwargs\n    ):\n        if stream:\n            server = ray.get_actor(\"BlockBinaryStreamServer\")\n            request_id = [None]\n\n            def writer():\n                try:\n                    request_id[0] = str(uuid.uuid4())\n                    with self.client.with_streaming_response.audio.speech.create(\n                        model=self.model, voice=voice, input=ins, **kwargs\n                    ) as response:\n                        for chunk in response.iter_bytes(chunk_size):\n                            input_tokens_count = 0\n                            generated_tokens_count = 0\n                            ray.get(\n                                server.add_item.remote(\n                                    request_id[0],\n                                    StreamOutputs(\n                                        outputs=[\n                                            SingleOutput(\n                                                text=chunk,\n                                                metadata=SingleOutputMeta(\n                                                    input_tokens_count=input_tokens_count,\n                                                    generated_tokens_count=generated_tokens_count,\n                                                ),\n                                            )\n                                        ]\n                                    ),\n                                )\n                            )\n                except:\n                    traceback.print_exc()\n                ray.get(server.mark_done.remote(request_id[0]))\n\n            threading.Thread(target=writer, daemon=True).start()\n\n            time_count = 10 * 100\n            while request_id[0] is None and time_count > 0:\n                time.sleep(0.01)\n                time_count -= 1\n\n            if request_id[0] is None:\n                raise Exception(\"Failed to get request id\")\n\n            def write_running():\n                return ray.get(server.add_item.remote(request_id[0], \"RUNNING\"))\n\n            await asyncio.to_thread(write_running)\n            return [\n                (\n                    \"\",\n                    {\n                        \"metadata\": {\n                            \"request_id\": request_id[0],\n                            \"stream_server\": \"BlockBinaryStreamServer\",\n                        }\n                    },\n                )\n            ]\n\n        start_time = time.monotonic()\n        with io.BytesIO() as output:\n            async with asyncfy_with_semaphore(\n                lambda: self.client.with_streaming_response.audio.speech.create(\n                    model=self.model, voice=voice, input=ins, **kwargs\n                )\n            )() as response:\n                for chunk in response.iter_bytes():\n                    output.write(chunk)\n\n            base64_audio = base64.b64encode(output.getvalue()).decode()\n            time_cost = time.monotonic() - start_time\n            return [\n                (\n                    base64_audio,\n                    {\n                        \"metadata\": {\n                            \"request_id\": \"\",\n                            \"input_tokens_count\": 0,\n                            \"generated_tokens_count\": 0,\n                            \"time_cost\": time_cost,\n                            \"first_token_time\": 0,\n                            \"speed\": 0,\n                        }\n                    },\n                )\n            ]\n\n    async def async_speech_to_text(\n        self,\n        audio: str,\n        response_format: str = \"verbose_json\",\n        timestamp_granularities: List[str] = [\"word\",\"segment\"],\n    ):               \n        # Extract audio format and base64 data\n        data_prefix = \"data:audio/\"\n        base64_prefix = \";base64,\"\n        if not audio.startswith(data_prefix):\n            raise ValueError(\"Invalid audio data format\")\n        \n        format_end = audio.index(base64_prefix)\n        audio_format = audio[len(data_prefix):format_end]\n        base64_data = audio[format_end + len(base64_prefix):]\n        \n        # Decode the base64 audio data\n        audio_data = base64.b64decode(base64_data)\n\n        # Create a temporary file with the correct extension\n        with tempfile.NamedTemporaryFile(delete=False, suffix=f'.{audio_format}') as temp_audio_file:\n            temp_audio_file.write(audio_data)\n            temp_audio_file_path = temp_audio_file.name\n\n        try:\n            start_time = time.monotonic()\n                   \n            with open(temp_audio_file_path, \"rb\") as audio_file:                \n                transcription = self.client.audio.transcriptions.create(\n                    model=self.model or \"whisper-1\",\n                    file=audio_file,\n                    response_format=response_format,\n                    timestamp_granularities=timestamp_granularities,\n                )\n            time_cost = time.monotonic() - start_time\n\n            return [\n                (\n                    json.dumps(transcription.to_dict(), ensure_ascii=True),\n                    {\n                        \"metadata\": {\n                            \"request_id\": \"\",\n                            \"input_tokens_count\": 0,\n                            \"generated_tokens_count\": 0,\n                            \"time_cost\": time_cost,\n                            \"first_token_time\": 0,\n                            \"speed\": 0,\n                        }\n                    },\n                )\n            ]\n        finally:\n            # Clean up the temporary file\n            import os\n            os.unlink(temp_audio_file_path)\n\n    def image_to_text(self, ins: str, **kwargs):\n        pass\n\n    async def async_text_to_image(\n        self, stream: bool, input: str, size: str, quality: str, n: int, **kwargs\n    ):\n        if stream:\n            raise Exception(\"Stream not supported for text to image\")\n        start_time = time.monotonic()\n        response = await asyncfy_with_semaphore(\n            lambda: self.client.images.generate(\n                model=self.model,\n                prompt=input,\n                size=size,\n                quality=quality,\n                n=1,\n                response_format=\"b64_json\",\n                **kwargs,\n            )\n        )()\n        time_cost = time.monotonic() - start_time\n        base64_image = response.data[0].b64_json\n        return [\n            (\n                base64_image,\n                {\n                    \"metadata\": {\n                        \"request_id\": \"\",\n                        \"input_tokens_count\": 0,\n                        \"generated_tokens_count\": 0,\n                        \"time_cost\": time_cost,\n                        \"first_token_time\": 0,\n                        \"speed\": 0,\n                    }\n                },\n            )\n        ]\n\n    def text_to_text(self, ins: str, **kwargs):\n        pass\n\n    async def async_stream_chat(\n        self,\n        tokenizer,\n        ins: str,\n        his: List[Dict[str, Any]] = [],\n        max_length: int = 4096,\n        top_p: float = 0.7,\n        temperature: float = 0.9,\n        **kwargs,\n    ):\n\n        model = self.model\n\n        if \"model\" in kwargs:\n            model = kwargs[\"model\"]\n\n        messages = [\n            {\"role\": message[\"role\"], \"content\": self.process_input(message[\"content\"])}\n            for message in his\n        ] + [{\"role\": \"user\", \"content\": self.process_input(ins)}]\n\n        stream = kwargs.get(\"stream\", False)\n\n        ## content = [\n        ##    \"voice\": \"alloy\",\"input\": \"Hello, World!\",response_format: \"mp3\"]\n        last_message = messages[-1][\"content\"]\n\n        if isinstance(last_message, dict) and \"voice\" in last_message:\n            voice = last_message[\"voice\"]\n            response_format = last_message.get(\"response_format\", \"mp3\")\n            chunk_size = last_message.get(\"chunk_size\", None)\n            input = last_message[\"input\"]\n            return await self.async_text_to_speech(\n                stream=stream,\n                ins=input,\n                voice=voice,\n                chunk_size=chunk_size,\n                response_format=response_format,\n            )\n\n        if isinstance(last_message, dict) and \"input\" in last_message:\n            input = last_message[\"input\"]\n            size = last_message.get(\"size\", \"1024x1024\")\n            quality = last_message.get(\"quality\", \"standard\")\n            n = last_message.get(\"n\", 1)\n            return await self.async_text_to_image(\n                stream=stream, input=input, size=size, quality=quality, n=n\n            )\n\n        if isinstance(last_message, dict) and \"audio\" in last_message:            \n            audio_data = last_message.get(\"audio\", \"\")            \n            tpe = last_message.get(\"type\",\"wav\")\n            if not audio_data.startswith(\"data:\"):\n                audio_data = f\"data:audio/${tpe};base64,\" + audio_data                \n            return await self.async_speech_to_text(audio=audio_data)\n        \n        server = ray.get_actor(\"BLOCK_VLLM_STREAM_SERVER\")\n        request_id = [None]\n\n        def writer():\n            try:\n                r = \"\"\n                response = self.client.chat.completions.create(\n                    messages=messages,\n                    model=model,\n                    stream=True,\n                    max_tokens=max_length,\n                    temperature=temperature,\n                    top_p=top_p,\n                )\n                # input_tokens_count = 0\n                # generated_tokens_count = 0\n\n                request_id[0] = str(uuid.uuid4())\n\n                for chunk in response:\n                    content = chunk.choices[0].delta.content or \"\"\n                    r += content\n                    if hasattr(chunk, \"usage\") and chunk.usage:\n                        input_tokens_count = chunk.usage.prompt_tokens\n                        generated_tokens_count = chunk.usage.completion_tokens\n                    else:\n                        input_tokens_count = 0\n                        generated_tokens_count = 0\n                    ray.get(\n                        server.add_item.remote(\n                            request_id[0],\n                            StreamOutputs(\n                                outputs=[\n                                    SingleOutput(\n                                        text=r,\n                                        metadata=SingleOutputMeta(\n                                            input_tokens_count=input_tokens_count,\n                                            generated_tokens_count=generated_tokens_count,\n                                        ),\n                                    )\n                                ]\n                            ),\n                        )\n                    )\n            except:\n                traceback.print_exc()\n            ray.get(server.mark_done.remote(request_id[0]))\n\n        if stream:\n            threading.Thread(target=writer, daemon=True).start()\n\n            time_count = 60 * 100\n            while request_id[0] is None and time_count > 0:\n                time.sleep(0.01)\n                time_count -= 1\n\n            if request_id[0] is None:\n                raise Exception(\"Failed to get request id\")\n\n            def write_running():\n                return ray.get(server.add_item.remote(request_id[0], \"RUNNING\"))\n\n            await asyncio.to_thread(write_running)\n            return [\n                (\n                    \"\",\n                    {\n                        \"metadata\": {\n                            \"request_id\": request_id[0],\n                            \"stream_server\": \"BLOCK_VLLM_STREAM_SERVER\",\n                        }\n                    },\n                )\n            ]\n        else:\n            try:\n                start_time = time.monotonic()\n                response = await asyncfy_with_semaphore(\n                    lambda: self.client.chat.completions.create(\n                        messages=messages,\n                        model=model,\n                        max_tokens=max_length,\n                        temperature=temperature,\n                        top_p=top_p,\n                    )\n                )()\n\n                generated_text = response.choices[0].message.content\n                generated_tokens_count = response.usage.completion_tokens\n                input_tokens_count = response.usage.prompt_tokens\n                time_cost = time.monotonic() - start_time\n                return [\n                    (\n                        generated_text,\n                        {\n                            \"metadata\": {\n                                \"request_id\": response.id,\n                                \"input_tokens_count\": input_tokens_count,\n                                \"generated_tokens_count\": generated_tokens_count,\n                                \"time_cost\": time_cost,\n                                \"first_token_time\": 0,\n                                \"speed\": float(generated_tokens_count) / time_cost,\n                            }\n                        },\n                    )\n                ]\n            except Exception as e:\n                print(f\"Error: {e}\")\n                raise e\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/whisper/__init__.py\nimport whisper\nfrom typing import Dict, List, Tuple\nfrom byzerllm.utils.types import StopSequencesCriteria\n\nfrom typing import Dict, Any, List, Union\nfrom pyjava.storage import streaming_tar as STar\nimport json\nimport base64\nimport tempfile\nimport time\n\n\ndef get_meta(self):\n    return [\n        {\n            \"model_deploy_type\": \"proprietary\",\n            \"backend\": \"transformers\",\n            \"message_format\": True\n        }\n    ]\n\n\ndef process_input(ins: Union[str, List[Dict[str, Any]], Dict[str, Any]]):\n    # print(ins) \n    if isinstance(ins, list) or isinstance(ins, dict):\n        return ins\n\n    content = []\n    try:\n        ins_json = json.loads(ins)\n    except:\n        return ins\n\n    ## speech\n    if isinstance(ins_json, dict):\n        return ins_json\n\n    content = []\n    #     [\n    #     {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n    #     {\n    #       \"type\": \"image_url\",\n    #       \"image_url\": {\n    #         \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n    #         \"detail\": \"high\"\n    #       },\n    #     },\n    #   ],\n    for item in ins_json:\n        # for format like this: {\"image\": \"xxxxxx\", \"text\": \"What’s in this image?\",\"detail\":\"high\"}\n        # or {\"image\": \"xxxxxx\"}, {\"text\": \"What’s in this image?\"}\n        if (\"image\" in item or \"image_url\" in item) and \"type\" not in item:\n            image_data = item.get(\"image\", item.get(\"image_url\", \"\"))\n            ## \"data:image/jpeg;base64,\"\n            if not image_data.startswith(\"data:\"):\n                image_data = \"data:image/jpeg;base64,\" + image_data\n\n            ## get the other fields except image/text/image_url\n            other_fields = {\n                k: v for k, v in item.items() if k not in [\"image\", \"text\", \"image_url\"]\n            }\n            content.append(\n                {\n                    \"image_url\": {\"url\": image_data, **other_fields},\n                    \"type\": \"image_url\",\n                }\n            )\n\n        if \"text\" in item and \"type\" not in item:\n            text_data = item[\"text\"]\n            content.append({\"text\": text_data, \"type\": \"text\"})\n\n        ## for format like this: {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n        ## {\"type\": \"image_url\", \"image_url\": {\"url\":\"\",\"detail\":\"high\"}}\n        ## this is the standard format, just return it\n        if \"type\" in item and item[\"type\"] == \"text\":\n            content.append(item)\n\n        if \"type\" in item and item[\"type\"] == \"image_url\":\n            content.append(item)\n\n    if not content:\n        return ins\n\n    return content\n\n\ndef stream_chat(\n    self,\n    tokenizer,\n    ins: str,\n    his: List[Dict[str, str]] = [],\n    max_length: int = 4090,\n    top_p: float = 0.95,\n    temperature: float = 0.1,\n    **kwargs,\n):\n    messages = [\n        {\"role\": message[\"role\"], \"content\": process_input(message[\"content\"])}\n        for message in his\n    ] + [{\"role\": \"user\", \"content\": process_input(ins)}]\n\n    audio_input = messages[-1][\"content\"]\n\n    audio_data = audio_input.get(\"audio\", \"\")\n    audio = audio_input[\"audio\"]\n    response_format = audio_input.get(\"response_format\", \"json\")\n    timestamp_granularities = audio_input.get(\n        \"timestamp_granularities\", [\"word\", \"segment\"]\n    )\n    data_prefix = \"data:audio/\"\n    base64_prefix = \";base64,\"\n    if not audio.startswith(data_prefix):\n        raise ValueError(\"Invalid audio data format\")\n\n    format_end = audio.index(base64_prefix)\n    audio_format = audio[len(data_prefix) : format_end]\n    base64_data = audio[format_end + len(base64_prefix) :]\n\n    # Decode the base64 audio data\n    audio_data = base64.b64decode(base64_data)\n\n    # Create a temporary file with the correct extension\n    with tempfile.NamedTemporaryFile(\n        delete=False, suffix=f\".{audio_format}\"\n    ) as temp_audio_file:\n        temp_audio_file.write(audio_data)\n        temp_audio_file_path = temp_audio_file.name\n\n    try:\n        start_time = time.monotonic()\n        word_timestamps = \"word\" in timestamp_granularities\n        \n        result = self.transcribe(temp_audio_file_path, word_timestamps=word_timestamps)        \n            \n        time_cost = time.monotonic() - start_time\n        return [\n            (\n                json.dumps(result, ensure_ascii=True),\n                {\n                    \"metadata\": {\n                        \"request_id\": \"\",\n                        \"input_tokens_count\": 0,\n                        \"generated_tokens_count\": 0,\n                        \"time_cost\": time_cost,\n                        \"first_token_time\": 0,\n                        \"speed\": 0,\n                    }\n                },\n            )\n        ]\n    finally:\n        # Clean up the temporary file\n        import os\n\n        os.unlink(temp_audio_file_path)\n\n\ndef init_model(\n    model_dir, infer_params: Dict[str, str] = {}, sys_conf: Dict[str, str] = {}\n):\n    model = whisper.load_model(\"large\", download_root=model_dir)\n    import types\n\n    model.stream_chat = types.MethodType(stream_chat, model)\n    model.get_meta = types.MethodType(get_meta, model)\n    return (model, None)\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/sensevoice/__init__.py\nimport json\nimport base64\nimport tempfile\nimport time\nfrom typing import Dict, List, Union, Any\nfrom funasr import AutoModel\n\nemo_dict = {\n    \"<|HAPPY|>\": \"😊\",\n    \"<|SAD|>\": \"😔\",\n    \"<|ANGRY|>\": \"😡\",\n    \"<|NEUTRAL|>\": \"\",\n    \"<|FEARFUL|>\": \"😰\",\n    \"<|DISGUSTED|>\": \"🤢\",\n    \"<|SURPRISED|>\": \"😮\",\n}\n\nevent_dict = {\n    \"<|BGM|>\": \"🎼\",\n    \"<|Speech|>\": \"\",\n    \"<|Applause|>\": \"👏\",\n    \"<|Laughter|>\": \"��\",\n    \"<|Cry|>\": \"😭\",\n    \"<|Sneeze|>\": \"🤧\",\n    \"<|Breath|>\": \"\",\n    \"<|Cough|>\": \"🤧\",\n}\n\nemoji_dict = {\n    \"<|nospeech|><|Event_UNK|>\": \"❓\",\n    \"<|zh|>\": \"\",\n    \"<|en|>\": \"\",\n    \"<|yue|>\": \"\",\n    \"<|ja|>\": \"\",\n    \"<|ko|>\": \"\",\n    \"<|nospeech|>\": \"\",\n    \"<|HAPPY|>\": \"😊\",\n    \"<|SAD|>\": \"😔\",\n    \"<|ANGRY|>\": \"😡\",\n    \"<|NEUTRAL|>\": \"\",\n    \"<|BGM|>\": \"🎼\",\n    \"<|Speech|>\": \"\",\n    \"<|Applause|>\": \"👏\",\n    \"<|Laughter|>\": \"😀\",\n    \"<|FEARFUL|>\": \"😰\",\n    \"<|DISGUSTED|>\": \"🤢\",\n    \"<|SURPRISED|>\": \"😮\",\n    \"<|Cry|>\": \"😭\",\n    \"<|EMO_UNKNOWN|>\": \"\",\n    \"<|Sneeze|>\": \"🤧\",\n    \"<|Breath|>\": \"\",\n    \"<|Cough|>\": \"😷\",\n    \"<|Sing|>\": \"\",\n    \"<|Speech_Noise|>\": \"\",\n    \"<|withitn|>\": \"\",\n    \"<|woitn|>\": \"\",\n    \"<|GBG|>\": \"\",\n    \"<|Event_UNK|>\": \"\",\n}\n\nlang_dict = {\n    \"<|zh|>\": \"<|lang|>\",\n    \"<|en|>\": \"<|lang|>\",\n    \"<|yue|>\": \"<|lang|>\",\n    \"<|ja|>\": \"<|lang|>\",\n    \"<|ko|>\": \"<|lang|>\",\n    \"<|nospeech|>\": \"<|lang|>\",\n}\n\nemo_set = {\"😊\", \"😔\", \"😡\", \"😰\", \"🤢\", \"😮\"}\nevent_set = {\"🎼\", \"👏\", \"😀\", \"😭\", \"🤧\", \"😷\"}\n\n\ndef format_str(s):\n    for sptk in emoji_dict:\n        s = s.replace(sptk, emoji_dict[sptk])\n    return s\n\n\ndef format_str_v2(s):\n    sptk_dict = {sptk: s.count(sptk) for sptk in emoji_dict}\n    s = \"\".join(c for c in s if c not in emoji_dict.values())\n    emo = max(emo_dict, key=lambda e: sptk_dict[e])\n    event = \"\".join(event_dict[e] for e in event_dict if sptk_dict[e] > 0)\n    s = event + s + emo_dict[emo]\n    for emoji in emo_set.union(event_set):\n        s = s.replace(\" \" + emoji, emoji).replace(emoji + \" \", emoji)\n    return s.strip()\n\n\ndef format_str_v3(s):\n    def get_emo(s):\n        return s[-1] if s[-1] in emo_set else None\n\n    def get_event(s):\n        return s[0] if s[0] in event_set else None\n\n    s = s.replace(\"<|nospeech|><|Event_UNK|>\", \"❓\")\n    for lang in lang_dict:\n        s = s.replace(lang, \"<|lang|>\")\n    s_list = [format_str_v2(s_i).strip() for s_i in s.split(\"<|lang|>\")]\n    new_s = \" \" + s_list[0]\n    cur_ent_event = get_event(new_s)\n    for i in range(1, len(s_list)):\n        if len(s_list[i]) == 0:\n            continue\n        if get_event(s_list[i]) == cur_ent_event and get_event(s_list[i]) is not None:\n            s_list[i] = s_list[i][1:]\n        cur_ent_event = get_event(s_list[i])\n        if get_emo(s_list[i]) is not None and get_emo(s_list[i]) == get_emo(new_s):\n            new_s = new_s[:-1]\n        new_s += s_list[i].strip()\n    new_s = new_s.replace(\"The.\", \" \")\n    return new_s.strip()\n\n\ndef get_meta(self):\n    return [\n        {\n            \"model_deploy_type\": \"proprietary\",\n            \"backend\": \"funasr\",\n            \"message_format\": True,\n        }\n    ]\n\n\ndef process_input(ins: Union[str, List[Dict[str, Any]], Dict[str, Any]]):\n    if isinstance(ins, list) or isinstance(ins, dict):\n        return ins\n\n    try:\n        ins_json = json.loads(ins)\n    except:\n        return ins\n\n    if isinstance(ins_json, dict):\n        return ins_json\n\n    content = []\n    for item in ins_json:\n        if \"audio\" in item and \"type\" not in item:\n            audio_data = item.get(\"audio\", \"\")\n            if not audio_data.startswith(\"data:\"):\n                audio_data = f\"data:audio/wav;base64,{audio_data}\"\n            content.append({\"audio\": audio_data, \"type\": \"audio\"})\n\n        if \"text\" in item and \"type\" not in item:\n            content.append({\"text\": item[\"text\"], \"type\": \"text\"})\n\n        if \"type\" in item:\n            content.append(item)\n\n    return content if content else ins\n\n\ndef stream_chat(\n    self,\n    tokenizer,\n    ins: str,\n    his: List[Dict[str, str]] = [],\n    max_length: int = 4090,\n    top_p: float = 0.95,\n    temperature: float = 0.1,\n    **kwargs,\n):\n    messages = [\n        {\"role\": message[\"role\"], \"content\": process_input(message[\"content\"])}\n        for message in his\n    ] + [{\"role\": \"user\", \"content\": process_input(ins)}]\n\n    audio_input = messages[-1][\"content\"]\n\n    audio = audio_input[\"audio\"]\n    data_prefix = \"data:audio/\"\n    base64_prefix = \";base64,\"\n    if not audio.startswith(data_prefix):\n        raise ValueError(\"Invalid audio data format\")\n\n    format_end = audio.index(base64_prefix)\n    audio_format = audio[len(data_prefix) : format_end]\n    base64_data = audio[format_end + len(base64_prefix) :]\n\n    audio_data = base64.b64decode(base64_data)\n\n    with tempfile.NamedTemporaryFile(\n        delete=False, suffix=f\".{audio_format}\"\n    ) as temp_audio_file:\n        temp_audio_file.write(audio_data)\n        temp_audio_file_path = temp_audio_file.name\n\n    try:\n        start_time = time.monotonic()\n\n        result = self.generate(input=temp_audio_file_path, cache={})\n\n        time_cost = time.monotonic() - start_time\n\n        formatted_result = format_str_v3(result[0][\"text\"])\n\n        return [\n            (\n                json.dumps({\"text\": formatted_result}, ensure_ascii=False),\n                {\n                    \"metadata\": {\n                        \"request_id\": \"\",\n                        \"input_tokens_count\": 0,\n                        \"generated_tokens_count\": 0,\n                        \"time_cost\": time_cost,\n                        \"first_token_time\": 0,\n                        \"speed\": 0,\n                    }\n                },\n            )\n        ]\n    finally:\n        import os\n\n        os.unlink(temp_audio_file_path)\n\n\ndef init_model(\n    model_dir, infer_params: Dict[str, str] = {}, sys_conf: Dict[str, str] = {}\n):\n    model_kwargs = {\n        \"model\": model_dir,\n        \"trust_remote_code\": True,\n    }\n\n    if \"vad_model\" in infer_params:\n        model_kwargs[\"vad_model\"] = infer_params[\"vad_model\"]\n        model_kwargs[\"vad_kwargs\"] = {\"max_single_segment_time\": 30000}\n\n    model = AutoModel(**model_kwargs)\n\n    import types\n\n    model.stream_chat = types.MethodType(stream_chat, model)\n    model.get_meta = types.MethodType(get_meta, model)\n\n    return (model, None)\n\n\n# Helper functions from web_ui.py\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/notebooks/test_sensevoice.py\nimport sys\nimport os\n\n# Add the project root to the Python path\nsys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n\nfrom src.byzerllm.sensevoice import init_model, format_str_v3\nimport base64\nimport json\n\ndef main():\n    # Initialize the model\n    model_dir = \"path/to/your/model/directory\"  # Replace with actual path\n    model, _ = init_model(model_dir)\n\n    # Prepare test audio\n    with open(\"path/to/test/audio.wav\", \"rb\") as audio_file:  # Replace with actual path\n        audio_data = audio_file.read()\n        base64_audio = base64.b64encode(audio_data).decode('utf-8')\n\n    # Prepare input in the expected format\n    test_input = json.dumps([{\n        \"role\": \"user\",\n        \"content\": [{\n            \"type\": \"audio\",\n            \"audio\": f\"data:audio/wav;base64,{base64_audio}\"\n        }]\n    }])\n\n    # Call stream_chat\n    result = model.stream_chat(None, test_input)\n\n    # Process and print the result\n    if result and len(result) > 0:\n        output, metadata = result[0]\n        parsed_output = json.loads(output)\n        formatted_text = parsed_output.get('text', '')\n        print(\"Transcription:\", formatted_text)\n        print(\"Metadata:\", metadata)\n    else:\n        print(\"No result returned from the model.\")\n\nif __name__ == \"__main__\":\n    main()\n\n     \n\n下面是用户的需求：\n\n@test_sensevoice.py 中测试数据改成 /Users/allwefantasy/models/SenseVoiceSmall/example/zh.mp3 模型路径改成 /Users/allwefantasy/models/SenseVoiceSmall\n"}, {"role": "assistant", "content": "为了满足用户的需求，我们需要修改 `test_sensevoice.py` 文件中的测试音频路径和模型路径。以下是相应的 `SEARCH/REPLACE` 块：\n\n```python\n##File: /Users/allwefantasy/projects/byzer-llm/notebooks/test_sensevoice.py\n<<<<<<< SEARCH\n    model_dir = \"path/to/your/model/directory\"  # Replace with actual path\n    with open(\"path/to/test/audio.wav\", \"rb\") as audio_file:  # Replace with actual path\n=======\n    model_dir = \"/Users/allwefantasy/models/SenseVoiceSmall\"\n    with open(\"/Users/allwefantasy/models/SenseVoiceSmall/example/zh.mp3\", \"rb\") as audio_file:\n>>>>>>> REPLACE\n```\n\n这个 `SEARCH/REPLACE` 块将会替换 `test_sensevoice.py` 文件中的模型路径和测试音频路径，以满足用户的需求。"}], "model": "deepseek_chat", "yaml_file": "actions/028_chat_action.yml"}
{"instruction": "@test_sensevoice.py 中测试数据改成 /Users/allwefantasy/models/SenseVoiceSmall/example/zh.mp3 模型路径改成 /Users/allwefantasy/models/SenseVoiceSmall", "conversations": [{"role": "user", "content": "\n如果你需要生成代码，对于每个需要更改的文件,你需要按 *SEARCH/REPLACE block* 的格式进行生成。\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The opening fence and code language, eg: ```python\n2. The file path alone on a line, starting with \"##File:\" and verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n*SEARCH/REPLACE* blocks will replace *all* matching occurrences.\nInclude enough lines to make the SEARCH blocks unique.\n\nInclude *ALL* the code being searched and replaced!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n下面我们来看一个例子：\n\n当前项目目录结构：\n1. 项目根目录： /tmp/projects/mathweb\n2. 项目子目录/文件列表(类似tree 命令输出)\nflask/\n    app.py\n    templates/\n        index.html\n    static/\n        style.css\n\n用户需求： Change get_factorial() to use math.factorial\n\n回答： To make this change we need to modify `/tmp/projects/mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n用户需求： Refactor hello() into its own file.\n\n回答：To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/hello.py\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/main.py\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n现在让我们开始一个新的任务:\n\n当前项目目录结构：\n1. 项目根目录： /Users/allwefantasy/projects/byzer-llm\n2. 项目子目录/文件列表(类似tree 命令输出)：\nsetup.py\ntests/\n    test_emb_rerank.py\n    test_deepseek_code.py\n    test_prompt.py\n    test_model_meta.py\n    __init__.py\n    test_transformers_deploy.py\n    test_data_analysis.py\n    test_yi.py\n    test_deploy.py\n    functions_suite.py\n    test_template.py\n    auto/\n        test_backend_llama_cpp.py\n        __init__.py\nsetup-machine/\n    convert_safetensor.py\nnotebooks/\n    test.py\n    test_sensevoice.py\nsrc/\n    byzerllm/\n        lang.py\n        store.py\n        version.py\n        log.py\n        __init__.py\n        types.py\n        easy_byzerllm_command.py\n        byzerllm_command.py\n        command_args.py\n        bark/\n            generation.py\n            __init__.py\n            model.py\n            api.py\n            model_fine.py\n            bark_voice.py\n        moss/\n            finetune_moss.py\n            __init__.py\n            moss_inference.py\n            models/\n                configuration_moss.py\n                quantization.py\n                custom_autotune.py\n                __init__.py\n                modeling_moss.py\n                tokenization_moss.py\n        llama2/\n            __init__.py\n        records/\n            __init__.py\n        visualglm/\n            __init__.py\n        auto/\n            __init__.py\n            backend_llama_cpp.py\n            backend_vllm.py\n            backend_ds.py\n        alpha_moss/\n            __init__.py\n        processor/\n            __init__.py\n        sensevoice/\n            __init__.py\n        qwen/\n            __init__.py\n        utils/\n            json_repaire.py\n            object_store_ref_util.py\n            __init__.py\n            types.py\n            tokenizer.py\n            openai_utils.py\n            ray_utils.py\n            text_generator.py\n            emb.py\n            connect_ray.py\n            testing.py\n            langutil.py\n            nontext.py\n            fulltune/\n                deepspeed_trainner.py\n                __init__.py\n                launch.py\n                trainner.py\n                base_model/\n                    __init__.py\n                    configuration_baichuan.py\n                    modeling_baichuan.py\n                pretrain/\n                    __init__.py\n                    convert_to_transformers.py\n            metrics/\n                __init__.py\n            config/\n                __init__.py\n            retrieval/\n                rest.py\n                __init__.py\n                udf.py\n            sft/\n                argument.py\n                collator.py\n                qlora.py\n                __init__.py\n                model.py\n                dataset.py\n                loss.py\n                merge_lora.py\n                trainer.py\n            client/\n                message_utils.py\n                math_utils.py\n                byzerllm_client.py\n                parallel_utils.py\n                __init__.py\n                types.py\n                code_utils.py\n                img_utils.py\n                entrypoints/\n                    __init__.py\n                    openai/\n                        protocol.py\n                        __init__.py\n                        serve.py\n                        serving_chat.py\n                        api_server.py\n                        serving_engine.py\n                        serving_completion.py\n                        tool.py\n        chatglm2/\n            __init__.py\n        baichuan/\n            __init__.py\n        m3e/\n            __init__.py\n        stable_diffusion/\n            config.py\n            __init__.py\n            logger.py\n            shared.py\n            model.py\n            utils.py\n            images.py\n            diffusion/\n                __init__.py\n                embeddings.py\n                utils.py\n                networks/\n                    lyco.py\n                    __init__.py\n                upscalers/\n                    __init__.py\n                    multidiffusion.py\n                    samplers.py\n                piplines/\n                    __init__.py\n                    lpw.py\n                    diffusers.py\n            lib/\n                __init__.py\n                diffusers/\n                    __init__.py\n                    scheduler.py\n            api/\n                __init__.py\n                plugin.py\n                models/\n                    diffusion.py\n                    __init__.py\n                diffusion/\n                    __init__.py\n                    pipelines/\n                        __init__.py\n                        diffusers.py\n                events/\n                    generation.py\n                    __init__.py\n        bge/\n            __init__.py\n        bge_rerank/\n            __init__.py\n        apps/\n            client.py\n            qa_strategy.py\n            __init__.py\n            builder.py\n            vector_db.py\n            utils.py\n            qa.py\n            byzer_storage/\n                memory_model_based.py\n                command.py\n                env.py\n                train_pt.py\n                train_sft.py\n                __init__.py\n                train_dpo.py\n                simple_api.py\n                generate_sft_data.py\n            byzer_sql/\n                __init__.py\n            llama_index/\n                byzerai_vectordb.py\n                byzerai.py\n                byzerai_embedding.py\n                __init__.py\n                collection_manager.py\n                byzerai_index_store.py\n                byzerai_docstore.py\n                simple_retrieval.py\n                byzerai_kvstore.py\n            agent/\n                conversable_agent.py\n                registry.py\n                groupchat.py\n                user_proxy_agent.py\n                __init__.py\n                agent.py\n                extensions/\n                    simple_retrieval_client.py\n                    planner.py\n                    python_codesandbox_agent.py\n                    preview_file_agent.py\n                    llama_index_retrieval_agent.py\n                    visualization_agent.py\n                    assistant_agent.py\n                    load_data_agent.py\n                    retrieval_agent.py\n                    output_agent.py\n                    __init__.py\n                    byzer_engine_agent.py\n                    subquestion_agent.py\n                    sql_reviewer_agent.py\n                    spark_sql_agent.py\n                    rhetorical_agent.py\n                    data_analysis.py\n                    data_analysis_pipeline_agent.py\n                    common_agent.py\n                    query_rewrite/\n                        time.py\n                        rhetorical.py\n                        __init__.py\n                        condition.py\n                        context.py\n                store/\n                    memory_store.py\n                    __init__.py\n                    stores.py\n        saas/\n            __init__.py\n            chatglm/\n                __init__.py\n            sparkdesk/\n                __init__.py\n            gemini/\n                __init__.py\n            azure_openai/\n                __init__.py\n            azure/\n                __init__.py\n            qianfan/\n                __init__.py\n            claude/\n                __init__.py\n            qianwen/\n                __init__.py\n            volcengine/\n                __init__.py\n            minimax/\n                __init__.py\n            baichuan/\n                __init__.py\n            qianwen_vl/\n                __init__.py\n            official_openai/\n                __init__.py\n            zhipu/\n                __init__.py\n            aws_bedrock/\n                __init__.py\n            openai/\n                __init__.py\n        falcon/\n            __init__.py\n        chatglm6b/\n            trainer_seq2seq.py\n            arguments.py\n            __init__.py\n            finetune.py\n            tunning/\n                __init__.py\n                infer.py\n                finetune.py\n                utils/\n                    config.py\n                    __init__.py\n                    other.py\n                    common.py\n                    seq2seq.py\n        starcode/\n            __init__.py\n        qwen_vl_chat/\n            __init__.py\n        zephyr/\n            __init__.py\n        dolly/\n            generate.py\n            __init__.py\n            consts.py\n            dolly_inference.py\n            trainer.py\n        llama/\n            __init__.py\n        whisper/\n            whisper_inference.py\n            __init__.py\n下面是一些文件路径以及每个文件对应的源码：\n\n##File: /Users/allwefantasy/projects/SenseVoice/webui.py\n# coding=utf-8\n\nimport os\nimport librosa\nimport base64\nimport io\nimport gradio as gr\nimport re\n\nimport numpy as np\nimport torch\nimport torchaudio\n\n\nfrom funasr import AutoModel\n\nmodel = \"iic/SenseVoiceSmall\"\nmodel_kwargs = {\n    \"model\": model,\n    \"trust_remote_code\": True,\n}\n\n# Check if vad_model is provided in infer_params\nif \"vad_model\" in infer_params:\n    model_kwargs[\"vad_model\"] = infer_params[\"vad_model\"]\n    model_kwargs[\"vad_kwargs\"] = {\"max_single_segment_time\": 30000}\n\nmodel = AutoModel(**model_kwargs)\n\nimport re\n\nemo_dict = {\n\t\"<|HAPPY|>\": \"😊\",\n\t\"<|SAD|>\": \"😔\",\n\t\"<|ANGRY|>\": \"😡\",\n\t\"<|NEUTRAL|>\": \"\",\n\t\"<|FEARFUL|>\": \"😰\",\n\t\"<|DISGUSTED|>\": \"🤢\",\n\t\"<|SURPRISED|>\": \"😮\",\n}\n\nevent_dict = {\n\t\"<|BGM|>\": \"🎼\",\n\t\"<|Speech|>\": \"\",\n\t\"<|Applause|>\": \"👏\",\n\t\"<|Laughter|>\": \"😀\",\n\t\"<|Cry|>\": \"😭\",\n\t\"<|Sneeze|>\": \"🤧\",\n\t\"<|Breath|>\": \"\",\n\t\"<|Cough|>\": \"🤧\",\n}\n\nemoji_dict = {\n\t\"<|nospeech|><|Event_UNK|>\": \"❓\",\n\t\"<|zh|>\": \"\",\n\t\"<|en|>\": \"\",\n\t\"<|yue|>\": \"\",\n\t\"<|ja|>\": \"\",\n\t\"<|ko|>\": \"\",\n\t\"<|nospeech|>\": \"\",\n\t\"<|HAPPY|>\": \"😊\",\n\t\"<|SAD|>\": \"😔\",\n\t\"<|ANGRY|>\": \"😡\",\n\t\"<|NEUTRAL|>\": \"\",\n\t\"<|BGM|>\": \"🎼\",\n\t\"<|Speech|>\": \"\",\n\t\"<|Applause|>\": \"👏\",\n\t\"<|Laughter|>\": \"😀\",\n\t\"<|FEARFUL|>\": \"😰\",\n\t\"<|DISGUSTED|>\": \"🤢\",\n\t\"<|SURPRISED|>\": \"😮\",\n\t\"<|Cry|>\": \"😭\",\n\t\"<|EMO_UNKNOWN|>\": \"\",\n\t\"<|Sneeze|>\": \"🤧\",\n\t\"<|Breath|>\": \"\",\n\t\"<|Cough|>\": \"😷\",\n\t\"<|Sing|>\": \"\",\n\t\"<|Speech_Noise|>\": \"\",\n\t\"<|withitn|>\": \"\",\n\t\"<|woitn|>\": \"\",\n\t\"<|GBG|>\": \"\",\n\t\"<|Event_UNK|>\": \"\",\n}\n\nlang_dict =  {\n    \"<|zh|>\": \"<|lang|>\",\n    \"<|en|>\": \"<|lang|>\",\n    \"<|yue|>\": \"<|lang|>\",\n    \"<|ja|>\": \"<|lang|>\",\n    \"<|ko|>\": \"<|lang|>\",\n    \"<|nospeech|>\": \"<|lang|>\",\n}\n\nemo_set = {\"😊\", \"😔\", \"😡\", \"😰\", \"🤢\", \"😮\"}\nevent_set = {\"🎼\", \"👏\", \"😀\", \"😭\", \"🤧\", \"😷\",}\n\ndef format_str(s):\n\tfor sptk in emoji_dict:\n\t\ts = s.replace(sptk, emoji_dict[sptk])\n\treturn s\n\n\ndef format_str_v2(s):\n\tsptk_dict = {}\n\tfor sptk in emoji_dict:\n\t\tsptk_dict[sptk] = s.count(sptk)\n\t\ts = s.replace(sptk, \"\")\n\temo = \"<|NEUTRAL|>\"\n\tfor e in emo_dict:\n\t\tif sptk_dict[e] > sptk_dict[emo]:\n\t\t\temo = e\n\tfor e in event_dict:\n\t\tif sptk_dict[e] > 0:\n\t\t\ts = event_dict[e] + s\n\ts = s + emo_dict[emo]\n\n\tfor emoji in emo_set.union(event_set):\n\t\ts = s.replace(\" \" + emoji, emoji)\n\t\ts = s.replace(emoji + \" \", emoji)\n\treturn s.strip()\n\ndef format_str_v3(s):\n\tdef get_emo(s):\n\t\treturn s[-1] if s[-1] in emo_set else None\n\tdef get_event(s):\n\t\treturn s[0] if s[0] in event_set else None\n\n\ts = s.replace(\"<|nospeech|><|Event_UNK|>\", \"❓\")\n\tfor lang in lang_dict:\n\t\ts = s.replace(lang, \"<|lang|>\")\n\ts_list = [format_str_v2(s_i).strip(\" \") for s_i in s.split(\"<|lang|>\")]\n\tnew_s = \" \" + s_list[0]\n\tcur_ent_event = get_event(new_s)\n\tfor i in range(1, len(s_list)):\n\t\tif len(s_list[i]) == 0:\n\t\t\tcontinue\n\t\tif get_event(s_list[i]) == cur_ent_event and get_event(s_list[i]) != None:\n\t\t\ts_list[i] = s_list[i][1:]\n\t\t#else:\n\t\tcur_ent_event = get_event(s_list[i])\n\t\tif get_emo(s_list[i]) != None and get_emo(s_list[i]) == get_emo(new_s):\n\t\t\tnew_s = new_s[:-1]\n\t\tnew_s += s_list[i].strip().lstrip()\n\tnew_s = new_s.replace(\"The.\", \" \")\n\treturn new_s.strip()\n\ndef model_inference(input_wav, language, fs=16000):\n\t# task_abbr = {\"Speech Recognition\": \"ASR\", \"Rich Text Transcription\": (\"ASR\", \"AED\", \"SER\")}\n\tlanguage_abbr = {\"auto\": \"auto\", \"zh\": \"zh\", \"en\": \"en\", \"yue\": \"yue\", \"ja\": \"ja\", \"ko\": \"ko\",\n\t\t\t\t\t \"nospeech\": \"nospeech\"}\n\t\n\t# task = \"Speech Recognition\" if task is None else task\n\tlanguage = \"auto\" if len(language) < 1 else language\n\tselected_language = language_abbr[language]\n\t# selected_task = task_abbr.get(task)\n\t\n\t# print(f\"input_wav: {type(input_wav)}, {input_wav[1].shape}, {input_wav}\")\n\t\n\tif isinstance(input_wav, tuple):\n\t\tfs, input_wav = input_wav\n\t\tinput_wav = input_wav.astype(np.float32) / np.iinfo(np.int16).max\n\t\tif len(input_wav.shape) > 1:\n\t\t\tinput_wav = input_wav.mean(-1)\n\t\tif fs != 16000:\n\t\t\tprint(f\"audio_fs: {fs}\")\n\t\t\tresampler = torchaudio.transforms.Resample(fs, 16000)\n\t\t\tinput_wav_t = torch.from_numpy(input_wav).to(torch.float32)\n\t\t\tinput_wav = resampler(input_wav_t[None, :])[0, :].numpy()\n\t\n\t\n\tmerge_vad = True #False if selected_task == \"ASR\" else True\n\tprint(f\"language: {language}, merge_vad: {merge_vad}\")\n\ttext = model.generate(input=input_wav,\n\t\t\t\t\t\t  cache={},\n\t\t\t\t\t\t  language=language,\n\t\t\t\t\t\t  use_itn=True,\n\t\t\t\t\t\t  batch_size_s=60, merge_vad=merge_vad)\n\t\n\tprint(text)\n\ttext = text[0][\"text\"]\n\ttext = format_str_v3(text)\n\t\n\tprint(text)\n\t\n\treturn text\n\n\naudio_examples = [\n    [\"example/zh.mp3\", \"zh\"],\n    [\"example/yue.mp3\", \"yue\"],\n    [\"example/en.mp3\", \"en\"],\n    [\"example/ja.mp3\", \"ja\"],\n    [\"example/ko.mp3\", \"ko\"],\n    [\"example/emo_1.wav\", \"auto\"],\n    [\"example/emo_2.wav\", \"auto\"],\n    [\"example/emo_3.wav\", \"auto\"],\n    #[\"example/emo_4.wav\", \"auto\"],\n    #[\"example/event_1.wav\", \"auto\"],\n    #[\"example/event_2.wav\", \"auto\"],\n    #[\"example/event_3.wav\", \"auto\"],\n    [\"example/rich_1.wav\", \"auto\"],\n    [\"example/rich_2.wav\", \"auto\"],\n    #[\"example/rich_3.wav\", \"auto\"],\n    [\"example/longwav_1.wav\", \"auto\"],\n    [\"example/longwav_2.wav\", \"auto\"],\n    [\"example/longwav_3.wav\", \"auto\"],\n    #[\"example/longwav_4.wav\", \"auto\"],\n]\n\n\n\nhtml_content = \"\"\"\n<div>\n    <h2 style=\"font-size: 22px;margin-left: 0px;\">Voice Understanding Model: SenseVoice-Small</h2>\n    <p style=\"font-size: 18px;margin-left: 20px;\">SenseVoice-Small is an encoder-only speech foundation model designed for rapid voice understanding. It encompasses a variety of features including automatic speech recognition (ASR), spoken language identification (LID), speech emotion recognition (SER), and acoustic event detection (AED). SenseVoice-Small supports multilingual recognition for Chinese, English, Cantonese, Japanese, and Korean. Additionally, it offers exceptionally low inference latency, performing 7 times faster than Whisper-small and 17 times faster than Whisper-large.</p>\n    <h2 style=\"font-size: 22px;margin-left: 0px;\">Usage</h2> <p style=\"font-size: 18px;margin-left: 20px;\">Upload an audio file or input through a microphone, then select the task and language. the audio is transcribed into corresponding text along with associated emotions (😊 happy, 😡 angry/exicting, 😔 sad) and types of sound events (😀 laughter, 🎼 music, 👏 applause, 🤧 cough&sneeze, 😭 cry). The event labels are placed in the front of the text and the emotion are in the back of the text.</p>\n\t<p style=\"font-size: 18px;margin-left: 20px;\">Recommended audio input duration is below 30 seconds. For audio longer than 30 seconds, local deployment is recommended.</p>\n\t<h2 style=\"font-size: 22px;margin-left: 0px;\">Repo</h2>\n\t<p style=\"font-size: 18px;margin-left: 20px;\"><a href=\"https://github.com/FunAudioLLM/SenseVoice\" target=\"_blank\">SenseVoice</a>: multilingual speech understanding model</p>\n\t<p style=\"font-size: 18px;margin-left: 20px;\"><a href=\"https://github.com/modelscope/FunASR\" target=\"_blank\">FunASR</a>: fundamental speech recognition toolkit</p>\n\t<p style=\"font-size: 18px;margin-left: 20px;\"><a href=\"https://github.com/FunAudioLLM/CosyVoice\" target=\"_blank\">CosyVoice</a>: high-quality multilingual TTS model</p>\n</div>\n\"\"\"\n\n\ndef launch():\n\twith gr.Blocks(theme=gr.themes.Soft()) as demo:\n\t\t# gr.Markdown(description)\n\t\tgr.HTML(html_content)\n\t\twith gr.Row():\n\t\t\twith gr.Column():\n\t\t\t\taudio_inputs = gr.Audio(label=\"Upload audio or use the microphone\")\n\t\t\t\t\n\t\t\t\twith gr.Accordion(\"Configuration\"):\n\t\t\t\t\tlanguage_inputs = gr.Dropdown(choices=[\"auto\", \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"],\n\t\t\t\t\t\t\t\t\t\t\t\t  value=\"auto\",\n\t\t\t\t\t\t\t\t\t\t\t\t  label=\"Language\")\n\t\t\t\tfn_button = gr.Button(\"Start\", variant=\"primary\")\n\t\t\t\ttext_outputs = gr.Textbox(label=\"Results\")\n\t\t\tgr.Examples(examples=audio_examples, inputs=[audio_inputs, language_inputs], examples_per_page=20)\n\t\t\n\t\tfn_button.click(model_inference, inputs=[audio_inputs, language_inputs], outputs=text_outputs)\n\n\tdemo.launch()\n\n\nif __name__ == \"__main__\":\n\t# iface.launch()\n\tlaunch()\n\n\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/saas/official_openai/__init__.py\nimport time\nfrom typing import List, Tuple, Dict, Any, Union\nimport httpx\nfrom openai import OpenAI\nimport base64\nimport io\nimport json\nimport ray\nfrom byzerllm.utils.types import (\n    BlockVLLMStreamServer,\n    StreamOutputs,\n    SingleOutput,\n    SingleOutputMeta,\n    BlockBinaryStreamServer,\n)\nfrom byzerllm.utils.langutil import asyncfy_with_semaphore\nimport threading\nimport asyncio\nimport traceback\nimport uuid\nimport tempfile        \n\nclass CustomSaasAPI:\n\n    def __init__(self, infer_params: Dict[str, str]) -> None:\n\n        self.api_key = infer_params[\"saas.api_key\"]\n        self.model = infer_params.get(\"saas.model\", \"gpt-3.5-turbo-1106\")\n\n        other_params = {}\n\n        if \"saas.api_base\" in infer_params:\n            other_params[\"api_base\"] = infer_params[\"saas.api_base\"]\n\n        if \"saas.api_version\" in infer_params:\n            other_params[\"api_version\"] = infer_params[\"saas.api_version\"]\n\n        if \"saas.api_type\" in infer_params:\n            other_params[\"api_type\"] = infer_params[\"saas.api_type\"]\n\n        if \"saas.base_url\" in infer_params:\n            other_params[\"base_url\"] = infer_params[\"saas.base_url\"]\n\n        if \"saas.timeout\" in infer_params:\n            other_params[\"timeout\"] = float(infer_params[\"saas.timeout\"])\n\n        self.max_retries = int(infer_params.get(\"saas.max_retries\", 10))\n\n        self.meta = {\n            \"model_deploy_type\": \"saas\",\n            \"backend\": \"saas\",\n            \"support_stream\": True,\n            \"model_name\": self.model,\n        }\n\n        self.meta[\"embedding_mode\"] = \"embedding\" in self.model.lower()\n\n        self.proxies = infer_params.get(\"saas.proxies\", None)\n        self.local_address = infer_params.get(\"saas.local_address\", \"0.0.0.0\")\n\n        if not self.proxies:\n            self.client = OpenAI(**other_params, api_key=self.api_key)\n        else:\n            self.client = OpenAI(\n                **other_params,\n                api_key=self.api_key,\n                http_client=httpx.Client(\n                    proxies=self.proxies,\n                    transport=httpx.HTTPTransport(local_address=self.local_address),\n                ),\n            )\n\n        try:\n            ray.get_actor(\"BLOCK_VLLM_STREAM_SERVER\")\n        except ValueError:\n            try:\n                ray.remote(BlockVLLMStreamServer).options(\n                    name=\"BLOCK_VLLM_STREAM_SERVER\",\n                    lifetime=\"detached\",\n                    max_concurrency=1000,\n                ).remote()\n            except Exception as e:\n                pass\n        try:\n            ray.get_actor(\"BlockBinaryStreamServer\")\n        except ValueError:\n            try:\n                ray.remote(BlockBinaryStreamServer).options(\n                    name=\"BlockBinaryStreamServer\",\n                    lifetime=\"detached\",\n                    max_concurrency=1000,\n                ).remote()\n            except Exception as e:\n                pass\n\n    # saas/proprietary\n    def get_meta(self):\n        return [self.meta]\n\n    def process_input(self, ins: Union[str, List[Dict[str, Any]], Dict[str, Any]]):\n\n        if isinstance(ins, list) or isinstance(ins, dict):\n            return ins\n\n        content = []\n        try:\n            ins_json = json.loads(ins)\n        except:\n            return ins\n\n        ## speech\n        if isinstance(ins_json, dict):\n            return ins_json\n\n        content = []\n        #     [\n        #     {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n        #     {\n        #       \"type\": \"image_url\",\n        #       \"image_url\": {\n        #         \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n        #         \"detail\": \"high\"\n        #       },\n        #     },\n        #   ],\n        for item in ins_json:\n            # for format like this: {\"image\": \"xxxxxx\", \"text\": \"What’s in this image?\",\"detail\":\"high\"}\n            # or {\"image\": \"xxxxxx\"}, {\"text\": \"What’s in this image?\"}\n            if (\"image\" in item or \"image_url\" in item) and \"type\" not in item:\n                image_data = item.get(\"image\", item.get(\"image_url\", \"\"))\n                ## \"data:image/jpeg;base64,\"\n                if not image_data.startswith(\"data:\"):\n                    image_data = \"data:image/jpeg;base64,\" + image_data\n\n                ## get the other fields except image/text/image_url\n                other_fields = {\n                    k: v\n                    for k, v in item.items()\n                    if k not in [\"image\", \"text\", \"image_url\"]\n                }\n                content.append(\n                    {\n                        \"image_url\": {\"url\": image_data, **other_fields},\n                        \"type\": \"image_url\",\n                    }\n                )            \n\n            if \"text\" in item and \"type\" not in item:\n                text_data = item[\"text\"]\n                content.append({\"text\": text_data, \"type\": \"text\"})\n\n            ## for format like this: {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n            ## {\"type\": \"image_url\", \"image_url\": {\"url\":\"\",\"detail\":\"high\"}}\n            ## this is the standard format, just return it\n            if \"type\" in item and item[\"type\"] == \"text\":\n                content.append(item)\n\n            if \"type\" in item and item[\"type\"] == \"image_url\":\n                content.append(item)\n\n        if not content:\n            return ins\n\n        return content\n\n    async def async_embed_query(self, ins: str, **kwargs):\n        resp = await asyncfy_with_semaphore(\n            lambda: self.client.embeddings.create(input=[ins], model=self.model)\n        )()\n        embedding = resp.data[0].embedding\n        usage = resp.usage\n        return (\n            embedding,\n            {\n                \"metadata\": {\n                    \"input_tokens_count\": usage.prompt_tokens,\n                    \"generated_tokens_count\": 0,\n                }\n            },\n        )\n\n    def embed_query(self, ins: str, **kwargs):\n        resp = self.client.embeddings.create(input=[ins], model=self.model)\n        embedding = resp.data[0].embedding\n        usage = resp.usage\n        return (\n            embedding,\n            {\n                \"metadata\": {\n                    \"input_tokens_count\": usage.prompt_tokens,\n                    \"generated_tokens_count\": 0,\n                }\n            },\n        )\n\n    async def async_text_to_speech(\n        self, stream: bool, ins: str, voice: str, chunk_size: int = None, **kwargs\n    ):\n        if stream:\n            server = ray.get_actor(\"BlockBinaryStreamServer\")\n            request_id = [None]\n\n            def writer():\n                try:\n                    request_id[0] = str(uuid.uuid4())\n                    with self.client.with_streaming_response.audio.speech.create(\n                        model=self.model, voice=voice, input=ins, **kwargs\n                    ) as response:\n                        for chunk in response.iter_bytes(chunk_size):\n                            input_tokens_count = 0\n                            generated_tokens_count = 0\n                            ray.get(\n                                server.add_item.remote(\n                                    request_id[0],\n                                    StreamOutputs(\n                                        outputs=[\n                                            SingleOutput(\n                                                text=chunk,\n                                                metadata=SingleOutputMeta(\n                                                    input_tokens_count=input_tokens_count,\n                                                    generated_tokens_count=generated_tokens_count,\n                                                ),\n                                            )\n                                        ]\n                                    ),\n                                )\n                            )\n                except:\n                    traceback.print_exc()\n                ray.get(server.mark_done.remote(request_id[0]))\n\n            threading.Thread(target=writer, daemon=True).start()\n\n            time_count = 10 * 100\n            while request_id[0] is None and time_count > 0:\n                time.sleep(0.01)\n                time_count -= 1\n\n            if request_id[0] is None:\n                raise Exception(\"Failed to get request id\")\n\n            def write_running():\n                return ray.get(server.add_item.remote(request_id[0], \"RUNNING\"))\n\n            await asyncio.to_thread(write_running)\n            return [\n                (\n                    \"\",\n                    {\n                        \"metadata\": {\n                            \"request_id\": request_id[0],\n                            \"stream_server\": \"BlockBinaryStreamServer\",\n                        }\n                    },\n                )\n            ]\n\n        start_time = time.monotonic()\n        with io.BytesIO() as output:\n            async with asyncfy_with_semaphore(\n                lambda: self.client.with_streaming_response.audio.speech.create(\n                    model=self.model, voice=voice, input=ins, **kwargs\n                )\n            )() as response:\n                for chunk in response.iter_bytes():\n                    output.write(chunk)\n\n            base64_audio = base64.b64encode(output.getvalue()).decode()\n            time_cost = time.monotonic() - start_time\n            return [\n                (\n                    base64_audio,\n                    {\n                        \"metadata\": {\n                            \"request_id\": \"\",\n                            \"input_tokens_count\": 0,\n                            \"generated_tokens_count\": 0,\n                            \"time_cost\": time_cost,\n                            \"first_token_time\": 0,\n                            \"speed\": 0,\n                        }\n                    },\n                )\n            ]\n\n    async def async_speech_to_text(\n        self,\n        audio: str,\n        response_format: str = \"verbose_json\",\n        timestamp_granularities: List[str] = [\"word\",\"segment\"],\n    ):               \n        # Extract audio format and base64 data\n        data_prefix = \"data:audio/\"\n        base64_prefix = \";base64,\"\n        if not audio.startswith(data_prefix):\n            raise ValueError(\"Invalid audio data format\")\n        \n        format_end = audio.index(base64_prefix)\n        audio_format = audio[len(data_prefix):format_end]\n        base64_data = audio[format_end + len(base64_prefix):]\n        \n        # Decode the base64 audio data\n        audio_data = base64.b64decode(base64_data)\n\n        # Create a temporary file with the correct extension\n        with tempfile.NamedTemporaryFile(delete=False, suffix=f'.{audio_format}') as temp_audio_file:\n            temp_audio_file.write(audio_data)\n            temp_audio_file_path = temp_audio_file.name\n\n        try:\n            start_time = time.monotonic()\n                   \n            with open(temp_audio_file_path, \"rb\") as audio_file:                \n                transcription = self.client.audio.transcriptions.create(\n                    model=self.model or \"whisper-1\",\n                    file=audio_file,\n                    response_format=response_format,\n                    timestamp_granularities=timestamp_granularities,\n                )\n            time_cost = time.monotonic() - start_time\n\n            return [\n                (\n                    json.dumps(transcription.to_dict(), ensure_ascii=True),\n                    {\n                        \"metadata\": {\n                            \"request_id\": \"\",\n                            \"input_tokens_count\": 0,\n                            \"generated_tokens_count\": 0,\n                            \"time_cost\": time_cost,\n                            \"first_token_time\": 0,\n                            \"speed\": 0,\n                        }\n                    },\n                )\n            ]\n        finally:\n            # Clean up the temporary file\n            import os\n            os.unlink(temp_audio_file_path)\n\n    def image_to_text(self, ins: str, **kwargs):\n        pass\n\n    async def async_text_to_image(\n        self, stream: bool, input: str, size: str, quality: str, n: int, **kwargs\n    ):\n        if stream:\n            raise Exception(\"Stream not supported for text to image\")\n        start_time = time.monotonic()\n        response = await asyncfy_with_semaphore(\n            lambda: self.client.images.generate(\n                model=self.model,\n                prompt=input,\n                size=size,\n                quality=quality,\n                n=1,\n                response_format=\"b64_json\",\n                **kwargs,\n            )\n        )()\n        time_cost = time.monotonic() - start_time\n        base64_image = response.data[0].b64_json\n        return [\n            (\n                base64_image,\n                {\n                    \"metadata\": {\n                        \"request_id\": \"\",\n                        \"input_tokens_count\": 0,\n                        \"generated_tokens_count\": 0,\n                        \"time_cost\": time_cost,\n                        \"first_token_time\": 0,\n                        \"speed\": 0,\n                    }\n                },\n            )\n        ]\n\n    def text_to_text(self, ins: str, **kwargs):\n        pass\n\n    async def async_stream_chat(\n        self,\n        tokenizer,\n        ins: str,\n        his: List[Dict[str, Any]] = [],\n        max_length: int = 4096,\n        top_p: float = 0.7,\n        temperature: float = 0.9,\n        **kwargs,\n    ):\n\n        model = self.model\n\n        if \"model\" in kwargs:\n            model = kwargs[\"model\"]\n\n        messages = [\n            {\"role\": message[\"role\"], \"content\": self.process_input(message[\"content\"])}\n            for message in his\n        ] + [{\"role\": \"user\", \"content\": self.process_input(ins)}]\n\n        stream = kwargs.get(\"stream\", False)\n\n        ## content = [\n        ##    \"voice\": \"alloy\",\"input\": \"Hello, World!\",response_format: \"mp3\"]\n        last_message = messages[-1][\"content\"]\n\n        if isinstance(last_message, dict) and \"voice\" in last_message:\n            voice = last_message[\"voice\"]\n            response_format = last_message.get(\"response_format\", \"mp3\")\n            chunk_size = last_message.get(\"chunk_size\", None)\n            input = last_message[\"input\"]\n            return await self.async_text_to_speech(\n                stream=stream,\n                ins=input,\n                voice=voice,\n                chunk_size=chunk_size,\n                response_format=response_format,\n            )\n\n        if isinstance(last_message, dict) and \"input\" in last_message:\n            input = last_message[\"input\"]\n            size = last_message.get(\"size\", \"1024x1024\")\n            quality = last_message.get(\"quality\", \"standard\")\n            n = last_message.get(\"n\", 1)\n            return await self.async_text_to_image(\n                stream=stream, input=input, size=size, quality=quality, n=n\n            )\n\n        if isinstance(last_message, dict) and \"audio\" in last_message:            \n            audio_data = last_message.get(\"audio\", \"\")            \n            tpe = last_message.get(\"type\",\"wav\")\n            if not audio_data.startswith(\"data:\"):\n                audio_data = f\"data:audio/${tpe};base64,\" + audio_data                \n            return await self.async_speech_to_text(audio=audio_data)\n        \n        server = ray.get_actor(\"BLOCK_VLLM_STREAM_SERVER\")\n        request_id = [None]\n\n        def writer():\n            try:\n                r = \"\"\n                response = self.client.chat.completions.create(\n                    messages=messages,\n                    model=model,\n                    stream=True,\n                    max_tokens=max_length,\n                    temperature=temperature,\n                    top_p=top_p,\n                )\n                # input_tokens_count = 0\n                # generated_tokens_count = 0\n\n                request_id[0] = str(uuid.uuid4())\n\n                for chunk in response:\n                    content = chunk.choices[0].delta.content or \"\"\n                    r += content\n                    if hasattr(chunk, \"usage\") and chunk.usage:\n                        input_tokens_count = chunk.usage.prompt_tokens\n                        generated_tokens_count = chunk.usage.completion_tokens\n                    else:\n                        input_tokens_count = 0\n                        generated_tokens_count = 0\n                    ray.get(\n                        server.add_item.remote(\n                            request_id[0],\n                            StreamOutputs(\n                                outputs=[\n                                    SingleOutput(\n                                        text=r,\n                                        metadata=SingleOutputMeta(\n                                            input_tokens_count=input_tokens_count,\n                                            generated_tokens_count=generated_tokens_count,\n                                        ),\n                                    )\n                                ]\n                            ),\n                        )\n                    )\n            except:\n                traceback.print_exc()\n            ray.get(server.mark_done.remote(request_id[0]))\n\n        if stream:\n            threading.Thread(target=writer, daemon=True).start()\n\n            time_count = 60 * 100\n            while request_id[0] is None and time_count > 0:\n                time.sleep(0.01)\n                time_count -= 1\n\n            if request_id[0] is None:\n                raise Exception(\"Failed to get request id\")\n\n            def write_running():\n                return ray.get(server.add_item.remote(request_id[0], \"RUNNING\"))\n\n            await asyncio.to_thread(write_running)\n            return [\n                (\n                    \"\",\n                    {\n                        \"metadata\": {\n                            \"request_id\": request_id[0],\n                            \"stream_server\": \"BLOCK_VLLM_STREAM_SERVER\",\n                        }\n                    },\n                )\n            ]\n        else:\n            try:\n                start_time = time.monotonic()\n                response = await asyncfy_with_semaphore(\n                    lambda: self.client.chat.completions.create(\n                        messages=messages,\n                        model=model,\n                        max_tokens=max_length,\n                        temperature=temperature,\n                        top_p=top_p,\n                    )\n                )()\n\n                generated_text = response.choices[0].message.content\n                generated_tokens_count = response.usage.completion_tokens\n                input_tokens_count = response.usage.prompt_tokens\n                time_cost = time.monotonic() - start_time\n                return [\n                    (\n                        generated_text,\n                        {\n                            \"metadata\": {\n                                \"request_id\": response.id,\n                                \"input_tokens_count\": input_tokens_count,\n                                \"generated_tokens_count\": generated_tokens_count,\n                                \"time_cost\": time_cost,\n                                \"first_token_time\": 0,\n                                \"speed\": float(generated_tokens_count) / time_cost,\n                            }\n                        },\n                    )\n                ]\n            except Exception as e:\n                print(f\"Error: {e}\")\n                raise e\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/whisper/__init__.py\nimport whisper\nfrom typing import Dict, List, Tuple\nfrom byzerllm.utils.types import StopSequencesCriteria\n\nfrom typing import Dict, Any, List, Union\nfrom pyjava.storage import streaming_tar as STar\nimport json\nimport base64\nimport tempfile\nimport time\n\n\ndef get_meta(self):\n    return [\n        {\n            \"model_deploy_type\": \"proprietary\",\n            \"backend\": \"transformers\",\n            \"message_format\": True\n        }\n    ]\n\n\ndef process_input(ins: Union[str, List[Dict[str, Any]], Dict[str, Any]]):\n    # print(ins) \n    if isinstance(ins, list) or isinstance(ins, dict):\n        return ins\n\n    content = []\n    try:\n        ins_json = json.loads(ins)\n    except:\n        return ins\n\n    ## speech\n    if isinstance(ins_json, dict):\n        return ins_json\n\n    content = []\n    #     [\n    #     {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n    #     {\n    #       \"type\": \"image_url\",\n    #       \"image_url\": {\n    #         \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n    #         \"detail\": \"high\"\n    #       },\n    #     },\n    #   ],\n    for item in ins_json:\n        # for format like this: {\"image\": \"xxxxxx\", \"text\": \"What’s in this image?\",\"detail\":\"high\"}\n        # or {\"image\": \"xxxxxx\"}, {\"text\": \"What’s in this image?\"}\n        if (\"image\" in item or \"image_url\" in item) and \"type\" not in item:\n            image_data = item.get(\"image\", item.get(\"image_url\", \"\"))\n            ## \"data:image/jpeg;base64,\"\n            if not image_data.startswith(\"data:\"):\n                image_data = \"data:image/jpeg;base64,\" + image_data\n\n            ## get the other fields except image/text/image_url\n            other_fields = {\n                k: v for k, v in item.items() if k not in [\"image\", \"text\", \"image_url\"]\n            }\n            content.append(\n                {\n                    \"image_url\": {\"url\": image_data, **other_fields},\n                    \"type\": \"image_url\",\n                }\n            )\n\n        if \"text\" in item and \"type\" not in item:\n            text_data = item[\"text\"]\n            content.append({\"text\": text_data, \"type\": \"text\"})\n\n        ## for format like this: {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n        ## {\"type\": \"image_url\", \"image_url\": {\"url\":\"\",\"detail\":\"high\"}}\n        ## this is the standard format, just return it\n        if \"type\" in item and item[\"type\"] == \"text\":\n            content.append(item)\n\n        if \"type\" in item and item[\"type\"] == \"image_url\":\n            content.append(item)\n\n    if not content:\n        return ins\n\n    return content\n\n\ndef stream_chat(\n    self,\n    tokenizer,\n    ins: str,\n    his: List[Dict[str, str]] = [],\n    max_length: int = 4090,\n    top_p: float = 0.95,\n    temperature: float = 0.1,\n    **kwargs,\n):\n    messages = [\n        {\"role\": message[\"role\"], \"content\": process_input(message[\"content\"])}\n        for message in his\n    ] + [{\"role\": \"user\", \"content\": process_input(ins)}]\n\n    audio_input = messages[-1][\"content\"]\n\n    audio_data = audio_input.get(\"audio\", \"\")\n    audio = audio_input[\"audio\"]\n    response_format = audio_input.get(\"response_format\", \"json\")\n    timestamp_granularities = audio_input.get(\n        \"timestamp_granularities\", [\"word\", \"segment\"]\n    )\n    data_prefix = \"data:audio/\"\n    base64_prefix = \";base64,\"\n    if not audio.startswith(data_prefix):\n        raise ValueError(\"Invalid audio data format\")\n\n    format_end = audio.index(base64_prefix)\n    audio_format = audio[len(data_prefix) : format_end]\n    base64_data = audio[format_end + len(base64_prefix) :]\n\n    # Decode the base64 audio data\n    audio_data = base64.b64decode(base64_data)\n\n    # Create a temporary file with the correct extension\n    with tempfile.NamedTemporaryFile(\n        delete=False, suffix=f\".{audio_format}\"\n    ) as temp_audio_file:\n        temp_audio_file.write(audio_data)\n        temp_audio_file_path = temp_audio_file.name\n\n    try:\n        start_time = time.monotonic()\n        word_timestamps = \"word\" in timestamp_granularities\n        \n        result = self.transcribe(temp_audio_file_path, word_timestamps=word_timestamps)        \n            \n        time_cost = time.monotonic() - start_time\n        return [\n            (\n                json.dumps(result, ensure_ascii=True),\n                {\n                    \"metadata\": {\n                        \"request_id\": \"\",\n                        \"input_tokens_count\": 0,\n                        \"generated_tokens_count\": 0,\n                        \"time_cost\": time_cost,\n                        \"first_token_time\": 0,\n                        \"speed\": 0,\n                    }\n                },\n            )\n        ]\n    finally:\n        # Clean up the temporary file\n        import os\n\n        os.unlink(temp_audio_file_path)\n\n\ndef init_model(\n    model_dir, infer_params: Dict[str, str] = {}, sys_conf: Dict[str, str] = {}\n):\n    model = whisper.load_model(\"large\", download_root=model_dir)\n    import types\n\n    model.stream_chat = types.MethodType(stream_chat, model)\n    model.get_meta = types.MethodType(get_meta, model)\n    return (model, None)\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/sensevoice/__init__.py\nimport json\nimport base64\nimport tempfile\nimport time\nfrom typing import Dict, List, Union, Any\nfrom funasr import AutoModel\n\nemo_dict = {\n    \"<|HAPPY|>\": \"😊\",\n    \"<|SAD|>\": \"😔\",\n    \"<|ANGRY|>\": \"😡\",\n    \"<|NEUTRAL|>\": \"\",\n    \"<|FEARFUL|>\": \"😰\",\n    \"<|DISGUSTED|>\": \"🤢\",\n    \"<|SURPRISED|>\": \"😮\",\n}\n\nevent_dict = {\n    \"<|BGM|>\": \"🎼\",\n    \"<|Speech|>\": \"\",\n    \"<|Applause|>\": \"👏\",\n    \"<|Laughter|>\": \"��\",\n    \"<|Cry|>\": \"😭\",\n    \"<|Sneeze|>\": \"🤧\",\n    \"<|Breath|>\": \"\",\n    \"<|Cough|>\": \"🤧\",\n}\n\nemoji_dict = {\n    \"<|nospeech|><|Event_UNK|>\": \"❓\",\n    \"<|zh|>\": \"\",\n    \"<|en|>\": \"\",\n    \"<|yue|>\": \"\",\n    \"<|ja|>\": \"\",\n    \"<|ko|>\": \"\",\n    \"<|nospeech|>\": \"\",\n    \"<|HAPPY|>\": \"😊\",\n    \"<|SAD|>\": \"😔\",\n    \"<|ANGRY|>\": \"😡\",\n    \"<|NEUTRAL|>\": \"\",\n    \"<|BGM|>\": \"🎼\",\n    \"<|Speech|>\": \"\",\n    \"<|Applause|>\": \"👏\",\n    \"<|Laughter|>\": \"😀\",\n    \"<|FEARFUL|>\": \"😰\",\n    \"<|DISGUSTED|>\": \"🤢\",\n    \"<|SURPRISED|>\": \"😮\",\n    \"<|Cry|>\": \"😭\",\n    \"<|EMO_UNKNOWN|>\": \"\",\n    \"<|Sneeze|>\": \"🤧\",\n    \"<|Breath|>\": \"\",\n    \"<|Cough|>\": \"😷\",\n    \"<|Sing|>\": \"\",\n    \"<|Speech_Noise|>\": \"\",\n    \"<|withitn|>\": \"\",\n    \"<|woitn|>\": \"\",\n    \"<|GBG|>\": \"\",\n    \"<|Event_UNK|>\": \"\",\n}\n\nlang_dict = {\n    \"<|zh|>\": \"<|lang|>\",\n    \"<|en|>\": \"<|lang|>\",\n    \"<|yue|>\": \"<|lang|>\",\n    \"<|ja|>\": \"<|lang|>\",\n    \"<|ko|>\": \"<|lang|>\",\n    \"<|nospeech|>\": \"<|lang|>\",\n}\n\nemo_set = {\"😊\", \"😔\", \"😡\", \"😰\", \"🤢\", \"😮\"}\nevent_set = {\"🎼\", \"👏\", \"😀\", \"😭\", \"🤧\", \"😷\"}\n\n\ndef format_str(s):\n    for sptk in emoji_dict:\n        s = s.replace(sptk, emoji_dict[sptk])\n    return s\n\n\ndef format_str_v2(s):\n    sptk_dict = {sptk: s.count(sptk) for sptk in emoji_dict}\n    s = \"\".join(c for c in s if c not in emoji_dict.values())\n    emo = max(emo_dict, key=lambda e: sptk_dict[e])\n    event = \"\".join(event_dict[e] for e in event_dict if sptk_dict[e] > 0)\n    s = event + s + emo_dict[emo]\n    for emoji in emo_set.union(event_set):\n        s = s.replace(\" \" + emoji, emoji).replace(emoji + \" \", emoji)\n    return s.strip()\n\n\ndef format_str_v3(s):\n    def get_emo(s):\n        return s[-1] if s[-1] in emo_set else None\n\n    def get_event(s):\n        return s[0] if s[0] in event_set else None\n\n    s = s.replace(\"<|nospeech|><|Event_UNK|>\", \"❓\")\n    for lang in lang_dict:\n        s = s.replace(lang, \"<|lang|>\")\n    s_list = [format_str_v2(s_i).strip() for s_i in s.split(\"<|lang|>\")]\n    new_s = \" \" + s_list[0]\n    cur_ent_event = get_event(new_s)\n    for i in range(1, len(s_list)):\n        if len(s_list[i]) == 0:\n            continue\n        if get_event(s_list[i]) == cur_ent_event and get_event(s_list[i]) is not None:\n            s_list[i] = s_list[i][1:]\n        cur_ent_event = get_event(s_list[i])\n        if get_emo(s_list[i]) is not None and get_emo(s_list[i]) == get_emo(new_s):\n            new_s = new_s[:-1]\n        new_s += s_list[i].strip()\n    new_s = new_s.replace(\"The.\", \" \")\n    return new_s.strip()\n\n\ndef get_meta(self):\n    return [\n        {\n            \"model_deploy_type\": \"proprietary\",\n            \"backend\": \"funasr\",\n            \"message_format\": True,\n        }\n    ]\n\n\ndef process_input(ins: Union[str, List[Dict[str, Any]], Dict[str, Any]]):\n    if isinstance(ins, list) or isinstance(ins, dict):\n        return ins\n\n    try:\n        ins_json = json.loads(ins)\n    except:\n        return ins\n\n    if isinstance(ins_json, dict):\n        return ins_json\n\n    content = []\n    for item in ins_json:\n        if \"audio\" in item and \"type\" not in item:\n            audio_data = item.get(\"audio\", \"\")\n            if not audio_data.startswith(\"data:\"):\n                audio_data = f\"data:audio/wav;base64,{audio_data}\"\n            content.append({\"audio\": audio_data, \"type\": \"audio\"})\n\n        if \"text\" in item and \"type\" not in item:\n            content.append({\"text\": item[\"text\"], \"type\": \"text\"})\n\n        if \"type\" in item:\n            content.append(item)\n\n    return content if content else ins\n\n\ndef stream_chat(\n    self,\n    tokenizer,\n    ins: str,\n    his: List[Dict[str, str]] = [],\n    max_length: int = 4090,\n    top_p: float = 0.95,\n    temperature: float = 0.1,\n    **kwargs,\n):\n    messages = [\n        {\"role\": message[\"role\"], \"content\": process_input(message[\"content\"])}\n        for message in his\n    ] + [{\"role\": \"user\", \"content\": process_input(ins)}]\n\n    audio_input = messages[-1][\"content\"]\n\n    audio = audio_input[\"audio\"]\n    data_prefix = \"data:audio/\"\n    base64_prefix = \";base64,\"\n    if not audio.startswith(data_prefix):\n        raise ValueError(\"Invalid audio data format\")\n\n    format_end = audio.index(base64_prefix)\n    audio_format = audio[len(data_prefix) : format_end]\n    base64_data = audio[format_end + len(base64_prefix) :]\n\n    audio_data = base64.b64decode(base64_data)\n\n    with tempfile.NamedTemporaryFile(\n        delete=False, suffix=f\".{audio_format}\"\n    ) as temp_audio_file:\n        temp_audio_file.write(audio_data)\n        temp_audio_file_path = temp_audio_file.name\n\n    try:\n        start_time = time.monotonic()\n\n        result = self.generate(input=temp_audio_file_path, cache={})\n\n        time_cost = time.monotonic() - start_time\n\n        formatted_result = format_str_v3(result[0][\"text\"])\n\n        return [\n            (\n                json.dumps({\"text\": formatted_result}, ensure_ascii=False),\n                {\n                    \"metadata\": {\n                        \"request_id\": \"\",\n                        \"input_tokens_count\": 0,\n                        \"generated_tokens_count\": 0,\n                        \"time_cost\": time_cost,\n                        \"first_token_time\": 0,\n                        \"speed\": 0,\n                    }\n                },\n            )\n        ]\n    finally:\n        import os\n\n        os.unlink(temp_audio_file_path)\n\n\ndef init_model(\n    model_dir, infer_params: Dict[str, str] = {}, sys_conf: Dict[str, str] = {}\n):\n    model_kwargs = {\n        \"model\": model_dir,\n        \"trust_remote_code\": True,\n    }\n\n    if \"vad_model\" in infer_params:\n        model_kwargs[\"vad_model\"] = infer_params[\"vad_model\"]\n        model_kwargs[\"vad_kwargs\"] = {\"max_single_segment_time\": 30000}\n\n    model = AutoModel(**model_kwargs)\n\n    import types\n\n    model.stream_chat = types.MethodType(stream_chat, model)\n    model.get_meta = types.MethodType(get_meta, model)\n\n    return (model, None)\n\n\n# Helper functions from web_ui.py\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/notebooks/test_sensevoice.py\nimport sys\nimport os\n\n# Add the project root to the Python path\nsys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n\nfrom src.byzerllm.sensevoice import init_model, format_str_v3\nimport base64\nimport json\n\ndef main():\n    # Initialize the model\n    model_dir = \"path/to/your/model/directory\"  # Replace with actual path\n    model, _ = init_model(model_dir)\n\n    # Prepare test audio\n    with open(\"path/to/test/audio.wav\", \"rb\") as audio_file:  # Replace with actual path\n        audio_data = audio_file.read()\n        base64_audio = base64.b64encode(audio_data).decode('utf-8')\n\n    # Prepare input in the expected format\n    test_input = json.dumps([{\n        \"role\": \"user\",\n        \"content\": [{\n            \"type\": \"audio\",\n            \"audio\": f\"data:audio/wav;base64,{base64_audio}\"\n        }]\n    }])\n\n    # Call stream_chat\n    result = model.stream_chat(None, test_input)\n\n    # Process and print the result\n    if result and len(result) > 0:\n        output, metadata = result[0]\n        parsed_output = json.loads(output)\n        formatted_text = parsed_output.get('text', '')\n        print(\"Transcription:\", formatted_text)\n        print(\"Metadata:\", metadata)\n    else:\n        print(\"No result returned from the model.\")\n\nif __name__ == \"__main__\":\n    main()\n\n     \n\n下面是用户的需求：\n\n@test_sensevoice.py 中测试数据改成 /Users/allwefantasy/models/SenseVoiceSmall/example/zh.mp3 模型路径改成 /Users/allwefantasy/models/SenseVoiceSmall\n"}, {"role": "assistant", "content": "```python\n##File: /Users/allwefantasy/projects/byzer-llm/notebooks/test_sensevoice.py\n<<<<<<< SEARCH\nwith open(\"path/to/test/audio.wav\", \"rb\") as audio_file:  # Replace with actual path\n    audio_data = audio_file.read()\n    base64_audio = base64.b64encode(audio_data).decode('utf-8')\nmodel_dir = \"path/to/your/model/directory\"  # Replace with actual path\n=======\nwith open(\"/Users/allwefantasy/models/SenseVoiceSmall/example/zh.mp3\", \"rb\") as audio_file:\n    audio_data = audio_file.read()\n    base64_audio = base64.b64encode(audio_data).decode('utf-8')\nmodel_dir = \"/Users/allwefantasy/models/SenseVoiceSmall\"\n>>>>>>> REPLACE\n```"}], "model": "deepseek_chat", "yaml_file": "actions/029_chat_action.yml"}
{"instruction": "@test_sensevoice.py 中测试数据改成 /Users/allwefantasy/models/SenseVoiceSmall/example/zh.mp3 模型路径改成 /Users/allwefantasy/models/SenseVoiceSmall", "conversations": [{"role": "user", "content": "\n如果你需要生成代码，对于每个需要更改的文件,你需要按 *SEARCH/REPLACE block* 的格式进行生成。\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The opening fence and code language, eg: ```python\n2. The file path alone on a line, starting with \"##File:\" and verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n*SEARCH/REPLACE* blocks will replace *all* matching occurrences.\nInclude enough lines to make the SEARCH blocks unique.\n\nInclude *ALL* the code being searched and replaced!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n下面我们来看一个例子：\n\n当前项目目录结构：\n1. 项目根目录： /tmp/projects/mathweb\n2. 项目子目录/文件列表(类似tree 命令输出)\nflask/\n    app.py\n    templates/\n        index.html\n    static/\n        style.css\n\n用户需求： Change get_factorial() to use math.factorial\n\n回答： To make this change we need to modify `/tmp/projects/mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n用户需求： Refactor hello() into its own file.\n\n回答：To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/hello.py\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/main.py\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n现在让我们开始一个新的任务:\n\n当前项目目录结构：\n1. 项目根目录： /Users/allwefantasy/projects/byzer-llm\n2. 项目子目录/文件列表(类似tree 命令输出)：\nsetup.py\ntests/\n    test_emb_rerank.py\n    test_deepseek_code.py\n    test_prompt.py\n    test_model_meta.py\n    __init__.py\n    test_transformers_deploy.py\n    test_data_analysis.py\n    test_yi.py\n    test_deploy.py\n    functions_suite.py\n    test_template.py\n    auto/\n        test_backend_llama_cpp.py\n        __init__.py\nsetup-machine/\n    convert_safetensor.py\nnotebooks/\n    test.py\n    test_sensevoice.py\nsrc/\n    byzerllm/\n        lang.py\n        store.py\n        version.py\n        log.py\n        __init__.py\n        types.py\n        easy_byzerllm_command.py\n        byzerllm_command.py\n        command_args.py\n        bark/\n            generation.py\n            __init__.py\n            model.py\n            api.py\n            model_fine.py\n            bark_voice.py\n        moss/\n            finetune_moss.py\n            __init__.py\n            moss_inference.py\n            models/\n                configuration_moss.py\n                quantization.py\n                custom_autotune.py\n                __init__.py\n                modeling_moss.py\n                tokenization_moss.py\n        llama2/\n            __init__.py\n        records/\n            __init__.py\n        visualglm/\n            __init__.py\n        auto/\n            __init__.py\n            backend_llama_cpp.py\n            backend_vllm.py\n            backend_ds.py\n        alpha_moss/\n            __init__.py\n        processor/\n            __init__.py\n        sensevoice/\n            __init__.py\n        qwen/\n            __init__.py\n        utils/\n            json_repaire.py\n            object_store_ref_util.py\n            __init__.py\n            types.py\n            tokenizer.py\n            openai_utils.py\n            ray_utils.py\n            text_generator.py\n            emb.py\n            connect_ray.py\n            testing.py\n            langutil.py\n            nontext.py\n            fulltune/\n                deepspeed_trainner.py\n                __init__.py\n                launch.py\n                trainner.py\n                base_model/\n                    __init__.py\n                    configuration_baichuan.py\n                    modeling_baichuan.py\n                pretrain/\n                    __init__.py\n                    convert_to_transformers.py\n            metrics/\n                __init__.py\n            config/\n                __init__.py\n            retrieval/\n                rest.py\n                __init__.py\n                udf.py\n            sft/\n                argument.py\n                collator.py\n                qlora.py\n                __init__.py\n                model.py\n                dataset.py\n                loss.py\n                merge_lora.py\n                trainer.py\n            client/\n                message_utils.py\n                math_utils.py\n                byzerllm_client.py\n                parallel_utils.py\n                __init__.py\n                types.py\n                code_utils.py\n                img_utils.py\n                entrypoints/\n                    __init__.py\n                    openai/\n                        protocol.py\n                        __init__.py\n                        serve.py\n                        serving_chat.py\n                        api_server.py\n                        serving_engine.py\n                        serving_completion.py\n                        tool.py\n        chatglm2/\n            __init__.py\n        baichuan/\n            __init__.py\n        m3e/\n            __init__.py\n        stable_diffusion/\n            config.py\n            __init__.py\n            logger.py\n            shared.py\n            model.py\n            utils.py\n            images.py\n            diffusion/\n                __init__.py\n                embeddings.py\n                utils.py\n                networks/\n                    lyco.py\n                    __init__.py\n                upscalers/\n                    __init__.py\n                    multidiffusion.py\n                    samplers.py\n                piplines/\n                    __init__.py\n                    lpw.py\n                    diffusers.py\n            lib/\n                __init__.py\n                diffusers/\n                    __init__.py\n                    scheduler.py\n            api/\n                __init__.py\n                plugin.py\n                models/\n                    diffusion.py\n                    __init__.py\n                diffusion/\n                    __init__.py\n                    pipelines/\n                        __init__.py\n                        diffusers.py\n                events/\n                    generation.py\n                    __init__.py\n        bge/\n            __init__.py\n        bge_rerank/\n            __init__.py\n        apps/\n            client.py\n            qa_strategy.py\n            __init__.py\n            builder.py\n            vector_db.py\n            utils.py\n            qa.py\n            byzer_storage/\n                memory_model_based.py\n                command.py\n                env.py\n                train_pt.py\n                train_sft.py\n                __init__.py\n                train_dpo.py\n                simple_api.py\n                generate_sft_data.py\n            byzer_sql/\n                __init__.py\n            llama_index/\n                byzerai_vectordb.py\n                byzerai.py\n                byzerai_embedding.py\n                __init__.py\n                collection_manager.py\n                byzerai_index_store.py\n                byzerai_docstore.py\n                simple_retrieval.py\n                byzerai_kvstore.py\n            agent/\n                conversable_agent.py\n                registry.py\n                groupchat.py\n                user_proxy_agent.py\n                __init__.py\n                agent.py\n                extensions/\n                    simple_retrieval_client.py\n                    planner.py\n                    python_codesandbox_agent.py\n                    preview_file_agent.py\n                    llama_index_retrieval_agent.py\n                    visualization_agent.py\n                    assistant_agent.py\n                    load_data_agent.py\n                    retrieval_agent.py\n                    output_agent.py\n                    __init__.py\n                    byzer_engine_agent.py\n                    subquestion_agent.py\n                    sql_reviewer_agent.py\n                    spark_sql_agent.py\n                    rhetorical_agent.py\n                    data_analysis.py\n                    data_analysis_pipeline_agent.py\n                    common_agent.py\n                    query_rewrite/\n                        time.py\n                        rhetorical.py\n                        __init__.py\n                        condition.py\n                        context.py\n                store/\n                    memory_store.py\n                    __init__.py\n                    stores.py\n        saas/\n            __init__.py\n            chatglm/\n                __init__.py\n            sparkdesk/\n                __init__.py\n            gemini/\n                __init__.py\n            azure_openai/\n                __init__.py\n            azure/\n                __init__.py\n            qianfan/\n                __init__.py\n            claude/\n                __init__.py\n            qianwen/\n                __init__.py\n            volcengine/\n                __init__.py\n            minimax/\n                __init__.py\n            baichuan/\n                __init__.py\n            qianwen_vl/\n                __init__.py\n            official_openai/\n                __init__.py\n            zhipu/\n                __init__.py\n            aws_bedrock/\n                __init__.py\n            openai/\n                __init__.py\n        falcon/\n            __init__.py\n        chatglm6b/\n            trainer_seq2seq.py\n            arguments.py\n            __init__.py\n            finetune.py\n            tunning/\n                __init__.py\n                infer.py\n                finetune.py\n                utils/\n                    config.py\n                    __init__.py\n                    other.py\n                    common.py\n                    seq2seq.py\n        starcode/\n            __init__.py\n        qwen_vl_chat/\n            __init__.py\n        zephyr/\n            __init__.py\n        dolly/\n            generate.py\n            __init__.py\n            consts.py\n            dolly_inference.py\n            trainer.py\n        llama/\n            __init__.py\n        whisper/\n            whisper_inference.py\n            __init__.py\n下面是一些文件路径以及每个文件对应的源码：\n\n##File: /Users/allwefantasy/projects/SenseVoice/webui.py\n# coding=utf-8\n\nimport os\nimport librosa\nimport base64\nimport io\nimport gradio as gr\nimport re\n\nimport numpy as np\nimport torch\nimport torchaudio\n\n\nfrom funasr import AutoModel\n\nmodel = \"iic/SenseVoiceSmall\"\nmodel_kwargs = {\n    \"model\": model,\n    \"trust_remote_code\": True,\n}\n\n# Check if vad_model is provided in infer_params\nif \"vad_model\" in infer_params:\n    model_kwargs[\"vad_model\"] = infer_params[\"vad_model\"]\n    model_kwargs[\"vad_kwargs\"] = {\"max_single_segment_time\": 30000}\n\nmodel = AutoModel(**model_kwargs)\n\nimport re\n\nemo_dict = {\n\t\"<|HAPPY|>\": \"😊\",\n\t\"<|SAD|>\": \"😔\",\n\t\"<|ANGRY|>\": \"😡\",\n\t\"<|NEUTRAL|>\": \"\",\n\t\"<|FEARFUL|>\": \"😰\",\n\t\"<|DISGUSTED|>\": \"🤢\",\n\t\"<|SURPRISED|>\": \"😮\",\n}\n\nevent_dict = {\n\t\"<|BGM|>\": \"🎼\",\n\t\"<|Speech|>\": \"\",\n\t\"<|Applause|>\": \"👏\",\n\t\"<|Laughter|>\": \"😀\",\n\t\"<|Cry|>\": \"😭\",\n\t\"<|Sneeze|>\": \"🤧\",\n\t\"<|Breath|>\": \"\",\n\t\"<|Cough|>\": \"🤧\",\n}\n\nemoji_dict = {\n\t\"<|nospeech|><|Event_UNK|>\": \"❓\",\n\t\"<|zh|>\": \"\",\n\t\"<|en|>\": \"\",\n\t\"<|yue|>\": \"\",\n\t\"<|ja|>\": \"\",\n\t\"<|ko|>\": \"\",\n\t\"<|nospeech|>\": \"\",\n\t\"<|HAPPY|>\": \"😊\",\n\t\"<|SAD|>\": \"😔\",\n\t\"<|ANGRY|>\": \"😡\",\n\t\"<|NEUTRAL|>\": \"\",\n\t\"<|BGM|>\": \"🎼\",\n\t\"<|Speech|>\": \"\",\n\t\"<|Applause|>\": \"👏\",\n\t\"<|Laughter|>\": \"😀\",\n\t\"<|FEARFUL|>\": \"😰\",\n\t\"<|DISGUSTED|>\": \"🤢\",\n\t\"<|SURPRISED|>\": \"😮\",\n\t\"<|Cry|>\": \"😭\",\n\t\"<|EMO_UNKNOWN|>\": \"\",\n\t\"<|Sneeze|>\": \"🤧\",\n\t\"<|Breath|>\": \"\",\n\t\"<|Cough|>\": \"😷\",\n\t\"<|Sing|>\": \"\",\n\t\"<|Speech_Noise|>\": \"\",\n\t\"<|withitn|>\": \"\",\n\t\"<|woitn|>\": \"\",\n\t\"<|GBG|>\": \"\",\n\t\"<|Event_UNK|>\": \"\",\n}\n\nlang_dict =  {\n    \"<|zh|>\": \"<|lang|>\",\n    \"<|en|>\": \"<|lang|>\",\n    \"<|yue|>\": \"<|lang|>\",\n    \"<|ja|>\": \"<|lang|>\",\n    \"<|ko|>\": \"<|lang|>\",\n    \"<|nospeech|>\": \"<|lang|>\",\n}\n\nemo_set = {\"😊\", \"😔\", \"😡\", \"😰\", \"🤢\", \"😮\"}\nevent_set = {\"🎼\", \"👏\", \"😀\", \"😭\", \"🤧\", \"😷\",}\n\ndef format_str(s):\n\tfor sptk in emoji_dict:\n\t\ts = s.replace(sptk, emoji_dict[sptk])\n\treturn s\n\n\ndef format_str_v2(s):\n\tsptk_dict = {}\n\tfor sptk in emoji_dict:\n\t\tsptk_dict[sptk] = s.count(sptk)\n\t\ts = s.replace(sptk, \"\")\n\temo = \"<|NEUTRAL|>\"\n\tfor e in emo_dict:\n\t\tif sptk_dict[e] > sptk_dict[emo]:\n\t\t\temo = e\n\tfor e in event_dict:\n\t\tif sptk_dict[e] > 0:\n\t\t\ts = event_dict[e] + s\n\ts = s + emo_dict[emo]\n\n\tfor emoji in emo_set.union(event_set):\n\t\ts = s.replace(\" \" + emoji, emoji)\n\t\ts = s.replace(emoji + \" \", emoji)\n\treturn s.strip()\n\ndef format_str_v3(s):\n\tdef get_emo(s):\n\t\treturn s[-1] if s[-1] in emo_set else None\n\tdef get_event(s):\n\t\treturn s[0] if s[0] in event_set else None\n\n\ts = s.replace(\"<|nospeech|><|Event_UNK|>\", \"❓\")\n\tfor lang in lang_dict:\n\t\ts = s.replace(lang, \"<|lang|>\")\n\ts_list = [format_str_v2(s_i).strip(\" \") for s_i in s.split(\"<|lang|>\")]\n\tnew_s = \" \" + s_list[0]\n\tcur_ent_event = get_event(new_s)\n\tfor i in range(1, len(s_list)):\n\t\tif len(s_list[i]) == 0:\n\t\t\tcontinue\n\t\tif get_event(s_list[i]) == cur_ent_event and get_event(s_list[i]) != None:\n\t\t\ts_list[i] = s_list[i][1:]\n\t\t#else:\n\t\tcur_ent_event = get_event(s_list[i])\n\t\tif get_emo(s_list[i]) != None and get_emo(s_list[i]) == get_emo(new_s):\n\t\t\tnew_s = new_s[:-1]\n\t\tnew_s += s_list[i].strip().lstrip()\n\tnew_s = new_s.replace(\"The.\", \" \")\n\treturn new_s.strip()\n\ndef model_inference(input_wav, language, fs=16000):\n\t# task_abbr = {\"Speech Recognition\": \"ASR\", \"Rich Text Transcription\": (\"ASR\", \"AED\", \"SER\")}\n\tlanguage_abbr = {\"auto\": \"auto\", \"zh\": \"zh\", \"en\": \"en\", \"yue\": \"yue\", \"ja\": \"ja\", \"ko\": \"ko\",\n\t\t\t\t\t \"nospeech\": \"nospeech\"}\n\t\n\t# task = \"Speech Recognition\" if task is None else task\n\tlanguage = \"auto\" if len(language) < 1 else language\n\tselected_language = language_abbr[language]\n\t# selected_task = task_abbr.get(task)\n\t\n\t# print(f\"input_wav: {type(input_wav)}, {input_wav[1].shape}, {input_wav}\")\n\t\n\tif isinstance(input_wav, tuple):\n\t\tfs, input_wav = input_wav\n\t\tinput_wav = input_wav.astype(np.float32) / np.iinfo(np.int16).max\n\t\tif len(input_wav.shape) > 1:\n\t\t\tinput_wav = input_wav.mean(-1)\n\t\tif fs != 16000:\n\t\t\tprint(f\"audio_fs: {fs}\")\n\t\t\tresampler = torchaudio.transforms.Resample(fs, 16000)\n\t\t\tinput_wav_t = torch.from_numpy(input_wav).to(torch.float32)\n\t\t\tinput_wav = resampler(input_wav_t[None, :])[0, :].numpy()\n\t\n\t\n\tmerge_vad = True #False if selected_task == \"ASR\" else True\n\tprint(f\"language: {language}, merge_vad: {merge_vad}\")\n\ttext = model.generate(input=input_wav,\n\t\t\t\t\t\t  cache={},\n\t\t\t\t\t\t  language=language,\n\t\t\t\t\t\t  use_itn=True,\n\t\t\t\t\t\t  batch_size_s=60, merge_vad=merge_vad)\n\t\n\tprint(text)\n\ttext = text[0][\"text\"]\n\ttext = format_str_v3(text)\n\t\n\tprint(text)\n\t\n\treturn text\n\n\naudio_examples = [\n    [\"example/zh.mp3\", \"zh\"],\n    [\"example/yue.mp3\", \"yue\"],\n    [\"example/en.mp3\", \"en\"],\n    [\"example/ja.mp3\", \"ja\"],\n    [\"example/ko.mp3\", \"ko\"],\n    [\"example/emo_1.wav\", \"auto\"],\n    [\"example/emo_2.wav\", \"auto\"],\n    [\"example/emo_3.wav\", \"auto\"],\n    #[\"example/emo_4.wav\", \"auto\"],\n    #[\"example/event_1.wav\", \"auto\"],\n    #[\"example/event_2.wav\", \"auto\"],\n    #[\"example/event_3.wav\", \"auto\"],\n    [\"example/rich_1.wav\", \"auto\"],\n    [\"example/rich_2.wav\", \"auto\"],\n    #[\"example/rich_3.wav\", \"auto\"],\n    [\"example/longwav_1.wav\", \"auto\"],\n    [\"example/longwav_2.wav\", \"auto\"],\n    [\"example/longwav_3.wav\", \"auto\"],\n    #[\"example/longwav_4.wav\", \"auto\"],\n]\n\n\n\nhtml_content = \"\"\"\n<div>\n    <h2 style=\"font-size: 22px;margin-left: 0px;\">Voice Understanding Model: SenseVoice-Small</h2>\n    <p style=\"font-size: 18px;margin-left: 20px;\">SenseVoice-Small is an encoder-only speech foundation model designed for rapid voice understanding. It encompasses a variety of features including automatic speech recognition (ASR), spoken language identification (LID), speech emotion recognition (SER), and acoustic event detection (AED). SenseVoice-Small supports multilingual recognition for Chinese, English, Cantonese, Japanese, and Korean. Additionally, it offers exceptionally low inference latency, performing 7 times faster than Whisper-small and 17 times faster than Whisper-large.</p>\n    <h2 style=\"font-size: 22px;margin-left: 0px;\">Usage</h2> <p style=\"font-size: 18px;margin-left: 20px;\">Upload an audio file or input through a microphone, then select the task and language. the audio is transcribed into corresponding text along with associated emotions (😊 happy, 😡 angry/exicting, 😔 sad) and types of sound events (😀 laughter, 🎼 music, 👏 applause, 🤧 cough&sneeze, 😭 cry). The event labels are placed in the front of the text and the emotion are in the back of the text.</p>\n\t<p style=\"font-size: 18px;margin-left: 20px;\">Recommended audio input duration is below 30 seconds. For audio longer than 30 seconds, local deployment is recommended.</p>\n\t<h2 style=\"font-size: 22px;margin-left: 0px;\">Repo</h2>\n\t<p style=\"font-size: 18px;margin-left: 20px;\"><a href=\"https://github.com/FunAudioLLM/SenseVoice\" target=\"_blank\">SenseVoice</a>: multilingual speech understanding model</p>\n\t<p style=\"font-size: 18px;margin-left: 20px;\"><a href=\"https://github.com/modelscope/FunASR\" target=\"_blank\">FunASR</a>: fundamental speech recognition toolkit</p>\n\t<p style=\"font-size: 18px;margin-left: 20px;\"><a href=\"https://github.com/FunAudioLLM/CosyVoice\" target=\"_blank\">CosyVoice</a>: high-quality multilingual TTS model</p>\n</div>\n\"\"\"\n\n\ndef launch():\n\twith gr.Blocks(theme=gr.themes.Soft()) as demo:\n\t\t# gr.Markdown(description)\n\t\tgr.HTML(html_content)\n\t\twith gr.Row():\n\t\t\twith gr.Column():\n\t\t\t\taudio_inputs = gr.Audio(label=\"Upload audio or use the microphone\")\n\t\t\t\t\n\t\t\t\twith gr.Accordion(\"Configuration\"):\n\t\t\t\t\tlanguage_inputs = gr.Dropdown(choices=[\"auto\", \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"],\n\t\t\t\t\t\t\t\t\t\t\t\t  value=\"auto\",\n\t\t\t\t\t\t\t\t\t\t\t\t  label=\"Language\")\n\t\t\t\tfn_button = gr.Button(\"Start\", variant=\"primary\")\n\t\t\t\ttext_outputs = gr.Textbox(label=\"Results\")\n\t\t\tgr.Examples(examples=audio_examples, inputs=[audio_inputs, language_inputs], examples_per_page=20)\n\t\t\n\t\tfn_button.click(model_inference, inputs=[audio_inputs, language_inputs], outputs=text_outputs)\n\n\tdemo.launch()\n\n\nif __name__ == \"__main__\":\n\t# iface.launch()\n\tlaunch()\n\n\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/saas/official_openai/__init__.py\nimport time\nfrom typing import List, Tuple, Dict, Any, Union\nimport httpx\nfrom openai import OpenAI\nimport base64\nimport io\nimport json\nimport ray\nfrom byzerllm.utils.types import (\n    BlockVLLMStreamServer,\n    StreamOutputs,\n    SingleOutput,\n    SingleOutputMeta,\n    BlockBinaryStreamServer,\n)\nfrom byzerllm.utils.langutil import asyncfy_with_semaphore\nimport threading\nimport asyncio\nimport traceback\nimport uuid\nimport tempfile        \n\nclass CustomSaasAPI:\n\n    def __init__(self, infer_params: Dict[str, str]) -> None:\n\n        self.api_key = infer_params[\"saas.api_key\"]\n        self.model = infer_params.get(\"saas.model\", \"gpt-3.5-turbo-1106\")\n\n        other_params = {}\n\n        if \"saas.api_base\" in infer_params:\n            other_params[\"api_base\"] = infer_params[\"saas.api_base\"]\n\n        if \"saas.api_version\" in infer_params:\n            other_params[\"api_version\"] = infer_params[\"saas.api_version\"]\n\n        if \"saas.api_type\" in infer_params:\n            other_params[\"api_type\"] = infer_params[\"saas.api_type\"]\n\n        if \"saas.base_url\" in infer_params:\n            other_params[\"base_url\"] = infer_params[\"saas.base_url\"]\n\n        if \"saas.timeout\" in infer_params:\n            other_params[\"timeout\"] = float(infer_params[\"saas.timeout\"])\n\n        self.max_retries = int(infer_params.get(\"saas.max_retries\", 10))\n\n        self.meta = {\n            \"model_deploy_type\": \"saas\",\n            \"backend\": \"saas\",\n            \"support_stream\": True,\n            \"model_name\": self.model,\n        }\n\n        self.meta[\"embedding_mode\"] = \"embedding\" in self.model.lower()\n\n        self.proxies = infer_params.get(\"saas.proxies\", None)\n        self.local_address = infer_params.get(\"saas.local_address\", \"0.0.0.0\")\n\n        if not self.proxies:\n            self.client = OpenAI(**other_params, api_key=self.api_key)\n        else:\n            self.client = OpenAI(\n                **other_params,\n                api_key=self.api_key,\n                http_client=httpx.Client(\n                    proxies=self.proxies,\n                    transport=httpx.HTTPTransport(local_address=self.local_address),\n                ),\n            )\n\n        try:\n            ray.get_actor(\"BLOCK_VLLM_STREAM_SERVER\")\n        except ValueError:\n            try:\n                ray.remote(BlockVLLMStreamServer).options(\n                    name=\"BLOCK_VLLM_STREAM_SERVER\",\n                    lifetime=\"detached\",\n                    max_concurrency=1000,\n                ).remote()\n            except Exception as e:\n                pass\n        try:\n            ray.get_actor(\"BlockBinaryStreamServer\")\n        except ValueError:\n            try:\n                ray.remote(BlockBinaryStreamServer).options(\n                    name=\"BlockBinaryStreamServer\",\n                    lifetime=\"detached\",\n                    max_concurrency=1000,\n                ).remote()\n            except Exception as e:\n                pass\n\n    # saas/proprietary\n    def get_meta(self):\n        return [self.meta]\n\n    def process_input(self, ins: Union[str, List[Dict[str, Any]], Dict[str, Any]]):\n\n        if isinstance(ins, list) or isinstance(ins, dict):\n            return ins\n\n        content = []\n        try:\n            ins_json = json.loads(ins)\n        except:\n            return ins\n\n        ## speech\n        if isinstance(ins_json, dict):\n            return ins_json\n\n        content = []\n        #     [\n        #     {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n        #     {\n        #       \"type\": \"image_url\",\n        #       \"image_url\": {\n        #         \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n        #         \"detail\": \"high\"\n        #       },\n        #     },\n        #   ],\n        for item in ins_json:\n            # for format like this: {\"image\": \"xxxxxx\", \"text\": \"What’s in this image?\",\"detail\":\"high\"}\n            # or {\"image\": \"xxxxxx\"}, {\"text\": \"What’s in this image?\"}\n            if (\"image\" in item or \"image_url\" in item) and \"type\" not in item:\n                image_data = item.get(\"image\", item.get(\"image_url\", \"\"))\n                ## \"data:image/jpeg;base64,\"\n                if not image_data.startswith(\"data:\"):\n                    image_data = \"data:image/jpeg;base64,\" + image_data\n\n                ## get the other fields except image/text/image_url\n                other_fields = {\n                    k: v\n                    for k, v in item.items()\n                    if k not in [\"image\", \"text\", \"image_url\"]\n                }\n                content.append(\n                    {\n                        \"image_url\": {\"url\": image_data, **other_fields},\n                        \"type\": \"image_url\",\n                    }\n                )            \n\n            if \"text\" in item and \"type\" not in item:\n                text_data = item[\"text\"]\n                content.append({\"text\": text_data, \"type\": \"text\"})\n\n            ## for format like this: {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n            ## {\"type\": \"image_url\", \"image_url\": {\"url\":\"\",\"detail\":\"high\"}}\n            ## this is the standard format, just return it\n            if \"type\" in item and item[\"type\"] == \"text\":\n                content.append(item)\n\n            if \"type\" in item and item[\"type\"] == \"image_url\":\n                content.append(item)\n\n        if not content:\n            return ins\n\n        return content\n\n    async def async_embed_query(self, ins: str, **kwargs):\n        resp = await asyncfy_with_semaphore(\n            lambda: self.client.embeddings.create(input=[ins], model=self.model)\n        )()\n        embedding = resp.data[0].embedding\n        usage = resp.usage\n        return (\n            embedding,\n            {\n                \"metadata\": {\n                    \"input_tokens_count\": usage.prompt_tokens,\n                    \"generated_tokens_count\": 0,\n                }\n            },\n        )\n\n    def embed_query(self, ins: str, **kwargs):\n        resp = self.client.embeddings.create(input=[ins], model=self.model)\n        embedding = resp.data[0].embedding\n        usage = resp.usage\n        return (\n            embedding,\n            {\n                \"metadata\": {\n                    \"input_tokens_count\": usage.prompt_tokens,\n                    \"generated_tokens_count\": 0,\n                }\n            },\n        )\n\n    async def async_text_to_speech(\n        self, stream: bool, ins: str, voice: str, chunk_size: int = None, **kwargs\n    ):\n        if stream:\n            server = ray.get_actor(\"BlockBinaryStreamServer\")\n            request_id = [None]\n\n            def writer():\n                try:\n                    request_id[0] = str(uuid.uuid4())\n                    with self.client.with_streaming_response.audio.speech.create(\n                        model=self.model, voice=voice, input=ins, **kwargs\n                    ) as response:\n                        for chunk in response.iter_bytes(chunk_size):\n                            input_tokens_count = 0\n                            generated_tokens_count = 0\n                            ray.get(\n                                server.add_item.remote(\n                                    request_id[0],\n                                    StreamOutputs(\n                                        outputs=[\n                                            SingleOutput(\n                                                text=chunk,\n                                                metadata=SingleOutputMeta(\n                                                    input_tokens_count=input_tokens_count,\n                                                    generated_tokens_count=generated_tokens_count,\n                                                ),\n                                            )\n                                        ]\n                                    ),\n                                )\n                            )\n                except:\n                    traceback.print_exc()\n                ray.get(server.mark_done.remote(request_id[0]))\n\n            threading.Thread(target=writer, daemon=True).start()\n\n            time_count = 10 * 100\n            while request_id[0] is None and time_count > 0:\n                time.sleep(0.01)\n                time_count -= 1\n\n            if request_id[0] is None:\n                raise Exception(\"Failed to get request id\")\n\n            def write_running():\n                return ray.get(server.add_item.remote(request_id[0], \"RUNNING\"))\n\n            await asyncio.to_thread(write_running)\n            return [\n                (\n                    \"\",\n                    {\n                        \"metadata\": {\n                            \"request_id\": request_id[0],\n                            \"stream_server\": \"BlockBinaryStreamServer\",\n                        }\n                    },\n                )\n            ]\n\n        start_time = time.monotonic()\n        with io.BytesIO() as output:\n            async with asyncfy_with_semaphore(\n                lambda: self.client.with_streaming_response.audio.speech.create(\n                    model=self.model, voice=voice, input=ins, **kwargs\n                )\n            )() as response:\n                for chunk in response.iter_bytes():\n                    output.write(chunk)\n\n            base64_audio = base64.b64encode(output.getvalue()).decode()\n            time_cost = time.monotonic() - start_time\n            return [\n                (\n                    base64_audio,\n                    {\n                        \"metadata\": {\n                            \"request_id\": \"\",\n                            \"input_tokens_count\": 0,\n                            \"generated_tokens_count\": 0,\n                            \"time_cost\": time_cost,\n                            \"first_token_time\": 0,\n                            \"speed\": 0,\n                        }\n                    },\n                )\n            ]\n\n    async def async_speech_to_text(\n        self,\n        audio: str,\n        response_format: str = \"verbose_json\",\n        timestamp_granularities: List[str] = [\"word\",\"segment\"],\n    ):               \n        # Extract audio format and base64 data\n        data_prefix = \"data:audio/\"\n        base64_prefix = \";base64,\"\n        if not audio.startswith(data_prefix):\n            raise ValueError(\"Invalid audio data format\")\n        \n        format_end = audio.index(base64_prefix)\n        audio_format = audio[len(data_prefix):format_end]\n        base64_data = audio[format_end + len(base64_prefix):]\n        \n        # Decode the base64 audio data\n        audio_data = base64.b64decode(base64_data)\n\n        # Create a temporary file with the correct extension\n        with tempfile.NamedTemporaryFile(delete=False, suffix=f'.{audio_format}') as temp_audio_file:\n            temp_audio_file.write(audio_data)\n            temp_audio_file_path = temp_audio_file.name\n\n        try:\n            start_time = time.monotonic()\n                   \n            with open(temp_audio_file_path, \"rb\") as audio_file:                \n                transcription = self.client.audio.transcriptions.create(\n                    model=self.model or \"whisper-1\",\n                    file=audio_file,\n                    response_format=response_format,\n                    timestamp_granularities=timestamp_granularities,\n                )\n            time_cost = time.monotonic() - start_time\n\n            return [\n                (\n                    json.dumps(transcription.to_dict(), ensure_ascii=True),\n                    {\n                        \"metadata\": {\n                            \"request_id\": \"\",\n                            \"input_tokens_count\": 0,\n                            \"generated_tokens_count\": 0,\n                            \"time_cost\": time_cost,\n                            \"first_token_time\": 0,\n                            \"speed\": 0,\n                        }\n                    },\n                )\n            ]\n        finally:\n            # Clean up the temporary file\n            import os\n            os.unlink(temp_audio_file_path)\n\n    def image_to_text(self, ins: str, **kwargs):\n        pass\n\n    async def async_text_to_image(\n        self, stream: bool, input: str, size: str, quality: str, n: int, **kwargs\n    ):\n        if stream:\n            raise Exception(\"Stream not supported for text to image\")\n        start_time = time.monotonic()\n        response = await asyncfy_with_semaphore(\n            lambda: self.client.images.generate(\n                model=self.model,\n                prompt=input,\n                size=size,\n                quality=quality,\n                n=1,\n                response_format=\"b64_json\",\n                **kwargs,\n            )\n        )()\n        time_cost = time.monotonic() - start_time\n        base64_image = response.data[0].b64_json\n        return [\n            (\n                base64_image,\n                {\n                    \"metadata\": {\n                        \"request_id\": \"\",\n                        \"input_tokens_count\": 0,\n                        \"generated_tokens_count\": 0,\n                        \"time_cost\": time_cost,\n                        \"first_token_time\": 0,\n                        \"speed\": 0,\n                    }\n                },\n            )\n        ]\n\n    def text_to_text(self, ins: str, **kwargs):\n        pass\n\n    async def async_stream_chat(\n        self,\n        tokenizer,\n        ins: str,\n        his: List[Dict[str, Any]] = [],\n        max_length: int = 4096,\n        top_p: float = 0.7,\n        temperature: float = 0.9,\n        **kwargs,\n    ):\n\n        model = self.model\n\n        if \"model\" in kwargs:\n            model = kwargs[\"model\"]\n\n        messages = [\n            {\"role\": message[\"role\"], \"content\": self.process_input(message[\"content\"])}\n            for message in his\n        ] + [{\"role\": \"user\", \"content\": self.process_input(ins)}]\n\n        stream = kwargs.get(\"stream\", False)\n\n        ## content = [\n        ##    \"voice\": \"alloy\",\"input\": \"Hello, World!\",response_format: \"mp3\"]\n        last_message = messages[-1][\"content\"]\n\n        if isinstance(last_message, dict) and \"voice\" in last_message:\n            voice = last_message[\"voice\"]\n            response_format = last_message.get(\"response_format\", \"mp3\")\n            chunk_size = last_message.get(\"chunk_size\", None)\n            input = last_message[\"input\"]\n            return await self.async_text_to_speech(\n                stream=stream,\n                ins=input,\n                voice=voice,\n                chunk_size=chunk_size,\n                response_format=response_format,\n            )\n\n        if isinstance(last_message, dict) and \"input\" in last_message:\n            input = last_message[\"input\"]\n            size = last_message.get(\"size\", \"1024x1024\")\n            quality = last_message.get(\"quality\", \"standard\")\n            n = last_message.get(\"n\", 1)\n            return await self.async_text_to_image(\n                stream=stream, input=input, size=size, quality=quality, n=n\n            )\n\n        if isinstance(last_message, dict) and \"audio\" in last_message:            \n            audio_data = last_message.get(\"audio\", \"\")            \n            tpe = last_message.get(\"type\",\"wav\")\n            if not audio_data.startswith(\"data:\"):\n                audio_data = f\"data:audio/${tpe};base64,\" + audio_data                \n            return await self.async_speech_to_text(audio=audio_data)\n        \n        server = ray.get_actor(\"BLOCK_VLLM_STREAM_SERVER\")\n        request_id = [None]\n\n        def writer():\n            try:\n                r = \"\"\n                response = self.client.chat.completions.create(\n                    messages=messages,\n                    model=model,\n                    stream=True,\n                    max_tokens=max_length,\n                    temperature=temperature,\n                    top_p=top_p,\n                )\n                # input_tokens_count = 0\n                # generated_tokens_count = 0\n\n                request_id[0] = str(uuid.uuid4())\n\n                for chunk in response:\n                    content = chunk.choices[0].delta.content or \"\"\n                    r += content\n                    if hasattr(chunk, \"usage\") and chunk.usage:\n                        input_tokens_count = chunk.usage.prompt_tokens\n                        generated_tokens_count = chunk.usage.completion_tokens\n                    else:\n                        input_tokens_count = 0\n                        generated_tokens_count = 0\n                    ray.get(\n                        server.add_item.remote(\n                            request_id[0],\n                            StreamOutputs(\n                                outputs=[\n                                    SingleOutput(\n                                        text=r,\n                                        metadata=SingleOutputMeta(\n                                            input_tokens_count=input_tokens_count,\n                                            generated_tokens_count=generated_tokens_count,\n                                        ),\n                                    )\n                                ]\n                            ),\n                        )\n                    )\n            except:\n                traceback.print_exc()\n            ray.get(server.mark_done.remote(request_id[0]))\n\n        if stream:\n            threading.Thread(target=writer, daemon=True).start()\n\n            time_count = 60 * 100\n            while request_id[0] is None and time_count > 0:\n                time.sleep(0.01)\n                time_count -= 1\n\n            if request_id[0] is None:\n                raise Exception(\"Failed to get request id\")\n\n            def write_running():\n                return ray.get(server.add_item.remote(request_id[0], \"RUNNING\"))\n\n            await asyncio.to_thread(write_running)\n            return [\n                (\n                    \"\",\n                    {\n                        \"metadata\": {\n                            \"request_id\": request_id[0],\n                            \"stream_server\": \"BLOCK_VLLM_STREAM_SERVER\",\n                        }\n                    },\n                )\n            ]\n        else:\n            try:\n                start_time = time.monotonic()\n                response = await asyncfy_with_semaphore(\n                    lambda: self.client.chat.completions.create(\n                        messages=messages,\n                        model=model,\n                        max_tokens=max_length,\n                        temperature=temperature,\n                        top_p=top_p,\n                    )\n                )()\n\n                generated_text = response.choices[0].message.content\n                generated_tokens_count = response.usage.completion_tokens\n                input_tokens_count = response.usage.prompt_tokens\n                time_cost = time.monotonic() - start_time\n                return [\n                    (\n                        generated_text,\n                        {\n                            \"metadata\": {\n                                \"request_id\": response.id,\n                                \"input_tokens_count\": input_tokens_count,\n                                \"generated_tokens_count\": generated_tokens_count,\n                                \"time_cost\": time_cost,\n                                \"first_token_time\": 0,\n                                \"speed\": float(generated_tokens_count) / time_cost,\n                            }\n                        },\n                    )\n                ]\n            except Exception as e:\n                print(f\"Error: {e}\")\n                raise e\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/whisper/__init__.py\nimport whisper\nfrom typing import Dict, List, Tuple\nfrom byzerllm.utils.types import StopSequencesCriteria\n\nfrom typing import Dict, Any, List, Union\nfrom pyjava.storage import streaming_tar as STar\nimport json\nimport base64\nimport tempfile\nimport time\n\n\ndef get_meta(self):\n    return [\n        {\n            \"model_deploy_type\": \"proprietary\",\n            \"backend\": \"transformers\",\n            \"message_format\": True\n        }\n    ]\n\n\ndef process_input(ins: Union[str, List[Dict[str, Any]], Dict[str, Any]]):\n    # print(ins) \n    if isinstance(ins, list) or isinstance(ins, dict):\n        return ins\n\n    content = []\n    try:\n        ins_json = json.loads(ins)\n    except:\n        return ins\n\n    ## speech\n    if isinstance(ins_json, dict):\n        return ins_json\n\n    content = []\n    #     [\n    #     {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n    #     {\n    #       \"type\": \"image_url\",\n    #       \"image_url\": {\n    #         \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n    #         \"detail\": \"high\"\n    #       },\n    #     },\n    #   ],\n    for item in ins_json:\n        # for format like this: {\"image\": \"xxxxxx\", \"text\": \"What’s in this image?\",\"detail\":\"high\"}\n        # or {\"image\": \"xxxxxx\"}, {\"text\": \"What’s in this image?\"}\n        if (\"image\" in item or \"image_url\" in item) and \"type\" not in item:\n            image_data = item.get(\"image\", item.get(\"image_url\", \"\"))\n            ## \"data:image/jpeg;base64,\"\n            if not image_data.startswith(\"data:\"):\n                image_data = \"data:image/jpeg;base64,\" + image_data\n\n            ## get the other fields except image/text/image_url\n            other_fields = {\n                k: v for k, v in item.items() if k not in [\"image\", \"text\", \"image_url\"]\n            }\n            content.append(\n                {\n                    \"image_url\": {\"url\": image_data, **other_fields},\n                    \"type\": \"image_url\",\n                }\n            )\n\n        if \"text\" in item and \"type\" not in item:\n            text_data = item[\"text\"]\n            content.append({\"text\": text_data, \"type\": \"text\"})\n\n        ## for format like this: {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n        ## {\"type\": \"image_url\", \"image_url\": {\"url\":\"\",\"detail\":\"high\"}}\n        ## this is the standard format, just return it\n        if \"type\" in item and item[\"type\"] == \"text\":\n            content.append(item)\n\n        if \"type\" in item and item[\"type\"] == \"image_url\":\n            content.append(item)\n\n    if not content:\n        return ins\n\n    return content\n\n\ndef stream_chat(\n    self,\n    tokenizer,\n    ins: str,\n    his: List[Dict[str, str]] = [],\n    max_length: int = 4090,\n    top_p: float = 0.95,\n    temperature: float = 0.1,\n    **kwargs,\n):\n    messages = [\n        {\"role\": message[\"role\"], \"content\": process_input(message[\"content\"])}\n        for message in his\n    ] + [{\"role\": \"user\", \"content\": process_input(ins)}]\n\n    audio_input = messages[-1][\"content\"]\n\n    audio_data = audio_input.get(\"audio\", \"\")\n    audio = audio_input[\"audio\"]\n    response_format = audio_input.get(\"response_format\", \"json\")\n    timestamp_granularities = audio_input.get(\n        \"timestamp_granularities\", [\"word\", \"segment\"]\n    )\n    data_prefix = \"data:audio/\"\n    base64_prefix = \";base64,\"\n    if not audio.startswith(data_prefix):\n        raise ValueError(\"Invalid audio data format\")\n\n    format_end = audio.index(base64_prefix)\n    audio_format = audio[len(data_prefix) : format_end]\n    base64_data = audio[format_end + len(base64_prefix) :]\n\n    # Decode the base64 audio data\n    audio_data = base64.b64decode(base64_data)\n\n    # Create a temporary file with the correct extension\n    with tempfile.NamedTemporaryFile(\n        delete=False, suffix=f\".{audio_format}\"\n    ) as temp_audio_file:\n        temp_audio_file.write(audio_data)\n        temp_audio_file_path = temp_audio_file.name\n\n    try:\n        start_time = time.monotonic()\n        word_timestamps = \"word\" in timestamp_granularities\n        \n        result = self.transcribe(temp_audio_file_path, word_timestamps=word_timestamps)        \n            \n        time_cost = time.monotonic() - start_time\n        return [\n            (\n                json.dumps(result, ensure_ascii=True),\n                {\n                    \"metadata\": {\n                        \"request_id\": \"\",\n                        \"input_tokens_count\": 0,\n                        \"generated_tokens_count\": 0,\n                        \"time_cost\": time_cost,\n                        \"first_token_time\": 0,\n                        \"speed\": 0,\n                    }\n                },\n            )\n        ]\n    finally:\n        # Clean up the temporary file\n        import os\n\n        os.unlink(temp_audio_file_path)\n\n\ndef init_model(\n    model_dir, infer_params: Dict[str, str] = {}, sys_conf: Dict[str, str] = {}\n):\n    model = whisper.load_model(\"large\", download_root=model_dir)\n    import types\n\n    model.stream_chat = types.MethodType(stream_chat, model)\n    model.get_meta = types.MethodType(get_meta, model)\n    return (model, None)\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/sensevoice/__init__.py\nimport json\nimport base64\nimport tempfile\nimport time\nfrom typing import Dict, List, Union, Any\nfrom funasr import AutoModel\n\nemo_dict = {\n    \"<|HAPPY|>\": \"😊\",\n    \"<|SAD|>\": \"😔\",\n    \"<|ANGRY|>\": \"😡\",\n    \"<|NEUTRAL|>\": \"\",\n    \"<|FEARFUL|>\": \"😰\",\n    \"<|DISGUSTED|>\": \"🤢\",\n    \"<|SURPRISED|>\": \"😮\",\n}\n\nevent_dict = {\n    \"<|BGM|>\": \"🎼\",\n    \"<|Speech|>\": \"\",\n    \"<|Applause|>\": \"👏\",\n    \"<|Laughter|>\": \"��\",\n    \"<|Cry|>\": \"😭\",\n    \"<|Sneeze|>\": \"🤧\",\n    \"<|Breath|>\": \"\",\n    \"<|Cough|>\": \"🤧\",\n}\n\nemoji_dict = {\n    \"<|nospeech|><|Event_UNK|>\": \"❓\",\n    \"<|zh|>\": \"\",\n    \"<|en|>\": \"\",\n    \"<|yue|>\": \"\",\n    \"<|ja|>\": \"\",\n    \"<|ko|>\": \"\",\n    \"<|nospeech|>\": \"\",\n    \"<|HAPPY|>\": \"😊\",\n    \"<|SAD|>\": \"😔\",\n    \"<|ANGRY|>\": \"😡\",\n    \"<|NEUTRAL|>\": \"\",\n    \"<|BGM|>\": \"🎼\",\n    \"<|Speech|>\": \"\",\n    \"<|Applause|>\": \"👏\",\n    \"<|Laughter|>\": \"😀\",\n    \"<|FEARFUL|>\": \"😰\",\n    \"<|DISGUSTED|>\": \"🤢\",\n    \"<|SURPRISED|>\": \"😮\",\n    \"<|Cry|>\": \"😭\",\n    \"<|EMO_UNKNOWN|>\": \"\",\n    \"<|Sneeze|>\": \"🤧\",\n    \"<|Breath|>\": \"\",\n    \"<|Cough|>\": \"😷\",\n    \"<|Sing|>\": \"\",\n    \"<|Speech_Noise|>\": \"\",\n    \"<|withitn|>\": \"\",\n    \"<|woitn|>\": \"\",\n    \"<|GBG|>\": \"\",\n    \"<|Event_UNK|>\": \"\",\n}\n\nlang_dict = {\n    \"<|zh|>\": \"<|lang|>\",\n    \"<|en|>\": \"<|lang|>\",\n    \"<|yue|>\": \"<|lang|>\",\n    \"<|ja|>\": \"<|lang|>\",\n    \"<|ko|>\": \"<|lang|>\",\n    \"<|nospeech|>\": \"<|lang|>\",\n}\n\nemo_set = {\"😊\", \"😔\", \"😡\", \"😰\", \"🤢\", \"😮\"}\nevent_set = {\"🎼\", \"👏\", \"😀\", \"😭\", \"🤧\", \"😷\"}\n\n\ndef format_str(s):\n    for sptk in emoji_dict:\n        s = s.replace(sptk, emoji_dict[sptk])\n    return s\n\n\ndef format_str_v2(s):\n    sptk_dict = {sptk: s.count(sptk) for sptk in emoji_dict}\n    s = \"\".join(c for c in s if c not in emoji_dict.values())\n    emo = max(emo_dict, key=lambda e: sptk_dict[e])\n    event = \"\".join(event_dict[e] for e in event_dict if sptk_dict[e] > 0)\n    s = event + s + emo_dict[emo]\n    for emoji in emo_set.union(event_set):\n        s = s.replace(\" \" + emoji, emoji).replace(emoji + \" \", emoji)\n    return s.strip()\n\n\ndef format_str_v3(s):\n    def get_emo(s):\n        return s[-1] if s[-1] in emo_set else None\n\n    def get_event(s):\n        return s[0] if s[0] in event_set else None\n\n    s = s.replace(\"<|nospeech|><|Event_UNK|>\", \"❓\")\n    for lang in lang_dict:\n        s = s.replace(lang, \"<|lang|>\")\n    s_list = [format_str_v2(s_i).strip() for s_i in s.split(\"<|lang|>\")]\n    new_s = \" \" + s_list[0]\n    cur_ent_event = get_event(new_s)\n    for i in range(1, len(s_list)):\n        if len(s_list[i]) == 0:\n            continue\n        if get_event(s_list[i]) == cur_ent_event and get_event(s_list[i]) is not None:\n            s_list[i] = s_list[i][1:]\n        cur_ent_event = get_event(s_list[i])\n        if get_emo(s_list[i]) is not None and get_emo(s_list[i]) == get_emo(new_s):\n            new_s = new_s[:-1]\n        new_s += s_list[i].strip()\n    new_s = new_s.replace(\"The.\", \" \")\n    return new_s.strip()\n\n\ndef get_meta(self):\n    return [\n        {\n            \"model_deploy_type\": \"proprietary\",\n            \"backend\": \"funasr\",\n            \"message_format\": True,\n        }\n    ]\n\n\ndef process_input(ins: Union[str, List[Dict[str, Any]], Dict[str, Any]]):\n    if isinstance(ins, list) or isinstance(ins, dict):\n        return ins\n\n    try:\n        ins_json = json.loads(ins)\n    except:\n        return ins\n\n    if isinstance(ins_json, dict):\n        return ins_json\n\n    content = []\n    for item in ins_json:\n        if \"audio\" in item and \"type\" not in item:\n            audio_data = item.get(\"audio\", \"\")\n            if not audio_data.startswith(\"data:\"):\n                audio_data = f\"data:audio/wav;base64,{audio_data}\"\n            content.append({\"audio\": audio_data, \"type\": \"audio\"})\n\n        if \"text\" in item and \"type\" not in item:\n            content.append({\"text\": item[\"text\"], \"type\": \"text\"})\n\n        if \"type\" in item:\n            content.append(item)\n\n    return content if content else ins\n\n\ndef stream_chat(\n    self,\n    tokenizer,\n    ins: str,\n    his: List[Dict[str, str]] = [],\n    max_length: int = 4090,\n    top_p: float = 0.95,\n    temperature: float = 0.1,\n    **kwargs,\n):\n    messages = [\n        {\"role\": message[\"role\"], \"content\": process_input(message[\"content\"])}\n        for message in his\n    ] + [{\"role\": \"user\", \"content\": process_input(ins)}]\n\n    audio_input = messages[-1][\"content\"]\n\n    audio = audio_input[\"audio\"]\n    data_prefix = \"data:audio/\"\n    base64_prefix = \";base64,\"\n    if not audio.startswith(data_prefix):\n        raise ValueError(\"Invalid audio data format\")\n\n    format_end = audio.index(base64_prefix)\n    audio_format = audio[len(data_prefix) : format_end]\n    base64_data = audio[format_end + len(base64_prefix) :]\n\n    audio_data = base64.b64decode(base64_data)\n\n    with tempfile.NamedTemporaryFile(\n        delete=False, suffix=f\".{audio_format}\"\n    ) as temp_audio_file:\n        temp_audio_file.write(audio_data)\n        temp_audio_file_path = temp_audio_file.name\n\n    try:\n        start_time = time.monotonic()\n\n        result = self.generate(input=temp_audio_file_path, cache={})\n\n        time_cost = time.monotonic() - start_time\n\n        formatted_result = format_str_v3(result[0][\"text\"])\n\n        return [\n            (\n                json.dumps({\"text\": formatted_result}, ensure_ascii=False),\n                {\n                    \"metadata\": {\n                        \"request_id\": \"\",\n                        \"input_tokens_count\": 0,\n                        \"generated_tokens_count\": 0,\n                        \"time_cost\": time_cost,\n                        \"first_token_time\": 0,\n                        \"speed\": 0,\n                    }\n                },\n            )\n        ]\n    finally:\n        import os\n\n        os.unlink(temp_audio_file_path)\n\n\ndef init_model(\n    model_dir, infer_params: Dict[str, str] = {}, sys_conf: Dict[str, str] = {}\n):\n    model_kwargs = {\n        \"model\": model_dir,\n        \"trust_remote_code\": True,\n    }\n\n    if \"vad_model\" in infer_params:\n        model_kwargs[\"vad_model\"] = infer_params[\"vad_model\"]\n        model_kwargs[\"vad_kwargs\"] = {\"max_single_segment_time\": 30000}\n\n    model = AutoModel(**model_kwargs)\n\n    import types\n\n    model.stream_chat = types.MethodType(stream_chat, model)\n    model.get_meta = types.MethodType(get_meta, model)\n\n    return (model, None)\n\n\n# Helper functions from web_ui.py\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/notebooks/test_sensevoice.py\nimport sys\nimport os\n\n# Add the project root to the Python path\nsys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\")))\n\nfrom src.byzerllm.sensevoice import init_model, format_str_v3\nimport base64\nimport json\n\n\ndef main():\n    # Initialize the model\n    model_dir = \"path/to/your/model/directory\"  # Replace with actual path\n    model, _ = init_model(model_dir)\n\n    # Prepare test audio\n    with open(\n        \"/Users/allwefantasy/models/SenseVoiceSmall/example/zh.mp3\", \"rb\"\n    ) as audio_file:\n        audio_data = audio_file.read()\n        base64_audio = base64.b64encode(audio_data).decode(\"utf-8\")\n        model_dir = \"/Users/allwefantasy/models/SenseVoiceSmall\"\n        # Prepare input in the expected format\n        test_input = json.dumps(\n            [\n                {\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\n                            \"type\": \"audio\",\n                            \"audio\": f\"data:audio/wav;base64,{base64_audio}\",\n                        }\n                    ],\n                }\n            ]\n        )\n\n        # Call stream_chat\n        result = model.stream_chat(None, test_input)\n\n        # Process and print the result\n        if result and len(result) > 0:\n            output, metadata = result[0]\n            parsed_output = json.loads(output)\n            formatted_text = parsed_output.get(\"text\", \"\")\n            print(\"Transcription:\", formatted_text)\n            print(\"Metadata:\", metadata)\n        else:\n            print(\"No result returned from the model.\")\n\n\nif __name__ == \"__main__\":\n    main()\n\n\n     \n\n下面是用户的需求：\n\n@test_sensevoice.py 中测试数据改成 /Users/allwefantasy/models/SenseVoiceSmall/example/zh.mp3 模型路径改成 /Users/allwefantasy/models/SenseVoiceSmall\n"}, {"role": "assistant", "content": "```python\n##File: /Users/allwefantasy/projects/byzer-llm/notebooks/test_sensevoice.py\n<<<<<<< SEARCH\nwith open(\"/Users/allwefantasy/models/SenseVoiceSmall/example/zh.mp3\", \"rb\") as audio_file:\n    audio_data = audio_file.read()\n    base64_audio = base64.b64encode(audio_data).decode(\"utf-8\")\n    model_dir = \"/Users/allwefantasy/models/SenseVoiceSmall\"\n=======\nwith open(\"/Users/allwefantasy/models/SenseVoiceSmall/example/zh.mp3\", \"rb\") as audio_file:\n    audio_data = audio_file.read()\n    base64_audio = base64.b64encode(audio_data).decode(\"utf-8\")\n    model_dir = \"/Users/allwefantasy/models/SenseVoiceSmall\"\n>>>>>>> REPLACE\n```"}], "model": "deepseek_chat", "yaml_file": "actions/030_chat_action.yml"}
{"instruction": "@/Users/allwefantasy/projects/byzer-llm/src/byzerllm/sensevoice/__init__.py 中model = AutoModel(\n    model=model_dir,\n    trust_remote_code=True,\n    remote_code=\"./model.py\",    \n    vad_model=\"fsmn-vad\",\n    vad_kwargs={\"max_single_segment_time\": 30000},\n    device=\"cuda:0\",\n)  我们需要参考上面的代码,支持 vad_kwargs 参数.可以通过  infer_params 参数中的 vad_kwargs.xxxx 来获取,比如 max_single_segment_time 就是 vad_kwargs.max_single_segment_time 注意,传递过来的时候都是字符串,需要自动检测下类型", "conversations": [{"role": "user", "content": "\n如果你需要生成代码，对于每个需要更改的文件,你需要按 *SEARCH/REPLACE block* 的格式进行生成。\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The opening fence and code language, eg: ```python\n2. The file path alone on a line, starting with \"##File:\" and verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n*SEARCH/REPLACE* blocks will replace *all* matching occurrences.\nInclude enough lines to make the SEARCH blocks unique.\n\nInclude *ALL* the code being searched and replaced!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n下面我们来看一个例子：\n\n当前项目目录结构：\n1. 项目根目录： /tmp/projects/mathweb\n2. 项目子目录/文件列表(类似tree 命令输出)\nflask/\n    app.py\n    templates/\n        index.html\n    static/\n        style.css\n\n用户需求： Change get_factorial() to use math.factorial\n\n回答： To make this change we need to modify `/tmp/projects/mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n用户需求： Refactor hello() into its own file.\n\n回答：To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/hello.py\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/main.py\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n现在让我们开始一个新的任务:\n\n当前项目目录结构：\n1. 项目根目录： /Users/allwefantasy/projects/byzer-llm\n2. 项目子目录/文件列表(类似tree 命令输出)：\nsetup.py\ntests/\n    test_emb_rerank.py\n    test_deepseek_code.py\n    test_prompt.py\n    test_model_meta.py\n    __init__.py\n    test_transformers_deploy.py\n    test_data_analysis.py\n    test_yi.py\n    test_deploy.py\n    functions_suite.py\n    test_template.py\n    auto/\n        test_backend_llama_cpp.py\n        __init__.py\nsetup-machine/\n    convert_safetensor.py\nnotebooks/\n    test.py\n    test_sensevoice.py\nsrc/\n    byzerllm/\n        lang.py\n        store.py\n        version.py\n        log.py\n        __init__.py\n        types.py\n        easy_byzerllm_command.py\n        byzerllm_command.py\n        command_args.py\n        bark/\n            generation.py\n            __init__.py\n            model.py\n            api.py\n            model_fine.py\n            bark_voice.py\n        moss/\n            finetune_moss.py\n            __init__.py\n            moss_inference.py\n            models/\n                configuration_moss.py\n                quantization.py\n                custom_autotune.py\n                __init__.py\n                modeling_moss.py\n                tokenization_moss.py\n        llama2/\n            __init__.py\n        records/\n            __init__.py\n        visualglm/\n            __init__.py\n        auto/\n            __init__.py\n            backend_llama_cpp.py\n            backend_vllm.py\n            backend_ds.py\n        alpha_moss/\n            __init__.py\n        processor/\n            __init__.py\n        sensevoice/\n            __init__.py\n        qwen/\n            __init__.py\n        utils/\n            json_repaire.py\n            object_store_ref_util.py\n            __init__.py\n            types.py\n            tokenizer.py\n            openai_utils.py\n            ray_utils.py\n            text_generator.py\n            emb.py\n            connect_ray.py\n            testing.py\n            langutil.py\n            nontext.py\n            fulltune/\n                deepspeed_trainner.py\n                __init__.py\n                launch.py\n                trainner.py\n                base_model/\n                    __init__.py\n                    configuration_baichuan.py\n                    modeling_baichuan.py\n                pretrain/\n                    __init__.py\n                    convert_to_transformers.py\n            metrics/\n                __init__.py\n            config/\n                __init__.py\n            retrieval/\n                rest.py\n                __init__.py\n                udf.py\n            sft/\n                argument.py\n                collator.py\n                qlora.py\n                __init__.py\n                model.py\n                dataset.py\n                loss.py\n                merge_lora.py\n                trainer.py\n            client/\n                message_utils.py\n                math_utils.py\n                byzerllm_client.py\n                parallel_utils.py\n                __init__.py\n                types.py\n                code_utils.py\n                img_utils.py\n                entrypoints/\n                    __init__.py\n                    openai/\n                        protocol.py\n                        __init__.py\n                        serve.py\n                        serving_chat.py\n                        api_server.py\n                        serving_engine.py\n                        serving_completion.py\n                        tool.py\n        chatglm2/\n            __init__.py\n        baichuan/\n            __init__.py\n        m3e/\n            __init__.py\n        stable_diffusion/\n            config.py\n            __init__.py\n            logger.py\n            shared.py\n            model.py\n            utils.py\n            images.py\n            diffusion/\n                __init__.py\n                embeddings.py\n                utils.py\n                networks/\n                    lyco.py\n                    __init__.py\n                upscalers/\n                    __init__.py\n                    multidiffusion.py\n                    samplers.py\n                piplines/\n                    __init__.py\n                    lpw.py\n                    diffusers.py\n            lib/\n                __init__.py\n                diffusers/\n                    __init__.py\n                    scheduler.py\n            api/\n                __init__.py\n                plugin.py\n                models/\n                    diffusion.py\n                    __init__.py\n                diffusion/\n                    __init__.py\n                    pipelines/\n                        __init__.py\n                        diffusers.py\n                events/\n                    generation.py\n                    __init__.py\n        bge/\n            __init__.py\n        bge_rerank/\n            __init__.py\n        apps/\n            client.py\n            qa_strategy.py\n            __init__.py\n            builder.py\n            vector_db.py\n            utils.py\n            qa.py\n            byzer_storage/\n                memory_model_based.py\n                command.py\n                env.py\n                train_pt.py\n                train_sft.py\n                __init__.py\n                train_dpo.py\n                simple_api.py\n                generate_sft_data.py\n            byzer_sql/\n                __init__.py\n            llama_index/\n                byzerai_vectordb.py\n                byzerai.py\n                byzerai_embedding.py\n                __init__.py\n                collection_manager.py\n                byzerai_index_store.py\n                byzerai_docstore.py\n                simple_retrieval.py\n                byzerai_kvstore.py\n            agent/\n                conversable_agent.py\n                registry.py\n                groupchat.py\n                user_proxy_agent.py\n                __init__.py\n                agent.py\n                extensions/\n                    simple_retrieval_client.py\n                    planner.py\n                    python_codesandbox_agent.py\n                    preview_file_agent.py\n                    llama_index_retrieval_agent.py\n                    visualization_agent.py\n                    assistant_agent.py\n                    load_data_agent.py\n                    retrieval_agent.py\n                    output_agent.py\n                    __init__.py\n                    byzer_engine_agent.py\n                    subquestion_agent.py\n                    sql_reviewer_agent.py\n                    spark_sql_agent.py\n                    rhetorical_agent.py\n                    data_analysis.py\n                    data_analysis_pipeline_agent.py\n                    common_agent.py\n                    query_rewrite/\n                        time.py\n                        rhetorical.py\n                        __init__.py\n                        condition.py\n                        context.py\n                store/\n                    memory_store.py\n                    __init__.py\n                    stores.py\n        saas/\n            __init__.py\n            chatglm/\n                __init__.py\n            sparkdesk/\n                __init__.py\n            gemini/\n                __init__.py\n            azure_openai/\n                __init__.py\n            azure/\n                __init__.py\n            qianfan/\n                __init__.py\n            claude/\n                __init__.py\n            qianwen/\n                __init__.py\n            volcengine/\n                __init__.py\n            minimax/\n                __init__.py\n            baichuan/\n                __init__.py\n            qianwen_vl/\n                __init__.py\n            official_openai/\n                __init__.py\n            zhipu/\n                __init__.py\n            aws_bedrock/\n                __init__.py\n            openai/\n                __init__.py\n        falcon/\n            __init__.py\n        chatglm6b/\n            trainer_seq2seq.py\n            arguments.py\n            __init__.py\n            finetune.py\n            tunning/\n                __init__.py\n                infer.py\n                finetune.py\n                utils/\n                    config.py\n                    __init__.py\n                    other.py\n                    common.py\n                    seq2seq.py\n        starcode/\n            __init__.py\n        qwen_vl_chat/\n            __init__.py\n        zephyr/\n            __init__.py\n        dolly/\n            generate.py\n            __init__.py\n            consts.py\n            dolly_inference.py\n            trainer.py\n        llama/\n            __init__.py\n        whisper/\n            whisper_inference.py\n            __init__.py\n下面是一些文件路径以及每个文件对应的源码：\n\n##File: /Users/allwefantasy/projects/SenseVoice/webui.py\n# coding=utf-8\n\nimport os\nimport librosa\nimport base64\nimport io\nimport gradio as gr\nimport re\n\nimport numpy as np\nimport torch\nimport torchaudio\n\n\nfrom funasr import AutoModel\n\nmodel = \"iic/SenseVoiceSmall\"\nmodel_kwargs = {\n    \"model\": model,\n    \"trust_remote_code\": True,\n}\n\n# Check if vad_model is provided in infer_params\nif \"vad_model\" in infer_params:\n    model_kwargs[\"vad_model\"] = infer_params[\"vad_model\"]\n    model_kwargs[\"vad_kwargs\"] = {\"max_single_segment_time\": 30000}\n\nmodel = AutoModel(**model_kwargs)\n\nimport re\n\nemo_dict = {\n\t\"<|HAPPY|>\": \"😊\",\n\t\"<|SAD|>\": \"😔\",\n\t\"<|ANGRY|>\": \"😡\",\n\t\"<|NEUTRAL|>\": \"\",\n\t\"<|FEARFUL|>\": \"😰\",\n\t\"<|DISGUSTED|>\": \"🤢\",\n\t\"<|SURPRISED|>\": \"😮\",\n}\n\nevent_dict = {\n\t\"<|BGM|>\": \"🎼\",\n\t\"<|Speech|>\": \"\",\n\t\"<|Applause|>\": \"👏\",\n\t\"<|Laughter|>\": \"😀\",\n\t\"<|Cry|>\": \"😭\",\n\t\"<|Sneeze|>\": \"🤧\",\n\t\"<|Breath|>\": \"\",\n\t\"<|Cough|>\": \"🤧\",\n}\n\nemoji_dict = {\n\t\"<|nospeech|><|Event_UNK|>\": \"❓\",\n\t\"<|zh|>\": \"\",\n\t\"<|en|>\": \"\",\n\t\"<|yue|>\": \"\",\n\t\"<|ja|>\": \"\",\n\t\"<|ko|>\": \"\",\n\t\"<|nospeech|>\": \"\",\n\t\"<|HAPPY|>\": \"😊\",\n\t\"<|SAD|>\": \"😔\",\n\t\"<|ANGRY|>\": \"😡\",\n\t\"<|NEUTRAL|>\": \"\",\n\t\"<|BGM|>\": \"🎼\",\n\t\"<|Speech|>\": \"\",\n\t\"<|Applause|>\": \"👏\",\n\t\"<|Laughter|>\": \"😀\",\n\t\"<|FEARFUL|>\": \"😰\",\n\t\"<|DISGUSTED|>\": \"🤢\",\n\t\"<|SURPRISED|>\": \"😮\",\n\t\"<|Cry|>\": \"😭\",\n\t\"<|EMO_UNKNOWN|>\": \"\",\n\t\"<|Sneeze|>\": \"🤧\",\n\t\"<|Breath|>\": \"\",\n\t\"<|Cough|>\": \"😷\",\n\t\"<|Sing|>\": \"\",\n\t\"<|Speech_Noise|>\": \"\",\n\t\"<|withitn|>\": \"\",\n\t\"<|woitn|>\": \"\",\n\t\"<|GBG|>\": \"\",\n\t\"<|Event_UNK|>\": \"\",\n}\n\nlang_dict =  {\n    \"<|zh|>\": \"<|lang|>\",\n    \"<|en|>\": \"<|lang|>\",\n    \"<|yue|>\": \"<|lang|>\",\n    \"<|ja|>\": \"<|lang|>\",\n    \"<|ko|>\": \"<|lang|>\",\n    \"<|nospeech|>\": \"<|lang|>\",\n}\n\nemo_set = {\"😊\", \"😔\", \"😡\", \"😰\", \"🤢\", \"😮\"}\nevent_set = {\"🎼\", \"👏\", \"😀\", \"😭\", \"🤧\", \"😷\",}\n\ndef format_str(s):\n\tfor sptk in emoji_dict:\n\t\ts = s.replace(sptk, emoji_dict[sptk])\n\treturn s\n\n\ndef format_str_v2(s):\n\tsptk_dict = {}\n\tfor sptk in emoji_dict:\n\t\tsptk_dict[sptk] = s.count(sptk)\n\t\ts = s.replace(sptk, \"\")\n\temo = \"<|NEUTRAL|>\"\n\tfor e in emo_dict:\n\t\tif sptk_dict[e] > sptk_dict[emo]:\n\t\t\temo = e\n\tfor e in event_dict:\n\t\tif sptk_dict[e] > 0:\n\t\t\ts = event_dict[e] + s\n\ts = s + emo_dict[emo]\n\n\tfor emoji in emo_set.union(event_set):\n\t\ts = s.replace(\" \" + emoji, emoji)\n\t\ts = s.replace(emoji + \" \", emoji)\n\treturn s.strip()\n\ndef format_str_v3(s):\n\tdef get_emo(s):\n\t\treturn s[-1] if s[-1] in emo_set else None\n\tdef get_event(s):\n\t\treturn s[0] if s[0] in event_set else None\n\n\ts = s.replace(\"<|nospeech|><|Event_UNK|>\", \"❓\")\n\tfor lang in lang_dict:\n\t\ts = s.replace(lang, \"<|lang|>\")\n\ts_list = [format_str_v2(s_i).strip(\" \") for s_i in s.split(\"<|lang|>\")]\n\tnew_s = \" \" + s_list[0]\n\tcur_ent_event = get_event(new_s)\n\tfor i in range(1, len(s_list)):\n\t\tif len(s_list[i]) == 0:\n\t\t\tcontinue\n\t\tif get_event(s_list[i]) == cur_ent_event and get_event(s_list[i]) != None:\n\t\t\ts_list[i] = s_list[i][1:]\n\t\t#else:\n\t\tcur_ent_event = get_event(s_list[i])\n\t\tif get_emo(s_list[i]) != None and get_emo(s_list[i]) == get_emo(new_s):\n\t\t\tnew_s = new_s[:-1]\n\t\tnew_s += s_list[i].strip().lstrip()\n\tnew_s = new_s.replace(\"The.\", \" \")\n\treturn new_s.strip()\n\ndef model_inference(input_wav, language, fs=16000):\n\t# task_abbr = {\"Speech Recognition\": \"ASR\", \"Rich Text Transcription\": (\"ASR\", \"AED\", \"SER\")}\n\tlanguage_abbr = {\"auto\": \"auto\", \"zh\": \"zh\", \"en\": \"en\", \"yue\": \"yue\", \"ja\": \"ja\", \"ko\": \"ko\",\n\t\t\t\t\t \"nospeech\": \"nospeech\"}\n\t\n\t# task = \"Speech Recognition\" if task is None else task\n\tlanguage = \"auto\" if len(language) < 1 else language\n\tselected_language = language_abbr[language]\n\t# selected_task = task_abbr.get(task)\n\t\n\t# print(f\"input_wav: {type(input_wav)}, {input_wav[1].shape}, {input_wav}\")\n\t\n\tif isinstance(input_wav, tuple):\n\t\tfs, input_wav = input_wav\n\t\tinput_wav = input_wav.astype(np.float32) / np.iinfo(np.int16).max\n\t\tif len(input_wav.shape) > 1:\n\t\t\tinput_wav = input_wav.mean(-1)\n\t\tif fs != 16000:\n\t\t\tprint(f\"audio_fs: {fs}\")\n\t\t\tresampler = torchaudio.transforms.Resample(fs, 16000)\n\t\t\tinput_wav_t = torch.from_numpy(input_wav).to(torch.float32)\n\t\t\tinput_wav = resampler(input_wav_t[None, :])[0, :].numpy()\n\t\n\t\n\tmerge_vad = True #False if selected_task == \"ASR\" else True\n\tprint(f\"language: {language}, merge_vad: {merge_vad}\")\n\ttext = model.generate(input=input_wav,\n\t\t\t\t\t\t  cache={},\n\t\t\t\t\t\t  language=language,\n\t\t\t\t\t\t  use_itn=True,\n\t\t\t\t\t\t  batch_size_s=60, merge_vad=merge_vad)\n\t\n\tprint(text)\n\ttext = text[0][\"text\"]\n\ttext = format_str_v3(text)\n\t\n\tprint(text)\n\t\n\treturn text\n\n\naudio_examples = [\n    [\"example/zh.mp3\", \"zh\"],\n    [\"example/yue.mp3\", \"yue\"],\n    [\"example/en.mp3\", \"en\"],\n    [\"example/ja.mp3\", \"ja\"],\n    [\"example/ko.mp3\", \"ko\"],\n    [\"example/emo_1.wav\", \"auto\"],\n    [\"example/emo_2.wav\", \"auto\"],\n    [\"example/emo_3.wav\", \"auto\"],\n    #[\"example/emo_4.wav\", \"auto\"],\n    #[\"example/event_1.wav\", \"auto\"],\n    #[\"example/event_2.wav\", \"auto\"],\n    #[\"example/event_3.wav\", \"auto\"],\n    [\"example/rich_1.wav\", \"auto\"],\n    [\"example/rich_2.wav\", \"auto\"],\n    #[\"example/rich_3.wav\", \"auto\"],\n    [\"example/longwav_1.wav\", \"auto\"],\n    [\"example/longwav_2.wav\", \"auto\"],\n    [\"example/longwav_3.wav\", \"auto\"],\n    #[\"example/longwav_4.wav\", \"auto\"],\n]\n\n\n\nhtml_content = \"\"\"\n<div>\n    <h2 style=\"font-size: 22px;margin-left: 0px;\">Voice Understanding Model: SenseVoice-Small</h2>\n    <p style=\"font-size: 18px;margin-left: 20px;\">SenseVoice-Small is an encoder-only speech foundation model designed for rapid voice understanding. It encompasses a variety of features including automatic speech recognition (ASR), spoken language identification (LID), speech emotion recognition (SER), and acoustic event detection (AED). SenseVoice-Small supports multilingual recognition for Chinese, English, Cantonese, Japanese, and Korean. Additionally, it offers exceptionally low inference latency, performing 7 times faster than Whisper-small and 17 times faster than Whisper-large.</p>\n    <h2 style=\"font-size: 22px;margin-left: 0px;\">Usage</h2> <p style=\"font-size: 18px;margin-left: 20px;\">Upload an audio file or input through a microphone, then select the task and language. the audio is transcribed into corresponding text along with associated emotions (😊 happy, 😡 angry/exicting, 😔 sad) and types of sound events (😀 laughter, 🎼 music, 👏 applause, 🤧 cough&sneeze, 😭 cry). The event labels are placed in the front of the text and the emotion are in the back of the text.</p>\n\t<p style=\"font-size: 18px;margin-left: 20px;\">Recommended audio input duration is below 30 seconds. For audio longer than 30 seconds, local deployment is recommended.</p>\n\t<h2 style=\"font-size: 22px;margin-left: 0px;\">Repo</h2>\n\t<p style=\"font-size: 18px;margin-left: 20px;\"><a href=\"https://github.com/FunAudioLLM/SenseVoice\" target=\"_blank\">SenseVoice</a>: multilingual speech understanding model</p>\n\t<p style=\"font-size: 18px;margin-left: 20px;\"><a href=\"https://github.com/modelscope/FunASR\" target=\"_blank\">FunASR</a>: fundamental speech recognition toolkit</p>\n\t<p style=\"font-size: 18px;margin-left: 20px;\"><a href=\"https://github.com/FunAudioLLM/CosyVoice\" target=\"_blank\">CosyVoice</a>: high-quality multilingual TTS model</p>\n</div>\n\"\"\"\n\n\ndef launch():\n\twith gr.Blocks(theme=gr.themes.Soft()) as demo:\n\t\t# gr.Markdown(description)\n\t\tgr.HTML(html_content)\n\t\twith gr.Row():\n\t\t\twith gr.Column():\n\t\t\t\taudio_inputs = gr.Audio(label=\"Upload audio or use the microphone\")\n\t\t\t\t\n\t\t\t\twith gr.Accordion(\"Configuration\"):\n\t\t\t\t\tlanguage_inputs = gr.Dropdown(choices=[\"auto\", \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"],\n\t\t\t\t\t\t\t\t\t\t\t\t  value=\"auto\",\n\t\t\t\t\t\t\t\t\t\t\t\t  label=\"Language\")\n\t\t\t\tfn_button = gr.Button(\"Start\", variant=\"primary\")\n\t\t\t\ttext_outputs = gr.Textbox(label=\"Results\")\n\t\t\tgr.Examples(examples=audio_examples, inputs=[audio_inputs, language_inputs], examples_per_page=20)\n\t\t\n\t\tfn_button.click(model_inference, inputs=[audio_inputs, language_inputs], outputs=text_outputs)\n\n\tdemo.launch()\n\n\nif __name__ == \"__main__\":\n\t# iface.launch()\n\tlaunch()\n\n\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/saas/official_openai/__init__.py\nimport time\nfrom typing import List, Tuple, Dict, Any, Union\nimport httpx\nfrom openai import OpenAI\nimport base64\nimport io\nimport json\nimport ray\nfrom byzerllm.utils.types import (\n    BlockVLLMStreamServer,\n    StreamOutputs,\n    SingleOutput,\n    SingleOutputMeta,\n    BlockBinaryStreamServer,\n)\nfrom byzerllm.utils.langutil import asyncfy_with_semaphore\nimport threading\nimport asyncio\nimport traceback\nimport uuid\nimport tempfile        \n\nclass CustomSaasAPI:\n\n    def __init__(self, infer_params: Dict[str, str]) -> None:\n\n        self.api_key = infer_params[\"saas.api_key\"]\n        self.model = infer_params.get(\"saas.model\", \"gpt-3.5-turbo-1106\")\n\n        other_params = {}\n\n        if \"saas.api_base\" in infer_params:\n            other_params[\"api_base\"] = infer_params[\"saas.api_base\"]\n\n        if \"saas.api_version\" in infer_params:\n            other_params[\"api_version\"] = infer_params[\"saas.api_version\"]\n\n        if \"saas.api_type\" in infer_params:\n            other_params[\"api_type\"] = infer_params[\"saas.api_type\"]\n\n        if \"saas.base_url\" in infer_params:\n            other_params[\"base_url\"] = infer_params[\"saas.base_url\"]\n\n        if \"saas.timeout\" in infer_params:\n            other_params[\"timeout\"] = float(infer_params[\"saas.timeout\"])\n\n        self.max_retries = int(infer_params.get(\"saas.max_retries\", 10))\n\n        self.meta = {\n            \"model_deploy_type\": \"saas\",\n            \"backend\": \"saas\",\n            \"support_stream\": True,\n            \"model_name\": self.model,\n        }\n\n        self.meta[\"embedding_mode\"] = \"embedding\" in self.model.lower()\n\n        self.proxies = infer_params.get(\"saas.proxies\", None)\n        self.local_address = infer_params.get(\"saas.local_address\", \"0.0.0.0\")\n\n        if not self.proxies:\n            self.client = OpenAI(**other_params, api_key=self.api_key)\n        else:\n            self.client = OpenAI(\n                **other_params,\n                api_key=self.api_key,\n                http_client=httpx.Client(\n                    proxies=self.proxies,\n                    transport=httpx.HTTPTransport(local_address=self.local_address),\n                ),\n            )\n\n        try:\n            ray.get_actor(\"BLOCK_VLLM_STREAM_SERVER\")\n        except ValueError:\n            try:\n                ray.remote(BlockVLLMStreamServer).options(\n                    name=\"BLOCK_VLLM_STREAM_SERVER\",\n                    lifetime=\"detached\",\n                    max_concurrency=1000,\n                ).remote()\n            except Exception as e:\n                pass\n        try:\n            ray.get_actor(\"BlockBinaryStreamServer\")\n        except ValueError:\n            try:\n                ray.remote(BlockBinaryStreamServer).options(\n                    name=\"BlockBinaryStreamServer\",\n                    lifetime=\"detached\",\n                    max_concurrency=1000,\n                ).remote()\n            except Exception as e:\n                pass\n\n    # saas/proprietary\n    def get_meta(self):\n        return [self.meta]\n\n    def process_input(self, ins: Union[str, List[Dict[str, Any]], Dict[str, Any]]):\n\n        if isinstance(ins, list) or isinstance(ins, dict):\n            return ins\n\n        content = []\n        try:\n            ins_json = json.loads(ins)\n        except:\n            return ins\n\n        ## speech\n        if isinstance(ins_json, dict):\n            return ins_json\n\n        content = []\n        #     [\n        #     {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n        #     {\n        #       \"type\": \"image_url\",\n        #       \"image_url\": {\n        #         \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n        #         \"detail\": \"high\"\n        #       },\n        #     },\n        #   ],\n        for item in ins_json:\n            # for format like this: {\"image\": \"xxxxxx\", \"text\": \"What’s in this image?\",\"detail\":\"high\"}\n            # or {\"image\": \"xxxxxx\"}, {\"text\": \"What’s in this image?\"}\n            if (\"image\" in item or \"image_url\" in item) and \"type\" not in item:\n                image_data = item.get(\"image\", item.get(\"image_url\", \"\"))\n                ## \"data:image/jpeg;base64,\"\n                if not image_data.startswith(\"data:\"):\n                    image_data = \"data:image/jpeg;base64,\" + image_data\n\n                ## get the other fields except image/text/image_url\n                other_fields = {\n                    k: v\n                    for k, v in item.items()\n                    if k not in [\"image\", \"text\", \"image_url\"]\n                }\n                content.append(\n                    {\n                        \"image_url\": {\"url\": image_data, **other_fields},\n                        \"type\": \"image_url\",\n                    }\n                )            \n\n            if \"text\" in item and \"type\" not in item:\n                text_data = item[\"text\"]\n                content.append({\"text\": text_data, \"type\": \"text\"})\n\n            ## for format like this: {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n            ## {\"type\": \"image_url\", \"image_url\": {\"url\":\"\",\"detail\":\"high\"}}\n            ## this is the standard format, just return it\n            if \"type\" in item and item[\"type\"] == \"text\":\n                content.append(item)\n\n            if \"type\" in item and item[\"type\"] == \"image_url\":\n                content.append(item)\n\n        if not content:\n            return ins\n\n        return content\n\n    async def async_embed_query(self, ins: str, **kwargs):\n        resp = await asyncfy_with_semaphore(\n            lambda: self.client.embeddings.create(input=[ins], model=self.model)\n        )()\n        embedding = resp.data[0].embedding\n        usage = resp.usage\n        return (\n            embedding,\n            {\n                \"metadata\": {\n                    \"input_tokens_count\": usage.prompt_tokens,\n                    \"generated_tokens_count\": 0,\n                }\n            },\n        )\n\n    def embed_query(self, ins: str, **kwargs):\n        resp = self.client.embeddings.create(input=[ins], model=self.model)\n        embedding = resp.data[0].embedding\n        usage = resp.usage\n        return (\n            embedding,\n            {\n                \"metadata\": {\n                    \"input_tokens_count\": usage.prompt_tokens,\n                    \"generated_tokens_count\": 0,\n                }\n            },\n        )\n\n    async def async_text_to_speech(\n        self, stream: bool, ins: str, voice: str, chunk_size: int = None, **kwargs\n    ):\n        if stream:\n            server = ray.get_actor(\"BlockBinaryStreamServer\")\n            request_id = [None]\n\n            def writer():\n                try:\n                    request_id[0] = str(uuid.uuid4())\n                    with self.client.with_streaming_response.audio.speech.create(\n                        model=self.model, voice=voice, input=ins, **kwargs\n                    ) as response:\n                        for chunk in response.iter_bytes(chunk_size):\n                            input_tokens_count = 0\n                            generated_tokens_count = 0\n                            ray.get(\n                                server.add_item.remote(\n                                    request_id[0],\n                                    StreamOutputs(\n                                        outputs=[\n                                            SingleOutput(\n                                                text=chunk,\n                                                metadata=SingleOutputMeta(\n                                                    input_tokens_count=input_tokens_count,\n                                                    generated_tokens_count=generated_tokens_count,\n                                                ),\n                                            )\n                                        ]\n                                    ),\n                                )\n                            )\n                except:\n                    traceback.print_exc()\n                ray.get(server.mark_done.remote(request_id[0]))\n\n            threading.Thread(target=writer, daemon=True).start()\n\n            time_count = 10 * 100\n            while request_id[0] is None and time_count > 0:\n                time.sleep(0.01)\n                time_count -= 1\n\n            if request_id[0] is None:\n                raise Exception(\"Failed to get request id\")\n\n            def write_running():\n                return ray.get(server.add_item.remote(request_id[0], \"RUNNING\"))\n\n            await asyncio.to_thread(write_running)\n            return [\n                (\n                    \"\",\n                    {\n                        \"metadata\": {\n                            \"request_id\": request_id[0],\n                            \"stream_server\": \"BlockBinaryStreamServer\",\n                        }\n                    },\n                )\n            ]\n\n        start_time = time.monotonic()\n        with io.BytesIO() as output:\n            async with asyncfy_with_semaphore(\n                lambda: self.client.with_streaming_response.audio.speech.create(\n                    model=self.model, voice=voice, input=ins, **kwargs\n                )\n            )() as response:\n                for chunk in response.iter_bytes():\n                    output.write(chunk)\n\n            base64_audio = base64.b64encode(output.getvalue()).decode()\n            time_cost = time.monotonic() - start_time\n            return [\n                (\n                    base64_audio,\n                    {\n                        \"metadata\": {\n                            \"request_id\": \"\",\n                            \"input_tokens_count\": 0,\n                            \"generated_tokens_count\": 0,\n                            \"time_cost\": time_cost,\n                            \"first_token_time\": 0,\n                            \"speed\": 0,\n                        }\n                    },\n                )\n            ]\n\n    async def async_speech_to_text(\n        self,\n        audio: str,\n        response_format: str = \"verbose_json\",\n        timestamp_granularities: List[str] = [\"word\",\"segment\"],\n    ):               \n        # Extract audio format and base64 data\n        data_prefix = \"data:audio/\"\n        base64_prefix = \";base64,\"\n        if not audio.startswith(data_prefix):\n            raise ValueError(\"Invalid audio data format\")\n        \n        format_end = audio.index(base64_prefix)\n        audio_format = audio[len(data_prefix):format_end]\n        base64_data = audio[format_end + len(base64_prefix):]\n        \n        # Decode the base64 audio data\n        audio_data = base64.b64decode(base64_data)\n\n        # Create a temporary file with the correct extension\n        with tempfile.NamedTemporaryFile(delete=False, suffix=f'.{audio_format}') as temp_audio_file:\n            temp_audio_file.write(audio_data)\n            temp_audio_file_path = temp_audio_file.name\n\n        try:\n            start_time = time.monotonic()\n                   \n            with open(temp_audio_file_path, \"rb\") as audio_file:                \n                transcription = self.client.audio.transcriptions.create(\n                    model=self.model or \"whisper-1\",\n                    file=audio_file,\n                    response_format=response_format,\n                    timestamp_granularities=timestamp_granularities,\n                )\n            time_cost = time.monotonic() - start_time\n\n            return [\n                (\n                    json.dumps(transcription.to_dict(), ensure_ascii=True),\n                    {\n                        \"metadata\": {\n                            \"request_id\": \"\",\n                            \"input_tokens_count\": 0,\n                            \"generated_tokens_count\": 0,\n                            \"time_cost\": time_cost,\n                            \"first_token_time\": 0,\n                            \"speed\": 0,\n                        }\n                    },\n                )\n            ]\n        finally:\n            # Clean up the temporary file\n            import os\n            os.unlink(temp_audio_file_path)\n\n    def image_to_text(self, ins: str, **kwargs):\n        pass\n\n    async def async_text_to_image(\n        self, stream: bool, input: str, size: str, quality: str, n: int, **kwargs\n    ):\n        if stream:\n            raise Exception(\"Stream not supported for text to image\")\n        start_time = time.monotonic()\n        response = await asyncfy_with_semaphore(\n            lambda: self.client.images.generate(\n                model=self.model,\n                prompt=input,\n                size=size,\n                quality=quality,\n                n=1,\n                response_format=\"b64_json\",\n                **kwargs,\n            )\n        )()\n        time_cost = time.monotonic() - start_time\n        base64_image = response.data[0].b64_json\n        return [\n            (\n                base64_image,\n                {\n                    \"metadata\": {\n                        \"request_id\": \"\",\n                        \"input_tokens_count\": 0,\n                        \"generated_tokens_count\": 0,\n                        \"time_cost\": time_cost,\n                        \"first_token_time\": 0,\n                        \"speed\": 0,\n                    }\n                },\n            )\n        ]\n\n    def text_to_text(self, ins: str, **kwargs):\n        pass\n\n    async def async_stream_chat(\n        self,\n        tokenizer,\n        ins: str,\n        his: List[Dict[str, Any]] = [],\n        max_length: int = 4096,\n        top_p: float = 0.7,\n        temperature: float = 0.9,\n        **kwargs,\n    ):\n\n        model = self.model\n\n        if \"model\" in kwargs:\n            model = kwargs[\"model\"]\n\n        messages = [\n            {\"role\": message[\"role\"], \"content\": self.process_input(message[\"content\"])}\n            for message in his\n        ] + [{\"role\": \"user\", \"content\": self.process_input(ins)}]\n\n        stream = kwargs.get(\"stream\", False)\n\n        ## content = [\n        ##    \"voice\": \"alloy\",\"input\": \"Hello, World!\",response_format: \"mp3\"]\n        last_message = messages[-1][\"content\"]\n\n        if isinstance(last_message, dict) and \"voice\" in last_message:\n            voice = last_message[\"voice\"]\n            response_format = last_message.get(\"response_format\", \"mp3\")\n            chunk_size = last_message.get(\"chunk_size\", None)\n            input = last_message[\"input\"]\n            return await self.async_text_to_speech(\n                stream=stream,\n                ins=input,\n                voice=voice,\n                chunk_size=chunk_size,\n                response_format=response_format,\n            )\n\n        if isinstance(last_message, dict) and \"input\" in last_message:\n            input = last_message[\"input\"]\n            size = last_message.get(\"size\", \"1024x1024\")\n            quality = last_message.get(\"quality\", \"standard\")\n            n = last_message.get(\"n\", 1)\n            return await self.async_text_to_image(\n                stream=stream, input=input, size=size, quality=quality, n=n\n            )\n\n        if isinstance(last_message, dict) and \"audio\" in last_message:            \n            audio_data = last_message.get(\"audio\", \"\")            \n            tpe = last_message.get(\"type\",\"wav\")\n            if not audio_data.startswith(\"data:\"):\n                audio_data = f\"data:audio/${tpe};base64,\" + audio_data                \n            return await self.async_speech_to_text(audio=audio_data)\n        \n        server = ray.get_actor(\"BLOCK_VLLM_STREAM_SERVER\")\n        request_id = [None]\n\n        def writer():\n            try:\n                r = \"\"\n                response = self.client.chat.completions.create(\n                    messages=messages,\n                    model=model,\n                    stream=True,\n                    max_tokens=max_length,\n                    temperature=temperature,\n                    top_p=top_p,\n                )\n                # input_tokens_count = 0\n                # generated_tokens_count = 0\n\n                request_id[0] = str(uuid.uuid4())\n\n                for chunk in response:\n                    content = chunk.choices[0].delta.content or \"\"\n                    r += content\n                    if hasattr(chunk, \"usage\") and chunk.usage:\n                        input_tokens_count = chunk.usage.prompt_tokens\n                        generated_tokens_count = chunk.usage.completion_tokens\n                    else:\n                        input_tokens_count = 0\n                        generated_tokens_count = 0\n                    ray.get(\n                        server.add_item.remote(\n                            request_id[0],\n                            StreamOutputs(\n                                outputs=[\n                                    SingleOutput(\n                                        text=r,\n                                        metadata=SingleOutputMeta(\n                                            input_tokens_count=input_tokens_count,\n                                            generated_tokens_count=generated_tokens_count,\n                                        ),\n                                    )\n                                ]\n                            ),\n                        )\n                    )\n            except:\n                traceback.print_exc()\n            ray.get(server.mark_done.remote(request_id[0]))\n\n        if stream:\n            threading.Thread(target=writer, daemon=True).start()\n\n            time_count = 60 * 100\n            while request_id[0] is None and time_count > 0:\n                time.sleep(0.01)\n                time_count -= 1\n\n            if request_id[0] is None:\n                raise Exception(\"Failed to get request id\")\n\n            def write_running():\n                return ray.get(server.add_item.remote(request_id[0], \"RUNNING\"))\n\n            await asyncio.to_thread(write_running)\n            return [\n                (\n                    \"\",\n                    {\n                        \"metadata\": {\n                            \"request_id\": request_id[0],\n                            \"stream_server\": \"BLOCK_VLLM_STREAM_SERVER\",\n                        }\n                    },\n                )\n            ]\n        else:\n            try:\n                start_time = time.monotonic()\n                response = await asyncfy_with_semaphore(\n                    lambda: self.client.chat.completions.create(\n                        messages=messages,\n                        model=model,\n                        max_tokens=max_length,\n                        temperature=temperature,\n                        top_p=top_p,\n                    )\n                )()\n\n                generated_text = response.choices[0].message.content\n                generated_tokens_count = response.usage.completion_tokens\n                input_tokens_count = response.usage.prompt_tokens\n                time_cost = time.monotonic() - start_time\n                return [\n                    (\n                        generated_text,\n                        {\n                            \"metadata\": {\n                                \"request_id\": response.id,\n                                \"input_tokens_count\": input_tokens_count,\n                                \"generated_tokens_count\": generated_tokens_count,\n                                \"time_cost\": time_cost,\n                                \"first_token_time\": 0,\n                                \"speed\": float(generated_tokens_count) / time_cost,\n                            }\n                        },\n                    )\n                ]\n            except Exception as e:\n                print(f\"Error: {e}\")\n                raise e\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/whisper/__init__.py\nimport whisper\nfrom typing import Dict, List, Tuple\nfrom byzerllm.utils.types import StopSequencesCriteria\n\nfrom typing import Dict, Any, List, Union\nfrom pyjava.storage import streaming_tar as STar\nimport json\nimport base64\nimport tempfile\nimport time\n\n\ndef get_meta(self):\n    return [\n        {\n            \"model_deploy_type\": \"proprietary\",\n            \"backend\": \"transformers\",\n            \"message_format\": True\n        }\n    ]\n\n\ndef process_input(ins: Union[str, List[Dict[str, Any]], Dict[str, Any]]):\n    # print(ins) \n    if isinstance(ins, list) or isinstance(ins, dict):\n        return ins\n\n    content = []\n    try:\n        ins_json = json.loads(ins)\n    except:\n        return ins\n\n    ## speech\n    if isinstance(ins_json, dict):\n        return ins_json\n\n    content = []\n    #     [\n    #     {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n    #     {\n    #       \"type\": \"image_url\",\n    #       \"image_url\": {\n    #         \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n    #         \"detail\": \"high\"\n    #       },\n    #     },\n    #   ],\n    for item in ins_json:\n        # for format like this: {\"image\": \"xxxxxx\", \"text\": \"What’s in this image?\",\"detail\":\"high\"}\n        # or {\"image\": \"xxxxxx\"}, {\"text\": \"What’s in this image?\"}\n        if (\"image\" in item or \"image_url\" in item) and \"type\" not in item:\n            image_data = item.get(\"image\", item.get(\"image_url\", \"\"))\n            ## \"data:image/jpeg;base64,\"\n            if not image_data.startswith(\"data:\"):\n                image_data = \"data:image/jpeg;base64,\" + image_data\n\n            ## get the other fields except image/text/image_url\n            other_fields = {\n                k: v for k, v in item.items() if k not in [\"image\", \"text\", \"image_url\"]\n            }\n            content.append(\n                {\n                    \"image_url\": {\"url\": image_data, **other_fields},\n                    \"type\": \"image_url\",\n                }\n            )\n\n        if \"text\" in item and \"type\" not in item:\n            text_data = item[\"text\"]\n            content.append({\"text\": text_data, \"type\": \"text\"})\n\n        ## for format like this: {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n        ## {\"type\": \"image_url\", \"image_url\": {\"url\":\"\",\"detail\":\"high\"}}\n        ## this is the standard format, just return it\n        if \"type\" in item and item[\"type\"] == \"text\":\n            content.append(item)\n\n        if \"type\" in item and item[\"type\"] == \"image_url\":\n            content.append(item)\n\n    if not content:\n        return ins\n\n    return content\n\n\ndef stream_chat(\n    self,\n    tokenizer,\n    ins: str,\n    his: List[Dict[str, str]] = [],\n    max_length: int = 4090,\n    top_p: float = 0.95,\n    temperature: float = 0.1,\n    **kwargs,\n):\n    messages = [\n        {\"role\": message[\"role\"], \"content\": process_input(message[\"content\"])}\n        for message in his\n    ] + [{\"role\": \"user\", \"content\": process_input(ins)}]\n\n    audio_input = messages[-1][\"content\"]\n\n    audio_data = audio_input.get(\"audio\", \"\")\n    audio = audio_input[\"audio\"]\n    response_format = audio_input.get(\"response_format\", \"json\")\n    timestamp_granularities = audio_input.get(\n        \"timestamp_granularities\", [\"word\", \"segment\"]\n    )\n    data_prefix = \"data:audio/\"\n    base64_prefix = \";base64,\"\n    if not audio.startswith(data_prefix):\n        raise ValueError(\"Invalid audio data format\")\n\n    format_end = audio.index(base64_prefix)\n    audio_format = audio[len(data_prefix) : format_end]\n    base64_data = audio[format_end + len(base64_prefix) :]\n\n    # Decode the base64 audio data\n    audio_data = base64.b64decode(base64_data)\n\n    # Create a temporary file with the correct extension\n    with tempfile.NamedTemporaryFile(\n        delete=False, suffix=f\".{audio_format}\"\n    ) as temp_audio_file:\n        temp_audio_file.write(audio_data)\n        temp_audio_file_path = temp_audio_file.name\n\n    try:\n        start_time = time.monotonic()\n        word_timestamps = \"word\" in timestamp_granularities\n        \n        result = self.transcribe(temp_audio_file_path, word_timestamps=word_timestamps)        \n            \n        time_cost = time.monotonic() - start_time\n        return [\n            (\n                json.dumps(result, ensure_ascii=False),\n                {\n                    \"metadata\": {\n                        \"request_id\": \"\",\n                        \"input_tokens_count\": 0,\n                        \"generated_tokens_count\": 0,\n                        \"time_cost\": time_cost,\n                        \"first_token_time\": 0,\n                        \"speed\": 0,\n                    }\n                },\n            )\n        ]\n    finally:\n        # Clean up the temporary file\n        import os\n\n        os.unlink(temp_audio_file_path)\n\n\ndef init_model(\n    model_dir, infer_params: Dict[str, str] = {}, sys_conf: Dict[str, str] = {}\n):\n    model = whisper.load_model(\"large\", download_root=model_dir)\n    import types\n\n    model.stream_chat = types.MethodType(stream_chat, model)\n    model.get_meta = types.MethodType(get_meta, model)\n    return (model, None)\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/sensevoice/__init__.py\nimport json\nimport base64\nimport tempfile\nimport time\nfrom typing import Dict, List, Union, Any\nfrom funasr import AutoModel\n\nemo_dict = {\n    \"<|HAPPY|>\": \"😊\",\n    \"<|SAD|>\": \"😔\",\n    \"<|ANGRY|>\": \"😡\",\n    \"<|NEUTRAL|>\": \"\",\n    \"<|FEARFUL|>\": \"😰\",\n    \"<|DISGUSTED|>\": \"🤢\",\n    \"<|SURPRISED|>\": \"😮\",\n}\n\nevent_dict = {\n    \"<|BGM|>\": \"🎼\",\n    \"<|Speech|>\": \"\",\n    \"<|Applause|>\": \"👏\",\n    \"<|Laughter|>\": \"��\",\n    \"<|Cry|>\": \"😭\",\n    \"<|Sneeze|>\": \"🤧\",\n    \"<|Breath|>\": \"\",\n    \"<|Cough|>\": \"🤧\",\n}\n\nemoji_dict = {\n    \"<|nospeech|><|Event_UNK|>\": \"❓\",\n    \"<|zh|>\": \"\",\n    \"<|en|>\": \"\",\n    \"<|yue|>\": \"\",\n    \"<|ja|>\": \"\",\n    \"<|ko|>\": \"\",\n    \"<|nospeech|>\": \"\",\n    \"<|HAPPY|>\": \"😊\",\n    \"<|SAD|>\": \"😔\",\n    \"<|ANGRY|>\": \"😡\",\n    \"<|NEUTRAL|>\": \"\",\n    \"<|BGM|>\": \"🎼\",\n    \"<|Speech|>\": \"\",\n    \"<|Applause|>\": \"👏\",\n    \"<|Laughter|>\": \"😀\",\n    \"<|FEARFUL|>\": \"😰\",\n    \"<|DISGUSTED|>\": \"🤢\",\n    \"<|SURPRISED|>\": \"😮\",\n    \"<|Cry|>\": \"😭\",\n    \"<|EMO_UNKNOWN|>\": \"\",\n    \"<|Sneeze|>\": \"🤧\",\n    \"<|Breath|>\": \"\",\n    \"<|Cough|>\": \"😷\",\n    \"<|Sing|>\": \"\",\n    \"<|Speech_Noise|>\": \"\",\n    \"<|withitn|>\": \"\",\n    \"<|woitn|>\": \"\",\n    \"<|GBG|>\": \"\",\n    \"<|Event_UNK|>\": \"\",\n}\n\nlang_dict = {\n    \"<|zh|>\": \"<|lang|>\",\n    \"<|en|>\": \"<|lang|>\",\n    \"<|yue|>\": \"<|lang|>\",\n    \"<|ja|>\": \"<|lang|>\",\n    \"<|ko|>\": \"<|lang|>\",\n    \"<|nospeech|>\": \"<|lang|>\",\n}\n\nemo_set = {\"😊\", \"😔\", \"😡\", \"😰\", \"🤢\", \"😮\"}\nevent_set = {\"🎼\", \"👏\", \"😀\", \"😭\", \"🤧\", \"😷\"}\n\n\ndef format_str(s):\n    for sptk in emoji_dict:\n        s = s.replace(sptk, emoji_dict[sptk])\n    return s\n\n\ndef format_str_v2(s):\n    sptk_dict = {sptk: s.count(sptk) for sptk in emoji_dict}\n    s = \"\".join(c for c in s if c not in emoji_dict.values())\n    emo = max(emo_dict, key=lambda e: sptk_dict[e])\n    event = \"\".join(event_dict[e] for e in event_dict if sptk_dict[e] > 0)\n    s = event + s + emo_dict[emo]\n    for emoji in emo_set.union(event_set):\n        s = s.replace(\" \" + emoji, emoji).replace(emoji + \" \", emoji)\n    return s.strip()\n\n\ndef format_str_v3(s):\n    def get_emo(s):\n        return s[-1] if s[-1] in emo_set else None\n\n    def get_event(s):\n        return s[0] if s[0] in event_set else None\n\n    s = s.replace(\"<|nospeech|><|Event_UNK|>\", \"❓\")\n    for lang in lang_dict:\n        s = s.replace(lang, \"<|lang|>\")\n    s_list = [format_str_v2(s_i).strip() for s_i in s.split(\"<|lang|>\")]\n    new_s = \" \" + s_list[0]\n    cur_ent_event = get_event(new_s)\n    for i in range(1, len(s_list)):\n        if len(s_list[i]) == 0:\n            continue\n        if get_event(s_list[i]) == cur_ent_event and get_event(s_list[i]) is not None:\n            s_list[i] = s_list[i][1:]\n        cur_ent_event = get_event(s_list[i])\n        if get_emo(s_list[i]) is not None and get_emo(s_list[i]) == get_emo(new_s):\n            new_s = new_s[:-1]\n        new_s += s_list[i].strip()\n    new_s = new_s.replace(\"The.\", \" \")\n    return new_s.strip()\n\n\ndef get_meta(self):\n    return [\n        {\n            \"model_deploy_type\": \"proprietary\",\n            \"backend\": \"funasr\",\n            \"message_format\": True,\n        }\n    ]\n\n\ndef process_input(ins: Union[str, List[Dict[str, Any]], Dict[str, Any]]):\n    if isinstance(ins, list) or isinstance(ins, dict):\n        return ins\n\n    try:\n        ins_json = json.loads(ins)\n    except:\n        return ins\n\n    if isinstance(ins_json, dict):\n        return ins_json\n\n    content = []\n    for item in ins_json:\n        if \"audio\" in item and \"type\" not in item:\n            audio_data = item.get(\"audio\", \"\")\n            if not audio_data.startswith(\"data:\"):\n                audio_data = f\"data:audio/wav;base64,{audio_data}\"\n            content.append({\"audio\": audio_data, \"type\": \"audio\"})\n\n        if \"text\" in item and \"type\" not in item:\n            content.append({\"text\": item[\"text\"], \"type\": \"text\"})\n\n        if \"type\" in item:\n            content.append(item)\n\n    return content if content else ins\n\n\ndef stream_chat(\n    self,\n    tokenizer,\n    ins: str,\n    his: List[Dict[str, str]] = [],\n    max_length: int = 4090,\n    top_p: float = 0.95,\n    temperature: float = 0.1,\n    **kwargs,\n):\n    messages = [\n        {\"role\": message[\"role\"], \"content\": process_input(message[\"content\"])}\n        for message in his\n    ] + [{\"role\": \"user\", \"content\": process_input(ins)}]\n\n    audio_input = messages[-1][\"content\"]\n\n    audio = audio_input[\"audio\"]\n    data_prefix = \"data:audio/\"\n    base64_prefix = \";base64,\"\n    if not audio.startswith(data_prefix):\n        raise ValueError(\"Invalid audio data format\")\n\n    format_end = audio.index(base64_prefix)\n    audio_format = audio[len(data_prefix) : format_end]\n    base64_data = audio[format_end + len(base64_prefix) :]\n\n    audio_data = base64.b64decode(base64_data)\n\n    with tempfile.NamedTemporaryFile(\n        delete=False, suffix=f\".{audio_format}\"\n    ) as temp_audio_file:\n        temp_audio_file.write(audio_data)\n        temp_audio_file_path = temp_audio_file.name\n\n    try:\n        start_time = time.monotonic()\n\n        result = self.generate(input=temp_audio_file_path, cache={})\n        print(result)\n\n        time_cost = time.monotonic() - start_time\n\n        formatted_result = format_str_v3(result[0][\"text\"])\n\n        return [\n            (\n                json.dumps({\"text\": formatted_result}, ensure_ascii=False),\n                {\n                    \"metadata\": {\n                        \"request_id\": \"\",\n                        \"input_tokens_count\": 0,\n                        \"generated_tokens_count\": 0,\n                        \"time_cost\": time_cost,\n                        \"first_token_time\": 0,\n                        \"speed\": 0,\n                    }\n                },\n            )\n        ]\n    finally:\n        import os\n\n        os.unlink(temp_audio_file_path)\n\n\ndef init_model(\n    model_dir, infer_params: Dict[str, str] = {}, sys_conf: Dict[str, str] = {}\n):\n    model_kwargs = {\n        \"model\": model_dir,\n        \"trust_remote_code\": True,\n    }\n\n    if \"vad_model\" in infer_params:\n        model_kwargs[\"vad_model\"] = infer_params[\"vad_model\"]\n        model_kwargs[\"vad_kwargs\"] = {\"max_single_segment_time\": 30000}\n\n    model = AutoModel(**model_kwargs)\n\n    import types\n\n    model.stream_chat = types.MethodType(stream_chat, model)\n    model.get_meta = types.MethodType(get_meta, model)\n\n    return (model, None)\n\n\n# Helper functions from web_ui.py\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/notebooks/test_sensevoice.py\nimport sys\nimport os\n\n# Add the project root to the Python path\nsys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\")))\n\nfrom src.byzerllm.sensevoice import init_model, format_str_v3\nimport base64\nimport json\n\n\ndef main():\n    # Initialize the model\n    model_dir = \"/Users/allwefantasy/models/SenseVoiceSmall\"\n    model, _ = init_model(model_dir)\n\n    # Prepare test audio\n    with open(\n        \"/Users/allwefantasy/models/SenseVoiceSmall/example/zh.mp3\", \"rb\"\n    ) as audio_file:\n        audio_data = audio_file.read()\n        base64_audio = base64.b64encode(audio_data).decode(\"utf-8\")        \n        # Prepare input in the expected format\n        test_input = json.dumps(\n            {\n                            \"type\": \"audio\",\n                            \"audio\": f\"data:audio/wav;base64,{base64_audio}\",\n                        }\n        )\n\n        # Call stream_chat\n        result = model.stream_chat(tokenizer=None, ins = test_input)\n\n        # Process and print the result\n        if result and len(result) > 0:\n            output, metadata = result[0]\n            parsed_output = json.loads(output)\n            formatted_text = parsed_output.get(\"text\", \"\")\n            print(\"Transcription:\", formatted_text)\n            print(\"Metadata:\", metadata)\n        else:\n            print(\"No result returned from the model.\")\n\n\nif __name__ == \"__main__\":\n    main()\n\n\n     \n\n下面是用户的需求：\n\n@/Users/allwefantasy/projects/byzer-llm/src/byzerllm/sensevoice/__init__.py 中model = AutoModel(\n    model=model_dir,\n    trust_remote_code=True,\n    remote_code=\"./model.py\",    \n    vad_model=\"fsmn-vad\",\n    vad_kwargs={\"max_single_segment_time\": 30000},\n    device=\"cuda:0\",\n)  我们需要参考上面的代码,支持 vad_kwargs 参数.可以通过  infer_params 参数中的 vad_kwargs.xxxx 来获取,比如 max_single_segment_time 就是 vad_kwargs.max_single_segment_time 注意,传递过来的时候都是字符串,需要自动检测下类型\n"}, {"role": "assistant", "content": "Certainly! Let's modify the `init_model` function in `/Users/allwefantasy/projects/byzer-llm/src/byzerllm/sensevoice/__init__.py` to support the `vad_kwargs` parameter and handle the type conversion. Here's the updated code:\n\n```python\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/sensevoice/__init__.py\n<<<<<<< SEARCH\ndef init_model(\n    model_dir, infer_params: Dict[str, str] = {}, sys_conf: Dict[str, str] = {}\n):\n    model_kwargs = {\n        \"model\": model_dir,\n        \"trust_remote_code\": True,\n    }\n\n    if \"vad_model\" in infer_params:\n        model_kwargs[\"vad_model\"] = infer_params[\"vad_model\"]\n        model_kwargs[\"vad_kwargs\"] = {\"max_single_segment_time\": 30000}\n\n    model = AutoModel(**model_kwargs)\n=======\ndef init_model(\n    model_dir, infer_params: Dict[str, str] = {}, sys_conf: Dict[str, str] = {}\n):\n    model_kwargs = {\n        \"model\": model_dir,\n        \"trust_remote_code\": True,\n    }\n\n    if \"remote_code\" in infer_params:\n        model_kwargs[\"remote_code\"] = infer_params[\"remote_code\"]\n\n    if \"vad_model\" in infer_params:\n        model_kwargs[\"vad_model\"] = infer_params[\"vad_model\"]\n        vad_kwargs = {}\n        for key, value in infer_params.items():\n            if key.startswith(\"vad_kwargs.\"):\n                param_name = key.split(\".\", 1)[1]\n                # Attempt to convert the value to int or float if possible\n                try:\n                    vad_kwargs[param_name] = int(value)\n                except ValueError:\n                    try:\n                        vad_kwargs[param_name] = float(value)\n                    except ValueError:\n                        vad_kwargs[param_name] = value  # Keep as string if not a number\n        model_kwargs[\"vad_kwargs\"] = vad_kwargs\n\n    if \"device\" in infer_params:\n        model_kwargs[\"device\"] = infer_params[\"device\"]\n\n    model = AutoModel(**model_kwargs)\n>>>>>>> REPLACE\n```\n\nThis modification does the following:\n\n1. We now check for a `remote_code` parameter in `infer_params` and add it to `model_kwargs` if present.\n\n2. We create a `vad_kwargs` dictionary to store all parameters that start with \"vad_kwargs.\".\n\n3. For each parameter starting with \"vad_kwargs.\", we:\n   - Extract the actual parameter name (everything after \"vad_kwargs.\")\n   - Attempt to convert the value to an integer first\n   - If that fails, attempt to convert it to a float\n   - If both conversions fail, keep it as a string\n\n4. We add the resulting `vad_kwargs` dictionary to `model_kwargs`.\n\n5. We also check for a `device` parameter in `infer_params` and add it to `model_kwargs` if present.\n\nThis implementation allows users to pass any number of VAD kwargs with automatic type conversion, and also supports specifying the device and remote code path. For example, users can now pass parameters like:\n\n- `vad_kwargs.max_single_segment_time`: \"30000\" (will be converted to int)\n- `vad_kwargs.threshold`: \"0.5\" (will be converted to float)\n- `vad_kwargs.some_string_param`: \"value\" (will remain a string)\n\nThe function will automatically detect and convert these parameters to the appropriate types when initializing the model."}], "model": "human", "yaml_file": "actions/032_chat_action.yml"}
{"instruction": "参考 @README-CN.md 里介绍的 qwen voice 的使用方式,根据 @/Users/allwefantasy/projects/byzer-llm/src/byzerllm/whisper/__init__.py 中的方法,实现一个新的 qwen_voice/__init__.py 文件. 注意 @README_CN.md 里使用了 audio_url 参数, 此参数我们允许用户填写base64格式的音频数据.此外,修改 process_input 方法类支持图片那样支持格式的定义", "conversations": [{"role": "user", "content": "\n如果你需要生成代码，对于每个需要更改的文件,你需要按 *SEARCH/REPLACE block* 的格式进行生成。\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The opening fence and code language, eg: ```python\n2. The file path alone on a line, starting with \"##File:\" and verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n*SEARCH/REPLACE* blocks will replace *all* matching occurrences.\nInclude enough lines to make the SEARCH blocks unique.\n\nInclude *ALL* the code being searched and replaced!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n下面我们来看一个例子：\n\n当前项目目录结构：\n1. 项目根目录： /tmp/projects/mathweb\n2. 项目子目录/文件列表(类似tree 命令输出)\nflask/\n    app.py\n    templates/\n        index.html\n    static/\n        style.css\n\n用户需求： Change get_factorial() to use math.factorial\n\n回答： To make this change we need to modify `/tmp/projects/mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n用户需求： Refactor hello() into its own file.\n\n回答：To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/hello.py\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/main.py\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n现在让我们开始一个新的任务:\n\n当前项目目录结构：\n1. 项目根目录： /Users/allwefantasy/projects/byzer-llm\n2. 项目子目录/文件列表(类似tree 命令输出)：\nsetup.py\ntests/\n    test_emb_rerank.py\n    test_deepseek_code.py\n    test_prompt.py\n    test_model_meta.py\n    __init__.py\n    test_transformers_deploy.py\n    test_data_analysis.py\n    test_yi.py\n    test_deploy.py\n    functions_suite.py\n    test_template.py\n    auto/\n        test_backend_llama_cpp.py\n        __init__.py\nsetup-machine/\n    convert_safetensor.py\nnotebooks/\n    test.py\n    test_sensevoice.py\nsrc/\n    byzerllm/\n        lang.py\n        store.py\n        version.py\n        log.py\n        __init__.py\n        types.py\n        easy_byzerllm_command.py\n        byzerllm_command.py\n        command_args.py\n        bark/\n            generation.py\n            __init__.py\n            model.py\n            api.py\n            model_fine.py\n            bark_voice.py\n        moss/\n            finetune_moss.py\n            __init__.py\n            moss_inference.py\n            models/\n                configuration_moss.py\n                quantization.py\n                custom_autotune.py\n                __init__.py\n                modeling_moss.py\n                tokenization_moss.py\n        llama2/\n            __init__.py\n        records/\n            __init__.py\n        visualglm/\n            __init__.py\n        auto/\n            __init__.py\n            backend_llama_cpp.py\n            backend_vllm.py\n            backend_ds.py\n        alpha_moss/\n            __init__.py\n        processor/\n            __init__.py\n        sensevoice/\n            __init__.py\n        qwen/\n            __init__.py\n        utils/\n            json_repaire.py\n            object_store_ref_util.py\n            __init__.py\n            types.py\n            tokenizer.py\n            openai_utils.py\n            ray_utils.py\n            text_generator.py\n            emb.py\n            connect_ray.py\n            testing.py\n            langutil.py\n            nontext.py\n            fulltune/\n                deepspeed_trainner.py\n                __init__.py\n                launch.py\n                trainner.py\n                base_model/\n                    __init__.py\n                    configuration_baichuan.py\n                    modeling_baichuan.py\n                pretrain/\n                    __init__.py\n                    convert_to_transformers.py\n            metrics/\n                __init__.py\n            config/\n                __init__.py\n            retrieval/\n                rest.py\n                __init__.py\n                udf.py\n            sft/\n                argument.py\n                collator.py\n                qlora.py\n                __init__.py\n                model.py\n                dataset.py\n                loss.py\n                merge_lora.py\n                trainer.py\n            client/\n                message_utils.py\n                math_utils.py\n                byzerllm_client.py\n                parallel_utils.py\n                __init__.py\n                types.py\n                code_utils.py\n                img_utils.py\n                entrypoints/\n                    __init__.py\n                    openai/\n                        protocol.py\n                        __init__.py\n                        serve.py\n                        serving_chat.py\n                        api_server.py\n                        serving_engine.py\n                        serving_completion.py\n                        tool.py\n        chatglm2/\n            __init__.py\n        baichuan/\n            __init__.py\n        m3e/\n            __init__.py\n        stable_diffusion/\n            config.py\n            __init__.py\n            logger.py\n            shared.py\n            model.py\n            utils.py\n            images.py\n            diffusion/\n                __init__.py\n                embeddings.py\n                utils.py\n                networks/\n                    lyco.py\n                    __init__.py\n                upscalers/\n                    __init__.py\n                    multidiffusion.py\n                    samplers.py\n                piplines/\n                    __init__.py\n                    lpw.py\n                    diffusers.py\n            lib/\n                __init__.py\n                diffusers/\n                    __init__.py\n                    scheduler.py\n            api/\n                __init__.py\n                plugin.py\n                models/\n                    diffusion.py\n                    __init__.py\n                diffusion/\n                    __init__.py\n                    pipelines/\n                        __init__.py\n                        diffusers.py\n                events/\n                    generation.py\n                    __init__.py\n        bge/\n            __init__.py\n        bge_rerank/\n            __init__.py\n        apps/\n            client.py\n            qa_strategy.py\n            __init__.py\n            builder.py\n            vector_db.py\n            utils.py\n            qa.py\n            byzer_storage/\n                memory_model_based.py\n                command.py\n                env.py\n                train_pt.py\n                train_sft.py\n                __init__.py\n                train_dpo.py\n                simple_api.py\n                generate_sft_data.py\n            byzer_sql/\n                __init__.py\n            llama_index/\n                byzerai_vectordb.py\n                byzerai.py\n                byzerai_embedding.py\n                __init__.py\n                collection_manager.py\n                byzerai_index_store.py\n                byzerai_docstore.py\n                simple_retrieval.py\n                byzerai_kvstore.py\n            agent/\n                conversable_agent.py\n                registry.py\n                groupchat.py\n                user_proxy_agent.py\n                __init__.py\n                agent.py\n                extensions/\n                    simple_retrieval_client.py\n                    planner.py\n                    python_codesandbox_agent.py\n                    preview_file_agent.py\n                    llama_index_retrieval_agent.py\n                    visualization_agent.py\n                    assistant_agent.py\n                    load_data_agent.py\n                    retrieval_agent.py\n                    output_agent.py\n                    __init__.py\n                    byzer_engine_agent.py\n                    subquestion_agent.py\n                    sql_reviewer_agent.py\n                    spark_sql_agent.py\n                    rhetorical_agent.py\n                    data_analysis.py\n                    data_analysis_pipeline_agent.py\n                    common_agent.py\n                    query_rewrite/\n                        time.py\n                        rhetorical.py\n                        __init__.py\n                        condition.py\n                        context.py\n                store/\n                    memory_store.py\n                    __init__.py\n                    stores.py\n        saas/\n            __init__.py\n            chatglm/\n                __init__.py\n            sparkdesk/\n                __init__.py\n            gemini/\n                __init__.py\n            azure_openai/\n                __init__.py\n            azure/\n                __init__.py\n            qianfan/\n                __init__.py\n            claude/\n                __init__.py\n            qianwen/\n                __init__.py\n            volcengine/\n                __init__.py\n            minimax/\n                __init__.py\n            baichuan/\n                __init__.py\n            qianwen_vl/\n                __init__.py\n            official_openai/\n                __init__.py\n            zhipu/\n                __init__.py\n            aws_bedrock/\n                __init__.py\n            openai/\n                __init__.py\n        falcon/\n            __init__.py\n        chatglm6b/\n            trainer_seq2seq.py\n            arguments.py\n            __init__.py\n            finetune.py\n            tunning/\n                __init__.py\n                infer.py\n                finetune.py\n                utils/\n                    config.py\n                    __init__.py\n                    other.py\n                    common.py\n                    seq2seq.py\n        starcode/\n            __init__.py\n        qwen_vl_chat/\n            __init__.py\n        zephyr/\n            __init__.py\n        dolly/\n            generate.py\n            __init__.py\n            consts.py\n            dolly_inference.py\n            trainer.py\n        llama/\n            __init__.py\n        whisper/\n            whisper_inference.py\n            __init__.py\n下面是一些文件路径以及每个文件对应的源码：\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/saas/official_openai/__init__.py\nimport time\nfrom typing import List, Tuple, Dict, Any, Union\nimport httpx\nfrom openai import OpenAI\nimport base64\nimport io\nimport json\nimport ray\nfrom byzerllm.utils.types import (\n    BlockVLLMStreamServer,\n    StreamOutputs,\n    SingleOutput,\n    SingleOutputMeta,\n    BlockBinaryStreamServer,\n)\nfrom byzerllm.utils.langutil import asyncfy_with_semaphore\nimport threading\nimport asyncio\nimport traceback\nimport uuid\nimport tempfile        \n\nclass CustomSaasAPI:\n\n    def __init__(self, infer_params: Dict[str, str]) -> None:\n\n        self.api_key = infer_params[\"saas.api_key\"]\n        self.model = infer_params.get(\"saas.model\", \"gpt-3.5-turbo-1106\")\n\n        other_params = {}\n\n        if \"saas.api_base\" in infer_params:\n            other_params[\"api_base\"] = infer_params[\"saas.api_base\"]\n\n        if \"saas.api_version\" in infer_params:\n            other_params[\"api_version\"] = infer_params[\"saas.api_version\"]\n\n        if \"saas.api_type\" in infer_params:\n            other_params[\"api_type\"] = infer_params[\"saas.api_type\"]\n\n        if \"saas.base_url\" in infer_params:\n            other_params[\"base_url\"] = infer_params[\"saas.base_url\"]\n\n        if \"saas.timeout\" in infer_params:\n            other_params[\"timeout\"] = float(infer_params[\"saas.timeout\"])\n\n        self.max_retries = int(infer_params.get(\"saas.max_retries\", 10))\n\n        self.meta = {\n            \"model_deploy_type\": \"saas\",\n            \"backend\": \"saas\",\n            \"support_stream\": True,\n            \"model_name\": self.model,\n        }\n\n        self.meta[\"embedding_mode\"] = \"embedding\" in self.model.lower()\n\n        self.proxies = infer_params.get(\"saas.proxies\", None)\n        self.local_address = infer_params.get(\"saas.local_address\", \"0.0.0.0\")\n\n        if not self.proxies:\n            self.client = OpenAI(**other_params, api_key=self.api_key)\n        else:\n            self.client = OpenAI(\n                **other_params,\n                api_key=self.api_key,\n                http_client=httpx.Client(\n                    proxies=self.proxies,\n                    transport=httpx.HTTPTransport(local_address=self.local_address),\n                ),\n            )\n\n        try:\n            ray.get_actor(\"BLOCK_VLLM_STREAM_SERVER\")\n        except ValueError:\n            try:\n                ray.remote(BlockVLLMStreamServer).options(\n                    name=\"BLOCK_VLLM_STREAM_SERVER\",\n                    lifetime=\"detached\",\n                    max_concurrency=1000,\n                ).remote()\n            except Exception as e:\n                pass\n        try:\n            ray.get_actor(\"BlockBinaryStreamServer\")\n        except ValueError:\n            try:\n                ray.remote(BlockBinaryStreamServer).options(\n                    name=\"BlockBinaryStreamServer\",\n                    lifetime=\"detached\",\n                    max_concurrency=1000,\n                ).remote()\n            except Exception as e:\n                pass\n\n    # saas/proprietary\n    def get_meta(self):\n        return [self.meta]\n\n    def process_input(self, ins: Union[str, List[Dict[str, Any]], Dict[str, Any]]):\n\n        if isinstance(ins, list) or isinstance(ins, dict):\n            return ins\n\n        content = []\n        try:\n            ins_json = json.loads(ins)\n        except:\n            return ins\n\n        ## speech\n        if isinstance(ins_json, dict):\n            return ins_json\n\n        content = []\n        #     [\n        #     {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n        #     {\n        #       \"type\": \"image_url\",\n        #       \"image_url\": {\n        #         \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n        #         \"detail\": \"high\"\n        #       },\n        #     },\n        #   ],\n        for item in ins_json:\n            # for format like this: {\"image\": \"xxxxxx\", \"text\": \"What’s in this image?\",\"detail\":\"high\"}\n            # or {\"image\": \"xxxxxx\"}, {\"text\": \"What’s in this image?\"}\n            if (\"image\" in item or \"image_url\" in item) and \"type\" not in item:\n                image_data = item.get(\"image\", item.get(\"image_url\", \"\"))\n                ## \"data:image/jpeg;base64,\"\n                if not image_data.startswith(\"data:\"):\n                    image_data = \"data:image/jpeg;base64,\" + image_data\n\n                ## get the other fields except image/text/image_url\n                other_fields = {\n                    k: v\n                    for k, v in item.items()\n                    if k not in [\"image\", \"text\", \"image_url\"]\n                }\n                content.append(\n                    {\n                        \"image_url\": {\"url\": image_data, **other_fields},\n                        \"type\": \"image_url\",\n                    }\n                )            \n\n            if \"text\" in item and \"type\" not in item:\n                text_data = item[\"text\"]\n                content.append({\"text\": text_data, \"type\": \"text\"})\n\n            ## for format like this: {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n            ## {\"type\": \"image_url\", \"image_url\": {\"url\":\"\",\"detail\":\"high\"}}\n            ## this is the standard format, just return it\n            if \"type\" in item and item[\"type\"] == \"text\":\n                content.append(item)\n\n            if \"type\" in item and item[\"type\"] == \"image_url\":\n                content.append(item)\n\n        if not content:\n            return ins\n\n        return content\n\n    async def async_embed_query(self, ins: str, **kwargs):\n        resp = await asyncfy_with_semaphore(\n            lambda: self.client.embeddings.create(input=[ins], model=self.model)\n        )()\n        embedding = resp.data[0].embedding\n        usage = resp.usage\n        return (\n            embedding,\n            {\n                \"metadata\": {\n                    \"input_tokens_count\": usage.prompt_tokens,\n                    \"generated_tokens_count\": 0,\n                }\n            },\n        )\n\n    def embed_query(self, ins: str, **kwargs):\n        resp = self.client.embeddings.create(input=[ins], model=self.model)\n        embedding = resp.data[0].embedding\n        usage = resp.usage\n        return (\n            embedding,\n            {\n                \"metadata\": {\n                    \"input_tokens_count\": usage.prompt_tokens,\n                    \"generated_tokens_count\": 0,\n                }\n            },\n        )\n\n    async def async_text_to_speech(\n        self, stream: bool, ins: str, voice: str, chunk_size: int = None, **kwargs\n    ):\n        if stream:\n            server = ray.get_actor(\"BlockBinaryStreamServer\")\n            request_id = [None]\n\n            def writer():\n                try:\n                    request_id[0] = str(uuid.uuid4())\n                    with self.client.with_streaming_response.audio.speech.create(\n                        model=self.model, voice=voice, input=ins, **kwargs\n                    ) as response:\n                        for chunk in response.iter_bytes(chunk_size):\n                            input_tokens_count = 0\n                            generated_tokens_count = 0\n                            ray.get(\n                                server.add_item.remote(\n                                    request_id[0],\n                                    StreamOutputs(\n                                        outputs=[\n                                            SingleOutput(\n                                                text=chunk,\n                                                metadata=SingleOutputMeta(\n                                                    input_tokens_count=input_tokens_count,\n                                                    generated_tokens_count=generated_tokens_count,\n                                                ),\n                                            )\n                                        ]\n                                    ),\n                                )\n                            )\n                except:\n                    traceback.print_exc()\n                ray.get(server.mark_done.remote(request_id[0]))\n\n            threading.Thread(target=writer, daemon=True).start()\n\n            time_count = 10 * 100\n            while request_id[0] is None and time_count > 0:\n                time.sleep(0.01)\n                time_count -= 1\n\n            if request_id[0] is None:\n                raise Exception(\"Failed to get request id\")\n\n            def write_running():\n                return ray.get(server.add_item.remote(request_id[0], \"RUNNING\"))\n\n            await asyncio.to_thread(write_running)\n            return [\n                (\n                    \"\",\n                    {\n                        \"metadata\": {\n                            \"request_id\": request_id[0],\n                            \"stream_server\": \"BlockBinaryStreamServer\",\n                        }\n                    },\n                )\n            ]\n\n        start_time = time.monotonic()\n        with io.BytesIO() as output:\n            async with asyncfy_with_semaphore(\n                lambda: self.client.with_streaming_response.audio.speech.create(\n                    model=self.model, voice=voice, input=ins, **kwargs\n                )\n            )() as response:\n                for chunk in response.iter_bytes():\n                    output.write(chunk)\n\n            base64_audio = base64.b64encode(output.getvalue()).decode()\n            time_cost = time.monotonic() - start_time\n            return [\n                (\n                    base64_audio,\n                    {\n                        \"metadata\": {\n                            \"request_id\": \"\",\n                            \"input_tokens_count\": 0,\n                            \"generated_tokens_count\": 0,\n                            \"time_cost\": time_cost,\n                            \"first_token_time\": 0,\n                            \"speed\": 0,\n                        }\n                    },\n                )\n            ]\n\n    async def async_speech_to_text(\n        self,\n        audio: str,\n        response_format: str = \"verbose_json\",\n        timestamp_granularities: List[str] = [\"word\",\"segment\"],\n    ):               \n        # Extract audio format and base64 data\n        data_prefix = \"data:audio/\"\n        base64_prefix = \";base64,\"\n        if not audio.startswith(data_prefix):\n            raise ValueError(\"Invalid audio data format\")\n        \n        format_end = audio.index(base64_prefix)\n        audio_format = audio[len(data_prefix):format_end]\n        base64_data = audio[format_end + len(base64_prefix):]\n        \n        # Decode the base64 audio data\n        audio_data = base64.b64decode(base64_data)\n\n        # Create a temporary file with the correct extension\n        with tempfile.NamedTemporaryFile(delete=False, suffix=f'.{audio_format}') as temp_audio_file:\n            temp_audio_file.write(audio_data)\n            temp_audio_file_path = temp_audio_file.name\n\n        try:\n            start_time = time.monotonic()\n                   \n            with open(temp_audio_file_path, \"rb\") as audio_file:                \n                transcription = self.client.audio.transcriptions.create(\n                    model=self.model or \"whisper-1\",\n                    file=audio_file,\n                    response_format=response_format,\n                    timestamp_granularities=timestamp_granularities,\n                )\n            time_cost = time.monotonic() - start_time\n\n            return [\n                (\n                    json.dumps(transcription.to_dict(), ensure_ascii=True),\n                    {\n                        \"metadata\": {\n                            \"request_id\": \"\",\n                            \"input_tokens_count\": 0,\n                            \"generated_tokens_count\": 0,\n                            \"time_cost\": time_cost,\n                            \"first_token_time\": 0,\n                            \"speed\": 0,\n                        }\n                    },\n                )\n            ]\n        finally:\n            # Clean up the temporary file\n            import os\n            os.unlink(temp_audio_file_path)\n\n    def image_to_text(self, ins: str, **kwargs):\n        pass\n\n    async def async_text_to_image(\n        self, stream: bool, input: str, size: str, quality: str, n: int, **kwargs\n    ):\n        if stream:\n            raise Exception(\"Stream not supported for text to image\")\n        start_time = time.monotonic()\n        response = await asyncfy_with_semaphore(\n            lambda: self.client.images.generate(\n                model=self.model,\n                prompt=input,\n                size=size,\n                quality=quality,\n                n=1,\n                response_format=\"b64_json\",\n                **kwargs,\n            )\n        )()\n        time_cost = time.monotonic() - start_time\n        base64_image = response.data[0].b64_json\n        return [\n            (\n                base64_image,\n                {\n                    \"metadata\": {\n                        \"request_id\": \"\",\n                        \"input_tokens_count\": 0,\n                        \"generated_tokens_count\": 0,\n                        \"time_cost\": time_cost,\n                        \"first_token_time\": 0,\n                        \"speed\": 0,\n                    }\n                },\n            )\n        ]\n\n    def text_to_text(self, ins: str, **kwargs):\n        pass\n\n    async def async_stream_chat(\n        self,\n        tokenizer,\n        ins: str,\n        his: List[Dict[str, Any]] = [],\n        max_length: int = 4096,\n        top_p: float = 0.7,\n        temperature: float = 0.9,\n        **kwargs,\n    ):\n\n        model = self.model\n\n        if \"model\" in kwargs:\n            model = kwargs[\"model\"]\n\n        messages = [\n            {\"role\": message[\"role\"], \"content\": self.process_input(message[\"content\"])}\n            for message in his\n        ] + [{\"role\": \"user\", \"content\": self.process_input(ins)}]\n\n        stream = kwargs.get(\"stream\", False)\n\n        ## content = [\n        ##    \"voice\": \"alloy\",\"input\": \"Hello, World!\",response_format: \"mp3\"]\n        last_message = messages[-1][\"content\"]\n\n        if isinstance(last_message, dict) and \"voice\" in last_message:\n            voice = last_message[\"voice\"]\n            response_format = last_message.get(\"response_format\", \"mp3\")\n            chunk_size = last_message.get(\"chunk_size\", None)\n            input = last_message[\"input\"]\n            return await self.async_text_to_speech(\n                stream=stream,\n                ins=input,\n                voice=voice,\n                chunk_size=chunk_size,\n                response_format=response_format,\n            )\n\n        if isinstance(last_message, dict) and \"input\" in last_message:\n            input = last_message[\"input\"]\n            size = last_message.get(\"size\", \"1024x1024\")\n            quality = last_message.get(\"quality\", \"standard\")\n            n = last_message.get(\"n\", 1)\n            return await self.async_text_to_image(\n                stream=stream, input=input, size=size, quality=quality, n=n\n            )\n\n        if isinstance(last_message, dict) and \"audio\" in last_message:            \n            audio_data = last_message.get(\"audio\", \"\")            \n            tpe = last_message.get(\"type\",\"wav\")\n            if not audio_data.startswith(\"data:\"):\n                audio_data = f\"data:audio/${tpe};base64,\" + audio_data                \n            return await self.async_speech_to_text(audio=audio_data)\n        \n        server = ray.get_actor(\"BLOCK_VLLM_STREAM_SERVER\")\n        request_id = [None]\n\n        def writer():\n            try:\n                r = \"\"\n                response = self.client.chat.completions.create(\n                    messages=messages,\n                    model=model,\n                    stream=True,\n                    max_tokens=max_length,\n                    temperature=temperature,\n                    top_p=top_p,\n                )\n                # input_tokens_count = 0\n                # generated_tokens_count = 0\n\n                request_id[0] = str(uuid.uuid4())\n\n                for chunk in response:\n                    content = chunk.choices[0].delta.content or \"\"\n                    r += content\n                    if hasattr(chunk, \"usage\") and chunk.usage:\n                        input_tokens_count = chunk.usage.prompt_tokens\n                        generated_tokens_count = chunk.usage.completion_tokens\n                    else:\n                        input_tokens_count = 0\n                        generated_tokens_count = 0\n                    ray.get(\n                        server.add_item.remote(\n                            request_id[0],\n                            StreamOutputs(\n                                outputs=[\n                                    SingleOutput(\n                                        text=r,\n                                        metadata=SingleOutputMeta(\n                                            input_tokens_count=input_tokens_count,\n                                            generated_tokens_count=generated_tokens_count,\n                                        ),\n                                    )\n                                ]\n                            ),\n                        )\n                    )\n            except:\n                traceback.print_exc()\n            ray.get(server.mark_done.remote(request_id[0]))\n\n        if stream:\n            threading.Thread(target=writer, daemon=True).start()\n\n            time_count = 60 * 100\n            while request_id[0] is None and time_count > 0:\n                time.sleep(0.01)\n                time_count -= 1\n\n            if request_id[0] is None:\n                raise Exception(\"Failed to get request id\")\n\n            def write_running():\n                return ray.get(server.add_item.remote(request_id[0], \"RUNNING\"))\n\n            await asyncio.to_thread(write_running)\n            return [\n                (\n                    \"\",\n                    {\n                        \"metadata\": {\n                            \"request_id\": request_id[0],\n                            \"stream_server\": \"BLOCK_VLLM_STREAM_SERVER\",\n                        }\n                    },\n                )\n            ]\n        else:\n            try:\n                start_time = time.monotonic()\n                response = await asyncfy_with_semaphore(\n                    lambda: self.client.chat.completions.create(\n                        messages=messages,\n                        model=model,\n                        max_tokens=max_length,\n                        temperature=temperature,\n                        top_p=top_p,\n                    )\n                )()\n\n                generated_text = response.choices[0].message.content\n                generated_tokens_count = response.usage.completion_tokens\n                input_tokens_count = response.usage.prompt_tokens\n                time_cost = time.monotonic() - start_time\n                return [\n                    (\n                        generated_text,\n                        {\n                            \"metadata\": {\n                                \"request_id\": response.id,\n                                \"input_tokens_count\": input_tokens_count,\n                                \"generated_tokens_count\": generated_tokens_count,\n                                \"time_cost\": time_cost,\n                                \"first_token_time\": 0,\n                                \"speed\": float(generated_tokens_count) / time_cost,\n                            }\n                        },\n                    )\n                ]\n            except Exception as e:\n                print(f\"Error: {e}\")\n                raise e\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/whisper/__init__.py\nimport whisper\nfrom typing import Dict, List, Tuple\nfrom byzerllm.utils.types import StopSequencesCriteria\n\nfrom typing import Dict, Any, List, Union\nfrom pyjava.storage import streaming_tar as STar\nimport json\nimport base64\nimport tempfile\nimport time\n\n\ndef get_meta(self):\n    return [\n        {\n            \"model_deploy_type\": \"proprietary\",\n            \"backend\": \"transformers\",\n            \"message_format\": True\n        }\n    ]\n\n\ndef process_input(ins: Union[str, List[Dict[str, Any]], Dict[str, Any]]):\n    # print(ins) \n    if isinstance(ins, list) or isinstance(ins, dict):\n        return ins\n\n    content = []\n    try:\n        ins_json = json.loads(ins)\n    except:\n        return ins\n\n    ## speech\n    if isinstance(ins_json, dict):\n        return ins_json\n\n    content = []\n    #     [\n    #     {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n    #     {\n    #       \"type\": \"image_url\",\n    #       \"image_url\": {\n    #         \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n    #         \"detail\": \"high\"\n    #       },\n    #     },\n    #   ],\n    for item in ins_json:\n        # for format like this: {\"image\": \"xxxxxx\", \"text\": \"What’s in this image?\",\"detail\":\"high\"}\n        # or {\"image\": \"xxxxxx\"}, {\"text\": \"What’s in this image?\"}\n        if (\"image\" in item or \"image_url\" in item) and \"type\" not in item:\n            image_data = item.get(\"image\", item.get(\"image_url\", \"\"))\n            ## \"data:image/jpeg;base64,\"\n            if not image_data.startswith(\"data:\"):\n                image_data = \"data:image/jpeg;base64,\" + image_data\n\n            ## get the other fields except image/text/image_url\n            other_fields = {\n                k: v for k, v in item.items() if k not in [\"image\", \"text\", \"image_url\"]\n            }\n            content.append(\n                {\n                    \"image_url\": {\"url\": image_data, **other_fields},\n                    \"type\": \"image_url\",\n                }\n            )\n\n        if \"text\" in item and \"type\" not in item:\n            text_data = item[\"text\"]\n            content.append({\"text\": text_data, \"type\": \"text\"})\n\n        ## for format like this: {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n        ## {\"type\": \"image_url\", \"image_url\": {\"url\":\"\",\"detail\":\"high\"}}\n        ## this is the standard format, just return it\n        if \"type\" in item and item[\"type\"] == \"text\":\n            content.append(item)\n\n        if \"type\" in item and item[\"type\"] == \"image_url\":\n            content.append(item)\n\n    if not content:\n        return ins\n\n    return content\n\n\ndef stream_chat(\n    self,\n    tokenizer,\n    ins: str,\n    his: List[Dict[str, str]] = [],\n    max_length: int = 4090,\n    top_p: float = 0.95,\n    temperature: float = 0.1,\n    **kwargs,\n):\n    messages = [\n        {\"role\": message[\"role\"], \"content\": process_input(message[\"content\"])}\n        for message in his\n    ] + [{\"role\": \"user\", \"content\": process_input(ins)}]\n\n    audio_input = messages[-1][\"content\"]\n\n    audio_data = audio_input.get(\"audio\", \"\")\n    audio = audio_input[\"audio\"]\n    response_format = audio_input.get(\"response_format\", \"json\")\n    timestamp_granularities = audio_input.get(\n        \"timestamp_granularities\", [\"word\", \"segment\"]\n    )\n    data_prefix = \"data:audio/\"\n    base64_prefix = \";base64,\"\n    if not audio.startswith(data_prefix):\n        raise ValueError(\"Invalid audio data format\")\n\n    format_end = audio.index(base64_prefix)\n    audio_format = audio[len(data_prefix) : format_end]\n    base64_data = audio[format_end + len(base64_prefix) :]\n\n    # Decode the base64 audio data\n    audio_data = base64.b64decode(base64_data)\n\n    # Create a temporary file with the correct extension\n    with tempfile.NamedTemporaryFile(\n        delete=False, suffix=f\".{audio_format}\"\n    ) as temp_audio_file:\n        temp_audio_file.write(audio_data)\n        temp_audio_file_path = temp_audio_file.name\n\n    try:\n        start_time = time.monotonic()\n        word_timestamps = \"word\" in timestamp_granularities\n        \n        result = self.transcribe(temp_audio_file_path, word_timestamps=word_timestamps)        \n            \n        time_cost = time.monotonic() - start_time\n        return [\n            (\n                json.dumps(result, ensure_ascii=False),\n                {\n                    \"metadata\": {\n                        \"request_id\": \"\",\n                        \"input_tokens_count\": 0,\n                        \"generated_tokens_count\": 0,\n                        \"time_cost\": time_cost,\n                        \"first_token_time\": 0,\n                        \"speed\": 0,\n                    }\n                },\n            )\n        ]\n    finally:\n        # Clean up the temporary file\n        import os\n\n        os.unlink(temp_audio_file_path)\n\n\ndef init_model(\n    model_dir, infer_params: Dict[str, str] = {}, sys_conf: Dict[str, str] = {}\n):\n    model = whisper.load_model(\"large\", download_root=model_dir)\n    import types\n\n    model.stream_chat = types.MethodType(stream_chat, model)\n    model.get_meta = types.MethodType(get_meta, model)\n    return (model, None)\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/notebooks/test_sensevoice.py\nimport sys\nimport os\n\n# Add the project root to the Python path\nsys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\")))\n\nfrom src.byzerllm.sensevoice import init_model, format_str_v3\nimport base64\nimport json\n\n\ndef main():\n    # Initialize the model\n    model_dir = \"/Users/allwefantasy/models/SenseVoiceSmall\"\n    model, _ = init_model(model_dir)\n\n    # Prepare test audio\n    with open(\n        \"/Users/allwefantasy/models/SenseVoiceSmall/example/zh.mp3\", \"rb\"\n    ) as audio_file:\n        audio_data = audio_file.read()\n        base64_audio = base64.b64encode(audio_data).decode(\"utf-8\")        \n        # Prepare input in the expected format\n        test_input = json.dumps(\n            {\n                            \"type\": \"audio\",\n                            \"audio\": f\"data:audio/wav;base64,{base64_audio}\",\n                        }\n        )\n\n        # Call stream_chat\n        result = model.stream_chat(tokenizer=None, ins = test_input)\n\n        # Process and print the result\n        if result and len(result) > 0:\n            output, metadata = result[0]\n            parsed_output = json.loads(output)\n            formatted_text = parsed_output.get(\"text\", \"\")\n            print(\"Transcription:\", formatted_text)\n            print(\"Metadata:\", metadata)\n        else:\n            print(\"No result returned from the model.\")\n\n\nif __name__ == \"__main__\":\n    main()\n\n\n##File: /Users/allwefantasy/projects/Qwen2-Audio/README_CN.md\n<p align=\"left\">\n        中文</a>&nbsp ｜ &nbsp<a href=\"README.md\">English</a>\n</p>\n<br><br>\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/assets/blog/qwenaudio/qwen2audio_logo.png\" width=\"400\"/>\n<p>\n\n<p align=\"center\">\nQwen2-Audio-7B <a href=\"https://modelscope.cn/models/qwen/Qwen2-Audio-7B\">🤖 </a> | <a href=\"https://huggingface.co/Qwen/Qwen2-Audio-7B\">🤗</a>&nbsp ｜ Qwen-Audio-7B-Instruct <a href=\"https://modelscope.cn/models/qwen/Qwen2-Audio-7B-Instruct\">🤖 </a>| <a href=\"https://huggingface.co/Qwen/Qwen2-Audio-7B-Instruct\">🤗</a>&nbsp ｜ Demo<a href=\"https://modelscope.cn/studios/qwen/Qwen2-Audio-Instruct-Demo\"> 🤖</a> | <a href=\"https://huggingface.co/spaces/Qwen/Qwen2-Audio-Instruct-Demo\">🤗</a>&nbsp\n<br>\n📑 <a href=\"https://arxiv.org/abs/2407.10759\">Paper</a> &nbsp&nbsp | &nbsp&nbsp 📑 <a href=\"https://qwenlm.github.io/blog/qwen2-audio\">Blog</a> &nbsp&nbsp | &nbsp&nbsp 💬 <a href=\"https://github.com/QwenLM/Qwen/blob/main/assets/wechat.png\">WeChat (微信)</a>&nbsp&nbsp | &nbsp&nbsp <a href=\"https://discord.gg/CV4E9rpNSD\">Discord</a>&nbsp&nbsp\n</p>\n\n\n\n我们介绍Qwen-Audio的最新进展：Qwen2-Audio。作为一个大规模音频语言模型，Qwen2-Audio能够接受各种音频信号输入，并根据语音指令执行音频分析或直接响应文本。我们介绍两种不同的音频交互模式：语音聊天 voice chat 和音频分析 audio analysis。\n\n* 语音聊天：用户可以自由地与 Qwen2-Audio 进行语音互动，而无需文本输入；\n* 音频分析：用户可以在互动过程中提供音频和文本指令对音频进行分析；\n\n**我们已经开源了 Qwen2-Audio 系列的两个模型：Qwen2-Audio-7B和Qwen2-Audio-7B-Instruct。**\n\n## 模型结构与训练范式\n\nQwen2-Audio 三阶段训练过程概述。\n\n<p align=\"left\">\n    <img src=\"assets/framework.png\" width=\"80%\"/>\n<p>\n\n\n\n## 新闻\n* 2024.8.9 🎉 我们在 ModelScope 和 Hugging Face 开源了`Qwen2-Audio-7B`和`Qwen2-Audio-7B-Instruct`的 checkpoint.\n* 2024.7.15 🎉 我们发布了 Qwen2-Audio 的[论文](https://arxiv.org/abs/2407.10759), 介绍了相关的模型结构，训练方法和模型表现。\n* 2023.11.30 🔥 我们发布了**Qwen-Audio**系列\n<br>\n\n## 评测\n我们在标准的13个学术数据集上评测了模型的能力如下：\n<table><thead><tr><th>Task</th><th>Description</th><th>Dataset</th><th>Split</th><th>Metric</th></tr></thead><tbody><tr><td rowspan=\"4\">ASR</td><td rowspan=\"4\">Automatic Speech Recognition</td><td>Fleurs</td><td>dev | test</td><td rowspan=\"4\">WER</td></tr><tr><td>Aishell2</td><td>test</td></tr><tr><td>Librispeech</td><td>dev | test</td></tr><tr><td>Common Voice</td><td>dev | test</td></tr><tr><td>S2TT</td><td>Speech-to-Text Translation</td><td>CoVoST2</td><td>test</td><td>BLEU </td></tr><tr><td>SER</td><td>Speech Emotion Recognition</td><td>Meld</td><td>test</td><td>ACC</td></tr><tr><td>VSC</td><td>Vocal Sound Classification</td><td>VocalSound</td><td>test</td><td>ACC</td></tr><tr><td rowspan=\"4\"><a href=\"https://github.com/OFA-Sys/AIR-Bench\">AIR-Bench</a><br></td><td>Chat-Benchmark-Speech</td><td>Fisher<br>SpokenWOZ<br>IEMOCAP<br>Common voice</td><td>dev | test</td><td>GPT-4 Eval</td></tr><tr><td>Chat-Benchmark-Sound</td><td>Clotho</td><td>dev | test</td><td>GPT-4 Eval</td></tr>\n<tr><td>Chat-Benchmark-Music</td><td>MusicCaps</td><td>dev | test</td><td>GPT-4 Eval</td></tr><tr><td>Chat-Benchmark-Mixed-Audio</td><td>Common voice<br>AudioCaps<br>MusicCaps</td><td>dev | test</td><td>GPT-4 Eval</td></tr></tbody></table>\n\n\n以下是整体表现：\n<p align=\"left\">\n    <img src=\"assets/radar_compare_qwen_audio.png\" width=\"70%\"/>\n<p>\n\n评测分数详情如下：\n<br>\n<b>（注意：我们所展示的评测结果是在原始训练框架的初始模型上的，然而在框架转换 Huggingface 后指标出现了部分波动，在这里我们展示我们的全部测评结果：首先是论文中的初始模型结果）</b>\n\n<table><thead><tr><th rowspan=\"2\">Task</th><th rowspan=\"2\">Dataset</th><th rowspan=\"2\">Model</th><th colspan=\"2\">Performance</th></tr><tr><th>Metrics</th><th>Results</th></tr></thead><tbody><tr><td rowspan=\"15\">ASR</td><td rowspan=\"7\"><b>Librispeech</b><br>dev-clean | dev-other | <br>test-clean | test-other</td><td>SpeechT5</td><td rowspan=\"7\">WER </td><td>2.1 | 5.5 | 2.4 | 5.8</td></tr><tr><td>SpeechNet</td><td>- | - | 30.7 | -</td></tr><tr><td>SLM-FT</td><td>- | - | 2.6 | 5.0</td></tr><tr><td>SALMONN</td><td>- | - | 2.1 | 4.9</td></tr><tr><td>SpeechVerse</td><td>- | - | 2.1 | 4.4</td></tr><tr><td>Qwen-Audio</td><td>1.8 | 4.0 | 2.0 | 4.2</td></tr><tr><td>Qwen2-Audio</td><td><b>1.3 | 3.4 | 1.6 | 3.6</b></td></tr><tr><td rowspan=\"2\"><b>Common Voice 15</b> <br>en | zh | yue | fr</td><td>Whisper-large-v3</td><td rowspan=\"2\">WER </td><td>9.3 | 12.8 | 10.9 | 10.8</td></tr><tr><td>Qwen2-Audio</td><td><b>8.6 | 6.9 | 5.9 | 9.6</b></td></tr>\n<tr><td rowspan=\"2\"><b>Fleurs</b> <br>zh</td><td>Whisper-large-v3</td><td rowspan=\"2\">WER </td><td>7.7</td></tr><tr><td>Qwen2-Audio</td><td><b>7.5</b></td></tr><tr><td rowspan=\"4\"><b>Aishell2</b> <br>Mic | iOS | Android</td><td>MMSpeech-base</td><td rowspan=\"4\">WER </td><td>4.5 | 3.9 | 4.0</td></tr><tr><td>Paraformer-large</td><td>- | <b>2.9</b> | -</td></tr><tr><td>Qwen-Audio</td><td>3.3 | 3.1 | 3.3</td></tr><tr><td>Qwen2-Audio</td><td><b>3.0</b> | 3.0 | <b>2.9</b></td></tr><tr><td rowspan=\"8\">S2TT</td><td rowspan=\"5\"><b>CoVoST2</b> <br>en-de | de-en | <br>en-zh | zh-en</td><td>SALMONN</td><td rowspan=\"5\">BLEU </td><td>18.6 | - | 33.1 | -</td></tr><tr><td>SpeechLLaMA</td><td>- | 27.1 | - | 12.3</td></tr><tr><td>BLSP</td><td>14.1 | - | - | -</td></tr><tr><td>Qwen-Audio</td><td>25.1 | 33.9 | 41.5 | 15.7</td></tr><tr><td>Qwen2-Audio</td><td><b>29.9 | 35.2 | 45.2 | 24.4</b></td></tr>\n<tr><td rowspan=\"3\"><b>CoVoST2</b> <br>es-en | fr-en | it-en |</td><td>SpeechLLaMA</td><td rowspan=\"3\">BLEU </td><td>27.9 | 25.2 | 25.9</td></tr><tr><td>Qwen-Audio</td><td>39.7 | <b>38.5</b> | 36.0</td></tr><tr><td>Qwen2-Audio</td><td><b>40.0 | 38.5 | 36.3</b></td></tr><tr><td rowspan=\"3\">SER</td><td rowspan=\"3\"><b>Meld</b></td><td>WavLM-large</td><td rowspan=\"3\">ACC </td><td>0.542</td></tr><tr><td>Qwen-Audio</td><td><b>0.557</b></td></tr><tr><td>Qwen2-Audio</td><td>0.553</td></tr><tr><td rowspan=\"4\">VSC</td><td rowspan=\"4\"><b>VocalSound</b></td><td>CLAP</td><td rowspan=\"4\">ACC </td><td>0.4945</td></tr><tr><td>Pengi</td><td>0.6035</td></tr><tr><td>Qwen-Audio</td><td>0.9289</td></tr><tr><td>Qwen2-Audio</td><td><b>0.9392</b></td></tr>\n<tr><td>AIR-Bench <br></td><td><b>Chat Benchmark</b><br>Speech | Sound |<br> Music | Mixed-Audio</td><td>SALMONN<br>BLSP<br>Pandagpt<br>Macaw-LLM<br>SpeechGPT<br>Next-gpt<br>Qwen-Audio<br>Gemini-1.5-pro<br>Qwen2-Audio</td><td>GPT-4 </td><td>6.16 | 6.28 | 5.95 | 6.08<br>6.17 | 5.55 | 5.08 | 5.33<br>3.58 | 5.46 | 5.06 | 4.25<br>0.97 | 1.01 | 0.91 | 1.01<br>1.57 | 0.95 | 0.95 | 4.13<br>3.86 | 4.76 | 4.18 | 4.13<br>6.47 | 6.95 | 5.52 | 6.08<br>6.97 | 5.49 | 5.06 | 5.27<br><b>7.18 | 6.99 | 6.79 | 6.77</b></td></tr></tbody></table>\n\n<b>（其次是转换 huggingface 后的）</b>\n\n<table><thead><tr><th rowspan=\"2\">Task</th><th rowspan=\"2\">Dataset</th><th rowspan=\"2\">Model</th><th colspan=\"2\">Performance</th></tr><tr><th>Metrics</th><th>Results</th></tr></thead><tbody><tr><td rowspan=\"15\">ASR</td><td rowspan=\"7\"><b>Librispeech</b><br>dev-clean | dev-other | <br>test-clean | test-other</td><td>SpeechT5</td><td rowspan=\"7\">WER </td><td>2.1 | 5.5 | 2.4 | 5.8</td></tr><tr><td>SpeechNet</td><td>- | - | 30.7 | -</td></tr><tr><td>SLM-FT</td><td>- | - | 2.6 | 5.0</td></tr><tr><td>SALMONN</td><td>- | - | 2.1 | 4.9</td></tr><tr><td>SpeechVerse</td><td>- | - | 2.1 | 4.4</td></tr><tr><td>Qwen-Audio</td><td>1.8 | 4.0 | 2.0 | 4.2</td></tr><tr><td>Qwen2-Audio</td><td><b>1.7 | 3.6 | 1.7 | 4.0</b></td></tr><tr><td rowspan=\"2\"><b>Common Voice 15</b> <br>en | zh | yue | fr</td><td>Whisper-large-v3</td><td rowspan=\"2\">WER </td><td>9.3 | 12.8 | 10.9 | 10.8</td></tr><tr><td>Qwen2-Audio</td><td><b>8.7 | 6.5 | 5.9 | 9.6</b></td></tr>\n<tr><td rowspan=\"2\"><b>Fleurs</b> <br>zh</td><td>Whisper-large-v3</td><td rowspan=\"2\">WER </td><td>7.7</td></tr><tr><td>Qwen2-Audio</td><td><b>7.0</b></td></tr><tr><td rowspan=\"4\"><b>Aishell2</b> <br>Mic | iOS | Android</td><td>MMSpeech-base</td><td rowspan=\"4\">WER </td><td>4.5 | 3.9 | 4.0</td></tr><tr><td>Paraformer-large</td><td>- | <b>2.9</b> | -</td></tr><tr><td>Qwen-Audio</td><td>3.3 | 3.1 | 3.3</td></tr><tr><td>Qwen2-Audio</td><td><b>3.2</b> | 3.1 | <b>2.9</b></td></tr><tr><td rowspan=\"8\">S2TT</td><td rowspan=\"5\"><b>CoVoST2</b> <br>en-de | de-en | <br>en-zh | zh-en</td><td>SALMONN</td><td rowspan=\"5\">BLEU </td><td>18.6 | - | 33.1 | -</td></tr><tr><td>SpeechLLaMA</td><td>- | 27.1 | - | 12.3</td></tr><tr><td>BLSP</td><td>14.1 | - | - | -</td></tr><tr><td>Qwen-Audio</td><td>25.1 | <b>33.9</b> | 41.5 | 15.7</td></tr><tr><td>Qwen2-Audio</td><td><b>29.6</b> | 33.6 | <b>45.6</b> | <b>24.0</b></td></tr>\n<tr><td rowspan=\"3\"><b>CoVoST2</b> <br>es-en | fr-en | it-en |</td><td>SpeechLLaMA</td><td rowspan=\"3\">BLEU </td><td>27.9 | 25.2 | 25.9</td></tr><tr><td>Qwen-Audio</td><td><b>39.7 | 38.5 | 36.0</b></td></tr><tr><td>Qwen2-Audio</td><td>38.7 | 37.2 | 35.2</td></tr><tr><td rowspan=\"3\">SER</td><td rowspan=\"3\"><b>Meld</b></td><td>WavLM-large</td><td rowspan=\"3\">ACC </td><td>0.542</td></tr><tr><td>Qwen-Audio</td><td><b>0.557</b></td></tr><tr><td>Qwen2-Audio</td><td>0.535</td></tr><tr><td rowspan=\"4\">VSC</td><td rowspan=\"4\"><b>VocalSound</b></td><td>CLAP</td><td rowspan=\"4\">ACC </td><td>0.4945</td></tr><tr><td>Pengi</td><td>0.6035</td></tr><tr><td>Qwen-Audio</td><td>0.9289</td></tr><tr><td>Qwen2-Audio</td><td><b>0.9395</b></td></tr>\n<tr><td>AIR-Bench <br></td><td><b>Chat Benchmark</b><br>Speech | Sound |<br> Music | Mixed-Audio</td><td>SALMONN<br>BLSP<br>Pandagpt<br>Macaw-LLM<br>SpeechGPT<br>Next-gpt<br>Qwen-Audio<br>Gemini-1.5-pro<br>Qwen2-Audio</td><td>GPT-4 </td><td>6.16 | 6.28 | 5.95 | 6.08<br>6.17 | 5.55 | 5.08 | 5.33<br>3.58 | 5.46 | 5.06 | 4.25<br>0.97 | 1.01 | 0.91 | 1.01<br>1.57 | 0.95 | 0.95 | 4.13<br>3.86 | 4.76 | 4.18 | 4.13<br>6.47 | <b>6.95</b> | 5.52 | 6.08<br>6.97 | 5.49 | 5.06 | 5.27<br><b>7.24</b> | 6.83 | <b>6.73</b> | <b>6.42</b></td></tr></tbody></table>\n\n\n我们提供了以上**所有**评测脚本以供复现我们的实验结果。请阅读 [eval_audio/EVALUATION.md](eval_audio/EVALUATION.md) 了解更多信息。\n\n## 部署要求\nThe code of Qwen2-Audio has been in the latest Hugging face transformers and we advise you to build from source with command `pip install git+https://github.com/huggingface/transformers`, or you might encounter the following error:\nQwen2-Audio的代码已经包含在最新的 Hugging Face Transformers 的主分支中，我们建议您使用命令`pip install git+https://github.com/huggingface/transformers`\n```\nKeyError: 'qwen2-audio'\n```\n## 快速使用\n我们提供简单的示例来说明如何利用 🤗 Transformers 快速使用 Qwen2-Audio-7B 和 Qwen2-Audio-7B-Instruct。\n在开始前，请确保你已经配置好环境并安装好相关的代码包。最重要的是，确保你满足上述要求，然后安装相关的依赖库。\n接下来你可以开始使用 Transformers 或者 ModelScope 来使用我们的模型。目前Qwen2-Audio-7B 及 Qwen2-Audio-7B-Instruct 模型处理30秒以内的音频表现更佳。\n#### 🤗 Hugging Face Transformers\n如希望使用 Qwen2-Audio-7B-Instruct 进行推理，我们分别演示语音聊天和音频分析的交互方式，所需要写的只是如下所示的数行代码。\n##### 语音聊天推理\n在语音聊天模式下，用户可以自由地与 Qwen2-Audio 进行语音交互，无需文字输入：\n```python\nfrom io import BytesIO\nfrom urllib.request import urlopen\nimport librosa\nfrom transformers import Qwen2AudioForConditionalGeneration, AutoProcessor\n\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\")\nmodel = Qwen2AudioForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\", device_map=\"auto\")\n\nconversation = [\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/guess_age_gender.wav\"},\n    ]},\n    {\"role\": \"assistant\", \"content\": \"Yes, the speaker is female and in her twenties.\"},\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/translate_to_chinese.wav\"},\n    ]},\n]\ntext = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\naudios = []\nfor message in conversation:\n    if isinstance(message[\"content\"], list):\n        for ele in message[\"content\"]:\n            if ele[\"type\"] == \"audio\":\n                audios.append(librosa.load(\n                    BytesIO(urlopen(ele['audio_url']).read()), \n                    sr=processor.feature_extractor.sampling_rate)[0]\n                )\n\ninputs = processor(text=text, audios=audios, return_tensors=\"pt\", padding=True)\ninputs.input_ids = inputs.input_ids.to(\"cuda\")\n\ngenerate_ids = model.generate(**inputs, max_length=256)\ngenerate_ids = generate_ids[:, inputs.input_ids.size(1):]\n\nresponse = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n```\n##### 音频分析推理\n在音频分析中，用户可以提供音频和文字问题来实现对音频的分析：\n```python\nfrom io import BytesIO\nfrom urllib.request import urlopen\nimport librosa\nfrom transformers import Qwen2AudioForConditionalGeneration, AutoProcessor\n\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\")\nmodel = Qwen2AudioForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\", device_map=\"auto\")\n\nconversation = [\n    {'role': 'system', 'content': 'You are a helpful assistant.'}, \n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/glass-breaking-151256.mp3\"},\n        {\"type\": \"text\", \"text\": \"What's that sound?\"},\n    ]},\n    {\"role\": \"assistant\", \"content\": \"It is the sound of glass shattering.\"},\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"text\", \"text\": \"What can you do when you hear that?\"},\n    ]},\n    {\"role\": \"assistant\", \"content\": \"Stay alert and cautious, and check if anyone is hurt or if there is any damage to property.\"},\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/1272-128104-0000.flac\"},\n        {\"type\": \"text\", \"text\": \"What does the person say?\"},\n    ]},\n]\ntext = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\naudios = []\nfor message in conversation:\n    if isinstance(message[\"content\"], list):\n        for ele in message[\"content\"]:\n            if ele[\"type\"] == \"audio\":\n                audios.append(\n                    librosa.load(\n                        BytesIO(urlopen(ele['audio_url']).read()), \n                        sr=processor.feature_extractor.sampling_rate)[0]\n                )\n\ninputs = processor(text=text, audios=audios, return_tensors=\"pt\", padding=True)\ninputs.input_ids = inputs.input_ids.to(\"cuda\")\n\ngenerate_ids = model.generate(**inputs, max_length=256)\ngenerate_ids = generate_ids[:, inputs.input_ids.size(1):]\n\nresponse = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n```\n##### 批量推理\n我们也支持批量推理：\n```python\nfrom io import BytesIO\nfrom urllib.request import urlopen\nimport librosa\nfrom transformers import Qwen2AudioForConditionalGeneration, AutoProcessor\n\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\")\nmodel = Qwen2AudioForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\", device_map=\"auto\")\n\nconversation1 = [\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/glass-breaking-151256.mp3\"},\n        {\"type\": \"text\", \"text\": \"What's that sound?\"},\n    ]},\n    {\"role\": \"assistant\", \"content\": \"It is the sound of glass shattering.\"},\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/f2641_0_throatclearing.wav\"},\n        {\"type\": \"text\", \"text\": \"What can you hear?\"},\n    ]}\n]\n\nconversation2 = [\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/1272-128104-0000.flac\"},\n        {\"type\": \"text\", \"text\": \"What does the person say?\"},\n    ]},\n]\n\nconversations = [conversation1, conversation2]\n\ntext = [processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False) for conversation in conversations]\n\naudios = []\nfor conversation in conversations:\n    for message in conversation:\n        if isinstance(message[\"content\"], list):\n            for ele in message[\"content\"]:\n                if ele[\"type\"] == \"audio\":\n                    audios.append(\n                        librosa.load(\n                            BytesIO(urlopen(ele['audio_url']).read()), \n                            sr=processor.feature_extractor.sampling_rate)[0]\n                    )\n\ninputs = processor(text=text, audios=audios, return_tensors=\"pt\", padding=True)\ninputs['input_ids'] = inputs['input_ids'].to(\"cuda\")\ninputs.input_ids = inputs.input_ids.to(\"cuda\")\n\ngenerate_ids = model.generate(**inputs, max_length=256)\ngenerate_ids = generate_ids[:, inputs.input_ids.size(1):]\n\nresponse = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n```\n运行Qwen2-Audio-7B同样非常简单。\n```python\nfrom io import BytesIO\nfrom urllib.request import urlopen\nimport librosa\nfrom transformers import AutoProcessor, Qwen2AudioForConditionalGeneration\n\nmodel = Qwen2AudioForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-Audio-7B\" ,trust_remote_code=True)\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-Audio-7B\" ,trust_remote_code=True)\n\nprompt = \"<|audio_bos|><|AUDIO|><|audio_eos|>Generate the caption in English:\"\nurl = \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Audio/glass-breaking-151256.mp3\"\naudio, sr = librosa.load(BytesIO(urlopen(url).read()), sr=processor.feature_extractor.sampling_rate)\ninputs = processor(text=prompt, audios=audio, return_tensors=\"pt\")\n\ngenerated_ids = model.generate(**inputs, max_length=256)\ngenerated_ids = generated_ids[:, inputs.input_ids.size(1):]\nresponse = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n```\n#### 🤖 ModelScope\n我们强烈建议用户，特别是中国大陆地区的用户，使用 ModelScope。`snapshot_download` 可以帮助您解决下载检查点时遇到的问题。\n<br>\n## Demo\n### Web UI\n我们提供了 Web UI 的 demo 供用户使用。在开始前，确保已经安装如下代码库：\n```\npip install -r requirements_web_demo.txt\n```\n随后运行如下命令，并点击生成链接：\n```\npython demo/web_demo_audio.py\n```\n<br>\n\n## 样例展示\n更多样例将更新于[通义千问博客](https://qwenlm.github.io/blog/qwen2-audio)中的 Qwen2-Audio 博客。\n\n## 团队招聘\n\n我们是通义千问语音多模态团队，致力于以通义千问为核心，拓展音频多模态理解和生成能力，实现自由灵活的音频交互。目前团队蓬勃发展中，如有意向实习或全职加入我们，请发送简历至 `qwen_audio@list.alibaba-inc.com`.\n<br>\n\n## 使用协议\n\n请查看每个模型在其 Hugging Face 仓库中的许可证。您无需提交商业使用申请。\n<br>\n\n\n\n## 引用\n\n如果你觉得我们的论文和代码对你的研究有帮助，请考虑 :star: 和引用 :pencil: :)\n\n```BibTeX\n@article{Qwen-Audio,\n  title={Qwen-Audio: Advancing Universal Audio Understanding via Unified Large-Scale Audio-Language Models},\n  author={Chu, Yunfei and Xu, Jin and Zhou, Xiaohuan and Yang, Qian and Zhang, Shiliang and Yan, Zhijie  and Zhou, Chang and Zhou, Jingren},\n  journal={arXiv preprint arXiv:2311.07919},\n  year={2023}\n}\n```\n\n```BibTeX\n@article{Qwen2-Audio,\n  title={Qwen2-Audio Technical Report},\n  author={Chu, Yunfei and Xu, Jin and Yang, Qian and Wei, Haojie and Wei, Xipin and Guo,  Zhifang and Leng, Yichong and Lv, Yuanjun and He, Jinzheng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},\n  journal={arXiv preprint arXiv:2407.10759},\n  year={2024}\n}\n```\n## 联系我们\n\n如果你想给我们的研发团队和产品团队留言，请通过邮件（`qianwen_opensource@alibabacloud.com`）联系我们。\n<br>\n\n\n     \n\n下面是用户的需求：\n\n参考 @README-CN.md 里介绍的 qwen voice 的使用方式,根据 @/Users/allwefantasy/projects/byzer-llm/src/byzerllm/whisper/__init__.py 中的方法,实现一个新的 qwen_voice/__init__.py 文件. 注意 @README_CN.md 里使用了 audio_url 参数, 此参数我们允许用户填写base64格式的音频数据.此外,修改 process_input 方法类支持图片那样支持格式的定义\n"}, {"role": "assistant", "content": "好的,我来根据您的要求实现一个新的 qwen_voice/__init__.py 文件。这个文件会参考 Qwen2-Audio 的使用方式,并基于 whisper/__init__.py 中的方法进行实现。我们会支持 audio_url 参数,允许用户填写 base64 格式的音频数据,并修改 process_input 方法以支持类似图片那样的格式定义。\n\n以下是 qwen_voice/__init__.py 的实现:\n\n```python\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/qwen_voice/__init__.py\n<<<<<<< SEARCH\n=======\nimport time\nfrom typing import List, Tuple, Dict, Any, Union\nimport base64\nimport json\nimport librosa\nimport io\nfrom transformers import Qwen2AudioForConditionalGeneration, AutoProcessor\n\ndef get_meta(self):\n    return [\n        {\n            \"model_deploy_type\": \"proprietary\",\n            \"backend\": \"transformers\",\n            \"message_format\": True\n        }\n    ]\n\ndef process_input(ins: Union[str, List[Dict[str, Any]], Dict[str, Any]]):\n    if isinstance(ins, list) or isinstance(ins, dict):\n        return ins\n\n    content = []\n    try:\n        ins_json = json.loads(ins)\n    except:\n        return ins\n\n    if isinstance(ins_json, dict):\n        return ins_json\n\n    for item in ins_json:\n        if (\"audio\" in item or \"audio_url\" in item) and \"type\" not in item:\n            audio_data = item.get(\"audio\", item.get(\"audio_url\", \"\"))\n            if not audio_data.startswith(\"data:\"):\n                audio_data = \"data:audio/wav;base64,\" + audio_data\n\n            other_fields = {k: v for k, v in item.items() if k not in [\"audio\", \"text\", \"audio_url\"]}\n            content.append(\n                {\n                    \"audio_url\": {\"url\": audio_data, **other_fields},\n                    \"type\": \"audio\",\n                }\n            )\n\n        if \"text\" in item and \"type\" not in item:\n            text_data = item[\"text\"]\n            content.append({\"text\": text_data, \"type\": \"text\"})\n\n        if \"type\" in item and item[\"type\"] in [\"text\", \"audio\"]:\n            content.append(item)\n\n    if not content:\n        return ins\n\n    return content\n\ndef init_model(model_dir, infer_params: Dict[str, str] = {}, sys_conf: Dict[str, str] = {}):\n    processor = AutoProcessor.from_pretrained(model_dir)\n    model = Qwen2AudioForConditionalGeneration.from_pretrained(model_dir, device_map=\"auto\")\n\n    def stream_chat(\n        self,\n        tokenizer,\n        ins: str,\n        his: List[Dict[str, str]] = [],\n        max_length: int = 4096,\n        top_p: float = 0.95,\n        temperature: float = 0.1,\n        **kwargs,\n    ):\n        messages = [\n            {\"role\": message[\"role\"], \"content\": process_input(message[\"content\"])}\n            for message in his\n        ] + [{\"role\": \"user\", \"content\": process_input(ins)}]\n\n        text = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n        audios = []\n        for message in messages:\n            if isinstance(message[\"content\"], list):\n                for ele in message[\"content\"]:\n                    if ele[\"type\"] == \"audio\":\n                        audio_data = ele[\"audio_url\"][\"url\"].split(\",\")[1]\n                        audio_bytes = base64.b64decode(audio_data)\n                        audio, _ = librosa.load(io.BytesIO(audio_bytes), sr=processor.feature_extractor.sampling_rate)\n                        audios.append(audio)\n\n        inputs = processor(text=text, audios=audios, return_tensors=\"pt\", padding=True)\n        inputs.input_ids = inputs.input_ids.to(\"cuda\")\n\n        start_time = time.monotonic()\n        generate_ids = model.generate(**inputs, max_length=max_length)\n        generate_ids = generate_ids[:, inputs.input_ids.size(1):]\n\n        response = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n        time_cost = time.monotonic() - start_time\n\n        return [\n            (\n                response,\n                {\n                    \"metadata\": {\n                        \"request_id\": \"\",\n                        \"input_tokens_count\": inputs.input_ids.size(1),\n                        \"generated_tokens_count\": generate_ids.size(1),\n                        \"time_cost\": time_cost,\n                        \"first_token_time\": 0,\n                        \"speed\": float(generate_ids.size(1)) / time_cost if time_cost > 0 else 0,\n                    }\n                },\n            )\n        ]\n\n    model.stream_chat = stream_chat.__get__(model)\n    model.get_meta = get_meta.__get__(model)\n    return (model, None)\n>>>>>>> REPLACE\n```\n\n这个实现包含以下主要部分：\n\n1. `get_meta` 函数：返回模型的元数据信息。\n\n2. `process_input` 函数：处理输入数据，支持音频和文本输入，并处理 base64 格式的音频数据。\n\n3. `init_model` 函数：初始化 Qwen2Audio 模型和处理器。\n\n4. `stream_chat` 函数：实现聊天功能，处理音频输入，生成响应。\n\n这个实现支持以下特性：\n\n- 支持音频和文本输入\n- 允许用户使用 base64 格式的音频数据\n- 使用 Qwen2Audio 模型进行音频处理和生成响应\n- 返回详细的元数据，包括处理时间和令牌计数\n\n使用这个实现，用户可以像使用 Qwen2-Audio 那样使用您的 qwen_voice 模型，同时支持 base64 格式的音频输入。"}], "model": "human", "yaml_file": "actions/034_chat_action.yml"}
{"instruction": "@/Users/allwefantasy/projects/byzer-llm/src/byzerllm/qwen_voice/__init__.py 中的 stream_chat 方法需要能够独立出来,严格遵循 @/Users/allwefantasy/projects/byzer-llm/src/byzerllm/whisper/__init__.py 中的接口定义标准", "conversations": [{"role": "user", "content": "\n如果你需要生成代码，对于每个需要更改的文件,你需要按 *SEARCH/REPLACE block* 的格式进行生成。\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The opening fence and code language, eg: ```python\n2. The file path alone on a line, starting with \"##File:\" and verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n*SEARCH/REPLACE* blocks will replace *all* matching occurrences.\nInclude enough lines to make the SEARCH blocks unique.\n\nInclude *ALL* the code being searched and replaced!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n下面我们来看一个例子：\n\n当前项目目录结构：\n1. 项目根目录： /tmp/projects/mathweb\n2. 项目子目录/文件列表(类似tree 命令输出)\nflask/\n    app.py\n    templates/\n        index.html\n    static/\n        style.css\n\n用户需求： Change get_factorial() to use math.factorial\n\n回答： To make this change we need to modify `/tmp/projects/mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n用户需求： Refactor hello() into its own file.\n\n回答：To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/hello.py\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/main.py\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n现在让我们开始一个新的任务:\n\n当前项目目录结构：\n1. 项目根目录： /Users/allwefantasy/projects/byzer-llm\n2. 项目子目录/文件列表(类似tree 命令输出)：\nsetup.py\ntests/\n    test_emb_rerank.py\n    test_deepseek_code.py\n    test_prompt.py\n    test_model_meta.py\n    __init__.py\n    test_transformers_deploy.py\n    test_data_analysis.py\n    test_yi.py\n    test_deploy.py\n    functions_suite.py\n    test_template.py\n    auto/\n        test_backend_llama_cpp.py\n        __init__.py\nsetup-machine/\n    convert_safetensor.py\nnotebooks/\n    test.py\n    test_sensevoice.py\nsrc/\n    byzerllm/\n        lang.py\n        store.py\n        version.py\n        log.py\n        __init__.py\n        types.py\n        easy_byzerllm_command.py\n        byzerllm_command.py\n        command_args.py\n        bark/\n            generation.py\n            __init__.py\n            model.py\n            api.py\n            model_fine.py\n            bark_voice.py\n        moss/\n            finetune_moss.py\n            __init__.py\n            moss_inference.py\n            models/\n                configuration_moss.py\n                quantization.py\n                custom_autotune.py\n                __init__.py\n                modeling_moss.py\n                tokenization_moss.py\n        llama2/\n            __init__.py\n        records/\n            __init__.py\n        visualglm/\n            __init__.py\n        auto/\n            __init__.py\n            backend_llama_cpp.py\n            backend_vllm.py\n            backend_ds.py\n        alpha_moss/\n            __init__.py\n        processor/\n            __init__.py\n        sensevoice/\n            __init__.py\n        qwen/\n            __init__.py\n        utils/\n            json_repaire.py\n            object_store_ref_util.py\n            __init__.py\n            types.py\n            tokenizer.py\n            openai_utils.py\n            ray_utils.py\n            text_generator.py\n            emb.py\n            connect_ray.py\n            testing.py\n            langutil.py\n            nontext.py\n            fulltune/\n                deepspeed_trainner.py\n                __init__.py\n                launch.py\n                trainner.py\n                base_model/\n                    __init__.py\n                    configuration_baichuan.py\n                    modeling_baichuan.py\n                pretrain/\n                    __init__.py\n                    convert_to_transformers.py\n            metrics/\n                __init__.py\n            config/\n                __init__.py\n            retrieval/\n                rest.py\n                __init__.py\n                udf.py\n            sft/\n                argument.py\n                collator.py\n                qlora.py\n                __init__.py\n                model.py\n                dataset.py\n                loss.py\n                merge_lora.py\n                trainer.py\n            client/\n                message_utils.py\n                math_utils.py\n                byzerllm_client.py\n                parallel_utils.py\n                __init__.py\n                types.py\n                code_utils.py\n                img_utils.py\n                entrypoints/\n                    __init__.py\n                    openai/\n                        protocol.py\n                        __init__.py\n                        serve.py\n                        serving_chat.py\n                        api_server.py\n                        serving_engine.py\n                        serving_completion.py\n                        tool.py\n        qwen_voice/\n            __init__.py\n        chatglm2/\n            __init__.py\n        baichuan/\n            __init__.py\n        m3e/\n            __init__.py\n        stable_diffusion/\n            config.py\n            __init__.py\n            logger.py\n            shared.py\n            model.py\n            utils.py\n            images.py\n            diffusion/\n                __init__.py\n                embeddings.py\n                utils.py\n                networks/\n                    lyco.py\n                    __init__.py\n                upscalers/\n                    __init__.py\n                    multidiffusion.py\n                    samplers.py\n                piplines/\n                    __init__.py\n                    lpw.py\n                    diffusers.py\n            lib/\n                __init__.py\n                diffusers/\n                    __init__.py\n                    scheduler.py\n            api/\n                __init__.py\n                plugin.py\n                models/\n                    diffusion.py\n                    __init__.py\n                diffusion/\n                    __init__.py\n                    pipelines/\n                        __init__.py\n                        diffusers.py\n                events/\n                    generation.py\n                    __init__.py\n        bge/\n            __init__.py\n        bge_rerank/\n            __init__.py\n        apps/\n            client.py\n            qa_strategy.py\n            __init__.py\n            builder.py\n            vector_db.py\n            utils.py\n            qa.py\n            byzer_storage/\n                memory_model_based.py\n                command.py\n                env.py\n                train_pt.py\n                train_sft.py\n                __init__.py\n                train_dpo.py\n                simple_api.py\n                generate_sft_data.py\n            byzer_sql/\n                __init__.py\n            llama_index/\n                byzerai_vectordb.py\n                byzerai.py\n                byzerai_embedding.py\n                __init__.py\n                collection_manager.py\n                byzerai_index_store.py\n                byzerai_docstore.py\n                simple_retrieval.py\n                byzerai_kvstore.py\n            agent/\n                conversable_agent.py\n                registry.py\n                groupchat.py\n                user_proxy_agent.py\n                __init__.py\n                agent.py\n                extensions/\n                    simple_retrieval_client.py\n                    planner.py\n                    python_codesandbox_agent.py\n                    preview_file_agent.py\n                    llama_index_retrieval_agent.py\n                    visualization_agent.py\n                    assistant_agent.py\n                    load_data_agent.py\n                    retrieval_agent.py\n                    output_agent.py\n                    __init__.py\n                    byzer_engine_agent.py\n                    subquestion_agent.py\n                    sql_reviewer_agent.py\n                    spark_sql_agent.py\n                    rhetorical_agent.py\n                    data_analysis.py\n                    data_analysis_pipeline_agent.py\n                    common_agent.py\n                    query_rewrite/\n                        time.py\n                        rhetorical.py\n                        __init__.py\n                        condition.py\n                        context.py\n                store/\n                    memory_store.py\n                    __init__.py\n                    stores.py\n        saas/\n            __init__.py\n            chatglm/\n                __init__.py\n            sparkdesk/\n                __init__.py\n            gemini/\n                __init__.py\n            azure_openai/\n                __init__.py\n            azure/\n                __init__.py\n            qianfan/\n                __init__.py\n            claude/\n                __init__.py\n            qianwen/\n                __init__.py\n            volcengine/\n                __init__.py\n            minimax/\n                __init__.py\n            baichuan/\n                __init__.py\n            qianwen_vl/\n                __init__.py\n            official_openai/\n                __init__.py\n            zhipu/\n                __init__.py\n            aws_bedrock/\n                __init__.py\n            openai/\n                __init__.py\n        falcon/\n            __init__.py\n        chatglm6b/\n            trainer_seq2seq.py\n            arguments.py\n            __init__.py\n            finetune.py\n            tunning/\n                __init__.py\n                infer.py\n                finetune.py\n                utils/\n                    config.py\n                    __init__.py\n                    other.py\n                    common.py\n                    seq2seq.py\n        starcode/\n            __init__.py\n        qwen_vl_chat/\n            __init__.py\n        zephyr/\n            __init__.py\n        dolly/\n            generate.py\n            __init__.py\n            consts.py\n            dolly_inference.py\n            trainer.py\n        llama/\n            __init__.py\n        whisper/\n            whisper_inference.py\n            __init__.py\n下面是一些文件路径以及每个文件对应的源码：\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/saas/official_openai/__init__.py\nimport time\nfrom typing import List, Tuple, Dict, Any, Union\nimport httpx\nfrom openai import OpenAI\nimport base64\nimport io\nimport json\nimport ray\nfrom byzerllm.utils.types import (\n    BlockVLLMStreamServer,\n    StreamOutputs,\n    SingleOutput,\n    SingleOutputMeta,\n    BlockBinaryStreamServer,\n)\nfrom byzerllm.utils.langutil import asyncfy_with_semaphore\nimport threading\nimport asyncio\nimport traceback\nimport uuid\nimport tempfile        \n\nclass CustomSaasAPI:\n\n    def __init__(self, infer_params: Dict[str, str]) -> None:\n\n        self.api_key = infer_params[\"saas.api_key\"]\n        self.model = infer_params.get(\"saas.model\", \"gpt-3.5-turbo-1106\")\n\n        other_params = {}\n\n        if \"saas.api_base\" in infer_params:\n            other_params[\"api_base\"] = infer_params[\"saas.api_base\"]\n\n        if \"saas.api_version\" in infer_params:\n            other_params[\"api_version\"] = infer_params[\"saas.api_version\"]\n\n        if \"saas.api_type\" in infer_params:\n            other_params[\"api_type\"] = infer_params[\"saas.api_type\"]\n\n        if \"saas.base_url\" in infer_params:\n            other_params[\"base_url\"] = infer_params[\"saas.base_url\"]\n\n        if \"saas.timeout\" in infer_params:\n            other_params[\"timeout\"] = float(infer_params[\"saas.timeout\"])\n\n        self.max_retries = int(infer_params.get(\"saas.max_retries\", 10))\n\n        self.meta = {\n            \"model_deploy_type\": \"saas\",\n            \"backend\": \"saas\",\n            \"support_stream\": True,\n            \"model_name\": self.model,\n        }\n\n        self.meta[\"embedding_mode\"] = \"embedding\" in self.model.lower()\n\n        self.proxies = infer_params.get(\"saas.proxies\", None)\n        self.local_address = infer_params.get(\"saas.local_address\", \"0.0.0.0\")\n\n        if not self.proxies:\n            self.client = OpenAI(**other_params, api_key=self.api_key)\n        else:\n            self.client = OpenAI(\n                **other_params,\n                api_key=self.api_key,\n                http_client=httpx.Client(\n                    proxies=self.proxies,\n                    transport=httpx.HTTPTransport(local_address=self.local_address),\n                ),\n            )\n\n        try:\n            ray.get_actor(\"BLOCK_VLLM_STREAM_SERVER\")\n        except ValueError:\n            try:\n                ray.remote(BlockVLLMStreamServer).options(\n                    name=\"BLOCK_VLLM_STREAM_SERVER\",\n                    lifetime=\"detached\",\n                    max_concurrency=1000,\n                ).remote()\n            except Exception as e:\n                pass\n        try:\n            ray.get_actor(\"BlockBinaryStreamServer\")\n        except ValueError:\n            try:\n                ray.remote(BlockBinaryStreamServer).options(\n                    name=\"BlockBinaryStreamServer\",\n                    lifetime=\"detached\",\n                    max_concurrency=1000,\n                ).remote()\n            except Exception as e:\n                pass\n\n    # saas/proprietary\n    def get_meta(self):\n        return [self.meta]\n\n    def process_input(self, ins: Union[str, List[Dict[str, Any]], Dict[str, Any]]):\n\n        if isinstance(ins, list) or isinstance(ins, dict):\n            return ins\n\n        content = []\n        try:\n            ins_json = json.loads(ins)\n        except:\n            return ins\n\n        ## speech\n        if isinstance(ins_json, dict):\n            return ins_json\n\n        content = []\n        #     [\n        #     {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n        #     {\n        #       \"type\": \"image_url\",\n        #       \"image_url\": {\n        #         \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n        #         \"detail\": \"high\"\n        #       },\n        #     },\n        #   ],\n        for item in ins_json:\n            # for format like this: {\"image\": \"xxxxxx\", \"text\": \"What’s in this image?\",\"detail\":\"high\"}\n            # or {\"image\": \"xxxxxx\"}, {\"text\": \"What’s in this image?\"}\n            if (\"image\" in item or \"image_url\" in item) and \"type\" not in item:\n                image_data = item.get(\"image\", item.get(\"image_url\", \"\"))\n                ## \"data:image/jpeg;base64,\"\n                if not image_data.startswith(\"data:\"):\n                    image_data = \"data:image/jpeg;base64,\" + image_data\n\n                ## get the other fields except image/text/image_url\n                other_fields = {\n                    k: v\n                    for k, v in item.items()\n                    if k not in [\"image\", \"text\", \"image_url\"]\n                }\n                content.append(\n                    {\n                        \"image_url\": {\"url\": image_data, **other_fields},\n                        \"type\": \"image_url\",\n                    }\n                )            \n\n            if \"text\" in item and \"type\" not in item:\n                text_data = item[\"text\"]\n                content.append({\"text\": text_data, \"type\": \"text\"})\n\n            ## for format like this: {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n            ## {\"type\": \"image_url\", \"image_url\": {\"url\":\"\",\"detail\":\"high\"}}\n            ## this is the standard format, just return it\n            if \"type\" in item and item[\"type\"] == \"text\":\n                content.append(item)\n\n            if \"type\" in item and item[\"type\"] == \"image_url\":\n                content.append(item)\n\n        if not content:\n            return ins\n\n        return content\n\n    async def async_embed_query(self, ins: str, **kwargs):\n        resp = await asyncfy_with_semaphore(\n            lambda: self.client.embeddings.create(input=[ins], model=self.model)\n        )()\n        embedding = resp.data[0].embedding\n        usage = resp.usage\n        return (\n            embedding,\n            {\n                \"metadata\": {\n                    \"input_tokens_count\": usage.prompt_tokens,\n                    \"generated_tokens_count\": 0,\n                }\n            },\n        )\n\n    def embed_query(self, ins: str, **kwargs):\n        resp = self.client.embeddings.create(input=[ins], model=self.model)\n        embedding = resp.data[0].embedding\n        usage = resp.usage\n        return (\n            embedding,\n            {\n                \"metadata\": {\n                    \"input_tokens_count\": usage.prompt_tokens,\n                    \"generated_tokens_count\": 0,\n                }\n            },\n        )\n\n    async def async_text_to_speech(\n        self, stream: bool, ins: str, voice: str, chunk_size: int = None, **kwargs\n    ):\n        if stream:\n            server = ray.get_actor(\"BlockBinaryStreamServer\")\n            request_id = [None]\n\n            def writer():\n                try:\n                    request_id[0] = str(uuid.uuid4())\n                    with self.client.with_streaming_response.audio.speech.create(\n                        model=self.model, voice=voice, input=ins, **kwargs\n                    ) as response:\n                        for chunk in response.iter_bytes(chunk_size):\n                            input_tokens_count = 0\n                            generated_tokens_count = 0\n                            ray.get(\n                                server.add_item.remote(\n                                    request_id[0],\n                                    StreamOutputs(\n                                        outputs=[\n                                            SingleOutput(\n                                                text=chunk,\n                                                metadata=SingleOutputMeta(\n                                                    input_tokens_count=input_tokens_count,\n                                                    generated_tokens_count=generated_tokens_count,\n                                                ),\n                                            )\n                                        ]\n                                    ),\n                                )\n                            )\n                except:\n                    traceback.print_exc()\n                ray.get(server.mark_done.remote(request_id[0]))\n\n            threading.Thread(target=writer, daemon=True).start()\n\n            time_count = 10 * 100\n            while request_id[0] is None and time_count > 0:\n                time.sleep(0.01)\n                time_count -= 1\n\n            if request_id[0] is None:\n                raise Exception(\"Failed to get request id\")\n\n            def write_running():\n                return ray.get(server.add_item.remote(request_id[0], \"RUNNING\"))\n\n            await asyncio.to_thread(write_running)\n            return [\n                (\n                    \"\",\n                    {\n                        \"metadata\": {\n                            \"request_id\": request_id[0],\n                            \"stream_server\": \"BlockBinaryStreamServer\",\n                        }\n                    },\n                )\n            ]\n\n        start_time = time.monotonic()\n        with io.BytesIO() as output:\n            async with asyncfy_with_semaphore(\n                lambda: self.client.with_streaming_response.audio.speech.create(\n                    model=self.model, voice=voice, input=ins, **kwargs\n                )\n            )() as response:\n                for chunk in response.iter_bytes():\n                    output.write(chunk)\n\n            base64_audio = base64.b64encode(output.getvalue()).decode()\n            time_cost = time.monotonic() - start_time\n            return [\n                (\n                    base64_audio,\n                    {\n                        \"metadata\": {\n                            \"request_id\": \"\",\n                            \"input_tokens_count\": 0,\n                            \"generated_tokens_count\": 0,\n                            \"time_cost\": time_cost,\n                            \"first_token_time\": 0,\n                            \"speed\": 0,\n                        }\n                    },\n                )\n            ]\n\n    async def async_speech_to_text(\n        self,\n        audio: str,\n        response_format: str = \"verbose_json\",\n        timestamp_granularities: List[str] = [\"word\",\"segment\"],\n    ):               \n        # Extract audio format and base64 data\n        data_prefix = \"data:audio/\"\n        base64_prefix = \";base64,\"\n        if not audio.startswith(data_prefix):\n            raise ValueError(\"Invalid audio data format\")\n        \n        format_end = audio.index(base64_prefix)\n        audio_format = audio[len(data_prefix):format_end]\n        base64_data = audio[format_end + len(base64_prefix):]\n        \n        # Decode the base64 audio data\n        audio_data = base64.b64decode(base64_data)\n\n        # Create a temporary file with the correct extension\n        with tempfile.NamedTemporaryFile(delete=False, suffix=f'.{audio_format}') as temp_audio_file:\n            temp_audio_file.write(audio_data)\n            temp_audio_file_path = temp_audio_file.name\n\n        try:\n            start_time = time.monotonic()\n                   \n            with open(temp_audio_file_path, \"rb\") as audio_file:                \n                transcription = self.client.audio.transcriptions.create(\n                    model=self.model or \"whisper-1\",\n                    file=audio_file,\n                    response_format=response_format,\n                    timestamp_granularities=timestamp_granularities,\n                )\n            time_cost = time.monotonic() - start_time\n\n            return [\n                (\n                    json.dumps(transcription.to_dict(), ensure_ascii=True),\n                    {\n                        \"metadata\": {\n                            \"request_id\": \"\",\n                            \"input_tokens_count\": 0,\n                            \"generated_tokens_count\": 0,\n                            \"time_cost\": time_cost,\n                            \"first_token_time\": 0,\n                            \"speed\": 0,\n                        }\n                    },\n                )\n            ]\n        finally:\n            # Clean up the temporary file\n            import os\n            os.unlink(temp_audio_file_path)\n\n    def image_to_text(self, ins: str, **kwargs):\n        pass\n\n    async def async_text_to_image(\n        self, stream: bool, input: str, size: str, quality: str, n: int, **kwargs\n    ):\n        if stream:\n            raise Exception(\"Stream not supported for text to image\")\n        start_time = time.monotonic()\n        response = await asyncfy_with_semaphore(\n            lambda: self.client.images.generate(\n                model=self.model,\n                prompt=input,\n                size=size,\n                quality=quality,\n                n=1,\n                response_format=\"b64_json\",\n                **kwargs,\n            )\n        )()\n        time_cost = time.monotonic() - start_time\n        base64_image = response.data[0].b64_json\n        return [\n            (\n                base64_image,\n                {\n                    \"metadata\": {\n                        \"request_id\": \"\",\n                        \"input_tokens_count\": 0,\n                        \"generated_tokens_count\": 0,\n                        \"time_cost\": time_cost,\n                        \"first_token_time\": 0,\n                        \"speed\": 0,\n                    }\n                },\n            )\n        ]\n\n    def text_to_text(self, ins: str, **kwargs):\n        pass\n\n    async def async_stream_chat(\n        self,\n        tokenizer,\n        ins: str,\n        his: List[Dict[str, Any]] = [],\n        max_length: int = 4096,\n        top_p: float = 0.7,\n        temperature: float = 0.9,\n        **kwargs,\n    ):\n\n        model = self.model\n\n        if \"model\" in kwargs:\n            model = kwargs[\"model\"]\n\n        messages = [\n            {\"role\": message[\"role\"], \"content\": self.process_input(message[\"content\"])}\n            for message in his\n        ] + [{\"role\": \"user\", \"content\": self.process_input(ins)}]\n\n        stream = kwargs.get(\"stream\", False)\n\n        ## content = [\n        ##    \"voice\": \"alloy\",\"input\": \"Hello, World!\",response_format: \"mp3\"]\n        last_message = messages[-1][\"content\"]\n\n        if isinstance(last_message, dict) and \"voice\" in last_message:\n            voice = last_message[\"voice\"]\n            response_format = last_message.get(\"response_format\", \"mp3\")\n            chunk_size = last_message.get(\"chunk_size\", None)\n            input = last_message[\"input\"]\n            return await self.async_text_to_speech(\n                stream=stream,\n                ins=input,\n                voice=voice,\n                chunk_size=chunk_size,\n                response_format=response_format,\n            )\n\n        if isinstance(last_message, dict) and \"input\" in last_message:\n            input = last_message[\"input\"]\n            size = last_message.get(\"size\", \"1024x1024\")\n            quality = last_message.get(\"quality\", \"standard\")\n            n = last_message.get(\"n\", 1)\n            return await self.async_text_to_image(\n                stream=stream, input=input, size=size, quality=quality, n=n\n            )\n\n        if isinstance(last_message, dict) and \"audio\" in last_message:            \n            audio_data = last_message.get(\"audio\", \"\")            \n            tpe = last_message.get(\"type\",\"wav\")\n            if not audio_data.startswith(\"data:\"):\n                audio_data = f\"data:audio/${tpe};base64,\" + audio_data                \n            return await self.async_speech_to_text(audio=audio_data)\n        \n        server = ray.get_actor(\"BLOCK_VLLM_STREAM_SERVER\")\n        request_id = [None]\n\n        def writer():\n            try:\n                r = \"\"\n                response = self.client.chat.completions.create(\n                    messages=messages,\n                    model=model,\n                    stream=True,\n                    max_tokens=max_length,\n                    temperature=temperature,\n                    top_p=top_p,\n                )\n                # input_tokens_count = 0\n                # generated_tokens_count = 0\n\n                request_id[0] = str(uuid.uuid4())\n\n                for chunk in response:\n                    content = chunk.choices[0].delta.content or \"\"\n                    r += content\n                    if hasattr(chunk, \"usage\") and chunk.usage:\n                        input_tokens_count = chunk.usage.prompt_tokens\n                        generated_tokens_count = chunk.usage.completion_tokens\n                    else:\n                        input_tokens_count = 0\n                        generated_tokens_count = 0\n                    ray.get(\n                        server.add_item.remote(\n                            request_id[0],\n                            StreamOutputs(\n                                outputs=[\n                                    SingleOutput(\n                                        text=r,\n                                        metadata=SingleOutputMeta(\n                                            input_tokens_count=input_tokens_count,\n                                            generated_tokens_count=generated_tokens_count,\n                                        ),\n                                    )\n                                ]\n                            ),\n                        )\n                    )\n            except:\n                traceback.print_exc()\n            ray.get(server.mark_done.remote(request_id[0]))\n\n        if stream:\n            threading.Thread(target=writer, daemon=True).start()\n\n            time_count = 60 * 100\n            while request_id[0] is None and time_count > 0:\n                time.sleep(0.01)\n                time_count -= 1\n\n            if request_id[0] is None:\n                raise Exception(\"Failed to get request id\")\n\n            def write_running():\n                return ray.get(server.add_item.remote(request_id[0], \"RUNNING\"))\n\n            await asyncio.to_thread(write_running)\n            return [\n                (\n                    \"\",\n                    {\n                        \"metadata\": {\n                            \"request_id\": request_id[0],\n                            \"stream_server\": \"BLOCK_VLLM_STREAM_SERVER\",\n                        }\n                    },\n                )\n            ]\n        else:\n            try:\n                start_time = time.monotonic()\n                response = await asyncfy_with_semaphore(\n                    lambda: self.client.chat.completions.create(\n                        messages=messages,\n                        model=model,\n                        max_tokens=max_length,\n                        temperature=temperature,\n                        top_p=top_p,\n                    )\n                )()\n\n                generated_text = response.choices[0].message.content\n                generated_tokens_count = response.usage.completion_tokens\n                input_tokens_count = response.usage.prompt_tokens\n                time_cost = time.monotonic() - start_time\n                return [\n                    (\n                        generated_text,\n                        {\n                            \"metadata\": {\n                                \"request_id\": response.id,\n                                \"input_tokens_count\": input_tokens_count,\n                                \"generated_tokens_count\": generated_tokens_count,\n                                \"time_cost\": time_cost,\n                                \"first_token_time\": 0,\n                                \"speed\": float(generated_tokens_count) / time_cost,\n                            }\n                        },\n                    )\n                ]\n            except Exception as e:\n                print(f\"Error: {e}\")\n                raise e\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/whisper/__init__.py\nimport whisper\nfrom typing import Dict, List, Tuple\nfrom byzerllm.utils.types import StopSequencesCriteria\n\nfrom typing import Dict, Any, List, Union\nfrom pyjava.storage import streaming_tar as STar\nimport json\nimport base64\nimport tempfile\nimport time\n\n\ndef get_meta(self):\n    return [\n        {\n            \"model_deploy_type\": \"proprietary\",\n            \"backend\": \"transformers\",\n            \"message_format\": True\n        }\n    ]\n\n\ndef process_input(ins: Union[str, List[Dict[str, Any]], Dict[str, Any]]):\n    # print(ins) \n    if isinstance(ins, list) or isinstance(ins, dict):\n        return ins\n\n    content = []\n    try:\n        ins_json = json.loads(ins)\n    except:\n        return ins\n\n    ## speech\n    if isinstance(ins_json, dict):\n        return ins_json\n\n    content = []\n    #     [\n    #     {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n    #     {\n    #       \"type\": \"image_url\",\n    #       \"image_url\": {\n    #         \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n    #         \"detail\": \"high\"\n    #       },\n    #     },\n    #   ],\n    for item in ins_json:\n        # for format like this: {\"image\": \"xxxxxx\", \"text\": \"What’s in this image?\",\"detail\":\"high\"}\n        # or {\"image\": \"xxxxxx\"}, {\"text\": \"What’s in this image?\"}\n        if (\"image\" in item or \"image_url\" in item) and \"type\" not in item:\n            image_data = item.get(\"image\", item.get(\"image_url\", \"\"))\n            ## \"data:image/jpeg;base64,\"\n            if not image_data.startswith(\"data:\"):\n                image_data = \"data:image/jpeg;base64,\" + image_data\n\n            ## get the other fields except image/text/image_url\n            other_fields = {\n                k: v for k, v in item.items() if k not in [\"image\", \"text\", \"image_url\"]\n            }\n            content.append(\n                {\n                    \"image_url\": {\"url\": image_data, **other_fields},\n                    \"type\": \"image_url\",\n                }\n            )\n\n        if \"text\" in item and \"type\" not in item:\n            text_data = item[\"text\"]\n            content.append({\"text\": text_data, \"type\": \"text\"})\n\n        ## for format like this: {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n        ## {\"type\": \"image_url\", \"image_url\": {\"url\":\"\",\"detail\":\"high\"}}\n        ## this is the standard format, just return it\n        if \"type\" in item and item[\"type\"] == \"text\":\n            content.append(item)\n\n        if \"type\" in item and item[\"type\"] == \"image_url\":\n            content.append(item)\n\n    if not content:\n        return ins\n\n    return content\n\n\ndef stream_chat(\n    self,\n    tokenizer,\n    ins: str,\n    his: List[Dict[str, str]] = [],\n    max_length: int = 4090,\n    top_p: float = 0.95,\n    temperature: float = 0.1,\n    **kwargs,\n):\n    messages = [\n        {\"role\": message[\"role\"], \"content\": process_input(message[\"content\"])}\n        for message in his\n    ] + [{\"role\": \"user\", \"content\": process_input(ins)}]\n\n    audio_input = messages[-1][\"content\"]\n\n    audio_data = audio_input.get(\"audio\", \"\")\n    audio = audio_input[\"audio\"]\n    response_format = audio_input.get(\"response_format\", \"json\")\n    timestamp_granularities = audio_input.get(\n        \"timestamp_granularities\", [\"word\", \"segment\"]\n    )\n    data_prefix = \"data:audio/\"\n    base64_prefix = \";base64,\"\n    if not audio.startswith(data_prefix):\n        raise ValueError(\"Invalid audio data format\")\n\n    format_end = audio.index(base64_prefix)\n    audio_format = audio[len(data_prefix) : format_end]\n    base64_data = audio[format_end + len(base64_prefix) :]\n\n    # Decode the base64 audio data\n    audio_data = base64.b64decode(base64_data)\n\n    # Create a temporary file with the correct extension\n    with tempfile.NamedTemporaryFile(\n        delete=False, suffix=f\".{audio_format}\"\n    ) as temp_audio_file:\n        temp_audio_file.write(audio_data)\n        temp_audio_file_path = temp_audio_file.name\n\n    try:\n        start_time = time.monotonic()\n        word_timestamps = \"word\" in timestamp_granularities\n        \n        result = self.transcribe(temp_audio_file_path, word_timestamps=word_timestamps)        \n            \n        time_cost = time.monotonic() - start_time\n        return [\n            (\n                json.dumps(result, ensure_ascii=False),\n                {\n                    \"metadata\": {\n                        \"request_id\": \"\",\n                        \"input_tokens_count\": 0,\n                        \"generated_tokens_count\": 0,\n                        \"time_cost\": time_cost,\n                        \"first_token_time\": 0,\n                        \"speed\": 0,\n                    }\n                },\n            )\n        ]\n    finally:\n        # Clean up the temporary file\n        import os\n\n        os.unlink(temp_audio_file_path)\n\n\ndef init_model(\n    model_dir, infer_params: Dict[str, str] = {}, sys_conf: Dict[str, str] = {}\n):\n    model = whisper.load_model(\"large\", download_root=model_dir)\n    import types\n\n    model.stream_chat = types.MethodType(stream_chat, model)\n    model.get_meta = types.MethodType(get_meta, model)\n    return (model, None)\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/notebooks/test_sensevoice.py\nimport sys\nimport os\n\n# Add the project root to the Python path\nsys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\")))\n\nfrom src.byzerllm.sensevoice import init_model, format_str_v3\nimport base64\nimport json\n\n\ndef main():\n    # Initialize the model\n    model_dir = \"/Users/allwefantasy/models/SenseVoiceSmall\"\n    model, _ = init_model(model_dir)\n\n    # Prepare test audio\n    with open(\n        \"/Users/allwefantasy/models/SenseVoiceSmall/example/zh.mp3\", \"rb\"\n    ) as audio_file:\n        audio_data = audio_file.read()\n        base64_audio = base64.b64encode(audio_data).decode(\"utf-8\")        \n        # Prepare input in the expected format\n        test_input = json.dumps(\n            {\n                            \"type\": \"audio\",\n                            \"audio\": f\"data:audio/wav;base64,{base64_audio}\",\n                        }\n        )\n\n        # Call stream_chat\n        result = model.stream_chat(tokenizer=None, ins = test_input)\n\n        # Process and print the result\n        if result and len(result) > 0:\n            output, metadata = result[0]\n            parsed_output = json.loads(output)\n            formatted_text = parsed_output.get(\"text\", \"\")\n            print(\"Transcription:\", formatted_text)\n            print(\"Metadata:\", metadata)\n        else:\n            print(\"No result returned from the model.\")\n\n\nif __name__ == \"__main__\":\n    main()\n\n\n##File: /Users/allwefantasy/projects/Qwen2-Audio/README_CN.md\n<p align=\"left\">\n        中文</a>&nbsp ｜ &nbsp<a href=\"README.md\">English</a>\n</p>\n<br><br>\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/assets/blog/qwenaudio/qwen2audio_logo.png\" width=\"400\"/>\n<p>\n\n<p align=\"center\">\nQwen2-Audio-7B <a href=\"https://modelscope.cn/models/qwen/Qwen2-Audio-7B\">🤖 </a> | <a href=\"https://huggingface.co/Qwen/Qwen2-Audio-7B\">🤗</a>&nbsp ｜ Qwen-Audio-7B-Instruct <a href=\"https://modelscope.cn/models/qwen/Qwen2-Audio-7B-Instruct\">🤖 </a>| <a href=\"https://huggingface.co/Qwen/Qwen2-Audio-7B-Instruct\">🤗</a>&nbsp ｜ Demo<a href=\"https://modelscope.cn/studios/qwen/Qwen2-Audio-Instruct-Demo\"> 🤖</a> | <a href=\"https://huggingface.co/spaces/Qwen/Qwen2-Audio-Instruct-Demo\">🤗</a>&nbsp\n<br>\n📑 <a href=\"https://arxiv.org/abs/2407.10759\">Paper</a> &nbsp&nbsp | &nbsp&nbsp 📑 <a href=\"https://qwenlm.github.io/blog/qwen2-audio\">Blog</a> &nbsp&nbsp | &nbsp&nbsp 💬 <a href=\"https://github.com/QwenLM/Qwen/blob/main/assets/wechat.png\">WeChat (微信)</a>&nbsp&nbsp | &nbsp&nbsp <a href=\"https://discord.gg/CV4E9rpNSD\">Discord</a>&nbsp&nbsp\n</p>\n\n\n\n我们介绍Qwen-Audio的最新进展：Qwen2-Audio。作为一个大规模音频语言模型，Qwen2-Audio能够接受各种音频信号输入，并根据语音指令执行音频分析或直接响应文本。我们介绍两种不同的音频交互模式：语音聊天 voice chat 和音频分析 audio analysis。\n\n* 语音聊天：用户可以自由地与 Qwen2-Audio 进行语音互动，而无需文本输入；\n* 音频分析：用户可以在互动过程中提供音频和文本指令对音频进行分析；\n\n**我们已经开源了 Qwen2-Audio 系列的两个模型：Qwen2-Audio-7B和Qwen2-Audio-7B-Instruct。**\n\n## 模型结构与训练范式\n\nQwen2-Audio 三阶段训练过程概述。\n\n<p align=\"left\">\n    <img src=\"assets/framework.png\" width=\"80%\"/>\n<p>\n\n\n\n## 新闻\n* 2024.8.9 🎉 我们在 ModelScope 和 Hugging Face 开源了`Qwen2-Audio-7B`和`Qwen2-Audio-7B-Instruct`的 checkpoint.\n* 2024.7.15 🎉 我们发布了 Qwen2-Audio 的[论文](https://arxiv.org/abs/2407.10759), 介绍了相关的模型结构，训练方法和模型表现。\n* 2023.11.30 🔥 我们发布了**Qwen-Audio**系列\n<br>\n\n## 评测\n我们在标准的13个学术数据集上评测了模型的能力如下：\n<table><thead><tr><th>Task</th><th>Description</th><th>Dataset</th><th>Split</th><th>Metric</th></tr></thead><tbody><tr><td rowspan=\"4\">ASR</td><td rowspan=\"4\">Automatic Speech Recognition</td><td>Fleurs</td><td>dev | test</td><td rowspan=\"4\">WER</td></tr><tr><td>Aishell2</td><td>test</td></tr><tr><td>Librispeech</td><td>dev | test</td></tr><tr><td>Common Voice</td><td>dev | test</td></tr><tr><td>S2TT</td><td>Speech-to-Text Translation</td><td>CoVoST2</td><td>test</td><td>BLEU </td></tr><tr><td>SER</td><td>Speech Emotion Recognition</td><td>Meld</td><td>test</td><td>ACC</td></tr><tr><td>VSC</td><td>Vocal Sound Classification</td><td>VocalSound</td><td>test</td><td>ACC</td></tr><tr><td rowspan=\"4\"><a href=\"https://github.com/OFA-Sys/AIR-Bench\">AIR-Bench</a><br></td><td>Chat-Benchmark-Speech</td><td>Fisher<br>SpokenWOZ<br>IEMOCAP<br>Common voice</td><td>dev | test</td><td>GPT-4 Eval</td></tr><tr><td>Chat-Benchmark-Sound</td><td>Clotho</td><td>dev | test</td><td>GPT-4 Eval</td></tr>\n<tr><td>Chat-Benchmark-Music</td><td>MusicCaps</td><td>dev | test</td><td>GPT-4 Eval</td></tr><tr><td>Chat-Benchmark-Mixed-Audio</td><td>Common voice<br>AudioCaps<br>MusicCaps</td><td>dev | test</td><td>GPT-4 Eval</td></tr></tbody></table>\n\n\n以下是整体表现：\n<p align=\"left\">\n    <img src=\"assets/radar_compare_qwen_audio.png\" width=\"70%\"/>\n<p>\n\n评测分数详情如下：\n<br>\n<b>（注意：我们所展示的评测结果是在原始训练框架的初始模型上的，然而在框架转换 Huggingface 后指标出现了部分波动，在这里我们展示我们的全部测评结果：首先是论文中的初始模型结果）</b>\n\n<table><thead><tr><th rowspan=\"2\">Task</th><th rowspan=\"2\">Dataset</th><th rowspan=\"2\">Model</th><th colspan=\"2\">Performance</th></tr><tr><th>Metrics</th><th>Results</th></tr></thead><tbody><tr><td rowspan=\"15\">ASR</td><td rowspan=\"7\"><b>Librispeech</b><br>dev-clean | dev-other | <br>test-clean | test-other</td><td>SpeechT5</td><td rowspan=\"7\">WER </td><td>2.1 | 5.5 | 2.4 | 5.8</td></tr><tr><td>SpeechNet</td><td>- | - | 30.7 | -</td></tr><tr><td>SLM-FT</td><td>- | - | 2.6 | 5.0</td></tr><tr><td>SALMONN</td><td>- | - | 2.1 | 4.9</td></tr><tr><td>SpeechVerse</td><td>- | - | 2.1 | 4.4</td></tr><tr><td>Qwen-Audio</td><td>1.8 | 4.0 | 2.0 | 4.2</td></tr><tr><td>Qwen2-Audio</td><td><b>1.3 | 3.4 | 1.6 | 3.6</b></td></tr><tr><td rowspan=\"2\"><b>Common Voice 15</b> <br>en | zh | yue | fr</td><td>Whisper-large-v3</td><td rowspan=\"2\">WER </td><td>9.3 | 12.8 | 10.9 | 10.8</td></tr><tr><td>Qwen2-Audio</td><td><b>8.6 | 6.9 | 5.9 | 9.6</b></td></tr>\n<tr><td rowspan=\"2\"><b>Fleurs</b> <br>zh</td><td>Whisper-large-v3</td><td rowspan=\"2\">WER </td><td>7.7</td></tr><tr><td>Qwen2-Audio</td><td><b>7.5</b></td></tr><tr><td rowspan=\"4\"><b>Aishell2</b> <br>Mic | iOS | Android</td><td>MMSpeech-base</td><td rowspan=\"4\">WER </td><td>4.5 | 3.9 | 4.0</td></tr><tr><td>Paraformer-large</td><td>- | <b>2.9</b> | -</td></tr><tr><td>Qwen-Audio</td><td>3.3 | 3.1 | 3.3</td></tr><tr><td>Qwen2-Audio</td><td><b>3.0</b> | 3.0 | <b>2.9</b></td></tr><tr><td rowspan=\"8\">S2TT</td><td rowspan=\"5\"><b>CoVoST2</b> <br>en-de | de-en | <br>en-zh | zh-en</td><td>SALMONN</td><td rowspan=\"5\">BLEU </td><td>18.6 | - | 33.1 | -</td></tr><tr><td>SpeechLLaMA</td><td>- | 27.1 | - | 12.3</td></tr><tr><td>BLSP</td><td>14.1 | - | - | -</td></tr><tr><td>Qwen-Audio</td><td>25.1 | 33.9 | 41.5 | 15.7</td></tr><tr><td>Qwen2-Audio</td><td><b>29.9 | 35.2 | 45.2 | 24.4</b></td></tr>\n<tr><td rowspan=\"3\"><b>CoVoST2</b> <br>es-en | fr-en | it-en |</td><td>SpeechLLaMA</td><td rowspan=\"3\">BLEU </td><td>27.9 | 25.2 | 25.9</td></tr><tr><td>Qwen-Audio</td><td>39.7 | <b>38.5</b> | 36.0</td></tr><tr><td>Qwen2-Audio</td><td><b>40.0 | 38.5 | 36.3</b></td></tr><tr><td rowspan=\"3\">SER</td><td rowspan=\"3\"><b>Meld</b></td><td>WavLM-large</td><td rowspan=\"3\">ACC </td><td>0.542</td></tr><tr><td>Qwen-Audio</td><td><b>0.557</b></td></tr><tr><td>Qwen2-Audio</td><td>0.553</td></tr><tr><td rowspan=\"4\">VSC</td><td rowspan=\"4\"><b>VocalSound</b></td><td>CLAP</td><td rowspan=\"4\">ACC </td><td>0.4945</td></tr><tr><td>Pengi</td><td>0.6035</td></tr><tr><td>Qwen-Audio</td><td>0.9289</td></tr><tr><td>Qwen2-Audio</td><td><b>0.9392</b></td></tr>\n<tr><td>AIR-Bench <br></td><td><b>Chat Benchmark</b><br>Speech | Sound |<br> Music | Mixed-Audio</td><td>SALMONN<br>BLSP<br>Pandagpt<br>Macaw-LLM<br>SpeechGPT<br>Next-gpt<br>Qwen-Audio<br>Gemini-1.5-pro<br>Qwen2-Audio</td><td>GPT-4 </td><td>6.16 | 6.28 | 5.95 | 6.08<br>6.17 | 5.55 | 5.08 | 5.33<br>3.58 | 5.46 | 5.06 | 4.25<br>0.97 | 1.01 | 0.91 | 1.01<br>1.57 | 0.95 | 0.95 | 4.13<br>3.86 | 4.76 | 4.18 | 4.13<br>6.47 | 6.95 | 5.52 | 6.08<br>6.97 | 5.49 | 5.06 | 5.27<br><b>7.18 | 6.99 | 6.79 | 6.77</b></td></tr></tbody></table>\n\n<b>（其次是转换 huggingface 后的）</b>\n\n<table><thead><tr><th rowspan=\"2\">Task</th><th rowspan=\"2\">Dataset</th><th rowspan=\"2\">Model</th><th colspan=\"2\">Performance</th></tr><tr><th>Metrics</th><th>Results</th></tr></thead><tbody><tr><td rowspan=\"15\">ASR</td><td rowspan=\"7\"><b>Librispeech</b><br>dev-clean | dev-other | <br>test-clean | test-other</td><td>SpeechT5</td><td rowspan=\"7\">WER </td><td>2.1 | 5.5 | 2.4 | 5.8</td></tr><tr><td>SpeechNet</td><td>- | - | 30.7 | -</td></tr><tr><td>SLM-FT</td><td>- | - | 2.6 | 5.0</td></tr><tr><td>SALMONN</td><td>- | - | 2.1 | 4.9</td></tr><tr><td>SpeechVerse</td><td>- | - | 2.1 | 4.4</td></tr><tr><td>Qwen-Audio</td><td>1.8 | 4.0 | 2.0 | 4.2</td></tr><tr><td>Qwen2-Audio</td><td><b>1.7 | 3.6 | 1.7 | 4.0</b></td></tr><tr><td rowspan=\"2\"><b>Common Voice 15</b> <br>en | zh | yue | fr</td><td>Whisper-large-v3</td><td rowspan=\"2\">WER </td><td>9.3 | 12.8 | 10.9 | 10.8</td></tr><tr><td>Qwen2-Audio</td><td><b>8.7 | 6.5 | 5.9 | 9.6</b></td></tr>\n<tr><td rowspan=\"2\"><b>Fleurs</b> <br>zh</td><td>Whisper-large-v3</td><td rowspan=\"2\">WER </td><td>7.7</td></tr><tr><td>Qwen2-Audio</td><td><b>7.0</b></td></tr><tr><td rowspan=\"4\"><b>Aishell2</b> <br>Mic | iOS | Android</td><td>MMSpeech-base</td><td rowspan=\"4\">WER </td><td>4.5 | 3.9 | 4.0</td></tr><tr><td>Paraformer-large</td><td>- | <b>2.9</b> | -</td></tr><tr><td>Qwen-Audio</td><td>3.3 | 3.1 | 3.3</td></tr><tr><td>Qwen2-Audio</td><td><b>3.2</b> | 3.1 | <b>2.9</b></td></tr><tr><td rowspan=\"8\">S2TT</td><td rowspan=\"5\"><b>CoVoST2</b> <br>en-de | de-en | <br>en-zh | zh-en</td><td>SALMONN</td><td rowspan=\"5\">BLEU </td><td>18.6 | - | 33.1 | -</td></tr><tr><td>SpeechLLaMA</td><td>- | 27.1 | - | 12.3</td></tr><tr><td>BLSP</td><td>14.1 | - | - | -</td></tr><tr><td>Qwen-Audio</td><td>25.1 | <b>33.9</b> | 41.5 | 15.7</td></tr><tr><td>Qwen2-Audio</td><td><b>29.6</b> | 33.6 | <b>45.6</b> | <b>24.0</b></td></tr>\n<tr><td rowspan=\"3\"><b>CoVoST2</b> <br>es-en | fr-en | it-en |</td><td>SpeechLLaMA</td><td rowspan=\"3\">BLEU </td><td>27.9 | 25.2 | 25.9</td></tr><tr><td>Qwen-Audio</td><td><b>39.7 | 38.5 | 36.0</b></td></tr><tr><td>Qwen2-Audio</td><td>38.7 | 37.2 | 35.2</td></tr><tr><td rowspan=\"3\">SER</td><td rowspan=\"3\"><b>Meld</b></td><td>WavLM-large</td><td rowspan=\"3\">ACC </td><td>0.542</td></tr><tr><td>Qwen-Audio</td><td><b>0.557</b></td></tr><tr><td>Qwen2-Audio</td><td>0.535</td></tr><tr><td rowspan=\"4\">VSC</td><td rowspan=\"4\"><b>VocalSound</b></td><td>CLAP</td><td rowspan=\"4\">ACC </td><td>0.4945</td></tr><tr><td>Pengi</td><td>0.6035</td></tr><tr><td>Qwen-Audio</td><td>0.9289</td></tr><tr><td>Qwen2-Audio</td><td><b>0.9395</b></td></tr>\n<tr><td>AIR-Bench <br></td><td><b>Chat Benchmark</b><br>Speech | Sound |<br> Music | Mixed-Audio</td><td>SALMONN<br>BLSP<br>Pandagpt<br>Macaw-LLM<br>SpeechGPT<br>Next-gpt<br>Qwen-Audio<br>Gemini-1.5-pro<br>Qwen2-Audio</td><td>GPT-4 </td><td>6.16 | 6.28 | 5.95 | 6.08<br>6.17 | 5.55 | 5.08 | 5.33<br>3.58 | 5.46 | 5.06 | 4.25<br>0.97 | 1.01 | 0.91 | 1.01<br>1.57 | 0.95 | 0.95 | 4.13<br>3.86 | 4.76 | 4.18 | 4.13<br>6.47 | <b>6.95</b> | 5.52 | 6.08<br>6.97 | 5.49 | 5.06 | 5.27<br><b>7.24</b> | 6.83 | <b>6.73</b> | <b>6.42</b></td></tr></tbody></table>\n\n\n我们提供了以上**所有**评测脚本以供复现我们的实验结果。请阅读 [eval_audio/EVALUATION.md](eval_audio/EVALUATION.md) 了解更多信息。\n\n## 部署要求\nThe code of Qwen2-Audio has been in the latest Hugging face transformers and we advise you to build from source with command `pip install git+https://github.com/huggingface/transformers`, or you might encounter the following error:\nQwen2-Audio的代码已经包含在最新的 Hugging Face Transformers 的主分支中，我们建议您使用命令`pip install git+https://github.com/huggingface/transformers`\n```\nKeyError: 'qwen2-audio'\n```\n## 快速使用\n我们提供简单的示例来说明如何利用 🤗 Transformers 快速使用 Qwen2-Audio-7B 和 Qwen2-Audio-7B-Instruct。\n在开始前，请确保你已经配置好环境并安装好相关的代码包。最重要的是，确保你满足上述要求，然后安装相关的依赖库。\n接下来你可以开始使用 Transformers 或者 ModelScope 来使用我们的模型。目前Qwen2-Audio-7B 及 Qwen2-Audio-7B-Instruct 模型处理30秒以内的音频表现更佳。\n#### 🤗 Hugging Face Transformers\n如希望使用 Qwen2-Audio-7B-Instruct 进行推理，我们分别演示语音聊天和音频分析的交互方式，所需要写的只是如下所示的数行代码。\n##### 语音聊天推理\n在语音聊天模式下，用户可以自由地与 Qwen2-Audio 进行语音交互，无需文字输入：\n```python\nfrom io import BytesIO\nfrom urllib.request import urlopen\nimport librosa\nfrom transformers import Qwen2AudioForConditionalGeneration, AutoProcessor\n\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\")\nmodel = Qwen2AudioForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\", device_map=\"auto\")\n\nconversation = [\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/guess_age_gender.wav\"},\n    ]},\n    {\"role\": \"assistant\", \"content\": \"Yes, the speaker is female and in her twenties.\"},\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/translate_to_chinese.wav\"},\n    ]},\n]\ntext = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\naudios = []\nfor message in conversation:\n    if isinstance(message[\"content\"], list):\n        for ele in message[\"content\"]:\n            if ele[\"type\"] == \"audio\":\n                audios.append(librosa.load(\n                    BytesIO(urlopen(ele['audio_url']).read()), \n                    sr=processor.feature_extractor.sampling_rate)[0]\n                )\n\ninputs = processor(text=text, audios=audios, return_tensors=\"pt\", padding=True)\ninputs.input_ids = inputs.input_ids.to(\"cuda\")\n\ngenerate_ids = model.generate(**inputs, max_length=256)\ngenerate_ids = generate_ids[:, inputs.input_ids.size(1):]\n\nresponse = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n```\n##### 音频分析推理\n在音频分析中，用户可以提供音频和文字问题来实现对音频的分析：\n```python\nfrom io import BytesIO\nfrom urllib.request import urlopen\nimport librosa\nfrom transformers import Qwen2AudioForConditionalGeneration, AutoProcessor\n\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\")\nmodel = Qwen2AudioForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\", device_map=\"auto\")\n\nconversation = [\n    {'role': 'system', 'content': 'You are a helpful assistant.'}, \n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/glass-breaking-151256.mp3\"},\n        {\"type\": \"text\", \"text\": \"What's that sound?\"},\n    ]},\n    {\"role\": \"assistant\", \"content\": \"It is the sound of glass shattering.\"},\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"text\", \"text\": \"What can you do when you hear that?\"},\n    ]},\n    {\"role\": \"assistant\", \"content\": \"Stay alert and cautious, and check if anyone is hurt or if there is any damage to property.\"},\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/1272-128104-0000.flac\"},\n        {\"type\": \"text\", \"text\": \"What does the person say?\"},\n    ]},\n]\ntext = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\naudios = []\nfor message in conversation:\n    if isinstance(message[\"content\"], list):\n        for ele in message[\"content\"]:\n            if ele[\"type\"] == \"audio\":\n                audios.append(\n                    librosa.load(\n                        BytesIO(urlopen(ele['audio_url']).read()), \n                        sr=processor.feature_extractor.sampling_rate)[0]\n                )\n\ninputs = processor(text=text, audios=audios, return_tensors=\"pt\", padding=True)\ninputs.input_ids = inputs.input_ids.to(\"cuda\")\n\ngenerate_ids = model.generate(**inputs, max_length=256)\ngenerate_ids = generate_ids[:, inputs.input_ids.size(1):]\n\nresponse = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n```\n##### 批量推理\n我们也支持批量推理：\n```python\nfrom io import BytesIO\nfrom urllib.request import urlopen\nimport librosa\nfrom transformers import Qwen2AudioForConditionalGeneration, AutoProcessor\n\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\")\nmodel = Qwen2AudioForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\", device_map=\"auto\")\n\nconversation1 = [\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/glass-breaking-151256.mp3\"},\n        {\"type\": \"text\", \"text\": \"What's that sound?\"},\n    ]},\n    {\"role\": \"assistant\", \"content\": \"It is the sound of glass shattering.\"},\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/f2641_0_throatclearing.wav\"},\n        {\"type\": \"text\", \"text\": \"What can you hear?\"},\n    ]}\n]\n\nconversation2 = [\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/1272-128104-0000.flac\"},\n        {\"type\": \"text\", \"text\": \"What does the person say?\"},\n    ]},\n]\n\nconversations = [conversation1, conversation2]\n\ntext = [processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False) for conversation in conversations]\n\naudios = []\nfor conversation in conversations:\n    for message in conversation:\n        if isinstance(message[\"content\"], list):\n            for ele in message[\"content\"]:\n                if ele[\"type\"] == \"audio\":\n                    audios.append(\n                        librosa.load(\n                            BytesIO(urlopen(ele['audio_url']).read()), \n                            sr=processor.feature_extractor.sampling_rate)[0]\n                    )\n\ninputs = processor(text=text, audios=audios, return_tensors=\"pt\", padding=True)\ninputs['input_ids'] = inputs['input_ids'].to(\"cuda\")\ninputs.input_ids = inputs.input_ids.to(\"cuda\")\n\ngenerate_ids = model.generate(**inputs, max_length=256)\ngenerate_ids = generate_ids[:, inputs.input_ids.size(1):]\n\nresponse = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n```\n运行Qwen2-Audio-7B同样非常简单。\n```python\nfrom io import BytesIO\nfrom urllib.request import urlopen\nimport librosa\nfrom transformers import AutoProcessor, Qwen2AudioForConditionalGeneration\n\nmodel = Qwen2AudioForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-Audio-7B\" ,trust_remote_code=True)\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-Audio-7B\" ,trust_remote_code=True)\n\nprompt = \"<|audio_bos|><|AUDIO|><|audio_eos|>Generate the caption in English:\"\nurl = \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Audio/glass-breaking-151256.mp3\"\naudio, sr = librosa.load(BytesIO(urlopen(url).read()), sr=processor.feature_extractor.sampling_rate)\ninputs = processor(text=prompt, audios=audio, return_tensors=\"pt\")\n\ngenerated_ids = model.generate(**inputs, max_length=256)\ngenerated_ids = generated_ids[:, inputs.input_ids.size(1):]\nresponse = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n```\n#### 🤖 ModelScope\n我们强烈建议用户，特别是中国大陆地区的用户，使用 ModelScope。`snapshot_download` 可以帮助您解决下载检查点时遇到的问题。\n<br>\n## Demo\n### Web UI\n我们提供了 Web UI 的 demo 供用户使用。在开始前，确保已经安装如下代码库：\n```\npip install -r requirements_web_demo.txt\n```\n随后运行如下命令，并点击生成链接：\n```\npython demo/web_demo_audio.py\n```\n<br>\n\n## 样例展示\n更多样例将更新于[通义千问博客](https://qwenlm.github.io/blog/qwen2-audio)中的 Qwen2-Audio 博客。\n\n## 团队招聘\n\n我们是通义千问语音多模态团队，致力于以通义千问为核心，拓展音频多模态理解和生成能力，实现自由灵活的音频交互。目前团队蓬勃发展中，如有意向实习或全职加入我们，请发送简历至 `qwen_audio@list.alibaba-inc.com`.\n<br>\n\n## 使用协议\n\n请查看每个模型在其 Hugging Face 仓库中的许可证。您无需提交商业使用申请。\n<br>\n\n\n\n## 引用\n\n如果你觉得我们的论文和代码对你的研究有帮助，请考虑 :star: 和引用 :pencil: :)\n\n```BibTeX\n@article{Qwen-Audio,\n  title={Qwen-Audio: Advancing Universal Audio Understanding via Unified Large-Scale Audio-Language Models},\n  author={Chu, Yunfei and Xu, Jin and Zhou, Xiaohuan and Yang, Qian and Zhang, Shiliang and Yan, Zhijie  and Zhou, Chang and Zhou, Jingren},\n  journal={arXiv preprint arXiv:2311.07919},\n  year={2023}\n}\n```\n\n```BibTeX\n@article{Qwen2-Audio,\n  title={Qwen2-Audio Technical Report},\n  author={Chu, Yunfei and Xu, Jin and Yang, Qian and Wei, Haojie and Wei, Xipin and Guo,  Zhifang and Leng, Yichong and Lv, Yuanjun and He, Jinzheng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},\n  journal={arXiv preprint arXiv:2407.10759},\n  year={2024}\n}\n```\n## 联系我们\n\n如果你想给我们的研发团队和产品团队留言，请通过邮件（`qianwen_opensource@alibabacloud.com`）联系我们。\n<br>\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/qwen_voice/__init__.py\nimport time\nfrom typing import List, Tuple, Dict, Any, Union\nimport base64\nimport json\nimport librosa\nimport io\nfrom transformers import Qwen2AudioForConditionalGeneration, AutoProcessor\n\ndef get_meta(self):\n    return [\n        {\n            \"model_deploy_type\": \"proprietary\",\n            \"backend\": \"transformers\",\n            \"message_format\": True\n        }\n    ]\n\ndef process_input(ins: Union[str, List[Dict[str, Any]], Dict[str, Any]]):\n    if isinstance(ins, list) or isinstance(ins, dict):\n        return ins\n\n    content = []\n    try:\n        ins_json = json.loads(ins)\n    except:\n        return ins\n\n    if isinstance(ins_json, dict):\n        return ins_json\n\n    for item in ins_json:\n        if (\"audio\" in item or \"audio_url\" in item) and \"type\" not in item:\n            audio_data = item.get(\"audio\", item.get(\"audio_url\", \"\"))\n            if not audio_data.startswith(\"data:\"):\n                audio_data = \"data:audio/wav;base64,\" + audio_data\n\n            other_fields = {k: v for k, v in item.items() if k not in [\"audio\", \"text\", \"audio_url\"]}\n            content.append(\n                {\n                    \"audio_url\": {\"url\": audio_data, **other_fields},\n                    \"type\": \"audio\",\n                }\n            )\n\n        if \"text\" in item and \"type\" not in item:\n            text_data = item[\"text\"]\n            content.append({\"text\": text_data, \"type\": \"text\"})\n\n        if \"type\" in item and item[\"type\"] in [\"text\", \"audio\"]:\n            content.append(item)\n\n    if not content:\n        return ins\n\n    return content\n\ndef init_model(model_dir, infer_params: Dict[str, str] = {}, sys_conf: Dict[str, str] = {}):\n    processor = AutoProcessor.from_pretrained(model_dir)\n    model = Qwen2AudioForConditionalGeneration.from_pretrained(model_dir, device_map=\"auto\")\n\n    def stream_chat(\n        self,\n        tokenizer,\n        ins: str,\n        his: List[Dict[str, str]] = [],\n        max_length: int = 4096,\n        top_p: float = 0.95,\n        temperature: float = 0.1,\n        **kwargs,\n    ):\n        messages = [\n            {\"role\": message[\"role\"], \"content\": process_input(message[\"content\"])}\n            for message in his\n        ] + [{\"role\": \"user\", \"content\": process_input(ins)}]\n\n        text = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n        audios = []\n        for message in messages:\n            if isinstance(message[\"content\"], list):\n                for ele in message[\"content\"]:\n                    if ele[\"type\"] == \"audio\":\n                        audio_data = ele[\"audio_url\"][\"url\"].split(\",\")[1]\n                        audio_bytes = base64.b64decode(audio_data)\n                        audio, _ = librosa.load(io.BytesIO(audio_bytes), sr=processor.feature_extractor.sampling_rate)\n                        audios.append(audio)\n\n        inputs = processor(text=text, audios=audios, return_tensors=\"pt\", padding=True)\n        inputs.input_ids = inputs.input_ids.to(\"cuda\")\n\n        start_time = time.monotonic()\n        generate_ids = model.generate(**inputs, max_length=max_length)\n        generate_ids = generate_ids[:, inputs.input_ids.size(1):]\n\n        response = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n        time_cost = time.monotonic() - start_time\n\n        return [\n            (\n                response,\n                {\n                    \"metadata\": {\n                        \"request_id\": \"\",\n                        \"input_tokens_count\": inputs.input_ids.size(1),\n                        \"generated_tokens_count\": generate_ids.size(1),\n                        \"time_cost\": time_cost,\n                        \"first_token_time\": 0,\n                        \"speed\": float(generate_ids.size(1)) / time_cost if time_cost > 0 else 0,\n                    }\n                },\n            )\n        ]\n\n    model.stream_chat = stream_chat.__get__(model)\n    model.get_meta = get_meta.__get__(model)\n    return (model, None)\n\n     \n\n下面是用户的需求：\n\n@/Users/allwefantasy/projects/byzer-llm/src/byzerllm/qwen_voice/__init__.py 中的 stream_chat 方法需要能够独立出来,严格遵循 @/Users/allwefantasy/projects/byzer-llm/src/byzerllm/whisper/__init__.py 中的接口定义标准\n"}, {"role": "assistant", "content": "好的,我理解您的需求是要将 `qwen_voice/__init__.py` 中的 `stream_chat` 方法独立出来,并严格遵循 `whisper/__init__.py` 中的接口定义标准。我会按照这个要求进行修改。以下是修改后的代码:\n\n```python\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/qwen_voice/__init__.py\nimport time\nfrom typing import List, Tuple, Dict, Any, Union\nimport base64\nimport json\nimport librosa\nimport io\nfrom transformers import Qwen2AudioForConditionalGeneration, AutoProcessor\n\ndef get_meta(self):\n    return [\n        {\n            \"model_deploy_type\": \"proprietary\",\n            \"backend\": \"transformers\",\n            \"message_format\": True\n        }\n    ]\n\ndef process_input(ins: Union[str, List[Dict[str, Any]], Dict[str, Any]]):\n    if isinstance(ins, list) or isinstance(ins, dict):\n        return ins\n\n    content = []\n    try:\n        ins_json = json.loads(ins)\n    except:\n        return ins\n\n    if isinstance(ins_json, dict):\n        return ins_json\n\n    for item in ins_json:\n        if (\"audio\" in item or \"audio_url\" in item) and \"type\" not in item:\n            audio_data = item.get(\"audio\", item.get(\"audio_url\", \"\"))\n            if not audio_data.startswith(\"data:\"):\n                audio_data = \"data:audio/wav;base64,\" + audio_data\n\n            other_fields = {k: v for k, v in item.items() if k not in [\"audio\", \"text\", \"audio_url\"]}\n            content.append(\n                {\n                    \"audio_url\": {\"url\": audio_data, **other_fields},\n                    \"type\": \"audio\",\n                }\n            )\n\n        if \"text\" in item and \"type\" not in item:\n            text_data = item[\"text\"]\n            content.append({\"text\": text_data, \"type\": \"text\"})\n\n        if \"type\" in item and item[\"type\"] in [\"text\", \"audio\"]:\n            content.append(item)\n\n    if not content:\n        return ins\n\n    return content\n\ndef stream_chat(\n    self,\n    tokenizer,\n    ins: str,\n    his: List[Dict[str, str]] = [],\n    max_length: int = 4096,\n    top_p: float = 0.95,\n    temperature: float = 0.1,\n    **kwargs,\n):\n    processor = self.processor\n    model = self.model\n\n    messages = [\n        {\"role\": message[\"role\"], \"content\": process_input(message[\"content\"])}\n        for message in his\n    ] + [{\"role\": \"user\", \"content\": process_input(ins)}]\n\n    text = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n    audios = []\n    for message in messages:\n        if isinstance(message[\"content\"], list):\n            for ele in message[\"content\"]:\n                if ele[\"type\"] == \"audio\":\n                    audio_data = ele[\"audio_url\"][\"url\"].split(\",\")[1]\n                    audio_bytes = base64.b64decode(audio_data)\n                    audio, _ = librosa.load(io.BytesIO(audio_bytes), sr=processor.feature_extractor.sampling_rate)\n                    audios.append(audio)\n\n    inputs = processor(text=text, audios=audios, return_tensors=\"pt\", padding=True)\n    inputs.input_ids = inputs.input_ids.to(\"cuda\")\n\n    start_time = time.monotonic()\n    generate_ids = model.generate(**inputs, max_length=max_length)\n    generate_ids = generate_ids[:, inputs.input_ids.size(1):]\n\n    response = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n    time_cost = time.monotonic() - start_time\n\n    return [\n        (\n            response,\n            {\n                \"metadata\": {\n                    \"request_id\": \"\",\n                    \"input_tokens_count\": inputs.input_ids.size(1),\n                    \"generated_tokens_count\": generate_ids.size(1),\n                    \"time_cost\": time_cost,\n                    \"first_token_time\": 0,\n                    \"speed\": float(generate_ids.size(1)) / time_cost if time_cost > 0 else 0,\n                }\n            },\n        )\n    ]\n\ndef init_model(model_dir, infer_params: Dict[str, str] = {}, sys_conf: Dict[str, str] = {}):\n    processor = AutoProcessor.from_pretrained(model_dir)\n    model = Qwen2AudioForConditionalGeneration.from_pretrained(model_dir, device_map=\"auto\")\n    \n    model.processor = processor\n    model.stream_chat = stream_chat.__get__(model)\n    model.get_meta = get_meta.__get__(model)\n    return (model, None)\n```\n\n这个修改后的版本做了以下几点主要改动:\n\n1. `stream_chat` 函数被独立出来,不再是 `init_model` 内部的嵌套函数。\n\n2. `stream_chat` 函数现在接受 `self` 作为第一个参数,这样它可以作为模型的方法被调用。\n\n3. 在 `stream_chat` 函数内部,我们通过 `self.processor` 和 `self.model` 来访问所需的处理器和模型。\n\n4. 在 `init_model` 函数中,我们将 `processor` 作为 `model` 的一个属性保存,这样 `stream_chat` 方法可以访问到它。\n\n5. `stream_chat` 和 `get_meta` 方法都通过 `__get__` 方法绑定到模型实例上。\n\n这种结构更接近 `whisper/__init__.py` 中的接口定义标准,同时保持了原有的功能。`stream_chat` 方法现在可以独立使用,也可以作为模型的一个方法被调用。"}], "model": "human", "yaml_file": "actions/035_chat_action.yml"}
{"instruction": "@/Users/allwefantasy/projects/byzer-llm/src/byzerllm/qwen_voice/__init__.py 中的 stream_chat 方法需要能够独立出来,严格遵循 @/Users/allwefantasy/projects/byzer-llm/src/byzerllm/whisper/__init__.py 中的接口定义标准", "conversations": [{"role": "user", "content": "\n如果你需要生成代码，对于每个需要更改的文件,你需要按 *SEARCH/REPLACE block* 的格式进行生成。\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The opening fence and code language, eg: ```python\n2. The file path alone on a line, starting with \"##File:\" and verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n*SEARCH/REPLACE* blocks will replace *all* matching occurrences.\nInclude enough lines to make the SEARCH blocks unique.\n\nInclude *ALL* the code being searched and replaced!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n下面我们来看一个例子：\n\n当前项目目录结构：\n1. 项目根目录： /tmp/projects/mathweb\n2. 项目子目录/文件列表(类似tree 命令输出)\nflask/\n    app.py\n    templates/\n        index.html\n    static/\n        style.css\n\n用户需求： Change get_factorial() to use math.factorial\n\n回答： To make this change we need to modify `/tmp/projects/mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n用户需求： Refactor hello() into its own file.\n\n回答：To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/hello.py\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/main.py\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n现在让我们开始一个新的任务:\n\n当前项目目录结构：\n1. 项目根目录： /Users/allwefantasy/projects/byzer-llm\n2. 项目子目录/文件列表(类似tree 命令输出)：\nsetup.py\ntests/\n    test_emb_rerank.py\n    test_deepseek_code.py\n    test_prompt.py\n    test_model_meta.py\n    __init__.py\n    test_transformers_deploy.py\n    test_data_analysis.py\n    test_yi.py\n    test_deploy.py\n    functions_suite.py\n    test_template.py\n    auto/\n        test_backend_llama_cpp.py\n        __init__.py\nsetup-machine/\n    convert_safetensor.py\nnotebooks/\n    test.py\n    test_sensevoice.py\nsrc/\n    byzerllm/\n        lang.py\n        store.py\n        version.py\n        log.py\n        __init__.py\n        types.py\n        easy_byzerllm_command.py\n        byzerllm_command.py\n        command_args.py\n        bark/\n            generation.py\n            __init__.py\n            model.py\n            api.py\n            model_fine.py\n            bark_voice.py\n        moss/\n            finetune_moss.py\n            __init__.py\n            moss_inference.py\n            models/\n                configuration_moss.py\n                quantization.py\n                custom_autotune.py\n                __init__.py\n                modeling_moss.py\n                tokenization_moss.py\n        llama2/\n            __init__.py\n        records/\n            __init__.py\n        visualglm/\n            __init__.py\n        auto/\n            __init__.py\n            backend_llama_cpp.py\n            backend_vllm.py\n            backend_ds.py\n        alpha_moss/\n            __init__.py\n        processor/\n            __init__.py\n        sensevoice/\n            __init__.py\n        qwen/\n            __init__.py\n        utils/\n            json_repaire.py\n            object_store_ref_util.py\n            __init__.py\n            types.py\n            tokenizer.py\n            openai_utils.py\n            ray_utils.py\n            text_generator.py\n            emb.py\n            connect_ray.py\n            testing.py\n            langutil.py\n            nontext.py\n            fulltune/\n                deepspeed_trainner.py\n                __init__.py\n                launch.py\n                trainner.py\n                base_model/\n                    __init__.py\n                    configuration_baichuan.py\n                    modeling_baichuan.py\n                pretrain/\n                    __init__.py\n                    convert_to_transformers.py\n            metrics/\n                __init__.py\n            config/\n                __init__.py\n            retrieval/\n                rest.py\n                __init__.py\n                udf.py\n            sft/\n                argument.py\n                collator.py\n                qlora.py\n                __init__.py\n                model.py\n                dataset.py\n                loss.py\n                merge_lora.py\n                trainer.py\n            client/\n                message_utils.py\n                math_utils.py\n                byzerllm_client.py\n                parallel_utils.py\n                __init__.py\n                types.py\n                code_utils.py\n                img_utils.py\n                entrypoints/\n                    __init__.py\n                    openai/\n                        protocol.py\n                        __init__.py\n                        serve.py\n                        serving_chat.py\n                        api_server.py\n                        serving_engine.py\n                        serving_completion.py\n                        tool.py\n        qwen_voice/\n            __init__.py\n        chatglm2/\n            __init__.py\n        baichuan/\n            __init__.py\n        m3e/\n            __init__.py\n        stable_diffusion/\n            config.py\n            __init__.py\n            logger.py\n            shared.py\n            model.py\n            utils.py\n            images.py\n            diffusion/\n                __init__.py\n                embeddings.py\n                utils.py\n                networks/\n                    lyco.py\n                    __init__.py\n                upscalers/\n                    __init__.py\n                    multidiffusion.py\n                    samplers.py\n                piplines/\n                    __init__.py\n                    lpw.py\n                    diffusers.py\n            lib/\n                __init__.py\n                diffusers/\n                    __init__.py\n                    scheduler.py\n            api/\n                __init__.py\n                plugin.py\n                models/\n                    diffusion.py\n                    __init__.py\n                diffusion/\n                    __init__.py\n                    pipelines/\n                        __init__.py\n                        diffusers.py\n                events/\n                    generation.py\n                    __init__.py\n        bge/\n            __init__.py\n        bge_rerank/\n            __init__.py\n        apps/\n            client.py\n            qa_strategy.py\n            __init__.py\n            builder.py\n            vector_db.py\n            utils.py\n            qa.py\n            byzer_storage/\n                memory_model_based.py\n                command.py\n                env.py\n                train_pt.py\n                train_sft.py\n                __init__.py\n                train_dpo.py\n                simple_api.py\n                generate_sft_data.py\n            byzer_sql/\n                __init__.py\n            llama_index/\n                byzerai_vectordb.py\n                byzerai.py\n                byzerai_embedding.py\n                __init__.py\n                collection_manager.py\n                byzerai_index_store.py\n                byzerai_docstore.py\n                simple_retrieval.py\n                byzerai_kvstore.py\n            agent/\n                conversable_agent.py\n                registry.py\n                groupchat.py\n                user_proxy_agent.py\n                __init__.py\n                agent.py\n                extensions/\n                    simple_retrieval_client.py\n                    planner.py\n                    python_codesandbox_agent.py\n                    preview_file_agent.py\n                    llama_index_retrieval_agent.py\n                    visualization_agent.py\n                    assistant_agent.py\n                    load_data_agent.py\n                    retrieval_agent.py\n                    output_agent.py\n                    __init__.py\n                    byzer_engine_agent.py\n                    subquestion_agent.py\n                    sql_reviewer_agent.py\n                    spark_sql_agent.py\n                    rhetorical_agent.py\n                    data_analysis.py\n                    data_analysis_pipeline_agent.py\n                    common_agent.py\n                    query_rewrite/\n                        time.py\n                        rhetorical.py\n                        __init__.py\n                        condition.py\n                        context.py\n                store/\n                    memory_store.py\n                    __init__.py\n                    stores.py\n        saas/\n            __init__.py\n            chatglm/\n                __init__.py\n            sparkdesk/\n                __init__.py\n            gemini/\n                __init__.py\n            azure_openai/\n                __init__.py\n            azure/\n                __init__.py\n            qianfan/\n                __init__.py\n            claude/\n                __init__.py\n            qianwen/\n                __init__.py\n            volcengine/\n                __init__.py\n            minimax/\n                __init__.py\n            baichuan/\n                __init__.py\n            qianwen_vl/\n                __init__.py\n            official_openai/\n                __init__.py\n            zhipu/\n                __init__.py\n            aws_bedrock/\n                __init__.py\n            openai/\n                __init__.py\n        falcon/\n            __init__.py\n        chatglm6b/\n            trainer_seq2seq.py\n            arguments.py\n            __init__.py\n            finetune.py\n            tunning/\n                __init__.py\n                infer.py\n                finetune.py\n                utils/\n                    config.py\n                    __init__.py\n                    other.py\n                    common.py\n                    seq2seq.py\n        starcode/\n            __init__.py\n        qwen_vl_chat/\n            __init__.py\n        zephyr/\n            __init__.py\n        dolly/\n            generate.py\n            __init__.py\n            consts.py\n            dolly_inference.py\n            trainer.py\n        llama/\n            __init__.py\n        whisper/\n            whisper_inference.py\n            __init__.py\n下面是一些文件路径以及每个文件对应的源码：\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/saas/official_openai/__init__.py\nimport time\nfrom typing import List, Tuple, Dict, Any, Union\nimport httpx\nfrom openai import OpenAI\nimport base64\nimport io\nimport json\nimport ray\nfrom byzerllm.utils.types import (\n    BlockVLLMStreamServer,\n    StreamOutputs,\n    SingleOutput,\n    SingleOutputMeta,\n    BlockBinaryStreamServer,\n)\nfrom byzerllm.utils.langutil import asyncfy_with_semaphore\nimport threading\nimport asyncio\nimport traceback\nimport uuid\nimport tempfile        \n\nclass CustomSaasAPI:\n\n    def __init__(self, infer_params: Dict[str, str]) -> None:\n\n        self.api_key = infer_params[\"saas.api_key\"]\n        self.model = infer_params.get(\"saas.model\", \"gpt-3.5-turbo-1106\")\n\n        other_params = {}\n\n        if \"saas.api_base\" in infer_params:\n            other_params[\"api_base\"] = infer_params[\"saas.api_base\"]\n\n        if \"saas.api_version\" in infer_params:\n            other_params[\"api_version\"] = infer_params[\"saas.api_version\"]\n\n        if \"saas.api_type\" in infer_params:\n            other_params[\"api_type\"] = infer_params[\"saas.api_type\"]\n\n        if \"saas.base_url\" in infer_params:\n            other_params[\"base_url\"] = infer_params[\"saas.base_url\"]\n\n        if \"saas.timeout\" in infer_params:\n            other_params[\"timeout\"] = float(infer_params[\"saas.timeout\"])\n\n        self.max_retries = int(infer_params.get(\"saas.max_retries\", 10))\n\n        self.meta = {\n            \"model_deploy_type\": \"saas\",\n            \"backend\": \"saas\",\n            \"support_stream\": True,\n            \"model_name\": self.model,\n        }\n\n        self.meta[\"embedding_mode\"] = \"embedding\" in self.model.lower()\n\n        self.proxies = infer_params.get(\"saas.proxies\", None)\n        self.local_address = infer_params.get(\"saas.local_address\", \"0.0.0.0\")\n\n        if not self.proxies:\n            self.client = OpenAI(**other_params, api_key=self.api_key)\n        else:\n            self.client = OpenAI(\n                **other_params,\n                api_key=self.api_key,\n                http_client=httpx.Client(\n                    proxies=self.proxies,\n                    transport=httpx.HTTPTransport(local_address=self.local_address),\n                ),\n            )\n\n        try:\n            ray.get_actor(\"BLOCK_VLLM_STREAM_SERVER\")\n        except ValueError:\n            try:\n                ray.remote(BlockVLLMStreamServer).options(\n                    name=\"BLOCK_VLLM_STREAM_SERVER\",\n                    lifetime=\"detached\",\n                    max_concurrency=1000,\n                ).remote()\n            except Exception as e:\n                pass\n        try:\n            ray.get_actor(\"BlockBinaryStreamServer\")\n        except ValueError:\n            try:\n                ray.remote(BlockBinaryStreamServer).options(\n                    name=\"BlockBinaryStreamServer\",\n                    lifetime=\"detached\",\n                    max_concurrency=1000,\n                ).remote()\n            except Exception as e:\n                pass\n\n    # saas/proprietary\n    def get_meta(self):\n        return [self.meta]\n\n    def process_input(self, ins: Union[str, List[Dict[str, Any]], Dict[str, Any]]):\n\n        if isinstance(ins, list) or isinstance(ins, dict):\n            return ins\n\n        content = []\n        try:\n            ins_json = json.loads(ins)\n        except:\n            return ins\n\n        ## speech\n        if isinstance(ins_json, dict):\n            return ins_json\n\n        content = []\n        #     [\n        #     {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n        #     {\n        #       \"type\": \"image_url\",\n        #       \"image_url\": {\n        #         \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n        #         \"detail\": \"high\"\n        #       },\n        #     },\n        #   ],\n        for item in ins_json:\n            # for format like this: {\"image\": \"xxxxxx\", \"text\": \"What’s in this image?\",\"detail\":\"high\"}\n            # or {\"image\": \"xxxxxx\"}, {\"text\": \"What’s in this image?\"}\n            if (\"image\" in item or \"image_url\" in item) and \"type\" not in item:\n                image_data = item.get(\"image\", item.get(\"image_url\", \"\"))\n                ## \"data:image/jpeg;base64,\"\n                if not image_data.startswith(\"data:\"):\n                    image_data = \"data:image/jpeg;base64,\" + image_data\n\n                ## get the other fields except image/text/image_url\n                other_fields = {\n                    k: v\n                    for k, v in item.items()\n                    if k not in [\"image\", \"text\", \"image_url\"]\n                }\n                content.append(\n                    {\n                        \"image_url\": {\"url\": image_data, **other_fields},\n                        \"type\": \"image_url\",\n                    }\n                )            \n\n            if \"text\" in item and \"type\" not in item:\n                text_data = item[\"text\"]\n                content.append({\"text\": text_data, \"type\": \"text\"})\n\n            ## for format like this: {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n            ## {\"type\": \"image_url\", \"image_url\": {\"url\":\"\",\"detail\":\"high\"}}\n            ## this is the standard format, just return it\n            if \"type\" in item and item[\"type\"] == \"text\":\n                content.append(item)\n\n            if \"type\" in item and item[\"type\"] == \"image_url\":\n                content.append(item)\n\n        if not content:\n            return ins\n\n        return content\n\n    async def async_embed_query(self, ins: str, **kwargs):\n        resp = await asyncfy_with_semaphore(\n            lambda: self.client.embeddings.create(input=[ins], model=self.model)\n        )()\n        embedding = resp.data[0].embedding\n        usage = resp.usage\n        return (\n            embedding,\n            {\n                \"metadata\": {\n                    \"input_tokens_count\": usage.prompt_tokens,\n                    \"generated_tokens_count\": 0,\n                }\n            },\n        )\n\n    def embed_query(self, ins: str, **kwargs):\n        resp = self.client.embeddings.create(input=[ins], model=self.model)\n        embedding = resp.data[0].embedding\n        usage = resp.usage\n        return (\n            embedding,\n            {\n                \"metadata\": {\n                    \"input_tokens_count\": usage.prompt_tokens,\n                    \"generated_tokens_count\": 0,\n                }\n            },\n        )\n\n    async def async_text_to_speech(\n        self, stream: bool, ins: str, voice: str, chunk_size: int = None, **kwargs\n    ):\n        if stream:\n            server = ray.get_actor(\"BlockBinaryStreamServer\")\n            request_id = [None]\n\n            def writer():\n                try:\n                    request_id[0] = str(uuid.uuid4())\n                    with self.client.with_streaming_response.audio.speech.create(\n                        model=self.model, voice=voice, input=ins, **kwargs\n                    ) as response:\n                        for chunk in response.iter_bytes(chunk_size):\n                            input_tokens_count = 0\n                            generated_tokens_count = 0\n                            ray.get(\n                                server.add_item.remote(\n                                    request_id[0],\n                                    StreamOutputs(\n                                        outputs=[\n                                            SingleOutput(\n                                                text=chunk,\n                                                metadata=SingleOutputMeta(\n                                                    input_tokens_count=input_tokens_count,\n                                                    generated_tokens_count=generated_tokens_count,\n                                                ),\n                                            )\n                                        ]\n                                    ),\n                                )\n                            )\n                except:\n                    traceback.print_exc()\n                ray.get(server.mark_done.remote(request_id[0]))\n\n            threading.Thread(target=writer, daemon=True).start()\n\n            time_count = 10 * 100\n            while request_id[0] is None and time_count > 0:\n                time.sleep(0.01)\n                time_count -= 1\n\n            if request_id[0] is None:\n                raise Exception(\"Failed to get request id\")\n\n            def write_running():\n                return ray.get(server.add_item.remote(request_id[0], \"RUNNING\"))\n\n            await asyncio.to_thread(write_running)\n            return [\n                (\n                    \"\",\n                    {\n                        \"metadata\": {\n                            \"request_id\": request_id[0],\n                            \"stream_server\": \"BlockBinaryStreamServer\",\n                        }\n                    },\n                )\n            ]\n\n        start_time = time.monotonic()\n        with io.BytesIO() as output:\n            async with asyncfy_with_semaphore(\n                lambda: self.client.with_streaming_response.audio.speech.create(\n                    model=self.model, voice=voice, input=ins, **kwargs\n                )\n            )() as response:\n                for chunk in response.iter_bytes():\n                    output.write(chunk)\n\n            base64_audio = base64.b64encode(output.getvalue()).decode()\n            time_cost = time.monotonic() - start_time\n            return [\n                (\n                    base64_audio,\n                    {\n                        \"metadata\": {\n                            \"request_id\": \"\",\n                            \"input_tokens_count\": 0,\n                            \"generated_tokens_count\": 0,\n                            \"time_cost\": time_cost,\n                            \"first_token_time\": 0,\n                            \"speed\": 0,\n                        }\n                    },\n                )\n            ]\n\n    async def async_speech_to_text(\n        self,\n        audio: str,\n        response_format: str = \"verbose_json\",\n        timestamp_granularities: List[str] = [\"word\",\"segment\"],\n    ):               \n        # Extract audio format and base64 data\n        data_prefix = \"data:audio/\"\n        base64_prefix = \";base64,\"\n        if not audio.startswith(data_prefix):\n            raise ValueError(\"Invalid audio data format\")\n        \n        format_end = audio.index(base64_prefix)\n        audio_format = audio[len(data_prefix):format_end]\n        base64_data = audio[format_end + len(base64_prefix):]\n        \n        # Decode the base64 audio data\n        audio_data = base64.b64decode(base64_data)\n\n        # Create a temporary file with the correct extension\n        with tempfile.NamedTemporaryFile(delete=False, suffix=f'.{audio_format}') as temp_audio_file:\n            temp_audio_file.write(audio_data)\n            temp_audio_file_path = temp_audio_file.name\n\n        try:\n            start_time = time.monotonic()\n                   \n            with open(temp_audio_file_path, \"rb\") as audio_file:                \n                transcription = self.client.audio.transcriptions.create(\n                    model=self.model or \"whisper-1\",\n                    file=audio_file,\n                    response_format=response_format,\n                    timestamp_granularities=timestamp_granularities,\n                )\n            time_cost = time.monotonic() - start_time\n\n            return [\n                (\n                    json.dumps(transcription.to_dict(), ensure_ascii=True),\n                    {\n                        \"metadata\": {\n                            \"request_id\": \"\",\n                            \"input_tokens_count\": 0,\n                            \"generated_tokens_count\": 0,\n                            \"time_cost\": time_cost,\n                            \"first_token_time\": 0,\n                            \"speed\": 0,\n                        }\n                    },\n                )\n            ]\n        finally:\n            # Clean up the temporary file\n            import os\n            os.unlink(temp_audio_file_path)\n\n    def image_to_text(self, ins: str, **kwargs):\n        pass\n\n    async def async_text_to_image(\n        self, stream: bool, input: str, size: str, quality: str, n: int, **kwargs\n    ):\n        if stream:\n            raise Exception(\"Stream not supported for text to image\")\n        start_time = time.monotonic()\n        response = await asyncfy_with_semaphore(\n            lambda: self.client.images.generate(\n                model=self.model,\n                prompt=input,\n                size=size,\n                quality=quality,\n                n=1,\n                response_format=\"b64_json\",\n                **kwargs,\n            )\n        )()\n        time_cost = time.monotonic() - start_time\n        base64_image = response.data[0].b64_json\n        return [\n            (\n                base64_image,\n                {\n                    \"metadata\": {\n                        \"request_id\": \"\",\n                        \"input_tokens_count\": 0,\n                        \"generated_tokens_count\": 0,\n                        \"time_cost\": time_cost,\n                        \"first_token_time\": 0,\n                        \"speed\": 0,\n                    }\n                },\n            )\n        ]\n\n    def text_to_text(self, ins: str, **kwargs):\n        pass\n\n    async def async_stream_chat(\n        self,\n        tokenizer,\n        ins: str,\n        his: List[Dict[str, Any]] = [],\n        max_length: int = 4096,\n        top_p: float = 0.7,\n        temperature: float = 0.9,\n        **kwargs,\n    ):\n\n        model = self.model\n\n        if \"model\" in kwargs:\n            model = kwargs[\"model\"]\n\n        messages = [\n            {\"role\": message[\"role\"], \"content\": self.process_input(message[\"content\"])}\n            for message in his\n        ] + [{\"role\": \"user\", \"content\": self.process_input(ins)}]\n\n        stream = kwargs.get(\"stream\", False)\n\n        ## content = [\n        ##    \"voice\": \"alloy\",\"input\": \"Hello, World!\",response_format: \"mp3\"]\n        last_message = messages[-1][\"content\"]\n\n        if isinstance(last_message, dict) and \"voice\" in last_message:\n            voice = last_message[\"voice\"]\n            response_format = last_message.get(\"response_format\", \"mp3\")\n            chunk_size = last_message.get(\"chunk_size\", None)\n            input = last_message[\"input\"]\n            return await self.async_text_to_speech(\n                stream=stream,\n                ins=input,\n                voice=voice,\n                chunk_size=chunk_size,\n                response_format=response_format,\n            )\n\n        if isinstance(last_message, dict) and \"input\" in last_message:\n            input = last_message[\"input\"]\n            size = last_message.get(\"size\", \"1024x1024\")\n            quality = last_message.get(\"quality\", \"standard\")\n            n = last_message.get(\"n\", 1)\n            return await self.async_text_to_image(\n                stream=stream, input=input, size=size, quality=quality, n=n\n            )\n\n        if isinstance(last_message, dict) and \"audio\" in last_message:            \n            audio_data = last_message.get(\"audio\", \"\")            \n            tpe = last_message.get(\"type\",\"wav\")\n            if not audio_data.startswith(\"data:\"):\n                audio_data = f\"data:audio/${tpe};base64,\" + audio_data                \n            return await self.async_speech_to_text(audio=audio_data)\n        \n        server = ray.get_actor(\"BLOCK_VLLM_STREAM_SERVER\")\n        request_id = [None]\n\n        def writer():\n            try:\n                r = \"\"\n                response = self.client.chat.completions.create(\n                    messages=messages,\n                    model=model,\n                    stream=True,\n                    max_tokens=max_length,\n                    temperature=temperature,\n                    top_p=top_p,\n                )\n                # input_tokens_count = 0\n                # generated_tokens_count = 0\n\n                request_id[0] = str(uuid.uuid4())\n\n                for chunk in response:\n                    content = chunk.choices[0].delta.content or \"\"\n                    r += content\n                    if hasattr(chunk, \"usage\") and chunk.usage:\n                        input_tokens_count = chunk.usage.prompt_tokens\n                        generated_tokens_count = chunk.usage.completion_tokens\n                    else:\n                        input_tokens_count = 0\n                        generated_tokens_count = 0\n                    ray.get(\n                        server.add_item.remote(\n                            request_id[0],\n                            StreamOutputs(\n                                outputs=[\n                                    SingleOutput(\n                                        text=r,\n                                        metadata=SingleOutputMeta(\n                                            input_tokens_count=input_tokens_count,\n                                            generated_tokens_count=generated_tokens_count,\n                                        ),\n                                    )\n                                ]\n                            ),\n                        )\n                    )\n            except:\n                traceback.print_exc()\n            ray.get(server.mark_done.remote(request_id[0]))\n\n        if stream:\n            threading.Thread(target=writer, daemon=True).start()\n\n            time_count = 60 * 100\n            while request_id[0] is None and time_count > 0:\n                time.sleep(0.01)\n                time_count -= 1\n\n            if request_id[0] is None:\n                raise Exception(\"Failed to get request id\")\n\n            def write_running():\n                return ray.get(server.add_item.remote(request_id[0], \"RUNNING\"))\n\n            await asyncio.to_thread(write_running)\n            return [\n                (\n                    \"\",\n                    {\n                        \"metadata\": {\n                            \"request_id\": request_id[0],\n                            \"stream_server\": \"BLOCK_VLLM_STREAM_SERVER\",\n                        }\n                    },\n                )\n            ]\n        else:\n            try:\n                start_time = time.monotonic()\n                response = await asyncfy_with_semaphore(\n                    lambda: self.client.chat.completions.create(\n                        messages=messages,\n                        model=model,\n                        max_tokens=max_length,\n                        temperature=temperature,\n                        top_p=top_p,\n                    )\n                )()\n\n                generated_text = response.choices[0].message.content\n                generated_tokens_count = response.usage.completion_tokens\n                input_tokens_count = response.usage.prompt_tokens\n                time_cost = time.monotonic() - start_time\n                return [\n                    (\n                        generated_text,\n                        {\n                            \"metadata\": {\n                                \"request_id\": response.id,\n                                \"input_tokens_count\": input_tokens_count,\n                                \"generated_tokens_count\": generated_tokens_count,\n                                \"time_cost\": time_cost,\n                                \"first_token_time\": 0,\n                                \"speed\": float(generated_tokens_count) / time_cost,\n                            }\n                        },\n                    )\n                ]\n            except Exception as e:\n                print(f\"Error: {e}\")\n                raise e\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/whisper/__init__.py\nimport whisper\nfrom typing import Dict, List, Tuple\nfrom byzerllm.utils.types import StopSequencesCriteria\n\nfrom typing import Dict, Any, List, Union\nfrom pyjava.storage import streaming_tar as STar\nimport json\nimport base64\nimport tempfile\nimport time\n\n\ndef get_meta(self):\n    return [\n        {\n            \"model_deploy_type\": \"proprietary\",\n            \"backend\": \"transformers\",\n            \"message_format\": True\n        }\n    ]\n\n\ndef process_input(ins: Union[str, List[Dict[str, Any]], Dict[str, Any]]):\n    # print(ins) \n    if isinstance(ins, list) or isinstance(ins, dict):\n        return ins\n\n    content = []\n    try:\n        ins_json = json.loads(ins)\n    except:\n        return ins\n\n    ## speech\n    if isinstance(ins_json, dict):\n        return ins_json\n\n    content = []\n    #     [\n    #     {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n    #     {\n    #       \"type\": \"image_url\",\n    #       \"image_url\": {\n    #         \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n    #         \"detail\": \"high\"\n    #       },\n    #     },\n    #   ],\n    for item in ins_json:\n        # for format like this: {\"image\": \"xxxxxx\", \"text\": \"What’s in this image?\",\"detail\":\"high\"}\n        # or {\"image\": \"xxxxxx\"}, {\"text\": \"What’s in this image?\"}\n        if (\"image\" in item or \"image_url\" in item) and \"type\" not in item:\n            image_data = item.get(\"image\", item.get(\"image_url\", \"\"))\n            ## \"data:image/jpeg;base64,\"\n            if not image_data.startswith(\"data:\"):\n                image_data = \"data:image/jpeg;base64,\" + image_data\n\n            ## get the other fields except image/text/image_url\n            other_fields = {\n                k: v for k, v in item.items() if k not in [\"image\", \"text\", \"image_url\"]\n            }\n            content.append(\n                {\n                    \"image_url\": {\"url\": image_data, **other_fields},\n                    \"type\": \"image_url\",\n                }\n            )\n\n        if \"text\" in item and \"type\" not in item:\n            text_data = item[\"text\"]\n            content.append({\"text\": text_data, \"type\": \"text\"})\n\n        ## for format like this: {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n        ## {\"type\": \"image_url\", \"image_url\": {\"url\":\"\",\"detail\":\"high\"}}\n        ## this is the standard format, just return it\n        if \"type\" in item and item[\"type\"] == \"text\":\n            content.append(item)\n\n        if \"type\" in item and item[\"type\"] == \"image_url\":\n            content.append(item)\n\n    if not content:\n        return ins\n\n    return content\n\n\ndef stream_chat(\n    self,\n    tokenizer,\n    ins: str,\n    his: List[Dict[str, str]] = [],\n    max_length: int = 4090,\n    top_p: float = 0.95,\n    temperature: float = 0.1,\n    **kwargs,\n):\n    messages = [\n        {\"role\": message[\"role\"], \"content\": process_input(message[\"content\"])}\n        for message in his\n    ] + [{\"role\": \"user\", \"content\": process_input(ins)}]\n\n    audio_input = messages[-1][\"content\"]\n\n    audio_data = audio_input.get(\"audio\", \"\")\n    audio = audio_input[\"audio\"]\n    response_format = audio_input.get(\"response_format\", \"json\")\n    timestamp_granularities = audio_input.get(\n        \"timestamp_granularities\", [\"word\", \"segment\"]\n    )\n    data_prefix = \"data:audio/\"\n    base64_prefix = \";base64,\"\n    if not audio.startswith(data_prefix):\n        raise ValueError(\"Invalid audio data format\")\n\n    format_end = audio.index(base64_prefix)\n    audio_format = audio[len(data_prefix) : format_end]\n    base64_data = audio[format_end + len(base64_prefix) :]\n\n    # Decode the base64 audio data\n    audio_data = base64.b64decode(base64_data)\n\n    # Create a temporary file with the correct extension\n    with tempfile.NamedTemporaryFile(\n        delete=False, suffix=f\".{audio_format}\"\n    ) as temp_audio_file:\n        temp_audio_file.write(audio_data)\n        temp_audio_file_path = temp_audio_file.name\n\n    try:\n        start_time = time.monotonic()\n        word_timestamps = \"word\" in timestamp_granularities\n        \n        result = self.transcribe(temp_audio_file_path, word_timestamps=word_timestamps)        \n            \n        time_cost = time.monotonic() - start_time\n        return [\n            (\n                json.dumps(result, ensure_ascii=False),\n                {\n                    \"metadata\": {\n                        \"request_id\": \"\",\n                        \"input_tokens_count\": 0,\n                        \"generated_tokens_count\": 0,\n                        \"time_cost\": time_cost,\n                        \"first_token_time\": 0,\n                        \"speed\": 0,\n                    }\n                },\n            )\n        ]\n    finally:\n        # Clean up the temporary file\n        import os\n\n        os.unlink(temp_audio_file_path)\n\n\ndef init_model(\n    model_dir, infer_params: Dict[str, str] = {}, sys_conf: Dict[str, str] = {}\n):\n    model = whisper.load_model(\"large\", download_root=model_dir)\n    import types\n\n    model.stream_chat = types.MethodType(stream_chat, model)\n    model.get_meta = types.MethodType(get_meta, model)\n    return (model, None)\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/notebooks/test_sensevoice.py\nimport sys\nimport os\n\n# Add the project root to the Python path\nsys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\")))\n\nfrom src.byzerllm.sensevoice import init_model, format_str_v3\nimport base64\nimport json\n\n\ndef main():\n    # Initialize the model\n    model_dir = \"/Users/allwefantasy/models/SenseVoiceSmall\"\n    model, _ = init_model(model_dir)\n\n    # Prepare test audio\n    with open(\n        \"/Users/allwefantasy/models/SenseVoiceSmall/example/zh.mp3\", \"rb\"\n    ) as audio_file:\n        audio_data = audio_file.read()\n        base64_audio = base64.b64encode(audio_data).decode(\"utf-8\")        \n        # Prepare input in the expected format\n        test_input = json.dumps(\n            {\n                            \"type\": \"audio\",\n                            \"audio\": f\"data:audio/wav;base64,{base64_audio}\",\n                        }\n        )\n\n        # Call stream_chat\n        result = model.stream_chat(tokenizer=None, ins = test_input)\n\n        # Process and print the result\n        if result and len(result) > 0:\n            output, metadata = result[0]\n            parsed_output = json.loads(output)\n            formatted_text = parsed_output.get(\"text\", \"\")\n            print(\"Transcription:\", formatted_text)\n            print(\"Metadata:\", metadata)\n        else:\n            print(\"No result returned from the model.\")\n\n\nif __name__ == \"__main__\":\n    main()\n\n\n##File: /Users/allwefantasy/projects/Qwen2-Audio/README_CN.md\n<p align=\"left\">\n        中文</a>&nbsp ｜ &nbsp<a href=\"README.md\">English</a>\n</p>\n<br><br>\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/assets/blog/qwenaudio/qwen2audio_logo.png\" width=\"400\"/>\n<p>\n\n<p align=\"center\">\nQwen2-Audio-7B <a href=\"https://modelscope.cn/models/qwen/Qwen2-Audio-7B\">🤖 </a> | <a href=\"https://huggingface.co/Qwen/Qwen2-Audio-7B\">🤗</a>&nbsp ｜ Qwen-Audio-7B-Instruct <a href=\"https://modelscope.cn/models/qwen/Qwen2-Audio-7B-Instruct\">🤖 </a>| <a href=\"https://huggingface.co/Qwen/Qwen2-Audio-7B-Instruct\">🤗</a>&nbsp ｜ Demo<a href=\"https://modelscope.cn/studios/qwen/Qwen2-Audio-Instruct-Demo\"> 🤖</a> | <a href=\"https://huggingface.co/spaces/Qwen/Qwen2-Audio-Instruct-Demo\">🤗</a>&nbsp\n<br>\n📑 <a href=\"https://arxiv.org/abs/2407.10759\">Paper</a> &nbsp&nbsp | &nbsp&nbsp 📑 <a href=\"https://qwenlm.github.io/blog/qwen2-audio\">Blog</a> &nbsp&nbsp | &nbsp&nbsp 💬 <a href=\"https://github.com/QwenLM/Qwen/blob/main/assets/wechat.png\">WeChat (微信)</a>&nbsp&nbsp | &nbsp&nbsp <a href=\"https://discord.gg/CV4E9rpNSD\">Discord</a>&nbsp&nbsp\n</p>\n\n\n\n我们介绍Qwen-Audio的最新进展：Qwen2-Audio。作为一个大规模音频语言模型，Qwen2-Audio能够接受各种音频信号输入，并根据语音指令执行音频分析或直接响应文本。我们介绍两种不同的音频交互模式：语音聊天 voice chat 和音频分析 audio analysis。\n\n* 语音聊天：用户可以自由地与 Qwen2-Audio 进行语音互动，而无需文本输入；\n* 音频分析：用户可以在互动过程中提供音频和文本指令对音频进行分析；\n\n**我们已经开源了 Qwen2-Audio 系列的两个模型：Qwen2-Audio-7B和Qwen2-Audio-7B-Instruct。**\n\n## 模型结构与训练范式\n\nQwen2-Audio 三阶段训练过程概述。\n\n<p align=\"left\">\n    <img src=\"assets/framework.png\" width=\"80%\"/>\n<p>\n\n\n\n## 新闻\n* 2024.8.9 🎉 我们在 ModelScope 和 Hugging Face 开源了`Qwen2-Audio-7B`和`Qwen2-Audio-7B-Instruct`的 checkpoint.\n* 2024.7.15 🎉 我们发布了 Qwen2-Audio 的[论文](https://arxiv.org/abs/2407.10759), 介绍了相关的模型结构，训练方法和模型表现。\n* 2023.11.30 🔥 我们发布了**Qwen-Audio**系列\n<br>\n\n## 评测\n我们在标准的13个学术数据集上评测了模型的能力如下：\n<table><thead><tr><th>Task</th><th>Description</th><th>Dataset</th><th>Split</th><th>Metric</th></tr></thead><tbody><tr><td rowspan=\"4\">ASR</td><td rowspan=\"4\">Automatic Speech Recognition</td><td>Fleurs</td><td>dev | test</td><td rowspan=\"4\">WER</td></tr><tr><td>Aishell2</td><td>test</td></tr><tr><td>Librispeech</td><td>dev | test</td></tr><tr><td>Common Voice</td><td>dev | test</td></tr><tr><td>S2TT</td><td>Speech-to-Text Translation</td><td>CoVoST2</td><td>test</td><td>BLEU </td></tr><tr><td>SER</td><td>Speech Emotion Recognition</td><td>Meld</td><td>test</td><td>ACC</td></tr><tr><td>VSC</td><td>Vocal Sound Classification</td><td>VocalSound</td><td>test</td><td>ACC</td></tr><tr><td rowspan=\"4\"><a href=\"https://github.com/OFA-Sys/AIR-Bench\">AIR-Bench</a><br></td><td>Chat-Benchmark-Speech</td><td>Fisher<br>SpokenWOZ<br>IEMOCAP<br>Common voice</td><td>dev | test</td><td>GPT-4 Eval</td></tr><tr><td>Chat-Benchmark-Sound</td><td>Clotho</td><td>dev | test</td><td>GPT-4 Eval</td></tr>\n<tr><td>Chat-Benchmark-Music</td><td>MusicCaps</td><td>dev | test</td><td>GPT-4 Eval</td></tr><tr><td>Chat-Benchmark-Mixed-Audio</td><td>Common voice<br>AudioCaps<br>MusicCaps</td><td>dev | test</td><td>GPT-4 Eval</td></tr></tbody></table>\n\n\n以下是整体表现：\n<p align=\"left\">\n    <img src=\"assets/radar_compare_qwen_audio.png\" width=\"70%\"/>\n<p>\n\n评测分数详情如下：\n<br>\n<b>（注意：我们所展示的评测结果是在原始训练框架的初始模型上的，然而在框架转换 Huggingface 后指标出现了部分波动，在这里我们展示我们的全部测评结果：首先是论文中的初始模型结果）</b>\n\n<table><thead><tr><th rowspan=\"2\">Task</th><th rowspan=\"2\">Dataset</th><th rowspan=\"2\">Model</th><th colspan=\"2\">Performance</th></tr><tr><th>Metrics</th><th>Results</th></tr></thead><tbody><tr><td rowspan=\"15\">ASR</td><td rowspan=\"7\"><b>Librispeech</b><br>dev-clean | dev-other | <br>test-clean | test-other</td><td>SpeechT5</td><td rowspan=\"7\">WER </td><td>2.1 | 5.5 | 2.4 | 5.8</td></tr><tr><td>SpeechNet</td><td>- | - | 30.7 | -</td></tr><tr><td>SLM-FT</td><td>- | - | 2.6 | 5.0</td></tr><tr><td>SALMONN</td><td>- | - | 2.1 | 4.9</td></tr><tr><td>SpeechVerse</td><td>- | - | 2.1 | 4.4</td></tr><tr><td>Qwen-Audio</td><td>1.8 | 4.0 | 2.0 | 4.2</td></tr><tr><td>Qwen2-Audio</td><td><b>1.3 | 3.4 | 1.6 | 3.6</b></td></tr><tr><td rowspan=\"2\"><b>Common Voice 15</b> <br>en | zh | yue | fr</td><td>Whisper-large-v3</td><td rowspan=\"2\">WER </td><td>9.3 | 12.8 | 10.9 | 10.8</td></tr><tr><td>Qwen2-Audio</td><td><b>8.6 | 6.9 | 5.9 | 9.6</b></td></tr>\n<tr><td rowspan=\"2\"><b>Fleurs</b> <br>zh</td><td>Whisper-large-v3</td><td rowspan=\"2\">WER </td><td>7.7</td></tr><tr><td>Qwen2-Audio</td><td><b>7.5</b></td></tr><tr><td rowspan=\"4\"><b>Aishell2</b> <br>Mic | iOS | Android</td><td>MMSpeech-base</td><td rowspan=\"4\">WER </td><td>4.5 | 3.9 | 4.0</td></tr><tr><td>Paraformer-large</td><td>- | <b>2.9</b> | -</td></tr><tr><td>Qwen-Audio</td><td>3.3 | 3.1 | 3.3</td></tr><tr><td>Qwen2-Audio</td><td><b>3.0</b> | 3.0 | <b>2.9</b></td></tr><tr><td rowspan=\"8\">S2TT</td><td rowspan=\"5\"><b>CoVoST2</b> <br>en-de | de-en | <br>en-zh | zh-en</td><td>SALMONN</td><td rowspan=\"5\">BLEU </td><td>18.6 | - | 33.1 | -</td></tr><tr><td>SpeechLLaMA</td><td>- | 27.1 | - | 12.3</td></tr><tr><td>BLSP</td><td>14.1 | - | - | -</td></tr><tr><td>Qwen-Audio</td><td>25.1 | 33.9 | 41.5 | 15.7</td></tr><tr><td>Qwen2-Audio</td><td><b>29.9 | 35.2 | 45.2 | 24.4</b></td></tr>\n<tr><td rowspan=\"3\"><b>CoVoST2</b> <br>es-en | fr-en | it-en |</td><td>SpeechLLaMA</td><td rowspan=\"3\">BLEU </td><td>27.9 | 25.2 | 25.9</td></tr><tr><td>Qwen-Audio</td><td>39.7 | <b>38.5</b> | 36.0</td></tr><tr><td>Qwen2-Audio</td><td><b>40.0 | 38.5 | 36.3</b></td></tr><tr><td rowspan=\"3\">SER</td><td rowspan=\"3\"><b>Meld</b></td><td>WavLM-large</td><td rowspan=\"3\">ACC </td><td>0.542</td></tr><tr><td>Qwen-Audio</td><td><b>0.557</b></td></tr><tr><td>Qwen2-Audio</td><td>0.553</td></tr><tr><td rowspan=\"4\">VSC</td><td rowspan=\"4\"><b>VocalSound</b></td><td>CLAP</td><td rowspan=\"4\">ACC </td><td>0.4945</td></tr><tr><td>Pengi</td><td>0.6035</td></tr><tr><td>Qwen-Audio</td><td>0.9289</td></tr><tr><td>Qwen2-Audio</td><td><b>0.9392</b></td></tr>\n<tr><td>AIR-Bench <br></td><td><b>Chat Benchmark</b><br>Speech | Sound |<br> Music | Mixed-Audio</td><td>SALMONN<br>BLSP<br>Pandagpt<br>Macaw-LLM<br>SpeechGPT<br>Next-gpt<br>Qwen-Audio<br>Gemini-1.5-pro<br>Qwen2-Audio</td><td>GPT-4 </td><td>6.16 | 6.28 | 5.95 | 6.08<br>6.17 | 5.55 | 5.08 | 5.33<br>3.58 | 5.46 | 5.06 | 4.25<br>0.97 | 1.01 | 0.91 | 1.01<br>1.57 | 0.95 | 0.95 | 4.13<br>3.86 | 4.76 | 4.18 | 4.13<br>6.47 | 6.95 | 5.52 | 6.08<br>6.97 | 5.49 | 5.06 | 5.27<br><b>7.18 | 6.99 | 6.79 | 6.77</b></td></tr></tbody></table>\n\n<b>（其次是转换 huggingface 后的）</b>\n\n<table><thead><tr><th rowspan=\"2\">Task</th><th rowspan=\"2\">Dataset</th><th rowspan=\"2\">Model</th><th colspan=\"2\">Performance</th></tr><tr><th>Metrics</th><th>Results</th></tr></thead><tbody><tr><td rowspan=\"15\">ASR</td><td rowspan=\"7\"><b>Librispeech</b><br>dev-clean | dev-other | <br>test-clean | test-other</td><td>SpeechT5</td><td rowspan=\"7\">WER </td><td>2.1 | 5.5 | 2.4 | 5.8</td></tr><tr><td>SpeechNet</td><td>- | - | 30.7 | -</td></tr><tr><td>SLM-FT</td><td>- | - | 2.6 | 5.0</td></tr><tr><td>SALMONN</td><td>- | - | 2.1 | 4.9</td></tr><tr><td>SpeechVerse</td><td>- | - | 2.1 | 4.4</td></tr><tr><td>Qwen-Audio</td><td>1.8 | 4.0 | 2.0 | 4.2</td></tr><tr><td>Qwen2-Audio</td><td><b>1.7 | 3.6 | 1.7 | 4.0</b></td></tr><tr><td rowspan=\"2\"><b>Common Voice 15</b> <br>en | zh | yue | fr</td><td>Whisper-large-v3</td><td rowspan=\"2\">WER </td><td>9.3 | 12.8 | 10.9 | 10.8</td></tr><tr><td>Qwen2-Audio</td><td><b>8.7 | 6.5 | 5.9 | 9.6</b></td></tr>\n<tr><td rowspan=\"2\"><b>Fleurs</b> <br>zh</td><td>Whisper-large-v3</td><td rowspan=\"2\">WER </td><td>7.7</td></tr><tr><td>Qwen2-Audio</td><td><b>7.0</b></td></tr><tr><td rowspan=\"4\"><b>Aishell2</b> <br>Mic | iOS | Android</td><td>MMSpeech-base</td><td rowspan=\"4\">WER </td><td>4.5 | 3.9 | 4.0</td></tr><tr><td>Paraformer-large</td><td>- | <b>2.9</b> | -</td></tr><tr><td>Qwen-Audio</td><td>3.3 | 3.1 | 3.3</td></tr><tr><td>Qwen2-Audio</td><td><b>3.2</b> | 3.1 | <b>2.9</b></td></tr><tr><td rowspan=\"8\">S2TT</td><td rowspan=\"5\"><b>CoVoST2</b> <br>en-de | de-en | <br>en-zh | zh-en</td><td>SALMONN</td><td rowspan=\"5\">BLEU </td><td>18.6 | - | 33.1 | -</td></tr><tr><td>SpeechLLaMA</td><td>- | 27.1 | - | 12.3</td></tr><tr><td>BLSP</td><td>14.1 | - | - | -</td></tr><tr><td>Qwen-Audio</td><td>25.1 | <b>33.9</b> | 41.5 | 15.7</td></tr><tr><td>Qwen2-Audio</td><td><b>29.6</b> | 33.6 | <b>45.6</b> | <b>24.0</b></td></tr>\n<tr><td rowspan=\"3\"><b>CoVoST2</b> <br>es-en | fr-en | it-en |</td><td>SpeechLLaMA</td><td rowspan=\"3\">BLEU </td><td>27.9 | 25.2 | 25.9</td></tr><tr><td>Qwen-Audio</td><td><b>39.7 | 38.5 | 36.0</b></td></tr><tr><td>Qwen2-Audio</td><td>38.7 | 37.2 | 35.2</td></tr><tr><td rowspan=\"3\">SER</td><td rowspan=\"3\"><b>Meld</b></td><td>WavLM-large</td><td rowspan=\"3\">ACC </td><td>0.542</td></tr><tr><td>Qwen-Audio</td><td><b>0.557</b></td></tr><tr><td>Qwen2-Audio</td><td>0.535</td></tr><tr><td rowspan=\"4\">VSC</td><td rowspan=\"4\"><b>VocalSound</b></td><td>CLAP</td><td rowspan=\"4\">ACC </td><td>0.4945</td></tr><tr><td>Pengi</td><td>0.6035</td></tr><tr><td>Qwen-Audio</td><td>0.9289</td></tr><tr><td>Qwen2-Audio</td><td><b>0.9395</b></td></tr>\n<tr><td>AIR-Bench <br></td><td><b>Chat Benchmark</b><br>Speech | Sound |<br> Music | Mixed-Audio</td><td>SALMONN<br>BLSP<br>Pandagpt<br>Macaw-LLM<br>SpeechGPT<br>Next-gpt<br>Qwen-Audio<br>Gemini-1.5-pro<br>Qwen2-Audio</td><td>GPT-4 </td><td>6.16 | 6.28 | 5.95 | 6.08<br>6.17 | 5.55 | 5.08 | 5.33<br>3.58 | 5.46 | 5.06 | 4.25<br>0.97 | 1.01 | 0.91 | 1.01<br>1.57 | 0.95 | 0.95 | 4.13<br>3.86 | 4.76 | 4.18 | 4.13<br>6.47 | <b>6.95</b> | 5.52 | 6.08<br>6.97 | 5.49 | 5.06 | 5.27<br><b>7.24</b> | 6.83 | <b>6.73</b> | <b>6.42</b></td></tr></tbody></table>\n\n\n我们提供了以上**所有**评测脚本以供复现我们的实验结果。请阅读 [eval_audio/EVALUATION.md](eval_audio/EVALUATION.md) 了解更多信息。\n\n## 部署要求\nThe code of Qwen2-Audio has been in the latest Hugging face transformers and we advise you to build from source with command `pip install git+https://github.com/huggingface/transformers`, or you might encounter the following error:\nQwen2-Audio的代码已经包含在最新的 Hugging Face Transformers 的主分支中，我们建议您使用命令`pip install git+https://github.com/huggingface/transformers`\n```\nKeyError: 'qwen2-audio'\n```\n## 快速使用\n我们提供简单的示例来说明如何利用 🤗 Transformers 快速使用 Qwen2-Audio-7B 和 Qwen2-Audio-7B-Instruct。\n在开始前，请确保你已经配置好环境并安装好相关的代码包。最重要的是，确保你满足上述要求，然后安装相关的依赖库。\n接下来你可以开始使用 Transformers 或者 ModelScope 来使用我们的模型。目前Qwen2-Audio-7B 及 Qwen2-Audio-7B-Instruct 模型处理30秒以内的音频表现更佳。\n#### 🤗 Hugging Face Transformers\n如希望使用 Qwen2-Audio-7B-Instruct 进行推理，我们分别演示语音聊天和音频分析的交互方式，所需要写的只是如下所示的数行代码。\n##### 语音聊天推理\n在语音聊天模式下，用户可以自由地与 Qwen2-Audio 进行语音交互，无需文字输入：\n```python\nfrom io import BytesIO\nfrom urllib.request import urlopen\nimport librosa\nfrom transformers import Qwen2AudioForConditionalGeneration, AutoProcessor\n\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\")\nmodel = Qwen2AudioForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\", device_map=\"auto\")\n\nconversation = [\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/guess_age_gender.wav\"},\n    ]},\n    {\"role\": \"assistant\", \"content\": \"Yes, the speaker is female and in her twenties.\"},\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/translate_to_chinese.wav\"},\n    ]},\n]\ntext = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\naudios = []\nfor message in conversation:\n    if isinstance(message[\"content\"], list):\n        for ele in message[\"content\"]:\n            if ele[\"type\"] == \"audio\":\n                audios.append(librosa.load(\n                    BytesIO(urlopen(ele['audio_url']).read()), \n                    sr=processor.feature_extractor.sampling_rate)[0]\n                )\n\ninputs = processor(text=text, audios=audios, return_tensors=\"pt\", padding=True)\ninputs.input_ids = inputs.input_ids.to(\"cuda\")\n\ngenerate_ids = model.generate(**inputs, max_length=256)\ngenerate_ids = generate_ids[:, inputs.input_ids.size(1):]\n\nresponse = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n```\n##### 音频分析推理\n在音频分析中，用户可以提供音频和文字问题来实现对音频的分析：\n```python\nfrom io import BytesIO\nfrom urllib.request import urlopen\nimport librosa\nfrom transformers import Qwen2AudioForConditionalGeneration, AutoProcessor\n\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\")\nmodel = Qwen2AudioForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\", device_map=\"auto\")\n\nconversation = [\n    {'role': 'system', 'content': 'You are a helpful assistant.'}, \n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/glass-breaking-151256.mp3\"},\n        {\"type\": \"text\", \"text\": \"What's that sound?\"},\n    ]},\n    {\"role\": \"assistant\", \"content\": \"It is the sound of glass shattering.\"},\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"text\", \"text\": \"What can you do when you hear that?\"},\n    ]},\n    {\"role\": \"assistant\", \"content\": \"Stay alert and cautious, and check if anyone is hurt or if there is any damage to property.\"},\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/1272-128104-0000.flac\"},\n        {\"type\": \"text\", \"text\": \"What does the person say?\"},\n    ]},\n]\ntext = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\naudios = []\nfor message in conversation:\n    if isinstance(message[\"content\"], list):\n        for ele in message[\"content\"]:\n            if ele[\"type\"] == \"audio\":\n                audios.append(\n                    librosa.load(\n                        BytesIO(urlopen(ele['audio_url']).read()), \n                        sr=processor.feature_extractor.sampling_rate)[0]\n                )\n\ninputs = processor(text=text, audios=audios, return_tensors=\"pt\", padding=True)\ninputs.input_ids = inputs.input_ids.to(\"cuda\")\n\ngenerate_ids = model.generate(**inputs, max_length=256)\ngenerate_ids = generate_ids[:, inputs.input_ids.size(1):]\n\nresponse = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n```\n##### 批量推理\n我们也支持批量推理：\n```python\nfrom io import BytesIO\nfrom urllib.request import urlopen\nimport librosa\nfrom transformers import Qwen2AudioForConditionalGeneration, AutoProcessor\n\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\")\nmodel = Qwen2AudioForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\", device_map=\"auto\")\n\nconversation1 = [\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/glass-breaking-151256.mp3\"},\n        {\"type\": \"text\", \"text\": \"What's that sound?\"},\n    ]},\n    {\"role\": \"assistant\", \"content\": \"It is the sound of glass shattering.\"},\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/f2641_0_throatclearing.wav\"},\n        {\"type\": \"text\", \"text\": \"What can you hear?\"},\n    ]}\n]\n\nconversation2 = [\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/1272-128104-0000.flac\"},\n        {\"type\": \"text\", \"text\": \"What does the person say?\"},\n    ]},\n]\n\nconversations = [conversation1, conversation2]\n\ntext = [processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False) for conversation in conversations]\n\naudios = []\nfor conversation in conversations:\n    for message in conversation:\n        if isinstance(message[\"content\"], list):\n            for ele in message[\"content\"]:\n                if ele[\"type\"] == \"audio\":\n                    audios.append(\n                        librosa.load(\n                            BytesIO(urlopen(ele['audio_url']).read()), \n                            sr=processor.feature_extractor.sampling_rate)[0]\n                    )\n\ninputs = processor(text=text, audios=audios, return_tensors=\"pt\", padding=True)\ninputs['input_ids'] = inputs['input_ids'].to(\"cuda\")\ninputs.input_ids = inputs.input_ids.to(\"cuda\")\n\ngenerate_ids = model.generate(**inputs, max_length=256)\ngenerate_ids = generate_ids[:, inputs.input_ids.size(1):]\n\nresponse = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n```\n运行Qwen2-Audio-7B同样非常简单。\n```python\nfrom io import BytesIO\nfrom urllib.request import urlopen\nimport librosa\nfrom transformers import AutoProcessor, Qwen2AudioForConditionalGeneration\n\nmodel = Qwen2AudioForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-Audio-7B\" ,trust_remote_code=True)\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-Audio-7B\" ,trust_remote_code=True)\n\nprompt = \"<|audio_bos|><|AUDIO|><|audio_eos|>Generate the caption in English:\"\nurl = \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Audio/glass-breaking-151256.mp3\"\naudio, sr = librosa.load(BytesIO(urlopen(url).read()), sr=processor.feature_extractor.sampling_rate)\ninputs = processor(text=prompt, audios=audio, return_tensors=\"pt\")\n\ngenerated_ids = model.generate(**inputs, max_length=256)\ngenerated_ids = generated_ids[:, inputs.input_ids.size(1):]\nresponse = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n```\n#### 🤖 ModelScope\n我们强烈建议用户，特别是中国大陆地区的用户，使用 ModelScope。`snapshot_download` 可以帮助您解决下载检查点时遇到的问题。\n<br>\n## Demo\n### Web UI\n我们提供了 Web UI 的 demo 供用户使用。在开始前，确保已经安装如下代码库：\n```\npip install -r requirements_web_demo.txt\n```\n随后运行如下命令，并点击生成链接：\n```\npython demo/web_demo_audio.py\n```\n<br>\n\n## 样例展示\n更多样例将更新于[通义千问博客](https://qwenlm.github.io/blog/qwen2-audio)中的 Qwen2-Audio 博客。\n\n## 团队招聘\n\n我们是通义千问语音多模态团队，致力于以通义千问为核心，拓展音频多模态理解和生成能力，实现自由灵活的音频交互。目前团队蓬勃发展中，如有意向实习或全职加入我们，请发送简历至 `qwen_audio@list.alibaba-inc.com`.\n<br>\n\n## 使用协议\n\n请查看每个模型在其 Hugging Face 仓库中的许可证。您无需提交商业使用申请。\n<br>\n\n\n\n## 引用\n\n如果你觉得我们的论文和代码对你的研究有帮助，请考虑 :star: 和引用 :pencil: :)\n\n```BibTeX\n@article{Qwen-Audio,\n  title={Qwen-Audio: Advancing Universal Audio Understanding via Unified Large-Scale Audio-Language Models},\n  author={Chu, Yunfei and Xu, Jin and Zhou, Xiaohuan and Yang, Qian and Zhang, Shiliang and Yan, Zhijie  and Zhou, Chang and Zhou, Jingren},\n  journal={arXiv preprint arXiv:2311.07919},\n  year={2023}\n}\n```\n\n```BibTeX\n@article{Qwen2-Audio,\n  title={Qwen2-Audio Technical Report},\n  author={Chu, Yunfei and Xu, Jin and Yang, Qian and Wei, Haojie and Wei, Xipin and Guo,  Zhifang and Leng, Yichong and Lv, Yuanjun and He, Jinzheng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},\n  journal={arXiv preprint arXiv:2407.10759},\n  year={2024}\n}\n```\n## 联系我们\n\n如果你想给我们的研发团队和产品团队留言，请通过邮件（`qianwen_opensource@alibabacloud.com`）联系我们。\n<br>\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/qwen_voice/__init__.py\nimport time\nfrom typing import List, Tuple, Dict, Any, Union\nimport base64\nimport json\nimport librosa\nimport io\nfrom transformers import Qwen2AudioForConditionalGeneration, AutoProcessor\n\ndef get_meta(self):\n    return [\n        {\n            \"model_deploy_type\": \"proprietary\",\n            \"backend\": \"transformers\",\n            \"message_format\": True\n        }\n    ]\n\ndef process_input(ins: Union[str, List[Dict[str, Any]], Dict[str, Any]]):\n    if isinstance(ins, list) or isinstance(ins, dict):\n        return ins\n\n    content = []\n    try:\n        ins_json = json.loads(ins)\n    except:\n        return ins\n\n    if isinstance(ins_json, dict):\n        return ins_json\n\n    for item in ins_json:\n        if (\"audio\" in item or \"audio_url\" in item) and \"type\" not in item:\n            audio_data = item.get(\"audio\", item.get(\"audio_url\", \"\"))\n            if not audio_data.startswith(\"data:\"):\n                audio_data = \"data:audio/wav;base64,\" + audio_data\n\n            other_fields = {k: v for k, v in item.items() if k not in [\"audio\", \"text\", \"audio_url\"]}\n            content.append(\n                {\n                    \"audio_url\": {\"url\": audio_data, **other_fields},\n                    \"type\": \"audio\",\n                }\n            )\n\n        if \"text\" in item and \"type\" not in item:\n            text_data = item[\"text\"]\n            content.append({\"text\": text_data, \"type\": \"text\"})\n\n        if \"type\" in item and item[\"type\"] in [\"text\", \"audio\"]:\n            content.append(item)\n\n    if not content:\n        return ins\n\n    return content\n\ndef init_model(model_dir, infer_params: Dict[str, str] = {}, sys_conf: Dict[str, str] = {}):\n    processor = AutoProcessor.from_pretrained(model_dir)\n    model = Qwen2AudioForConditionalGeneration.from_pretrained(model_dir, device_map=\"auto\")\n\n    def stream_chat(\n        self,\n        tokenizer,\n        ins: str,\n        his: List[Dict[str, str]] = [],\n        max_length: int = 4096,\n        top_p: float = 0.95,\n        temperature: float = 0.1,\n        **kwargs,\n    ):\n        messages = [\n            {\"role\": message[\"role\"], \"content\": process_input(message[\"content\"])}\n            for message in his\n        ] + [{\"role\": \"user\", \"content\": process_input(ins)}]\n\n        text = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n        audios = []\n        for message in messages:\n            if isinstance(message[\"content\"], list):\n                for ele in message[\"content\"]:\n                    if ele[\"type\"] == \"audio\":\n                        audio_data = ele[\"audio_url\"][\"url\"].split(\",\")[1]\n                        audio_bytes = base64.b64decode(audio_data)\n                        audio, _ = librosa.load(io.BytesIO(audio_bytes), sr=processor.feature_extractor.sampling_rate)\n                        audios.append(audio)\n\n        inputs = processor(text=text, audios=audios, return_tensors=\"pt\", padding=True)\n        inputs.input_ids = inputs.input_ids.to(\"cuda\")\n\n        start_time = time.monotonic()\n        generate_ids = model.generate(**inputs, max_length=max_length)\n        generate_ids = generate_ids[:, inputs.input_ids.size(1):]\n\n        response = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n        time_cost = time.monotonic() - start_time\n\n        return [\n            (\n                response,\n                {\n                    \"metadata\": {\n                        \"request_id\": \"\",\n                        \"input_tokens_count\": inputs.input_ids.size(1),\n                        \"generated_tokens_count\": generate_ids.size(1),\n                        \"time_cost\": time_cost,\n                        \"first_token_time\": 0,\n                        \"speed\": float(generate_ids.size(1)) / time_cost if time_cost > 0 else 0,\n                    }\n                },\n            )\n        ]\n\n    model.stream_chat = stream_chat.__get__(model)\n    model.get_meta = get_meta.__get__(model)\n    return (model, None)\n\n     \n\n下面是用户的需求：\n\n@/Users/allwefantasy/projects/byzer-llm/src/byzerllm/qwen_voice/__init__.py 中的 stream_chat 方法需要能够独立出来,严格遵循 @/Users/allwefantasy/projects/byzer-llm/src/byzerllm/whisper/__init__.py 中的接口定义标准\n"}, {"role": "assistant", "content": "好的，我理解您的需求是要将 `qwen_voice/__init__.py` 中的 `stream_chat` 方法独立出来，并严格遵循 `whisper/__init__.py` 中的接口定义标准。我们可以通过修改 `qwen_voice/__init__.py` 文件来实现这一点。以下是修改后的代码：\n\n```python\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/qwen_voice/__init__.py\n<<<<<<< SEARCH\nimport time\nfrom typing import List, Tuple, Dict, Any, Union\nimport base64\nimport json\nimport librosa\nimport io\nfrom transformers import Qwen2AudioForConditionalGeneration, AutoProcessor\n\ndef get_meta(self):\n    return [\n        {\n            \"model_deploy_type\": \"proprietary\",\n            \"backend\": \"transformers\",\n            \"message_format\": True\n        }\n    ]\n\ndef process_input(ins: Union[str, List[Dict[str, Any]], Dict[str, Any]]):\n    if isinstance(ins, list) or isinstance(ins, dict):\n        return ins\n\n    content = []\n    try:\n        ins_json = json.loads(ins)\n    except:\n        return ins\n\n    if isinstance(ins_json, dict):\n        return ins_json\n\n    for item in ins_json:\n        if (\"audio\" in item or \"audio_url\" in item) and \"type\" not in item:\n            audio_data = item.get(\"audio\", item.get(\"audio_url\", \"\"))\n            if not audio_data.startswith(\"data:\"):\n                audio_data = \"data:audio/wav;base64,\" + audio_data\n\n            other_fields = {k: v for k, v in item.items() if k not in [\"audio\", \"text\", \"audio_url\"]}\n            content.append(\n                {\n                    \"audio_url\": {\"url\": audio_data, **other_fields},\n                    \"type\": \"audio\",\n                }\n            )\n\n        if \"text\" in item and \"type\" not in item:\n            text_data = item[\"text\"]\n            content.append({\"text\": text_data, \"type\": \"text\"})\n\n        if \"type\" in item and item[\"type\"] in [\"text\", \"audio\"]:\n            content.append(item)\n\n    if not content:\n        return ins\n\n    return content\n\ndef init_model(model_dir, infer_params: Dict[str, str] = {}, sys_conf: Dict[str, str] = {}):\n    processor = AutoProcessor.from_pretrained(model_dir)\n    model = Qwen2AudioForConditionalGeneration.from_pretrained(model_dir, device_map=\"auto\")\n\n    def stream_chat(\n        self,\n        tokenizer,\n        ins: str,\n        his: List[Dict[str, str]] = [],\n        max_length: int = 4096,\n        top_p: float = 0.95,\n        temperature: float = 0.1,\n        **kwargs,\n    ):\n        messages = [\n            {\"role\": message[\"role\"], \"content\": process_input(message[\"content\"])}\n            for message in his\n        ] + [{\"role\": \"user\", \"content\": process_input(ins)}]\n\n        text = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n        audios = []\n        for message in messages:\n            if isinstance(message[\"content\"], list):\n                for ele in message[\"content\"]:\n                    if ele[\"type\"] == \"audio\":\n                        audio_data = ele[\"audio_url\"][\"url\"].split(\",\")[1]\n                        audio_bytes = base64.b64decode(audio_data)\n                        audio, _ = librosa.load(io.BytesIO(audio_bytes), sr=processor.feature_extractor.sampling_rate)\n                        audios.append(audio)\n\n        inputs = processor(text=text, audios=audios, return_tensors=\"pt\", padding=True)\n        inputs.input_ids = inputs.input_ids.to(\"cuda\")\n\n        start_time = time.monotonic()\n        generate_ids = model.generate(**inputs, max_length=max_length)\n        generate_ids = generate_ids[:, inputs.input_ids.size(1):]\n\n        response = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n        time_cost = time.monotonic() - start_time\n\n        return [\n            (\n                response,\n                {\n                    \"metadata\": {\n                        \"request_id\": \"\",\n                        \"input_tokens_count\": inputs.input_ids.size(1),\n                        \"generated_tokens_count\": generate_ids.size(1),\n                        \"time_cost\": time_cost,\n                        \"first_token_time\": 0,\n                        \"speed\": float(generate_ids.size(1)) / time_cost if time_cost > 0 else 0,\n                    }\n                },\n            )\n        ]\n\n    model.stream_chat = stream_chat.__get__(model)\n    model.get_meta = get_meta.__get__(model)\n    return (model, None)\n=======\nimport time\nfrom typing import List, Tuple, Dict, Any, Union\nimport base64\nimport json\nimport librosa\nimport io\nfrom transformers import Qwen2AudioForConditionalGeneration, AutoProcessor\n\ndef get_meta(self):\n    return [\n        {\n            \"model_deploy_type\": \"proprietary\",\n            \"backend\": \"transformers\",\n            \"message_format\": True\n        }\n    ]\n\ndef process_input(ins: Union[str, List[Dict[str, Any]], Dict[str, Any]]):\n    if isinstance(ins, list) or isinstance(ins, dict):\n        return ins\n\n    content = []\n    try:\n        ins_json = json.loads(ins)\n    except:\n        return ins\n\n    if isinstance(ins_json, dict):\n        return ins_json\n\n    for item in ins_json:\n        if (\"audio\" in item or \"audio_url\" in item) and \"type\" not in item:\n            audio_data = item.get(\"audio\", item.get(\"audio_url\", \"\"))\n            if not audio_data.startswith(\"data:\"):\n                audio_data = \"data:audio/wav;base64,\" + audio_data\n\n            other_fields = {k: v for k, v in item.items() if k not in [\"audio\", \"text\", \"audio_url\"]}\n            content.append(\n                {\n                    \"audio_url\": {\"url\": audio_data, **other_fields},\n                    \"type\": \"audio\",\n                }\n            )\n\n        if \"text\" in item and \"type\" not in item:\n            text_data = item[\"text\"]\n            content.append({\"text\": text_data, \"type\": \"text\"})\n\n        if \"type\" in item and item[\"type\"] in [\"text\", \"audio\"]:\n            content.append(item)\n\n    if not content:\n        return ins\n\n    return content\n\ndef stream_chat(\n    self,\n    tokenizer,\n    ins: str,\n    his: List[Dict[str, str]] = [],\n    max_length: int = 4096,\n    top_p: float = 0.95,\n    temperature: float = 0.1,\n    **kwargs,\n):\n    messages = [\n        {\"role\": message[\"role\"], \"content\": process_input(message[\"content\"])}\n        for message in his\n    ] + [{\"role\": \"user\", \"content\": process_input(ins)}]\n\n    text = self.processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n    audios = []\n    for message in messages:\n        if isinstance(message[\"content\"], list):\n            for ele in message[\"content\"]:\n                if ele[\"type\"] == \"audio\":\n                    audio_data = ele[\"audio_url\"][\"url\"].split(\",\")[1]\n                    audio_bytes = base64.b64decode(audio_data)\n                    audio, _ = librosa.load(io.BytesIO(audio_bytes), sr=self.processor.feature_extractor.sampling_rate)\n                    audios.append(audio)\n\n    inputs = self.processor(text=text, audios=audios, return_tensors=\"pt\", padding=True)\n    inputs.input_ids = inputs.input_ids.to(\"cuda\")\n\n    start_time = time.monotonic()\n    generate_ids = self.generate(**inputs, max_length=max_length)\n    generate_ids = generate_ids[:, inputs.input_ids.size(1):]\n\n    response = self.processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n    time_cost = time.monotonic() - start_time\n\n    return [\n        (\n            response,\n            {\n                \"metadata\": {\n                    \"request_id\": \"\",\n                    \"input_tokens_count\": inputs.input_ids.size(1),\n                    \"generated_tokens_count\": generate_ids.size(1),\n                    \"time_cost\": time_cost,\n                    \"first_token_time\": 0,\n                    \"speed\": float(generate_ids.size(1)) / time_cost if time_cost > 0 else 0,\n                }\n            },\n        )\n    ]\n\ndef init_model(model_dir, infer_params: Dict[str, str] = {}, sys_conf: Dict[str, str] = {}):\n    processor = AutoProcessor.from_pretrained(model_dir)\n    model = Qwen2AudioForConditionalGeneration.from_pretrained(model_dir, device_map=\"auto\")\n    model.processor = processor\n    model.stream_chat = stream_chat.__get__(model)\n    model.get_meta = get_meta.__get__(model)\n    return (model, None)\n>>>>>>> REPLACE\n```\n\n这个修改主要做了以下几点：\n\n1. 将 `stream_chat` 函数从 `init_model` 中提取出来，成为一个独立的函数。\n\n2. 在 `stream_chat` 函数中，将原本通过闭包访问的 `processor` 和 `model` 改为通过 `self` 访问。这意味着我们需要在初始化时将 `processor` 添加为 `model` 的一个属性。\n\n3. 在 `init_model` 函数中，我们现在直接将 `processor` 赋值给 `model.processor`，而不是将其作为闭包变量。\n\n4. `stream_chat` 和 `get_meta` 函数仍然通过 `__get__` 方法绑定到 `model` 上，保持了原有的调用方式。\n\n这样修改后，`stream_chat` 方法就成为了一个独立的函数，并且严格遵循了 `whisper/__init__.py` 中的接口定义标准。同时，它仍然可以通过 `model.stream_chat` 的方式调用，保持了与原代码相同的使用方式。"}], "model": "human", "yaml_file": "actions/036_chat_action.yml"}
{"instruction": "@/Users/allwefantasy/projects/byzer-llm/src/byzerllm/qwen_voice/__init__.py 给模型添加方法使用 types.MethodType 的模式", "conversations": [{"role": "user", "content": "\n如果你需要生成代码，对于每个需要更改的文件,你需要按 *SEARCH/REPLACE block* 的格式进行生成。\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The opening fence and code language, eg: ```python\n2. The file path alone on a line, starting with \"##File:\" and verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n*SEARCH/REPLACE* blocks will replace *all* matching occurrences.\nInclude enough lines to make the SEARCH blocks unique.\n\nInclude *ALL* the code being searched and replaced!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n下面我们来看一个例子：\n\n当前项目目录结构：\n1. 项目根目录： /tmp/projects/mathweb\n2. 项目子目录/文件列表(类似tree 命令输出)\nflask/\n    app.py\n    templates/\n        index.html\n    static/\n        style.css\n\n用户需求： Change get_factorial() to use math.factorial\n\n回答： To make this change we need to modify `/tmp/projects/mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n用户需求： Refactor hello() into its own file.\n\n回答：To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/hello.py\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/main.py\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n现在让我们开始一个新的任务:\n\n当前项目目录结构：\n1. 项目根目录： /Users/allwefantasy/projects/byzer-llm\n2. 项目子目录/文件列表(类似tree 命令输出)：\nsetup.py\ntests/\n    test_emb_rerank.py\n    test_deepseek_code.py\n    test_prompt.py\n    test_model_meta.py\n    __init__.py\n    test_transformers_deploy.py\n    test_data_analysis.py\n    test_yi.py\n    test_deploy.py\n    functions_suite.py\n    test_template.py\n    auto/\n        test_backend_llama_cpp.py\n        __init__.py\nsetup-machine/\n    convert_safetensor.py\nnotebooks/\n    test.py\n    test_sensevoice.py\nsrc/\n    byzerllm/\n        lang.py\n        store.py\n        version.py\n        log.py\n        __init__.py\n        types.py\n        easy_byzerllm_command.py\n        byzerllm_command.py\n        command_args.py\n        bark/\n            generation.py\n            __init__.py\n            model.py\n            api.py\n            model_fine.py\n            bark_voice.py\n        moss/\n            finetune_moss.py\n            __init__.py\n            moss_inference.py\n            models/\n                configuration_moss.py\n                quantization.py\n                custom_autotune.py\n                __init__.py\n                modeling_moss.py\n                tokenization_moss.py\n        llama2/\n            __init__.py\n        records/\n            __init__.py\n        visualglm/\n            __init__.py\n        auto/\n            __init__.py\n            backend_llama_cpp.py\n            backend_vllm.py\n            backend_ds.py\n        alpha_moss/\n            __init__.py\n        processor/\n            __init__.py\n        sensevoice/\n            __init__.py\n        qwen/\n            __init__.py\n        utils/\n            json_repaire.py\n            object_store_ref_util.py\n            __init__.py\n            types.py\n            tokenizer.py\n            openai_utils.py\n            ray_utils.py\n            text_generator.py\n            emb.py\n            connect_ray.py\n            testing.py\n            langutil.py\n            nontext.py\n            fulltune/\n                deepspeed_trainner.py\n                __init__.py\n                launch.py\n                trainner.py\n                base_model/\n                    __init__.py\n                    configuration_baichuan.py\n                    modeling_baichuan.py\n                pretrain/\n                    __init__.py\n                    convert_to_transformers.py\n            metrics/\n                __init__.py\n            config/\n                __init__.py\n            retrieval/\n                rest.py\n                __init__.py\n                udf.py\n            sft/\n                argument.py\n                collator.py\n                qlora.py\n                __init__.py\n                model.py\n                dataset.py\n                loss.py\n                merge_lora.py\n                trainer.py\n            client/\n                message_utils.py\n                math_utils.py\n                byzerllm_client.py\n                parallel_utils.py\n                __init__.py\n                types.py\n                code_utils.py\n                img_utils.py\n                entrypoints/\n                    __init__.py\n                    openai/\n                        protocol.py\n                        __init__.py\n                        serve.py\n                        serving_chat.py\n                        api_server.py\n                        serving_engine.py\n                        serving_completion.py\n                        tool.py\n        qwen_voice/\n            __init__.py\n        chatglm2/\n            __init__.py\n        baichuan/\n            __init__.py\n        m3e/\n            __init__.py\n        stable_diffusion/\n            config.py\n            __init__.py\n            logger.py\n            shared.py\n            model.py\n            utils.py\n            images.py\n            diffusion/\n                __init__.py\n                embeddings.py\n                utils.py\n                networks/\n                    lyco.py\n                    __init__.py\n                upscalers/\n                    __init__.py\n                    multidiffusion.py\n                    samplers.py\n                piplines/\n                    __init__.py\n                    lpw.py\n                    diffusers.py\n            lib/\n                __init__.py\n                diffusers/\n                    __init__.py\n                    scheduler.py\n            api/\n                __init__.py\n                plugin.py\n                models/\n                    diffusion.py\n                    __init__.py\n                diffusion/\n                    __init__.py\n                    pipelines/\n                        __init__.py\n                        diffusers.py\n                events/\n                    generation.py\n                    __init__.py\n        bge/\n            __init__.py\n        bge_rerank/\n            __init__.py\n        apps/\n            client.py\n            qa_strategy.py\n            __init__.py\n            builder.py\n            vector_db.py\n            utils.py\n            qa.py\n            byzer_storage/\n                memory_model_based.py\n                command.py\n                env.py\n                train_pt.py\n                train_sft.py\n                __init__.py\n                train_dpo.py\n                simple_api.py\n                generate_sft_data.py\n            byzer_sql/\n                __init__.py\n            llama_index/\n                byzerai_vectordb.py\n                byzerai.py\n                byzerai_embedding.py\n                __init__.py\n                collection_manager.py\n                byzerai_index_store.py\n                byzerai_docstore.py\n                simple_retrieval.py\n                byzerai_kvstore.py\n            agent/\n                conversable_agent.py\n                registry.py\n                groupchat.py\n                user_proxy_agent.py\n                __init__.py\n                agent.py\n                extensions/\n                    simple_retrieval_client.py\n                    planner.py\n                    python_codesandbox_agent.py\n                    preview_file_agent.py\n                    llama_index_retrieval_agent.py\n                    visualization_agent.py\n                    assistant_agent.py\n                    load_data_agent.py\n                    retrieval_agent.py\n                    output_agent.py\n                    __init__.py\n                    byzer_engine_agent.py\n                    subquestion_agent.py\n                    sql_reviewer_agent.py\n                    spark_sql_agent.py\n                    rhetorical_agent.py\n                    data_analysis.py\n                    data_analysis_pipeline_agent.py\n                    common_agent.py\n                    query_rewrite/\n                        time.py\n                        rhetorical.py\n                        __init__.py\n                        condition.py\n                        context.py\n                store/\n                    memory_store.py\n                    __init__.py\n                    stores.py\n        saas/\n            __init__.py\n            chatglm/\n                __init__.py\n            sparkdesk/\n                __init__.py\n            gemini/\n                __init__.py\n            azure_openai/\n                __init__.py\n            azure/\n                __init__.py\n            qianfan/\n                __init__.py\n            claude/\n                __init__.py\n            qianwen/\n                __init__.py\n            volcengine/\n                __init__.py\n            minimax/\n                __init__.py\n            baichuan/\n                __init__.py\n            qianwen_vl/\n                __init__.py\n            official_openai/\n                __init__.py\n            zhipu/\n                __init__.py\n            aws_bedrock/\n                __init__.py\n            openai/\n                __init__.py\n        falcon/\n            __init__.py\n        chatglm6b/\n            trainer_seq2seq.py\n            arguments.py\n            __init__.py\n            finetune.py\n            tunning/\n                __init__.py\n                infer.py\n                finetune.py\n                utils/\n                    config.py\n                    __init__.py\n                    other.py\n                    common.py\n                    seq2seq.py\n        starcode/\n            __init__.py\n        qwen_vl_chat/\n            __init__.py\n        zephyr/\n            __init__.py\n        dolly/\n            generate.py\n            __init__.py\n            consts.py\n            dolly_inference.py\n            trainer.py\n        llama/\n            __init__.py\n        whisper/\n            whisper_inference.py\n            __init__.py\n下面是一些文件路径以及每个文件对应的源码：\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/saas/official_openai/__init__.py\nimport time\nfrom typing import List, Tuple, Dict, Any, Union\nimport httpx\nfrom openai import OpenAI\nimport base64\nimport io\nimport json\nimport ray\nfrom byzerllm.utils.types import (\n    BlockVLLMStreamServer,\n    StreamOutputs,\n    SingleOutput,\n    SingleOutputMeta,\n    BlockBinaryStreamServer,\n)\nfrom byzerllm.utils.langutil import asyncfy_with_semaphore\nimport threading\nimport asyncio\nimport traceback\nimport uuid\nimport tempfile        \n\nclass CustomSaasAPI:\n\n    def __init__(self, infer_params: Dict[str, str]) -> None:\n\n        self.api_key = infer_params[\"saas.api_key\"]\n        self.model = infer_params.get(\"saas.model\", \"gpt-3.5-turbo-1106\")\n\n        other_params = {}\n\n        if \"saas.api_base\" in infer_params:\n            other_params[\"api_base\"] = infer_params[\"saas.api_base\"]\n\n        if \"saas.api_version\" in infer_params:\n            other_params[\"api_version\"] = infer_params[\"saas.api_version\"]\n\n        if \"saas.api_type\" in infer_params:\n            other_params[\"api_type\"] = infer_params[\"saas.api_type\"]\n\n        if \"saas.base_url\" in infer_params:\n            other_params[\"base_url\"] = infer_params[\"saas.base_url\"]\n\n        if \"saas.timeout\" in infer_params:\n            other_params[\"timeout\"] = float(infer_params[\"saas.timeout\"])\n\n        self.max_retries = int(infer_params.get(\"saas.max_retries\", 10))\n\n        self.meta = {\n            \"model_deploy_type\": \"saas\",\n            \"backend\": \"saas\",\n            \"support_stream\": True,\n            \"model_name\": self.model,\n        }\n\n        self.meta[\"embedding_mode\"] = \"embedding\" in self.model.lower()\n\n        self.proxies = infer_params.get(\"saas.proxies\", None)\n        self.local_address = infer_params.get(\"saas.local_address\", \"0.0.0.0\")\n\n        if not self.proxies:\n            self.client = OpenAI(**other_params, api_key=self.api_key)\n        else:\n            self.client = OpenAI(\n                **other_params,\n                api_key=self.api_key,\n                http_client=httpx.Client(\n                    proxies=self.proxies,\n                    transport=httpx.HTTPTransport(local_address=self.local_address),\n                ),\n            )\n\n        try:\n            ray.get_actor(\"BLOCK_VLLM_STREAM_SERVER\")\n        except ValueError:\n            try:\n                ray.remote(BlockVLLMStreamServer).options(\n                    name=\"BLOCK_VLLM_STREAM_SERVER\",\n                    lifetime=\"detached\",\n                    max_concurrency=1000,\n                ).remote()\n            except Exception as e:\n                pass\n        try:\n            ray.get_actor(\"BlockBinaryStreamServer\")\n        except ValueError:\n            try:\n                ray.remote(BlockBinaryStreamServer).options(\n                    name=\"BlockBinaryStreamServer\",\n                    lifetime=\"detached\",\n                    max_concurrency=1000,\n                ).remote()\n            except Exception as e:\n                pass\n\n    # saas/proprietary\n    def get_meta(self):\n        return [self.meta]\n\n    def process_input(self, ins: Union[str, List[Dict[str, Any]], Dict[str, Any]]):\n\n        if isinstance(ins, list) or isinstance(ins, dict):\n            return ins\n\n        content = []\n        try:\n            ins_json = json.loads(ins)\n        except:\n            return ins\n\n        ## speech\n        if isinstance(ins_json, dict):\n            return ins_json\n\n        content = []\n        #     [\n        #     {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n        #     {\n        #       \"type\": \"image_url\",\n        #       \"image_url\": {\n        #         \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n        #         \"detail\": \"high\"\n        #       },\n        #     },\n        #   ],\n        for item in ins_json:\n            # for format like this: {\"image\": \"xxxxxx\", \"text\": \"What’s in this image?\",\"detail\":\"high\"}\n            # or {\"image\": \"xxxxxx\"}, {\"text\": \"What’s in this image?\"}\n            if (\"image\" in item or \"image_url\" in item) and \"type\" not in item:\n                image_data = item.get(\"image\", item.get(\"image_url\", \"\"))\n                ## \"data:image/jpeg;base64,\"\n                if not image_data.startswith(\"data:\"):\n                    image_data = \"data:image/jpeg;base64,\" + image_data\n\n                ## get the other fields except image/text/image_url\n                other_fields = {\n                    k: v\n                    for k, v in item.items()\n                    if k not in [\"image\", \"text\", \"image_url\"]\n                }\n                content.append(\n                    {\n                        \"image_url\": {\"url\": image_data, **other_fields},\n                        \"type\": \"image_url\",\n                    }\n                )            \n\n            if \"text\" in item and \"type\" not in item:\n                text_data = item[\"text\"]\n                content.append({\"text\": text_data, \"type\": \"text\"})\n\n            ## for format like this: {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n            ## {\"type\": \"image_url\", \"image_url\": {\"url\":\"\",\"detail\":\"high\"}}\n            ## this is the standard format, just return it\n            if \"type\" in item and item[\"type\"] == \"text\":\n                content.append(item)\n\n            if \"type\" in item and item[\"type\"] == \"image_url\":\n                content.append(item)\n\n        if not content:\n            return ins\n\n        return content\n\n    async def async_embed_query(self, ins: str, **kwargs):\n        resp = await asyncfy_with_semaphore(\n            lambda: self.client.embeddings.create(input=[ins], model=self.model)\n        )()\n        embedding = resp.data[0].embedding\n        usage = resp.usage\n        return (\n            embedding,\n            {\n                \"metadata\": {\n                    \"input_tokens_count\": usage.prompt_tokens,\n                    \"generated_tokens_count\": 0,\n                }\n            },\n        )\n\n    def embed_query(self, ins: str, **kwargs):\n        resp = self.client.embeddings.create(input=[ins], model=self.model)\n        embedding = resp.data[0].embedding\n        usage = resp.usage\n        return (\n            embedding,\n            {\n                \"metadata\": {\n                    \"input_tokens_count\": usage.prompt_tokens,\n                    \"generated_tokens_count\": 0,\n                }\n            },\n        )\n\n    async def async_text_to_speech(\n        self, stream: bool, ins: str, voice: str, chunk_size: int = None, **kwargs\n    ):\n        if stream:\n            server = ray.get_actor(\"BlockBinaryStreamServer\")\n            request_id = [None]\n\n            def writer():\n                try:\n                    request_id[0] = str(uuid.uuid4())\n                    with self.client.with_streaming_response.audio.speech.create(\n                        model=self.model, voice=voice, input=ins, **kwargs\n                    ) as response:\n                        for chunk in response.iter_bytes(chunk_size):\n                            input_tokens_count = 0\n                            generated_tokens_count = 0\n                            ray.get(\n                                server.add_item.remote(\n                                    request_id[0],\n                                    StreamOutputs(\n                                        outputs=[\n                                            SingleOutput(\n                                                text=chunk,\n                                                metadata=SingleOutputMeta(\n                                                    input_tokens_count=input_tokens_count,\n                                                    generated_tokens_count=generated_tokens_count,\n                                                ),\n                                            )\n                                        ]\n                                    ),\n                                )\n                            )\n                except:\n                    traceback.print_exc()\n                ray.get(server.mark_done.remote(request_id[0]))\n\n            threading.Thread(target=writer, daemon=True).start()\n\n            time_count = 10 * 100\n            while request_id[0] is None and time_count > 0:\n                time.sleep(0.01)\n                time_count -= 1\n\n            if request_id[0] is None:\n                raise Exception(\"Failed to get request id\")\n\n            def write_running():\n                return ray.get(server.add_item.remote(request_id[0], \"RUNNING\"))\n\n            await asyncio.to_thread(write_running)\n            return [\n                (\n                    \"\",\n                    {\n                        \"metadata\": {\n                            \"request_id\": request_id[0],\n                            \"stream_server\": \"BlockBinaryStreamServer\",\n                        }\n                    },\n                )\n            ]\n\n        start_time = time.monotonic()\n        with io.BytesIO() as output:\n            async with asyncfy_with_semaphore(\n                lambda: self.client.with_streaming_response.audio.speech.create(\n                    model=self.model, voice=voice, input=ins, **kwargs\n                )\n            )() as response:\n                for chunk in response.iter_bytes():\n                    output.write(chunk)\n\n            base64_audio = base64.b64encode(output.getvalue()).decode()\n            time_cost = time.monotonic() - start_time\n            return [\n                (\n                    base64_audio,\n                    {\n                        \"metadata\": {\n                            \"request_id\": \"\",\n                            \"input_tokens_count\": 0,\n                            \"generated_tokens_count\": 0,\n                            \"time_cost\": time_cost,\n                            \"first_token_time\": 0,\n                            \"speed\": 0,\n                        }\n                    },\n                )\n            ]\n\n    async def async_speech_to_text(\n        self,\n        audio: str,\n        response_format: str = \"verbose_json\",\n        timestamp_granularities: List[str] = [\"word\",\"segment\"],\n    ):               \n        # Extract audio format and base64 data\n        data_prefix = \"data:audio/\"\n        base64_prefix = \";base64,\"\n        if not audio.startswith(data_prefix):\n            raise ValueError(\"Invalid audio data format\")\n        \n        format_end = audio.index(base64_prefix)\n        audio_format = audio[len(data_prefix):format_end]\n        base64_data = audio[format_end + len(base64_prefix):]\n        \n        # Decode the base64 audio data\n        audio_data = base64.b64decode(base64_data)\n\n        # Create a temporary file with the correct extension\n        with tempfile.NamedTemporaryFile(delete=False, suffix=f'.{audio_format}') as temp_audio_file:\n            temp_audio_file.write(audio_data)\n            temp_audio_file_path = temp_audio_file.name\n\n        try:\n            start_time = time.monotonic()\n                   \n            with open(temp_audio_file_path, \"rb\") as audio_file:                \n                transcription = self.client.audio.transcriptions.create(\n                    model=self.model or \"whisper-1\",\n                    file=audio_file,\n                    response_format=response_format,\n                    timestamp_granularities=timestamp_granularities,\n                )\n            time_cost = time.monotonic() - start_time\n\n            return [\n                (\n                    json.dumps(transcription.to_dict(), ensure_ascii=True),\n                    {\n                        \"metadata\": {\n                            \"request_id\": \"\",\n                            \"input_tokens_count\": 0,\n                            \"generated_tokens_count\": 0,\n                            \"time_cost\": time_cost,\n                            \"first_token_time\": 0,\n                            \"speed\": 0,\n                        }\n                    },\n                )\n            ]\n        finally:\n            # Clean up the temporary file\n            import os\n            os.unlink(temp_audio_file_path)\n\n    def image_to_text(self, ins: str, **kwargs):\n        pass\n\n    async def async_text_to_image(\n        self, stream: bool, input: str, size: str, quality: str, n: int, **kwargs\n    ):\n        if stream:\n            raise Exception(\"Stream not supported for text to image\")\n        start_time = time.monotonic()\n        response = await asyncfy_with_semaphore(\n            lambda: self.client.images.generate(\n                model=self.model,\n                prompt=input,\n                size=size,\n                quality=quality,\n                n=1,\n                response_format=\"b64_json\",\n                **kwargs,\n            )\n        )()\n        time_cost = time.monotonic() - start_time\n        base64_image = response.data[0].b64_json\n        return [\n            (\n                base64_image,\n                {\n                    \"metadata\": {\n                        \"request_id\": \"\",\n                        \"input_tokens_count\": 0,\n                        \"generated_tokens_count\": 0,\n                        \"time_cost\": time_cost,\n                        \"first_token_time\": 0,\n                        \"speed\": 0,\n                    }\n                },\n            )\n        ]\n\n    def text_to_text(self, ins: str, **kwargs):\n        pass\n\n    async def async_stream_chat(\n        self,\n        tokenizer,\n        ins: str,\n        his: List[Dict[str, Any]] = [],\n        max_length: int = 4096,\n        top_p: float = 0.7,\n        temperature: float = 0.9,\n        **kwargs,\n    ):\n\n        model = self.model\n\n        if \"model\" in kwargs:\n            model = kwargs[\"model\"]\n\n        messages = [\n            {\"role\": message[\"role\"], \"content\": self.process_input(message[\"content\"])}\n            for message in his\n        ] + [{\"role\": \"user\", \"content\": self.process_input(ins)}]\n\n        stream = kwargs.get(\"stream\", False)\n\n        ## content = [\n        ##    \"voice\": \"alloy\",\"input\": \"Hello, World!\",response_format: \"mp3\"]\n        last_message = messages[-1][\"content\"]\n\n        if isinstance(last_message, dict) and \"voice\" in last_message:\n            voice = last_message[\"voice\"]\n            response_format = last_message.get(\"response_format\", \"mp3\")\n            chunk_size = last_message.get(\"chunk_size\", None)\n            input = last_message[\"input\"]\n            return await self.async_text_to_speech(\n                stream=stream,\n                ins=input,\n                voice=voice,\n                chunk_size=chunk_size,\n                response_format=response_format,\n            )\n\n        if isinstance(last_message, dict) and \"input\" in last_message:\n            input = last_message[\"input\"]\n            size = last_message.get(\"size\", \"1024x1024\")\n            quality = last_message.get(\"quality\", \"standard\")\n            n = last_message.get(\"n\", 1)\n            return await self.async_text_to_image(\n                stream=stream, input=input, size=size, quality=quality, n=n\n            )\n\n        if isinstance(last_message, dict) and \"audio\" in last_message:            \n            audio_data = last_message.get(\"audio\", \"\")            \n            tpe = last_message.get(\"type\",\"wav\")\n            if not audio_data.startswith(\"data:\"):\n                audio_data = f\"data:audio/${tpe};base64,\" + audio_data                \n            return await self.async_speech_to_text(audio=audio_data)\n        \n        server = ray.get_actor(\"BLOCK_VLLM_STREAM_SERVER\")\n        request_id = [None]\n\n        def writer():\n            try:\n                r = \"\"\n                response = self.client.chat.completions.create(\n                    messages=messages,\n                    model=model,\n                    stream=True,\n                    max_tokens=max_length,\n                    temperature=temperature,\n                    top_p=top_p,\n                )\n                # input_tokens_count = 0\n                # generated_tokens_count = 0\n\n                request_id[0] = str(uuid.uuid4())\n\n                for chunk in response:\n                    content = chunk.choices[0].delta.content or \"\"\n                    r += content\n                    if hasattr(chunk, \"usage\") and chunk.usage:\n                        input_tokens_count = chunk.usage.prompt_tokens\n                        generated_tokens_count = chunk.usage.completion_tokens\n                    else:\n                        input_tokens_count = 0\n                        generated_tokens_count = 0\n                    ray.get(\n                        server.add_item.remote(\n                            request_id[0],\n                            StreamOutputs(\n                                outputs=[\n                                    SingleOutput(\n                                        text=r,\n                                        metadata=SingleOutputMeta(\n                                            input_tokens_count=input_tokens_count,\n                                            generated_tokens_count=generated_tokens_count,\n                                        ),\n                                    )\n                                ]\n                            ),\n                        )\n                    )\n            except:\n                traceback.print_exc()\n            ray.get(server.mark_done.remote(request_id[0]))\n\n        if stream:\n            threading.Thread(target=writer, daemon=True).start()\n\n            time_count = 60 * 100\n            while request_id[0] is None and time_count > 0:\n                time.sleep(0.01)\n                time_count -= 1\n\n            if request_id[0] is None:\n                raise Exception(\"Failed to get request id\")\n\n            def write_running():\n                return ray.get(server.add_item.remote(request_id[0], \"RUNNING\"))\n\n            await asyncio.to_thread(write_running)\n            return [\n                (\n                    \"\",\n                    {\n                        \"metadata\": {\n                            \"request_id\": request_id[0],\n                            \"stream_server\": \"BLOCK_VLLM_STREAM_SERVER\",\n                        }\n                    },\n                )\n            ]\n        else:\n            try:\n                start_time = time.monotonic()\n                response = await asyncfy_with_semaphore(\n                    lambda: self.client.chat.completions.create(\n                        messages=messages,\n                        model=model,\n                        max_tokens=max_length,\n                        temperature=temperature,\n                        top_p=top_p,\n                    )\n                )()\n\n                generated_text = response.choices[0].message.content\n                generated_tokens_count = response.usage.completion_tokens\n                input_tokens_count = response.usage.prompt_tokens\n                time_cost = time.monotonic() - start_time\n                return [\n                    (\n                        generated_text,\n                        {\n                            \"metadata\": {\n                                \"request_id\": response.id,\n                                \"input_tokens_count\": input_tokens_count,\n                                \"generated_tokens_count\": generated_tokens_count,\n                                \"time_cost\": time_cost,\n                                \"first_token_time\": 0,\n                                \"speed\": float(generated_tokens_count) / time_cost,\n                            }\n                        },\n                    )\n                ]\n            except Exception as e:\n                print(f\"Error: {e}\")\n                raise e\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/whisper/__init__.py\nimport whisper\nfrom typing import Dict, List, Tuple\nfrom byzerllm.utils.types import StopSequencesCriteria\n\nfrom typing import Dict, Any, List, Union\nfrom pyjava.storage import streaming_tar as STar\nimport json\nimport base64\nimport tempfile\nimport time\n\n\ndef get_meta(self):\n    return [\n        {\n            \"model_deploy_type\": \"proprietary\",\n            \"backend\": \"transformers\",\n            \"message_format\": True\n        }\n    ]\n\n\ndef process_input(ins: Union[str, List[Dict[str, Any]], Dict[str, Any]]):\n    # print(ins) \n    if isinstance(ins, list) or isinstance(ins, dict):\n        return ins\n\n    content = []\n    try:\n        ins_json = json.loads(ins)\n    except:\n        return ins\n\n    ## speech\n    if isinstance(ins_json, dict):\n        return ins_json\n\n    content = []\n    #     [\n    #     {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n    #     {\n    #       \"type\": \"image_url\",\n    #       \"image_url\": {\n    #         \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n    #         \"detail\": \"high\"\n    #       },\n    #     },\n    #   ],\n    for item in ins_json:\n        # for format like this: {\"image\": \"xxxxxx\", \"text\": \"What’s in this image?\",\"detail\":\"high\"}\n        # or {\"image\": \"xxxxxx\"}, {\"text\": \"What’s in this image?\"}\n        if (\"image\" in item or \"image_url\" in item) and \"type\" not in item:\n            image_data = item.get(\"image\", item.get(\"image_url\", \"\"))\n            ## \"data:image/jpeg;base64,\"\n            if not image_data.startswith(\"data:\"):\n                image_data = \"data:image/jpeg;base64,\" + image_data\n\n            ## get the other fields except image/text/image_url\n            other_fields = {\n                k: v for k, v in item.items() if k not in [\"image\", \"text\", \"image_url\"]\n            }\n            content.append(\n                {\n                    \"image_url\": {\"url\": image_data, **other_fields},\n                    \"type\": \"image_url\",\n                }\n            )\n\n        if \"text\" in item and \"type\" not in item:\n            text_data = item[\"text\"]\n            content.append({\"text\": text_data, \"type\": \"text\"})\n\n        ## for format like this: {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n        ## {\"type\": \"image_url\", \"image_url\": {\"url\":\"\",\"detail\":\"high\"}}\n        ## this is the standard format, just return it\n        if \"type\" in item and item[\"type\"] == \"text\":\n            content.append(item)\n\n        if \"type\" in item and item[\"type\"] == \"image_url\":\n            content.append(item)\n\n    if not content:\n        return ins\n\n    return content\n\n\ndef stream_chat(\n    self,\n    tokenizer,\n    ins: str,\n    his: List[Dict[str, str]] = [],\n    max_length: int = 4090,\n    top_p: float = 0.95,\n    temperature: float = 0.1,\n    **kwargs,\n):\n    messages = [\n        {\"role\": message[\"role\"], \"content\": process_input(message[\"content\"])}\n        for message in his\n    ] + [{\"role\": \"user\", \"content\": process_input(ins)}]\n\n    audio_input = messages[-1][\"content\"]\n\n    audio_data = audio_input.get(\"audio\", \"\")\n    audio = audio_input[\"audio\"]\n    response_format = audio_input.get(\"response_format\", \"json\")\n    timestamp_granularities = audio_input.get(\n        \"timestamp_granularities\", [\"word\", \"segment\"]\n    )\n    data_prefix = \"data:audio/\"\n    base64_prefix = \";base64,\"\n    if not audio.startswith(data_prefix):\n        raise ValueError(\"Invalid audio data format\")\n\n    format_end = audio.index(base64_prefix)\n    audio_format = audio[len(data_prefix) : format_end]\n    base64_data = audio[format_end + len(base64_prefix) :]\n\n    # Decode the base64 audio data\n    audio_data = base64.b64decode(base64_data)\n\n    # Create a temporary file with the correct extension\n    with tempfile.NamedTemporaryFile(\n        delete=False, suffix=f\".{audio_format}\"\n    ) as temp_audio_file:\n        temp_audio_file.write(audio_data)\n        temp_audio_file_path = temp_audio_file.name\n\n    try:\n        start_time = time.monotonic()\n        word_timestamps = \"word\" in timestamp_granularities\n        \n        result = self.transcribe(temp_audio_file_path, word_timestamps=word_timestamps)        \n            \n        time_cost = time.monotonic() - start_time\n        return [\n            (\n                json.dumps(result, ensure_ascii=False),\n                {\n                    \"metadata\": {\n                        \"request_id\": \"\",\n                        \"input_tokens_count\": 0,\n                        \"generated_tokens_count\": 0,\n                        \"time_cost\": time_cost,\n                        \"first_token_time\": 0,\n                        \"speed\": 0,\n                    }\n                },\n            )\n        ]\n    finally:\n        # Clean up the temporary file\n        import os\n\n        os.unlink(temp_audio_file_path)\n\n\ndef init_model(\n    model_dir, infer_params: Dict[str, str] = {}, sys_conf: Dict[str, str] = {}\n):\n    model = whisper.load_model(\"large\", download_root=model_dir)\n    import types\n\n    model.stream_chat = types.MethodType(stream_chat, model)\n    model.get_meta = types.MethodType(get_meta, model)\n    return (model, None)\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/notebooks/test_sensevoice.py\nimport sys\nimport os\n\n# Add the project root to the Python path\nsys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\")))\n\nfrom src.byzerllm.sensevoice import init_model, format_str_v3\nimport base64\nimport json\n\n\ndef main():\n    # Initialize the model\n    model_dir = \"/Users/allwefantasy/models/SenseVoiceSmall\"\n    model, _ = init_model(model_dir)\n\n    # Prepare test audio\n    with open(\n        \"/Users/allwefantasy/models/SenseVoiceSmall/example/zh.mp3\", \"rb\"\n    ) as audio_file:\n        audio_data = audio_file.read()\n        base64_audio = base64.b64encode(audio_data).decode(\"utf-8\")        \n        # Prepare input in the expected format\n        test_input = json.dumps(\n            {\n                            \"type\": \"audio\",\n                            \"audio\": f\"data:audio/wav;base64,{base64_audio}\",\n                        }\n        )\n\n        # Call stream_chat\n        result = model.stream_chat(tokenizer=None, ins = test_input)\n\n        # Process and print the result\n        if result and len(result) > 0:\n            output, metadata = result[0]\n            parsed_output = json.loads(output)\n            formatted_text = parsed_output.get(\"text\", \"\")\n            print(\"Transcription:\", formatted_text)\n            print(\"Metadata:\", metadata)\n        else:\n            print(\"No result returned from the model.\")\n\n\nif __name__ == \"__main__\":\n    main()\n\n\n##File: /Users/allwefantasy/projects/Qwen2-Audio/README_CN.md\n<p align=\"left\">\n        中文</a>&nbsp ｜ &nbsp<a href=\"README.md\">English</a>\n</p>\n<br><br>\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/assets/blog/qwenaudio/qwen2audio_logo.png\" width=\"400\"/>\n<p>\n\n<p align=\"center\">\nQwen2-Audio-7B <a href=\"https://modelscope.cn/models/qwen/Qwen2-Audio-7B\">🤖 </a> | <a href=\"https://huggingface.co/Qwen/Qwen2-Audio-7B\">🤗</a>&nbsp ｜ Qwen-Audio-7B-Instruct <a href=\"https://modelscope.cn/models/qwen/Qwen2-Audio-7B-Instruct\">🤖 </a>| <a href=\"https://huggingface.co/Qwen/Qwen2-Audio-7B-Instruct\">🤗</a>&nbsp ｜ Demo<a href=\"https://modelscope.cn/studios/qwen/Qwen2-Audio-Instruct-Demo\"> 🤖</a> | <a href=\"https://huggingface.co/spaces/Qwen/Qwen2-Audio-Instruct-Demo\">🤗</a>&nbsp\n<br>\n📑 <a href=\"https://arxiv.org/abs/2407.10759\">Paper</a> &nbsp&nbsp | &nbsp&nbsp 📑 <a href=\"https://qwenlm.github.io/blog/qwen2-audio\">Blog</a> &nbsp&nbsp | &nbsp&nbsp 💬 <a href=\"https://github.com/QwenLM/Qwen/blob/main/assets/wechat.png\">WeChat (微信)</a>&nbsp&nbsp | &nbsp&nbsp <a href=\"https://discord.gg/CV4E9rpNSD\">Discord</a>&nbsp&nbsp\n</p>\n\n\n\n我们介绍Qwen-Audio的最新进展：Qwen2-Audio。作为一个大规模音频语言模型，Qwen2-Audio能够接受各种音频信号输入，并根据语音指令执行音频分析或直接响应文本。我们介绍两种不同的音频交互模式：语音聊天 voice chat 和音频分析 audio analysis。\n\n* 语音聊天：用户可以自由地与 Qwen2-Audio 进行语音互动，而无需文本输入；\n* 音频分析：用户可以在互动过程中提供音频和文本指令对音频进行分析；\n\n**我们已经开源了 Qwen2-Audio 系列的两个模型：Qwen2-Audio-7B和Qwen2-Audio-7B-Instruct。**\n\n## 模型结构与训练范式\n\nQwen2-Audio 三阶段训练过程概述。\n\n<p align=\"left\">\n    <img src=\"assets/framework.png\" width=\"80%\"/>\n<p>\n\n\n\n## 新闻\n* 2024.8.9 🎉 我们在 ModelScope 和 Hugging Face 开源了`Qwen2-Audio-7B`和`Qwen2-Audio-7B-Instruct`的 checkpoint.\n* 2024.7.15 🎉 我们发布了 Qwen2-Audio 的[论文](https://arxiv.org/abs/2407.10759), 介绍了相关的模型结构，训练方法和模型表现。\n* 2023.11.30 🔥 我们发布了**Qwen-Audio**系列\n<br>\n\n## 评测\n我们在标准的13个学术数据集上评测了模型的能力如下：\n<table><thead><tr><th>Task</th><th>Description</th><th>Dataset</th><th>Split</th><th>Metric</th></tr></thead><tbody><tr><td rowspan=\"4\">ASR</td><td rowspan=\"4\">Automatic Speech Recognition</td><td>Fleurs</td><td>dev | test</td><td rowspan=\"4\">WER</td></tr><tr><td>Aishell2</td><td>test</td></tr><tr><td>Librispeech</td><td>dev | test</td></tr><tr><td>Common Voice</td><td>dev | test</td></tr><tr><td>S2TT</td><td>Speech-to-Text Translation</td><td>CoVoST2</td><td>test</td><td>BLEU </td></tr><tr><td>SER</td><td>Speech Emotion Recognition</td><td>Meld</td><td>test</td><td>ACC</td></tr><tr><td>VSC</td><td>Vocal Sound Classification</td><td>VocalSound</td><td>test</td><td>ACC</td></tr><tr><td rowspan=\"4\"><a href=\"https://github.com/OFA-Sys/AIR-Bench\">AIR-Bench</a><br></td><td>Chat-Benchmark-Speech</td><td>Fisher<br>SpokenWOZ<br>IEMOCAP<br>Common voice</td><td>dev | test</td><td>GPT-4 Eval</td></tr><tr><td>Chat-Benchmark-Sound</td><td>Clotho</td><td>dev | test</td><td>GPT-4 Eval</td></tr>\n<tr><td>Chat-Benchmark-Music</td><td>MusicCaps</td><td>dev | test</td><td>GPT-4 Eval</td></tr><tr><td>Chat-Benchmark-Mixed-Audio</td><td>Common voice<br>AudioCaps<br>MusicCaps</td><td>dev | test</td><td>GPT-4 Eval</td></tr></tbody></table>\n\n\n以下是整体表现：\n<p align=\"left\">\n    <img src=\"assets/radar_compare_qwen_audio.png\" width=\"70%\"/>\n<p>\n\n评测分数详情如下：\n<br>\n<b>（注意：我们所展示的评测结果是在原始训练框架的初始模型上的，然而在框架转换 Huggingface 后指标出现了部分波动，在这里我们展示我们的全部测评结果：首先是论文中的初始模型结果）</b>\n\n<table><thead><tr><th rowspan=\"2\">Task</th><th rowspan=\"2\">Dataset</th><th rowspan=\"2\">Model</th><th colspan=\"2\">Performance</th></tr><tr><th>Metrics</th><th>Results</th></tr></thead><tbody><tr><td rowspan=\"15\">ASR</td><td rowspan=\"7\"><b>Librispeech</b><br>dev-clean | dev-other | <br>test-clean | test-other</td><td>SpeechT5</td><td rowspan=\"7\">WER </td><td>2.1 | 5.5 | 2.4 | 5.8</td></tr><tr><td>SpeechNet</td><td>- | - | 30.7 | -</td></tr><tr><td>SLM-FT</td><td>- | - | 2.6 | 5.0</td></tr><tr><td>SALMONN</td><td>- | - | 2.1 | 4.9</td></tr><tr><td>SpeechVerse</td><td>- | - | 2.1 | 4.4</td></tr><tr><td>Qwen-Audio</td><td>1.8 | 4.0 | 2.0 | 4.2</td></tr><tr><td>Qwen2-Audio</td><td><b>1.3 | 3.4 | 1.6 | 3.6</b></td></tr><tr><td rowspan=\"2\"><b>Common Voice 15</b> <br>en | zh | yue | fr</td><td>Whisper-large-v3</td><td rowspan=\"2\">WER </td><td>9.3 | 12.8 | 10.9 | 10.8</td></tr><tr><td>Qwen2-Audio</td><td><b>8.6 | 6.9 | 5.9 | 9.6</b></td></tr>\n<tr><td rowspan=\"2\"><b>Fleurs</b> <br>zh</td><td>Whisper-large-v3</td><td rowspan=\"2\">WER </td><td>7.7</td></tr><tr><td>Qwen2-Audio</td><td><b>7.5</b></td></tr><tr><td rowspan=\"4\"><b>Aishell2</b> <br>Mic | iOS | Android</td><td>MMSpeech-base</td><td rowspan=\"4\">WER </td><td>4.5 | 3.9 | 4.0</td></tr><tr><td>Paraformer-large</td><td>- | <b>2.9</b> | -</td></tr><tr><td>Qwen-Audio</td><td>3.3 | 3.1 | 3.3</td></tr><tr><td>Qwen2-Audio</td><td><b>3.0</b> | 3.0 | <b>2.9</b></td></tr><tr><td rowspan=\"8\">S2TT</td><td rowspan=\"5\"><b>CoVoST2</b> <br>en-de | de-en | <br>en-zh | zh-en</td><td>SALMONN</td><td rowspan=\"5\">BLEU </td><td>18.6 | - | 33.1 | -</td></tr><tr><td>SpeechLLaMA</td><td>- | 27.1 | - | 12.3</td></tr><tr><td>BLSP</td><td>14.1 | - | - | -</td></tr><tr><td>Qwen-Audio</td><td>25.1 | 33.9 | 41.5 | 15.7</td></tr><tr><td>Qwen2-Audio</td><td><b>29.9 | 35.2 | 45.2 | 24.4</b></td></tr>\n<tr><td rowspan=\"3\"><b>CoVoST2</b> <br>es-en | fr-en | it-en |</td><td>SpeechLLaMA</td><td rowspan=\"3\">BLEU </td><td>27.9 | 25.2 | 25.9</td></tr><tr><td>Qwen-Audio</td><td>39.7 | <b>38.5</b> | 36.0</td></tr><tr><td>Qwen2-Audio</td><td><b>40.0 | 38.5 | 36.3</b></td></tr><tr><td rowspan=\"3\">SER</td><td rowspan=\"3\"><b>Meld</b></td><td>WavLM-large</td><td rowspan=\"3\">ACC </td><td>0.542</td></tr><tr><td>Qwen-Audio</td><td><b>0.557</b></td></tr><tr><td>Qwen2-Audio</td><td>0.553</td></tr><tr><td rowspan=\"4\">VSC</td><td rowspan=\"4\"><b>VocalSound</b></td><td>CLAP</td><td rowspan=\"4\">ACC </td><td>0.4945</td></tr><tr><td>Pengi</td><td>0.6035</td></tr><tr><td>Qwen-Audio</td><td>0.9289</td></tr><tr><td>Qwen2-Audio</td><td><b>0.9392</b></td></tr>\n<tr><td>AIR-Bench <br></td><td><b>Chat Benchmark</b><br>Speech | Sound |<br> Music | Mixed-Audio</td><td>SALMONN<br>BLSP<br>Pandagpt<br>Macaw-LLM<br>SpeechGPT<br>Next-gpt<br>Qwen-Audio<br>Gemini-1.5-pro<br>Qwen2-Audio</td><td>GPT-4 </td><td>6.16 | 6.28 | 5.95 | 6.08<br>6.17 | 5.55 | 5.08 | 5.33<br>3.58 | 5.46 | 5.06 | 4.25<br>0.97 | 1.01 | 0.91 | 1.01<br>1.57 | 0.95 | 0.95 | 4.13<br>3.86 | 4.76 | 4.18 | 4.13<br>6.47 | 6.95 | 5.52 | 6.08<br>6.97 | 5.49 | 5.06 | 5.27<br><b>7.18 | 6.99 | 6.79 | 6.77</b></td></tr></tbody></table>\n\n<b>（其次是转换 huggingface 后的）</b>\n\n<table><thead><tr><th rowspan=\"2\">Task</th><th rowspan=\"2\">Dataset</th><th rowspan=\"2\">Model</th><th colspan=\"2\">Performance</th></tr><tr><th>Metrics</th><th>Results</th></tr></thead><tbody><tr><td rowspan=\"15\">ASR</td><td rowspan=\"7\"><b>Librispeech</b><br>dev-clean | dev-other | <br>test-clean | test-other</td><td>SpeechT5</td><td rowspan=\"7\">WER </td><td>2.1 | 5.5 | 2.4 | 5.8</td></tr><tr><td>SpeechNet</td><td>- | - | 30.7 | -</td></tr><tr><td>SLM-FT</td><td>- | - | 2.6 | 5.0</td></tr><tr><td>SALMONN</td><td>- | - | 2.1 | 4.9</td></tr><tr><td>SpeechVerse</td><td>- | - | 2.1 | 4.4</td></tr><tr><td>Qwen-Audio</td><td>1.8 | 4.0 | 2.0 | 4.2</td></tr><tr><td>Qwen2-Audio</td><td><b>1.7 | 3.6 | 1.7 | 4.0</b></td></tr><tr><td rowspan=\"2\"><b>Common Voice 15</b> <br>en | zh | yue | fr</td><td>Whisper-large-v3</td><td rowspan=\"2\">WER </td><td>9.3 | 12.8 | 10.9 | 10.8</td></tr><tr><td>Qwen2-Audio</td><td><b>8.7 | 6.5 | 5.9 | 9.6</b></td></tr>\n<tr><td rowspan=\"2\"><b>Fleurs</b> <br>zh</td><td>Whisper-large-v3</td><td rowspan=\"2\">WER </td><td>7.7</td></tr><tr><td>Qwen2-Audio</td><td><b>7.0</b></td></tr><tr><td rowspan=\"4\"><b>Aishell2</b> <br>Mic | iOS | Android</td><td>MMSpeech-base</td><td rowspan=\"4\">WER </td><td>4.5 | 3.9 | 4.0</td></tr><tr><td>Paraformer-large</td><td>- | <b>2.9</b> | -</td></tr><tr><td>Qwen-Audio</td><td>3.3 | 3.1 | 3.3</td></tr><tr><td>Qwen2-Audio</td><td><b>3.2</b> | 3.1 | <b>2.9</b></td></tr><tr><td rowspan=\"8\">S2TT</td><td rowspan=\"5\"><b>CoVoST2</b> <br>en-de | de-en | <br>en-zh | zh-en</td><td>SALMONN</td><td rowspan=\"5\">BLEU </td><td>18.6 | - | 33.1 | -</td></tr><tr><td>SpeechLLaMA</td><td>- | 27.1 | - | 12.3</td></tr><tr><td>BLSP</td><td>14.1 | - | - | -</td></tr><tr><td>Qwen-Audio</td><td>25.1 | <b>33.9</b> | 41.5 | 15.7</td></tr><tr><td>Qwen2-Audio</td><td><b>29.6</b> | 33.6 | <b>45.6</b> | <b>24.0</b></td></tr>\n<tr><td rowspan=\"3\"><b>CoVoST2</b> <br>es-en | fr-en | it-en |</td><td>SpeechLLaMA</td><td rowspan=\"3\">BLEU </td><td>27.9 | 25.2 | 25.9</td></tr><tr><td>Qwen-Audio</td><td><b>39.7 | 38.5 | 36.0</b></td></tr><tr><td>Qwen2-Audio</td><td>38.7 | 37.2 | 35.2</td></tr><tr><td rowspan=\"3\">SER</td><td rowspan=\"3\"><b>Meld</b></td><td>WavLM-large</td><td rowspan=\"3\">ACC </td><td>0.542</td></tr><tr><td>Qwen-Audio</td><td><b>0.557</b></td></tr><tr><td>Qwen2-Audio</td><td>0.535</td></tr><tr><td rowspan=\"4\">VSC</td><td rowspan=\"4\"><b>VocalSound</b></td><td>CLAP</td><td rowspan=\"4\">ACC </td><td>0.4945</td></tr><tr><td>Pengi</td><td>0.6035</td></tr><tr><td>Qwen-Audio</td><td>0.9289</td></tr><tr><td>Qwen2-Audio</td><td><b>0.9395</b></td></tr>\n<tr><td>AIR-Bench <br></td><td><b>Chat Benchmark</b><br>Speech | Sound |<br> Music | Mixed-Audio</td><td>SALMONN<br>BLSP<br>Pandagpt<br>Macaw-LLM<br>SpeechGPT<br>Next-gpt<br>Qwen-Audio<br>Gemini-1.5-pro<br>Qwen2-Audio</td><td>GPT-4 </td><td>6.16 | 6.28 | 5.95 | 6.08<br>6.17 | 5.55 | 5.08 | 5.33<br>3.58 | 5.46 | 5.06 | 4.25<br>0.97 | 1.01 | 0.91 | 1.01<br>1.57 | 0.95 | 0.95 | 4.13<br>3.86 | 4.76 | 4.18 | 4.13<br>6.47 | <b>6.95</b> | 5.52 | 6.08<br>6.97 | 5.49 | 5.06 | 5.27<br><b>7.24</b> | 6.83 | <b>6.73</b> | <b>6.42</b></td></tr></tbody></table>\n\n\n我们提供了以上**所有**评测脚本以供复现我们的实验结果。请阅读 [eval_audio/EVALUATION.md](eval_audio/EVALUATION.md) 了解更多信息。\n\n## 部署要求\nThe code of Qwen2-Audio has been in the latest Hugging face transformers and we advise you to build from source with command `pip install git+https://github.com/huggingface/transformers`, or you might encounter the following error:\nQwen2-Audio的代码已经包含在最新的 Hugging Face Transformers 的主分支中，我们建议您使用命令`pip install git+https://github.com/huggingface/transformers`\n```\nKeyError: 'qwen2-audio'\n```\n## 快速使用\n我们提供简单的示例来说明如何利用 🤗 Transformers 快速使用 Qwen2-Audio-7B 和 Qwen2-Audio-7B-Instruct。\n在开始前，请确保你已经配置好环境并安装好相关的代码包。最重要的是，确保你满足上述要求，然后安装相关的依赖库。\n接下来你可以开始使用 Transformers 或者 ModelScope 来使用我们的模型。目前Qwen2-Audio-7B 及 Qwen2-Audio-7B-Instruct 模型处理30秒以内的音频表现更佳。\n#### 🤗 Hugging Face Transformers\n如希望使用 Qwen2-Audio-7B-Instruct 进行推理，我们分别演示语音聊天和音频分析的交互方式，所需要写的只是如下所示的数行代码。\n##### 语音聊天推理\n在语音聊天模式下，用户可以自由地与 Qwen2-Audio 进行语音交互，无需文字输入：\n```python\nfrom io import BytesIO\nfrom urllib.request import urlopen\nimport librosa\nfrom transformers import Qwen2AudioForConditionalGeneration, AutoProcessor\n\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\")\nmodel = Qwen2AudioForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\", device_map=\"auto\")\n\nconversation = [\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/guess_age_gender.wav\"},\n    ]},\n    {\"role\": \"assistant\", \"content\": \"Yes, the speaker is female and in her twenties.\"},\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/translate_to_chinese.wav\"},\n    ]},\n]\ntext = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\naudios = []\nfor message in conversation:\n    if isinstance(message[\"content\"], list):\n        for ele in message[\"content\"]:\n            if ele[\"type\"] == \"audio\":\n                audios.append(librosa.load(\n                    BytesIO(urlopen(ele['audio_url']).read()), \n                    sr=processor.feature_extractor.sampling_rate)[0]\n                )\n\ninputs = processor(text=text, audios=audios, return_tensors=\"pt\", padding=True)\ninputs.input_ids = inputs.input_ids.to(\"cuda\")\n\ngenerate_ids = model.generate(**inputs, max_length=256)\ngenerate_ids = generate_ids[:, inputs.input_ids.size(1):]\n\nresponse = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n```\n##### 音频分析推理\n在音频分析中，用户可以提供音频和文字问题来实现对音频的分析：\n```python\nfrom io import BytesIO\nfrom urllib.request import urlopen\nimport librosa\nfrom transformers import Qwen2AudioForConditionalGeneration, AutoProcessor\n\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\")\nmodel = Qwen2AudioForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\", device_map=\"auto\")\n\nconversation = [\n    {'role': 'system', 'content': 'You are a helpful assistant.'}, \n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/glass-breaking-151256.mp3\"},\n        {\"type\": \"text\", \"text\": \"What's that sound?\"},\n    ]},\n    {\"role\": \"assistant\", \"content\": \"It is the sound of glass shattering.\"},\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"text\", \"text\": \"What can you do when you hear that?\"},\n    ]},\n    {\"role\": \"assistant\", \"content\": \"Stay alert and cautious, and check if anyone is hurt or if there is any damage to property.\"},\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/1272-128104-0000.flac\"},\n        {\"type\": \"text\", \"text\": \"What does the person say?\"},\n    ]},\n]\ntext = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\naudios = []\nfor message in conversation:\n    if isinstance(message[\"content\"], list):\n        for ele in message[\"content\"]:\n            if ele[\"type\"] == \"audio\":\n                audios.append(\n                    librosa.load(\n                        BytesIO(urlopen(ele['audio_url']).read()), \n                        sr=processor.feature_extractor.sampling_rate)[0]\n                )\n\ninputs = processor(text=text, audios=audios, return_tensors=\"pt\", padding=True)\ninputs.input_ids = inputs.input_ids.to(\"cuda\")\n\ngenerate_ids = model.generate(**inputs, max_length=256)\ngenerate_ids = generate_ids[:, inputs.input_ids.size(1):]\n\nresponse = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n```\n##### 批量推理\n我们也支持批量推理：\n```python\nfrom io import BytesIO\nfrom urllib.request import urlopen\nimport librosa\nfrom transformers import Qwen2AudioForConditionalGeneration, AutoProcessor\n\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\")\nmodel = Qwen2AudioForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\", device_map=\"auto\")\n\nconversation1 = [\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/glass-breaking-151256.mp3\"},\n        {\"type\": \"text\", \"text\": \"What's that sound?\"},\n    ]},\n    {\"role\": \"assistant\", \"content\": \"It is the sound of glass shattering.\"},\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/f2641_0_throatclearing.wav\"},\n        {\"type\": \"text\", \"text\": \"What can you hear?\"},\n    ]}\n]\n\nconversation2 = [\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/1272-128104-0000.flac\"},\n        {\"type\": \"text\", \"text\": \"What does the person say?\"},\n    ]},\n]\n\nconversations = [conversation1, conversation2]\n\ntext = [processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False) for conversation in conversations]\n\naudios = []\nfor conversation in conversations:\n    for message in conversation:\n        if isinstance(message[\"content\"], list):\n            for ele in message[\"content\"]:\n                if ele[\"type\"] == \"audio\":\n                    audios.append(\n                        librosa.load(\n                            BytesIO(urlopen(ele['audio_url']).read()), \n                            sr=processor.feature_extractor.sampling_rate)[0]\n                    )\n\ninputs = processor(text=text, audios=audios, return_tensors=\"pt\", padding=True)\ninputs['input_ids'] = inputs['input_ids'].to(\"cuda\")\ninputs.input_ids = inputs.input_ids.to(\"cuda\")\n\ngenerate_ids = model.generate(**inputs, max_length=256)\ngenerate_ids = generate_ids[:, inputs.input_ids.size(1):]\n\nresponse = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n```\n运行Qwen2-Audio-7B同样非常简单。\n```python\nfrom io import BytesIO\nfrom urllib.request import urlopen\nimport librosa\nfrom transformers import AutoProcessor, Qwen2AudioForConditionalGeneration\n\nmodel = Qwen2AudioForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-Audio-7B\" ,trust_remote_code=True)\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-Audio-7B\" ,trust_remote_code=True)\n\nprompt = \"<|audio_bos|><|AUDIO|><|audio_eos|>Generate the caption in English:\"\nurl = \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Audio/glass-breaking-151256.mp3\"\naudio, sr = librosa.load(BytesIO(urlopen(url).read()), sr=processor.feature_extractor.sampling_rate)\ninputs = processor(text=prompt, audios=audio, return_tensors=\"pt\")\n\ngenerated_ids = model.generate(**inputs, max_length=256)\ngenerated_ids = generated_ids[:, inputs.input_ids.size(1):]\nresponse = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n```\n#### 🤖 ModelScope\n我们强烈建议用户，特别是中国大陆地区的用户，使用 ModelScope。`snapshot_download` 可以帮助您解决下载检查点时遇到的问题。\n<br>\n## Demo\n### Web UI\n我们提供了 Web UI 的 demo 供用户使用。在开始前，确保已经安装如下代码库：\n```\npip install -r requirements_web_demo.txt\n```\n随后运行如下命令，并点击生成链接：\n```\npython demo/web_demo_audio.py\n```\n<br>\n\n## 样例展示\n更多样例将更新于[通义千问博客](https://qwenlm.github.io/blog/qwen2-audio)中的 Qwen2-Audio 博客。\n\n## 团队招聘\n\n我们是通义千问语音多模态团队，致力于以通义千问为核心，拓展音频多模态理解和生成能力，实现自由灵活的音频交互。目前团队蓬勃发展中，如有意向实习或全职加入我们，请发送简历至 `qwen_audio@list.alibaba-inc.com`.\n<br>\n\n## 使用协议\n\n请查看每个模型在其 Hugging Face 仓库中的许可证。您无需提交商业使用申请。\n<br>\n\n\n\n## 引用\n\n如果你觉得我们的论文和代码对你的研究有帮助，请考虑 :star: 和引用 :pencil: :)\n\n```BibTeX\n@article{Qwen-Audio,\n  title={Qwen-Audio: Advancing Universal Audio Understanding via Unified Large-Scale Audio-Language Models},\n  author={Chu, Yunfei and Xu, Jin and Zhou, Xiaohuan and Yang, Qian and Zhang, Shiliang and Yan, Zhijie  and Zhou, Chang and Zhou, Jingren},\n  journal={arXiv preprint arXiv:2311.07919},\n  year={2023}\n}\n```\n\n```BibTeX\n@article{Qwen2-Audio,\n  title={Qwen2-Audio Technical Report},\n  author={Chu, Yunfei and Xu, Jin and Yang, Qian and Wei, Haojie and Wei, Xipin and Guo,  Zhifang and Leng, Yichong and Lv, Yuanjun and He, Jinzheng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},\n  journal={arXiv preprint arXiv:2407.10759},\n  year={2024}\n}\n```\n## 联系我们\n\n如果你想给我们的研发团队和产品团队留言，请通过邮件（`qianwen_opensource@alibabacloud.com`）联系我们。\n<br>\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/qwen_voice/__init__.py\nimport time\nfrom typing import List, Tuple, Dict, Any, Union\nimport base64\nimport json\nimport librosa\nimport io\nfrom transformers import Qwen2AudioForConditionalGeneration, AutoProcessor\n\ndef get_meta(self):\n    return [\n        {\n            \"model_deploy_type\": \"proprietary\",\n            \"backend\": \"transformers\",\n            \"message_format\": True\n        }\n    ]\n\ndef process_input(ins: Union[str, List[Dict[str, Any]], Dict[str, Any]]):\n    if isinstance(ins, list) or isinstance(ins, dict):\n        return ins\n\n    content = []\n    try:\n        ins_json = json.loads(ins)\n    except:\n        return ins\n\n    if isinstance(ins_json, dict):\n        return ins_json\n\n    for item in ins_json:\n        if (\"audio\" in item or \"audio_url\" in item) and \"type\" not in item:\n            audio_data = item.get(\"audio\", item.get(\"audio_url\", \"\"))\n            if not audio_data.startswith(\"data:\"):\n                audio_data = \"data:audio/wav;base64,\" + audio_data\n\n            other_fields = {k: v for k, v in item.items() if k not in [\"audio\", \"text\", \"audio_url\"]}\n            content.append(\n                {\n                    \"audio_url\": {\"url\": audio_data, **other_fields},\n                    \"type\": \"audio\",\n                }\n            )\n\n        if \"text\" in item and \"type\" not in item:\n            text_data = item[\"text\"]\n            content.append({\"text\": text_data, \"type\": \"text\"})\n\n        if \"type\" in item and item[\"type\"] in [\"text\", \"audio\"]:\n            content.append(item)\n\n    if not content:\n        return ins\n\n    return content\n\ndef stream_chat(\n    self,\n    tokenizer,\n    ins: str,\n    his: List[Dict[str, str]] = [],\n    max_length: int = 4096,\n    top_p: float = 0.95,\n    temperature: float = 0.1,\n    **kwargs,\n):\n    messages = [\n        {\"role\": message[\"role\"], \"content\": process_input(message[\"content\"])}\n        for message in his\n    ] + [{\"role\": \"user\", \"content\": process_input(ins)}]\n\n    text = self.processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n    audios = []\n    for message in messages:\n        if isinstance(message[\"content\"], list):\n            for ele in message[\"content\"]:\n                if ele[\"type\"] == \"audio\":\n                    audio_data = ele[\"audio_url\"][\"url\"].split(\",\")[1]\n                    audio_bytes = base64.b64decode(audio_data)\n                    audio, _ = librosa.load(io.BytesIO(audio_bytes), sr=self.processor.feature_extractor.sampling_rate)\n                    audios.append(audio)\n\n    inputs = self.processor(text=text, audios=audios, return_tensors=\"pt\", padding=True)\n    inputs.input_ids = inputs.input_ids.to(\"cuda\")\n\n    start_time = time.monotonic()\n    generate_ids = self.generate(**inputs, max_length=max_length)\n    generate_ids = generate_ids[:, inputs.input_ids.size(1):]\n\n    response = self.processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n    time_cost = time.monotonic() - start_time\n\n    return [\n        (\n            response,\n            {\n                \"metadata\": {\n                    \"request_id\": \"\",\n                    \"input_tokens_count\": inputs.input_ids.size(1),\n                    \"generated_tokens_count\": generate_ids.size(1),\n                    \"time_cost\": time_cost,\n                    \"first_token_time\": 0,\n                    \"speed\": float(generate_ids.size(1)) / time_cost if time_cost > 0 else 0,\n                }\n            },\n        )\n    ]\n\ndef init_model(model_dir, infer_params: Dict[str, str] = {}, sys_conf: Dict[str, str] = {}):\n    processor = AutoProcessor.from_pretrained(model_dir)\n    model = Qwen2AudioForConditionalGeneration.from_pretrained(model_dir, device_map=\"auto\")\n    model.processor = processor\n    model.stream_chat = stream_chat.__get__(model)\n    model.get_meta = get_meta.__get__(model)\n    return (model, None)\n\n     \n\n下面是用户的需求：\n\n@/Users/allwefantasy/projects/byzer-llm/src/byzerllm/qwen_voice/__init__.py 给模型添加方法使用 types.MethodType 的模式\n"}, {"role": "assistant", "content": "```python\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/qwen_voice/__init__.py\n<<<<<<< SEARCH\nmodel.stream_chat = stream_chat.__get__(model)\nmodel.get_meta = get_meta.__get__(model)\n=======\nimport types\nmodel.stream_chat = types.MethodType(stream_chat, model)\nmodel.get_meta = types.MethodType(get_meta, model)\n>>>>>>> REPLACE\n```"}], "model": "deepseek_chat", "yaml_file": "actions/037_chat_action.yml"}
{"instruction": "@/Users/allwefantasy/projects/byzer-llm/src/byzerllm/qwen_voice/__init__.py 需要避免 .to(\"cuda\") 这样的动作抛出错误\n严格遵循 SEARCH/REPLACE 格式,如果是python代码,注意空格和缩进", "conversations": [{"role": "user", "content": "\n如果你需要生成代码，对于每个需要更改的文件,你需要按 *SEARCH/REPLACE block* 的格式进行生成。\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The opening fence and code language, eg: ```python\n2. The file path alone on a line, starting with \"##File:\" and verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n*SEARCH/REPLACE* blocks will replace *all* matching occurrences.\nInclude enough lines to make the SEARCH blocks unique.\n\nInclude *ALL* the code being searched and replaced!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n下面我们来看一个例子：\n\n当前项目目录结构：\n1. 项目根目录： /tmp/projects/mathweb\n2. 项目子目录/文件列表(类似tree 命令输出)\nflask/\n    app.py\n    templates/\n        index.html\n    static/\n        style.css\n\n用户需求： Change get_factorial() to use math.factorial\n\n回答： To make this change we need to modify `/tmp/projects/mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n用户需求： Refactor hello() into its own file.\n\n回答：To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/hello.py\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/main.py\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n现在让我们开始一个新的任务:\n\n当前项目目录结构：\n1. 项目根目录： /Users/allwefantasy/projects/byzer-llm\n2. 项目子目录/文件列表(类似tree 命令输出)：\nsetup.py\ntests/\n    test_emb_rerank.py\n    test_deepseek_code.py\n    test_prompt.py\n    test_model_meta.py\n    __init__.py\n    test_transformers_deploy.py\n    test_data_analysis.py\n    test_yi.py\n    test_deploy.py\n    functions_suite.py\n    test_template.py\n    auto/\n        test_backend_llama_cpp.py\n        __init__.py\nsetup-machine/\n    convert_safetensor.py\nnotebooks/\n    test.py\n    test_sensevoice.py\nsrc/\n    byzerllm/\n        lang.py\n        store.py\n        version.py\n        log.py\n        __init__.py\n        types.py\n        easy_byzerllm_command.py\n        byzerllm_command.py\n        command_args.py\n        bark/\n            generation.py\n            __init__.py\n            model.py\n            api.py\n            model_fine.py\n            bark_voice.py\n        moss/\n            finetune_moss.py\n            __init__.py\n            moss_inference.py\n            models/\n                configuration_moss.py\n                quantization.py\n                custom_autotune.py\n                __init__.py\n                modeling_moss.py\n                tokenization_moss.py\n        llama2/\n            __init__.py\n        records/\n            __init__.py\n        visualglm/\n            __init__.py\n        auto/\n            __init__.py\n            backend_llama_cpp.py\n            backend_vllm.py\n            backend_ds.py\n        alpha_moss/\n            __init__.py\n        processor/\n            __init__.py\n        sensevoice/\n            __init__.py\n        qwen/\n            __init__.py\n        utils/\n            json_repaire.py\n            object_store_ref_util.py\n            __init__.py\n            types.py\n            tokenizer.py\n            openai_utils.py\n            ray_utils.py\n            text_generator.py\n            emb.py\n            connect_ray.py\n            testing.py\n            langutil.py\n            nontext.py\n            fulltune/\n                deepspeed_trainner.py\n                __init__.py\n                launch.py\n                trainner.py\n                base_model/\n                    __init__.py\n                    configuration_baichuan.py\n                    modeling_baichuan.py\n                pretrain/\n                    __init__.py\n                    convert_to_transformers.py\n            metrics/\n                __init__.py\n            config/\n                __init__.py\n            retrieval/\n                rest.py\n                __init__.py\n                udf.py\n            sft/\n                argument.py\n                collator.py\n                qlora.py\n                __init__.py\n                model.py\n                dataset.py\n                loss.py\n                merge_lora.py\n                trainer.py\n            client/\n                message_utils.py\n                math_utils.py\n                byzerllm_client.py\n                parallel_utils.py\n                __init__.py\n                types.py\n                code_utils.py\n                img_utils.py\n                entrypoints/\n                    __init__.py\n                    openai/\n                        protocol.py\n                        __init__.py\n                        serve.py\n                        serving_chat.py\n                        api_server.py\n                        serving_engine.py\n                        serving_completion.py\n                        tool.py\n        qwen_voice/\n            __init__.py\n        chatglm2/\n            __init__.py\n        baichuan/\n            __init__.py\n        m3e/\n            __init__.py\n        stable_diffusion/\n            config.py\n            __init__.py\n            logger.py\n            shared.py\n            model.py\n            utils.py\n            images.py\n            diffusion/\n                __init__.py\n                embeddings.py\n                utils.py\n                networks/\n                    lyco.py\n                    __init__.py\n                upscalers/\n                    __init__.py\n                    multidiffusion.py\n                    samplers.py\n                piplines/\n                    __init__.py\n                    lpw.py\n                    diffusers.py\n            lib/\n                __init__.py\n                diffusers/\n                    __init__.py\n                    scheduler.py\n            api/\n                __init__.py\n                plugin.py\n                models/\n                    diffusion.py\n                    __init__.py\n                diffusion/\n                    __init__.py\n                    pipelines/\n                        __init__.py\n                        diffusers.py\n                events/\n                    generation.py\n                    __init__.py\n        bge/\n            __init__.py\n        bge_rerank/\n            __init__.py\n        apps/\n            client.py\n            qa_strategy.py\n            __init__.py\n            builder.py\n            vector_db.py\n            utils.py\n            qa.py\n            byzer_storage/\n                memory_model_based.py\n                command.py\n                env.py\n                train_pt.py\n                train_sft.py\n                __init__.py\n                train_dpo.py\n                simple_api.py\n                generate_sft_data.py\n            byzer_sql/\n                __init__.py\n            llama_index/\n                byzerai_vectordb.py\n                byzerai.py\n                byzerai_embedding.py\n                __init__.py\n                collection_manager.py\n                byzerai_index_store.py\n                byzerai_docstore.py\n                simple_retrieval.py\n                byzerai_kvstore.py\n            agent/\n                conversable_agent.py\n                registry.py\n                groupchat.py\n                user_proxy_agent.py\n                __init__.py\n                agent.py\n                extensions/\n                    simple_retrieval_client.py\n                    planner.py\n                    python_codesandbox_agent.py\n                    preview_file_agent.py\n                    llama_index_retrieval_agent.py\n                    visualization_agent.py\n                    assistant_agent.py\n                    load_data_agent.py\n                    retrieval_agent.py\n                    output_agent.py\n                    __init__.py\n                    byzer_engine_agent.py\n                    subquestion_agent.py\n                    sql_reviewer_agent.py\n                    spark_sql_agent.py\n                    rhetorical_agent.py\n                    data_analysis.py\n                    data_analysis_pipeline_agent.py\n                    common_agent.py\n                    query_rewrite/\n                        time.py\n                        rhetorical.py\n                        __init__.py\n                        condition.py\n                        context.py\n                store/\n                    memory_store.py\n                    __init__.py\n                    stores.py\n        saas/\n            __init__.py\n            chatglm/\n                __init__.py\n            sparkdesk/\n                __init__.py\n            gemini/\n                __init__.py\n            azure_openai/\n                __init__.py\n            azure/\n                __init__.py\n            qianfan/\n                __init__.py\n            claude/\n                __init__.py\n            qianwen/\n                __init__.py\n            volcengine/\n                __init__.py\n            minimax/\n                __init__.py\n            baichuan/\n                __init__.py\n            qianwen_vl/\n                __init__.py\n            official_openai/\n                __init__.py\n            zhipu/\n                __init__.py\n            aws_bedrock/\n                __init__.py\n            openai/\n                __init__.py\n        falcon/\n            __init__.py\n        chatglm6b/\n            trainer_seq2seq.py\n            arguments.py\n            __init__.py\n            finetune.py\n            tunning/\n                __init__.py\n                infer.py\n                finetune.py\n                utils/\n                    config.py\n                    __init__.py\n                    other.py\n                    common.py\n                    seq2seq.py\n        starcode/\n            __init__.py\n        qwen_vl_chat/\n            __init__.py\n        zephyr/\n            __init__.py\n        dolly/\n            generate.py\n            __init__.py\n            consts.py\n            dolly_inference.py\n            trainer.py\n        llama/\n            __init__.py\n        whisper/\n            whisper_inference.py\n            __init__.py\n下面是一些文件路径以及每个文件对应的源码：\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/saas/official_openai/__init__.py\nimport time\nfrom typing import List, Tuple, Dict, Any, Union\nimport httpx\nfrom openai import OpenAI\nimport base64\nimport io\nimport json\nimport ray\nfrom byzerllm.utils.types import (\n    BlockVLLMStreamServer,\n    StreamOutputs,\n    SingleOutput,\n    SingleOutputMeta,\n    BlockBinaryStreamServer,\n)\nfrom byzerllm.utils.langutil import asyncfy_with_semaphore\nimport threading\nimport asyncio\nimport traceback\nimport uuid\nimport tempfile        \n\nclass CustomSaasAPI:\n\n    def __init__(self, infer_params: Dict[str, str]) -> None:\n\n        self.api_key = infer_params[\"saas.api_key\"]\n        self.model = infer_params.get(\"saas.model\", \"gpt-3.5-turbo-1106\")\n\n        other_params = {}\n\n        if \"saas.api_base\" in infer_params:\n            other_params[\"api_base\"] = infer_params[\"saas.api_base\"]\n\n        if \"saas.api_version\" in infer_params:\n            other_params[\"api_version\"] = infer_params[\"saas.api_version\"]\n\n        if \"saas.api_type\" in infer_params:\n            other_params[\"api_type\"] = infer_params[\"saas.api_type\"]\n\n        if \"saas.base_url\" in infer_params:\n            other_params[\"base_url\"] = infer_params[\"saas.base_url\"]\n\n        if \"saas.timeout\" in infer_params:\n            other_params[\"timeout\"] = float(infer_params[\"saas.timeout\"])\n\n        self.max_retries = int(infer_params.get(\"saas.max_retries\", 10))\n\n        self.meta = {\n            \"model_deploy_type\": \"saas\",\n            \"backend\": \"saas\",\n            \"support_stream\": True,\n            \"model_name\": self.model,\n        }\n\n        self.meta[\"embedding_mode\"] = \"embedding\" in self.model.lower()\n\n        self.proxies = infer_params.get(\"saas.proxies\", None)\n        self.local_address = infer_params.get(\"saas.local_address\", \"0.0.0.0\")\n\n        if not self.proxies:\n            self.client = OpenAI(**other_params, api_key=self.api_key)\n        else:\n            self.client = OpenAI(\n                **other_params,\n                api_key=self.api_key,\n                http_client=httpx.Client(\n                    proxies=self.proxies,\n                    transport=httpx.HTTPTransport(local_address=self.local_address),\n                ),\n            )\n\n        try:\n            ray.get_actor(\"BLOCK_VLLM_STREAM_SERVER\")\n        except ValueError:\n            try:\n                ray.remote(BlockVLLMStreamServer).options(\n                    name=\"BLOCK_VLLM_STREAM_SERVER\",\n                    lifetime=\"detached\",\n                    max_concurrency=1000,\n                ).remote()\n            except Exception as e:\n                pass\n        try:\n            ray.get_actor(\"BlockBinaryStreamServer\")\n        except ValueError:\n            try:\n                ray.remote(BlockBinaryStreamServer).options(\n                    name=\"BlockBinaryStreamServer\",\n                    lifetime=\"detached\",\n                    max_concurrency=1000,\n                ).remote()\n            except Exception as e:\n                pass\n\n    # saas/proprietary\n    def get_meta(self):\n        return [self.meta]\n\n    def process_input(self, ins: Union[str, List[Dict[str, Any]], Dict[str, Any]]):\n\n        if isinstance(ins, list) or isinstance(ins, dict):\n            return ins\n\n        content = []\n        try:\n            ins_json = json.loads(ins)\n        except:\n            return ins\n\n        ## speech\n        if isinstance(ins_json, dict):\n            return ins_json\n\n        content = []\n        #     [\n        #     {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n        #     {\n        #       \"type\": \"image_url\",\n        #       \"image_url\": {\n        #         \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n        #         \"detail\": \"high\"\n        #       },\n        #     },\n        #   ],\n        for item in ins_json:\n            # for format like this: {\"image\": \"xxxxxx\", \"text\": \"What’s in this image?\",\"detail\":\"high\"}\n            # or {\"image\": \"xxxxxx\"}, {\"text\": \"What’s in this image?\"}\n            if (\"image\" in item or \"image_url\" in item) and \"type\" not in item:\n                image_data = item.get(\"image\", item.get(\"image_url\", \"\"))\n                ## \"data:image/jpeg;base64,\"\n                if not image_data.startswith(\"data:\"):\n                    image_data = \"data:image/jpeg;base64,\" + image_data\n\n                ## get the other fields except image/text/image_url\n                other_fields = {\n                    k: v\n                    for k, v in item.items()\n                    if k not in [\"image\", \"text\", \"image_url\"]\n                }\n                content.append(\n                    {\n                        \"image_url\": {\"url\": image_data, **other_fields},\n                        \"type\": \"image_url\",\n                    }\n                )            \n\n            if \"text\" in item and \"type\" not in item:\n                text_data = item[\"text\"]\n                content.append({\"text\": text_data, \"type\": \"text\"})\n\n            ## for format like this: {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n            ## {\"type\": \"image_url\", \"image_url\": {\"url\":\"\",\"detail\":\"high\"}}\n            ## this is the standard format, just return it\n            if \"type\" in item and item[\"type\"] == \"text\":\n                content.append(item)\n\n            if \"type\" in item and item[\"type\"] == \"image_url\":\n                content.append(item)\n\n        if not content:\n            return ins\n\n        return content\n\n    async def async_embed_query(self, ins: str, **kwargs):\n        resp = await asyncfy_with_semaphore(\n            lambda: self.client.embeddings.create(input=[ins], model=self.model)\n        )()\n        embedding = resp.data[0].embedding\n        usage = resp.usage\n        return (\n            embedding,\n            {\n                \"metadata\": {\n                    \"input_tokens_count\": usage.prompt_tokens,\n                    \"generated_tokens_count\": 0,\n                }\n            },\n        )\n\n    def embed_query(self, ins: str, **kwargs):\n        resp = self.client.embeddings.create(input=[ins], model=self.model)\n        embedding = resp.data[0].embedding\n        usage = resp.usage\n        return (\n            embedding,\n            {\n                \"metadata\": {\n                    \"input_tokens_count\": usage.prompt_tokens,\n                    \"generated_tokens_count\": 0,\n                }\n            },\n        )\n\n    async def async_text_to_speech(\n        self, stream: bool, ins: str, voice: str, chunk_size: int = None, **kwargs\n    ):\n        if stream:\n            server = ray.get_actor(\"BlockBinaryStreamServer\")\n            request_id = [None]\n\n            def writer():\n                try:\n                    request_id[0] = str(uuid.uuid4())\n                    with self.client.with_streaming_response.audio.speech.create(\n                        model=self.model, voice=voice, input=ins, **kwargs\n                    ) as response:\n                        for chunk in response.iter_bytes(chunk_size):\n                            input_tokens_count = 0\n                            generated_tokens_count = 0\n                            ray.get(\n                                server.add_item.remote(\n                                    request_id[0],\n                                    StreamOutputs(\n                                        outputs=[\n                                            SingleOutput(\n                                                text=chunk,\n                                                metadata=SingleOutputMeta(\n                                                    input_tokens_count=input_tokens_count,\n                                                    generated_tokens_count=generated_tokens_count,\n                                                ),\n                                            )\n                                        ]\n                                    ),\n                                )\n                            )\n                except:\n                    traceback.print_exc()\n                ray.get(server.mark_done.remote(request_id[0]))\n\n            threading.Thread(target=writer, daemon=True).start()\n\n            time_count = 10 * 100\n            while request_id[0] is None and time_count > 0:\n                time.sleep(0.01)\n                time_count -= 1\n\n            if request_id[0] is None:\n                raise Exception(\"Failed to get request id\")\n\n            def write_running():\n                return ray.get(server.add_item.remote(request_id[0], \"RUNNING\"))\n\n            await asyncio.to_thread(write_running)\n            return [\n                (\n                    \"\",\n                    {\n                        \"metadata\": {\n                            \"request_id\": request_id[0],\n                            \"stream_server\": \"BlockBinaryStreamServer\",\n                        }\n                    },\n                )\n            ]\n\n        start_time = time.monotonic()\n        with io.BytesIO() as output:\n            async with asyncfy_with_semaphore(\n                lambda: self.client.with_streaming_response.audio.speech.create(\n                    model=self.model, voice=voice, input=ins, **kwargs\n                )\n            )() as response:\n                for chunk in response.iter_bytes():\n                    output.write(chunk)\n\n            base64_audio = base64.b64encode(output.getvalue()).decode()\n            time_cost = time.monotonic() - start_time\n            return [\n                (\n                    base64_audio,\n                    {\n                        \"metadata\": {\n                            \"request_id\": \"\",\n                            \"input_tokens_count\": 0,\n                            \"generated_tokens_count\": 0,\n                            \"time_cost\": time_cost,\n                            \"first_token_time\": 0,\n                            \"speed\": 0,\n                        }\n                    },\n                )\n            ]\n\n    async def async_speech_to_text(\n        self,\n        audio: str,\n        response_format: str = \"verbose_json\",\n        timestamp_granularities: List[str] = [\"word\",\"segment\"],\n    ):               \n        # Extract audio format and base64 data\n        data_prefix = \"data:audio/\"\n        base64_prefix = \";base64,\"\n        if not audio.startswith(data_prefix):\n            raise ValueError(\"Invalid audio data format\")\n        \n        format_end = audio.index(base64_prefix)\n        audio_format = audio[len(data_prefix):format_end]\n        base64_data = audio[format_end + len(base64_prefix):]\n        \n        # Decode the base64 audio data\n        audio_data = base64.b64decode(base64_data)\n\n        # Create a temporary file with the correct extension\n        with tempfile.NamedTemporaryFile(delete=False, suffix=f'.{audio_format}') as temp_audio_file:\n            temp_audio_file.write(audio_data)\n            temp_audio_file_path = temp_audio_file.name\n\n        try:\n            start_time = time.monotonic()\n                   \n            with open(temp_audio_file_path, \"rb\") as audio_file:                \n                transcription = self.client.audio.transcriptions.create(\n                    model=self.model or \"whisper-1\",\n                    file=audio_file,\n                    response_format=response_format,\n                    timestamp_granularities=timestamp_granularities,\n                )\n            time_cost = time.monotonic() - start_time\n\n            return [\n                (\n                    json.dumps(transcription.to_dict(), ensure_ascii=True),\n                    {\n                        \"metadata\": {\n                            \"request_id\": \"\",\n                            \"input_tokens_count\": 0,\n                            \"generated_tokens_count\": 0,\n                            \"time_cost\": time_cost,\n                            \"first_token_time\": 0,\n                            \"speed\": 0,\n                        }\n                    },\n                )\n            ]\n        finally:\n            # Clean up the temporary file\n            import os\n            os.unlink(temp_audio_file_path)\n\n    def image_to_text(self, ins: str, **kwargs):\n        pass\n\n    async def async_text_to_image(\n        self, stream: bool, input: str, size: str, quality: str, n: int, **kwargs\n    ):\n        if stream:\n            raise Exception(\"Stream not supported for text to image\")\n        start_time = time.monotonic()\n        response = await asyncfy_with_semaphore(\n            lambda: self.client.images.generate(\n                model=self.model,\n                prompt=input,\n                size=size,\n                quality=quality,\n                n=1,\n                response_format=\"b64_json\",\n                **kwargs,\n            )\n        )()\n        time_cost = time.monotonic() - start_time\n        base64_image = response.data[0].b64_json\n        return [\n            (\n                base64_image,\n                {\n                    \"metadata\": {\n                        \"request_id\": \"\",\n                        \"input_tokens_count\": 0,\n                        \"generated_tokens_count\": 0,\n                        \"time_cost\": time_cost,\n                        \"first_token_time\": 0,\n                        \"speed\": 0,\n                    }\n                },\n            )\n        ]\n\n    def text_to_text(self, ins: str, **kwargs):\n        pass\n\n    async def async_stream_chat(\n        self,\n        tokenizer,\n        ins: str,\n        his: List[Dict[str, Any]] = [],\n        max_length: int = 4096,\n        top_p: float = 0.7,\n        temperature: float = 0.9,\n        **kwargs,\n    ):\n\n        model = self.model\n\n        if \"model\" in kwargs:\n            model = kwargs[\"model\"]\n\n        messages = [\n            {\"role\": message[\"role\"], \"content\": self.process_input(message[\"content\"])}\n            for message in his\n        ] + [{\"role\": \"user\", \"content\": self.process_input(ins)}]\n\n        stream = kwargs.get(\"stream\", False)\n\n        ## content = [\n        ##    \"voice\": \"alloy\",\"input\": \"Hello, World!\",response_format: \"mp3\"]\n        last_message = messages[-1][\"content\"]\n\n        if isinstance(last_message, dict) and \"voice\" in last_message:\n            voice = last_message[\"voice\"]\n            response_format = last_message.get(\"response_format\", \"mp3\")\n            chunk_size = last_message.get(\"chunk_size\", None)\n            input = last_message[\"input\"]\n            return await self.async_text_to_speech(\n                stream=stream,\n                ins=input,\n                voice=voice,\n                chunk_size=chunk_size,\n                response_format=response_format,\n            )\n\n        if isinstance(last_message, dict) and \"input\" in last_message:\n            input = last_message[\"input\"]\n            size = last_message.get(\"size\", \"1024x1024\")\n            quality = last_message.get(\"quality\", \"standard\")\n            n = last_message.get(\"n\", 1)\n            return await self.async_text_to_image(\n                stream=stream, input=input, size=size, quality=quality, n=n\n            )\n\n        if isinstance(last_message, dict) and \"audio\" in last_message:            \n            audio_data = last_message.get(\"audio\", \"\")            \n            tpe = last_message.get(\"type\",\"wav\")\n            if not audio_data.startswith(\"data:\"):\n                audio_data = f\"data:audio/${tpe};base64,\" + audio_data                \n            return await self.async_speech_to_text(audio=audio_data)\n        \n        server = ray.get_actor(\"BLOCK_VLLM_STREAM_SERVER\")\n        request_id = [None]\n\n        def writer():\n            try:\n                r = \"\"\n                response = self.client.chat.completions.create(\n                    messages=messages,\n                    model=model,\n                    stream=True,\n                    max_tokens=max_length,\n                    temperature=temperature,\n                    top_p=top_p,\n                )\n                # input_tokens_count = 0\n                # generated_tokens_count = 0\n\n                request_id[0] = str(uuid.uuid4())\n\n                for chunk in response:\n                    content = chunk.choices[0].delta.content or \"\"\n                    r += content\n                    if hasattr(chunk, \"usage\") and chunk.usage:\n                        input_tokens_count = chunk.usage.prompt_tokens\n                        generated_tokens_count = chunk.usage.completion_tokens\n                    else:\n                        input_tokens_count = 0\n                        generated_tokens_count = 0\n                    ray.get(\n                        server.add_item.remote(\n                            request_id[0],\n                            StreamOutputs(\n                                outputs=[\n                                    SingleOutput(\n                                        text=r,\n                                        metadata=SingleOutputMeta(\n                                            input_tokens_count=input_tokens_count,\n                                            generated_tokens_count=generated_tokens_count,\n                                        ),\n                                    )\n                                ]\n                            ),\n                        )\n                    )\n            except:\n                traceback.print_exc()\n            ray.get(server.mark_done.remote(request_id[0]))\n\n        if stream:\n            threading.Thread(target=writer, daemon=True).start()\n\n            time_count = 60 * 100\n            while request_id[0] is None and time_count > 0:\n                time.sleep(0.01)\n                time_count -= 1\n\n            if request_id[0] is None:\n                raise Exception(\"Failed to get request id\")\n\n            def write_running():\n                return ray.get(server.add_item.remote(request_id[0], \"RUNNING\"))\n\n            await asyncio.to_thread(write_running)\n            return [\n                (\n                    \"\",\n                    {\n                        \"metadata\": {\n                            \"request_id\": request_id[0],\n                            \"stream_server\": \"BLOCK_VLLM_STREAM_SERVER\",\n                        }\n                    },\n                )\n            ]\n        else:\n            try:\n                start_time = time.monotonic()\n                response = await asyncfy_with_semaphore(\n                    lambda: self.client.chat.completions.create(\n                        messages=messages,\n                        model=model,\n                        max_tokens=max_length,\n                        temperature=temperature,\n                        top_p=top_p,\n                    )\n                )()\n\n                generated_text = response.choices[0].message.content\n                generated_tokens_count = response.usage.completion_tokens\n                input_tokens_count = response.usage.prompt_tokens\n                time_cost = time.monotonic() - start_time\n                return [\n                    (\n                        generated_text,\n                        {\n                            \"metadata\": {\n                                \"request_id\": response.id,\n                                \"input_tokens_count\": input_tokens_count,\n                                \"generated_tokens_count\": generated_tokens_count,\n                                \"time_cost\": time_cost,\n                                \"first_token_time\": 0,\n                                \"speed\": float(generated_tokens_count) / time_cost,\n                            }\n                        },\n                    )\n                ]\n            except Exception as e:\n                print(f\"Error: {e}\")\n                raise e\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/whisper/__init__.py\nimport whisper\nfrom typing import Dict, List, Tuple\nfrom byzerllm.utils.types import StopSequencesCriteria\n\nfrom typing import Dict, Any, List, Union\nfrom pyjava.storage import streaming_tar as STar\nimport json\nimport base64\nimport tempfile\nimport time\n\n\ndef get_meta(self):\n    return [\n        {\n            \"model_deploy_type\": \"proprietary\",\n            \"backend\": \"transformers\",\n            \"message_format\": True\n        }\n    ]\n\n\ndef process_input(ins: Union[str, List[Dict[str, Any]], Dict[str, Any]]):\n    # print(ins) \n    if isinstance(ins, list) or isinstance(ins, dict):\n        return ins\n\n    content = []\n    try:\n        ins_json = json.loads(ins)\n    except:\n        return ins\n\n    ## speech\n    if isinstance(ins_json, dict):\n        return ins_json\n\n    content = []\n    #     [\n    #     {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n    #     {\n    #       \"type\": \"image_url\",\n    #       \"image_url\": {\n    #         \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n    #         \"detail\": \"high\"\n    #       },\n    #     },\n    #   ],\n    for item in ins_json:\n        # for format like this: {\"image\": \"xxxxxx\", \"text\": \"What’s in this image?\",\"detail\":\"high\"}\n        # or {\"image\": \"xxxxxx\"}, {\"text\": \"What’s in this image?\"}\n        if (\"image\" in item or \"image_url\" in item) and \"type\" not in item:\n            image_data = item.get(\"image\", item.get(\"image_url\", \"\"))\n            ## \"data:image/jpeg;base64,\"\n            if not image_data.startswith(\"data:\"):\n                image_data = \"data:image/jpeg;base64,\" + image_data\n\n            ## get the other fields except image/text/image_url\n            other_fields = {\n                k: v for k, v in item.items() if k not in [\"image\", \"text\", \"image_url\"]\n            }\n            content.append(\n                {\n                    \"image_url\": {\"url\": image_data, **other_fields},\n                    \"type\": \"image_url\",\n                }\n            )\n\n        if \"text\" in item and \"type\" not in item:\n            text_data = item[\"text\"]\n            content.append({\"text\": text_data, \"type\": \"text\"})\n\n        ## for format like this: {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n        ## {\"type\": \"image_url\", \"image_url\": {\"url\":\"\",\"detail\":\"high\"}}\n        ## this is the standard format, just return it\n        if \"type\" in item and item[\"type\"] == \"text\":\n            content.append(item)\n\n        if \"type\" in item and item[\"type\"] == \"image_url\":\n            content.append(item)\n\n    if not content:\n        return ins\n\n    return content\n\n\ndef stream_chat(\n    self,\n    tokenizer,\n    ins: str,\n    his: List[Dict[str, str]] = [],\n    max_length: int = 4090,\n    top_p: float = 0.95,\n    temperature: float = 0.1,\n    **kwargs,\n):\n    messages = [\n        {\"role\": message[\"role\"], \"content\": process_input(message[\"content\"])}\n        for message in his\n    ] + [{\"role\": \"user\", \"content\": process_input(ins)}]\n\n    audio_input = messages[-1][\"content\"]\n\n    audio_data = audio_input.get(\"audio\", \"\")\n    audio = audio_input[\"audio\"]\n    response_format = audio_input.get(\"response_format\", \"json\")\n    timestamp_granularities = audio_input.get(\n        \"timestamp_granularities\", [\"word\", \"segment\"]\n    )\n    data_prefix = \"data:audio/\"\n    base64_prefix = \";base64,\"\n    if not audio.startswith(data_prefix):\n        raise ValueError(\"Invalid audio data format\")\n\n    format_end = audio.index(base64_prefix)\n    audio_format = audio[len(data_prefix) : format_end]\n    base64_data = audio[format_end + len(base64_prefix) :]\n\n    # Decode the base64 audio data\n    audio_data = base64.b64decode(base64_data)\n\n    # Create a temporary file with the correct extension\n    with tempfile.NamedTemporaryFile(\n        delete=False, suffix=f\".{audio_format}\"\n    ) as temp_audio_file:\n        temp_audio_file.write(audio_data)\n        temp_audio_file_path = temp_audio_file.name\n\n    try:\n        start_time = time.monotonic()\n        word_timestamps = \"word\" in timestamp_granularities\n        \n        result = self.transcribe(temp_audio_file_path, word_timestamps=word_timestamps)        \n            \n        time_cost = time.monotonic() - start_time\n        return [\n            (\n                json.dumps(result, ensure_ascii=False),\n                {\n                    \"metadata\": {\n                        \"request_id\": \"\",\n                        \"input_tokens_count\": 0,\n                        \"generated_tokens_count\": 0,\n                        \"time_cost\": time_cost,\n                        \"first_token_time\": 0,\n                        \"speed\": 0,\n                    }\n                },\n            )\n        ]\n    finally:\n        # Clean up the temporary file\n        import os\n\n        os.unlink(temp_audio_file_path)\n\n\ndef init_model(\n    model_dir, infer_params: Dict[str, str] = {}, sys_conf: Dict[str, str] = {}\n):\n    model = whisper.load_model(\"large\", download_root=model_dir)\n    import types\n\n    model.stream_chat = types.MethodType(stream_chat, model)\n    model.get_meta = types.MethodType(get_meta, model)\n    return (model, None)\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/notebooks/test_sensevoice.py\nimport sys\nimport os\n\n# Add the project root to the Python path\nsys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\")))\n\nfrom src.byzerllm.sensevoice import init_model, format_str_v3\nimport base64\nimport json\n\n\ndef main():\n    # Initialize the model\n    model_dir = \"/Users/allwefantasy/models/SenseVoiceSmall\"\n    model, _ = init_model(model_dir)\n\n    # Prepare test audio\n    with open(\n        \"/Users/allwefantasy/models/SenseVoiceSmall/example/zh.mp3\", \"rb\"\n    ) as audio_file:\n        audio_data = audio_file.read()\n        base64_audio = base64.b64encode(audio_data).decode(\"utf-8\")        \n        # Prepare input in the expected format\n        test_input = json.dumps(\n            {\n                            \"type\": \"audio\",\n                            \"audio\": f\"data:audio/wav;base64,{base64_audio}\",\n                        }\n        )\n\n        # Call stream_chat\n        result = model.stream_chat(tokenizer=None, ins = test_input)\n\n        # Process and print the result\n        if result and len(result) > 0:\n            output, metadata = result[0]\n            parsed_output = json.loads(output)\n            formatted_text = parsed_output.get(\"text\", \"\")\n            print(\"Transcription:\", formatted_text)\n            print(\"Metadata:\", metadata)\n        else:\n            print(\"No result returned from the model.\")\n\n\nif __name__ == \"__main__\":\n    main()\n\n\n##File: /Users/allwefantasy/projects/Qwen2-Audio/README_CN.md\n<p align=\"left\">\n        中文</a>&nbsp ｜ &nbsp<a href=\"README.md\">English</a>\n</p>\n<br><br>\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/assets/blog/qwenaudio/qwen2audio_logo.png\" width=\"400\"/>\n<p>\n\n<p align=\"center\">\nQwen2-Audio-7B <a href=\"https://modelscope.cn/models/qwen/Qwen2-Audio-7B\">🤖 </a> | <a href=\"https://huggingface.co/Qwen/Qwen2-Audio-7B\">🤗</a>&nbsp ｜ Qwen-Audio-7B-Instruct <a href=\"https://modelscope.cn/models/qwen/Qwen2-Audio-7B-Instruct\">🤖 </a>| <a href=\"https://huggingface.co/Qwen/Qwen2-Audio-7B-Instruct\">🤗</a>&nbsp ｜ Demo<a href=\"https://modelscope.cn/studios/qwen/Qwen2-Audio-Instruct-Demo\"> 🤖</a> | <a href=\"https://huggingface.co/spaces/Qwen/Qwen2-Audio-Instruct-Demo\">🤗</a>&nbsp\n<br>\n📑 <a href=\"https://arxiv.org/abs/2407.10759\">Paper</a> &nbsp&nbsp | &nbsp&nbsp 📑 <a href=\"https://qwenlm.github.io/blog/qwen2-audio\">Blog</a> &nbsp&nbsp | &nbsp&nbsp 💬 <a href=\"https://github.com/QwenLM/Qwen/blob/main/assets/wechat.png\">WeChat (微信)</a>&nbsp&nbsp | &nbsp&nbsp <a href=\"https://discord.gg/CV4E9rpNSD\">Discord</a>&nbsp&nbsp\n</p>\n\n\n\n我们介绍Qwen-Audio的最新进展：Qwen2-Audio。作为一个大规模音频语言模型，Qwen2-Audio能够接受各种音频信号输入，并根据语音指令执行音频分析或直接响应文本。我们介绍两种不同的音频交互模式：语音聊天 voice chat 和音频分析 audio analysis。\n\n* 语音聊天：用户可以自由地与 Qwen2-Audio 进行语音互动，而无需文本输入；\n* 音频分析：用户可以在互动过程中提供音频和文本指令对音频进行分析；\n\n**我们已经开源了 Qwen2-Audio 系列的两个模型：Qwen2-Audio-7B和Qwen2-Audio-7B-Instruct。**\n\n## 模型结构与训练范式\n\nQwen2-Audio 三阶段训练过程概述。\n\n<p align=\"left\">\n    <img src=\"assets/framework.png\" width=\"80%\"/>\n<p>\n\n\n\n## 新闻\n* 2024.8.9 🎉 我们在 ModelScope 和 Hugging Face 开源了`Qwen2-Audio-7B`和`Qwen2-Audio-7B-Instruct`的 checkpoint.\n* 2024.7.15 🎉 我们发布了 Qwen2-Audio 的[论文](https://arxiv.org/abs/2407.10759), 介绍了相关的模型结构，训练方法和模型表现。\n* 2023.11.30 🔥 我们发布了**Qwen-Audio**系列\n<br>\n\n## 评测\n我们在标准的13个学术数据集上评测了模型的能力如下：\n<table><thead><tr><th>Task</th><th>Description</th><th>Dataset</th><th>Split</th><th>Metric</th></tr></thead><tbody><tr><td rowspan=\"4\">ASR</td><td rowspan=\"4\">Automatic Speech Recognition</td><td>Fleurs</td><td>dev | test</td><td rowspan=\"4\">WER</td></tr><tr><td>Aishell2</td><td>test</td></tr><tr><td>Librispeech</td><td>dev | test</td></tr><tr><td>Common Voice</td><td>dev | test</td></tr><tr><td>S2TT</td><td>Speech-to-Text Translation</td><td>CoVoST2</td><td>test</td><td>BLEU </td></tr><tr><td>SER</td><td>Speech Emotion Recognition</td><td>Meld</td><td>test</td><td>ACC</td></tr><tr><td>VSC</td><td>Vocal Sound Classification</td><td>VocalSound</td><td>test</td><td>ACC</td></tr><tr><td rowspan=\"4\"><a href=\"https://github.com/OFA-Sys/AIR-Bench\">AIR-Bench</a><br></td><td>Chat-Benchmark-Speech</td><td>Fisher<br>SpokenWOZ<br>IEMOCAP<br>Common voice</td><td>dev | test</td><td>GPT-4 Eval</td></tr><tr><td>Chat-Benchmark-Sound</td><td>Clotho</td><td>dev | test</td><td>GPT-4 Eval</td></tr>\n<tr><td>Chat-Benchmark-Music</td><td>MusicCaps</td><td>dev | test</td><td>GPT-4 Eval</td></tr><tr><td>Chat-Benchmark-Mixed-Audio</td><td>Common voice<br>AudioCaps<br>MusicCaps</td><td>dev | test</td><td>GPT-4 Eval</td></tr></tbody></table>\n\n\n以下是整体表现：\n<p align=\"left\">\n    <img src=\"assets/radar_compare_qwen_audio.png\" width=\"70%\"/>\n<p>\n\n评测分数详情如下：\n<br>\n<b>（注意：我们所展示的评测结果是在原始训练框架的初始模型上的，然而在框架转换 Huggingface 后指标出现了部分波动，在这里我们展示我们的全部测评结果：首先是论文中的初始模型结果）</b>\n\n<table><thead><tr><th rowspan=\"2\">Task</th><th rowspan=\"2\">Dataset</th><th rowspan=\"2\">Model</th><th colspan=\"2\">Performance</th></tr><tr><th>Metrics</th><th>Results</th></tr></thead><tbody><tr><td rowspan=\"15\">ASR</td><td rowspan=\"7\"><b>Librispeech</b><br>dev-clean | dev-other | <br>test-clean | test-other</td><td>SpeechT5</td><td rowspan=\"7\">WER </td><td>2.1 | 5.5 | 2.4 | 5.8</td></tr><tr><td>SpeechNet</td><td>- | - | 30.7 | -</td></tr><tr><td>SLM-FT</td><td>- | - | 2.6 | 5.0</td></tr><tr><td>SALMONN</td><td>- | - | 2.1 | 4.9</td></tr><tr><td>SpeechVerse</td><td>- | - | 2.1 | 4.4</td></tr><tr><td>Qwen-Audio</td><td>1.8 | 4.0 | 2.0 | 4.2</td></tr><tr><td>Qwen2-Audio</td><td><b>1.3 | 3.4 | 1.6 | 3.6</b></td></tr><tr><td rowspan=\"2\"><b>Common Voice 15</b> <br>en | zh | yue | fr</td><td>Whisper-large-v3</td><td rowspan=\"2\">WER </td><td>9.3 | 12.8 | 10.9 | 10.8</td></tr><tr><td>Qwen2-Audio</td><td><b>8.6 | 6.9 | 5.9 | 9.6</b></td></tr>\n<tr><td rowspan=\"2\"><b>Fleurs</b> <br>zh</td><td>Whisper-large-v3</td><td rowspan=\"2\">WER </td><td>7.7</td></tr><tr><td>Qwen2-Audio</td><td><b>7.5</b></td></tr><tr><td rowspan=\"4\"><b>Aishell2</b> <br>Mic | iOS | Android</td><td>MMSpeech-base</td><td rowspan=\"4\">WER </td><td>4.5 | 3.9 | 4.0</td></tr><tr><td>Paraformer-large</td><td>- | <b>2.9</b> | -</td></tr><tr><td>Qwen-Audio</td><td>3.3 | 3.1 | 3.3</td></tr><tr><td>Qwen2-Audio</td><td><b>3.0</b> | 3.0 | <b>2.9</b></td></tr><tr><td rowspan=\"8\">S2TT</td><td rowspan=\"5\"><b>CoVoST2</b> <br>en-de | de-en | <br>en-zh | zh-en</td><td>SALMONN</td><td rowspan=\"5\">BLEU </td><td>18.6 | - | 33.1 | -</td></tr><tr><td>SpeechLLaMA</td><td>- | 27.1 | - | 12.3</td></tr><tr><td>BLSP</td><td>14.1 | - | - | -</td></tr><tr><td>Qwen-Audio</td><td>25.1 | 33.9 | 41.5 | 15.7</td></tr><tr><td>Qwen2-Audio</td><td><b>29.9 | 35.2 | 45.2 | 24.4</b></td></tr>\n<tr><td rowspan=\"3\"><b>CoVoST2</b> <br>es-en | fr-en | it-en |</td><td>SpeechLLaMA</td><td rowspan=\"3\">BLEU </td><td>27.9 | 25.2 | 25.9</td></tr><tr><td>Qwen-Audio</td><td>39.7 | <b>38.5</b> | 36.0</td></tr><tr><td>Qwen2-Audio</td><td><b>40.0 | 38.5 | 36.3</b></td></tr><tr><td rowspan=\"3\">SER</td><td rowspan=\"3\"><b>Meld</b></td><td>WavLM-large</td><td rowspan=\"3\">ACC </td><td>0.542</td></tr><tr><td>Qwen-Audio</td><td><b>0.557</b></td></tr><tr><td>Qwen2-Audio</td><td>0.553</td></tr><tr><td rowspan=\"4\">VSC</td><td rowspan=\"4\"><b>VocalSound</b></td><td>CLAP</td><td rowspan=\"4\">ACC </td><td>0.4945</td></tr><tr><td>Pengi</td><td>0.6035</td></tr><tr><td>Qwen-Audio</td><td>0.9289</td></tr><tr><td>Qwen2-Audio</td><td><b>0.9392</b></td></tr>\n<tr><td>AIR-Bench <br></td><td><b>Chat Benchmark</b><br>Speech | Sound |<br> Music | Mixed-Audio</td><td>SALMONN<br>BLSP<br>Pandagpt<br>Macaw-LLM<br>SpeechGPT<br>Next-gpt<br>Qwen-Audio<br>Gemini-1.5-pro<br>Qwen2-Audio</td><td>GPT-4 </td><td>6.16 | 6.28 | 5.95 | 6.08<br>6.17 | 5.55 | 5.08 | 5.33<br>3.58 | 5.46 | 5.06 | 4.25<br>0.97 | 1.01 | 0.91 | 1.01<br>1.57 | 0.95 | 0.95 | 4.13<br>3.86 | 4.76 | 4.18 | 4.13<br>6.47 | 6.95 | 5.52 | 6.08<br>6.97 | 5.49 | 5.06 | 5.27<br><b>7.18 | 6.99 | 6.79 | 6.77</b></td></tr></tbody></table>\n\n<b>（其次是转换 huggingface 后的）</b>\n\n<table><thead><tr><th rowspan=\"2\">Task</th><th rowspan=\"2\">Dataset</th><th rowspan=\"2\">Model</th><th colspan=\"2\">Performance</th></tr><tr><th>Metrics</th><th>Results</th></tr></thead><tbody><tr><td rowspan=\"15\">ASR</td><td rowspan=\"7\"><b>Librispeech</b><br>dev-clean | dev-other | <br>test-clean | test-other</td><td>SpeechT5</td><td rowspan=\"7\">WER </td><td>2.1 | 5.5 | 2.4 | 5.8</td></tr><tr><td>SpeechNet</td><td>- | - | 30.7 | -</td></tr><tr><td>SLM-FT</td><td>- | - | 2.6 | 5.0</td></tr><tr><td>SALMONN</td><td>- | - | 2.1 | 4.9</td></tr><tr><td>SpeechVerse</td><td>- | - | 2.1 | 4.4</td></tr><tr><td>Qwen-Audio</td><td>1.8 | 4.0 | 2.0 | 4.2</td></tr><tr><td>Qwen2-Audio</td><td><b>1.7 | 3.6 | 1.7 | 4.0</b></td></tr><tr><td rowspan=\"2\"><b>Common Voice 15</b> <br>en | zh | yue | fr</td><td>Whisper-large-v3</td><td rowspan=\"2\">WER </td><td>9.3 | 12.8 | 10.9 | 10.8</td></tr><tr><td>Qwen2-Audio</td><td><b>8.7 | 6.5 | 5.9 | 9.6</b></td></tr>\n<tr><td rowspan=\"2\"><b>Fleurs</b> <br>zh</td><td>Whisper-large-v3</td><td rowspan=\"2\">WER </td><td>7.7</td></tr><tr><td>Qwen2-Audio</td><td><b>7.0</b></td></tr><tr><td rowspan=\"4\"><b>Aishell2</b> <br>Mic | iOS | Android</td><td>MMSpeech-base</td><td rowspan=\"4\">WER </td><td>4.5 | 3.9 | 4.0</td></tr><tr><td>Paraformer-large</td><td>- | <b>2.9</b> | -</td></tr><tr><td>Qwen-Audio</td><td>3.3 | 3.1 | 3.3</td></tr><tr><td>Qwen2-Audio</td><td><b>3.2</b> | 3.1 | <b>2.9</b></td></tr><tr><td rowspan=\"8\">S2TT</td><td rowspan=\"5\"><b>CoVoST2</b> <br>en-de | de-en | <br>en-zh | zh-en</td><td>SALMONN</td><td rowspan=\"5\">BLEU </td><td>18.6 | - | 33.1 | -</td></tr><tr><td>SpeechLLaMA</td><td>- | 27.1 | - | 12.3</td></tr><tr><td>BLSP</td><td>14.1 | - | - | -</td></tr><tr><td>Qwen-Audio</td><td>25.1 | <b>33.9</b> | 41.5 | 15.7</td></tr><tr><td>Qwen2-Audio</td><td><b>29.6</b> | 33.6 | <b>45.6</b> | <b>24.0</b></td></tr>\n<tr><td rowspan=\"3\"><b>CoVoST2</b> <br>es-en | fr-en | it-en |</td><td>SpeechLLaMA</td><td rowspan=\"3\">BLEU </td><td>27.9 | 25.2 | 25.9</td></tr><tr><td>Qwen-Audio</td><td><b>39.7 | 38.5 | 36.0</b></td></tr><tr><td>Qwen2-Audio</td><td>38.7 | 37.2 | 35.2</td></tr><tr><td rowspan=\"3\">SER</td><td rowspan=\"3\"><b>Meld</b></td><td>WavLM-large</td><td rowspan=\"3\">ACC </td><td>0.542</td></tr><tr><td>Qwen-Audio</td><td><b>0.557</b></td></tr><tr><td>Qwen2-Audio</td><td>0.535</td></tr><tr><td rowspan=\"4\">VSC</td><td rowspan=\"4\"><b>VocalSound</b></td><td>CLAP</td><td rowspan=\"4\">ACC </td><td>0.4945</td></tr><tr><td>Pengi</td><td>0.6035</td></tr><tr><td>Qwen-Audio</td><td>0.9289</td></tr><tr><td>Qwen2-Audio</td><td><b>0.9395</b></td></tr>\n<tr><td>AIR-Bench <br></td><td><b>Chat Benchmark</b><br>Speech | Sound |<br> Music | Mixed-Audio</td><td>SALMONN<br>BLSP<br>Pandagpt<br>Macaw-LLM<br>SpeechGPT<br>Next-gpt<br>Qwen-Audio<br>Gemini-1.5-pro<br>Qwen2-Audio</td><td>GPT-4 </td><td>6.16 | 6.28 | 5.95 | 6.08<br>6.17 | 5.55 | 5.08 | 5.33<br>3.58 | 5.46 | 5.06 | 4.25<br>0.97 | 1.01 | 0.91 | 1.01<br>1.57 | 0.95 | 0.95 | 4.13<br>3.86 | 4.76 | 4.18 | 4.13<br>6.47 | <b>6.95</b> | 5.52 | 6.08<br>6.97 | 5.49 | 5.06 | 5.27<br><b>7.24</b> | 6.83 | <b>6.73</b> | <b>6.42</b></td></tr></tbody></table>\n\n\n我们提供了以上**所有**评测脚本以供复现我们的实验结果。请阅读 [eval_audio/EVALUATION.md](eval_audio/EVALUATION.md) 了解更多信息。\n\n## 部署要求\nThe code of Qwen2-Audio has been in the latest Hugging face transformers and we advise you to build from source with command `pip install git+https://github.com/huggingface/transformers`, or you might encounter the following error:\nQwen2-Audio的代码已经包含在最新的 Hugging Face Transformers 的主分支中，我们建议您使用命令`pip install git+https://github.com/huggingface/transformers`\n```\nKeyError: 'qwen2-audio'\n```\n## 快速使用\n我们提供简单的示例来说明如何利用 🤗 Transformers 快速使用 Qwen2-Audio-7B 和 Qwen2-Audio-7B-Instruct。\n在开始前，请确保你已经配置好环境并安装好相关的代码包。最重要的是，确保你满足上述要求，然后安装相关的依赖库。\n接下来你可以开始使用 Transformers 或者 ModelScope 来使用我们的模型。目前Qwen2-Audio-7B 及 Qwen2-Audio-7B-Instruct 模型处理30秒以内的音频表现更佳。\n#### 🤗 Hugging Face Transformers\n如希望使用 Qwen2-Audio-7B-Instruct 进行推理，我们分别演示语音聊天和音频分析的交互方式，所需要写的只是如下所示的数行代码。\n##### 语音聊天推理\n在语音聊天模式下，用户可以自由地与 Qwen2-Audio 进行语音交互，无需文字输入：\n```python\nfrom io import BytesIO\nfrom urllib.request import urlopen\nimport librosa\nfrom transformers import Qwen2AudioForConditionalGeneration, AutoProcessor\n\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\")\nmodel = Qwen2AudioForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\", device_map=\"auto\")\n\nconversation = [\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/guess_age_gender.wav\"},\n    ]},\n    {\"role\": \"assistant\", \"content\": \"Yes, the speaker is female and in her twenties.\"},\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/translate_to_chinese.wav\"},\n    ]},\n]\ntext = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\naudios = []\nfor message in conversation:\n    if isinstance(message[\"content\"], list):\n        for ele in message[\"content\"]:\n            if ele[\"type\"] == \"audio\":\n                audios.append(librosa.load(\n                    BytesIO(urlopen(ele['audio_url']).read()), \n                    sr=processor.feature_extractor.sampling_rate)[0]\n                )\n\ninputs = processor(text=text, audios=audios, return_tensors=\"pt\", padding=True)\ninputs.input_ids = inputs.input_ids.to(\"cuda\")\n\ngenerate_ids = model.generate(**inputs, max_length=256)\ngenerate_ids = generate_ids[:, inputs.input_ids.size(1):]\n\nresponse = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n```\n##### 音频分析推理\n在音频分析中，用户可以提供音频和文字问题来实现对音频的分析：\n```python\nfrom io import BytesIO\nfrom urllib.request import urlopen\nimport librosa\nfrom transformers import Qwen2AudioForConditionalGeneration, AutoProcessor\n\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\")\nmodel = Qwen2AudioForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\", device_map=\"auto\")\n\nconversation = [\n    {'role': 'system', 'content': 'You are a helpful assistant.'}, \n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/glass-breaking-151256.mp3\"},\n        {\"type\": \"text\", \"text\": \"What's that sound?\"},\n    ]},\n    {\"role\": \"assistant\", \"content\": \"It is the sound of glass shattering.\"},\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"text\", \"text\": \"What can you do when you hear that?\"},\n    ]},\n    {\"role\": \"assistant\", \"content\": \"Stay alert and cautious, and check if anyone is hurt or if there is any damage to property.\"},\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/1272-128104-0000.flac\"},\n        {\"type\": \"text\", \"text\": \"What does the person say?\"},\n    ]},\n]\ntext = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\naudios = []\nfor message in conversation:\n    if isinstance(message[\"content\"], list):\n        for ele in message[\"content\"]:\n            if ele[\"type\"] == \"audio\":\n                audios.append(\n                    librosa.load(\n                        BytesIO(urlopen(ele['audio_url']).read()), \n                        sr=processor.feature_extractor.sampling_rate)[0]\n                )\n\ninputs = processor(text=text, audios=audios, return_tensors=\"pt\", padding=True)\ninputs.input_ids = inputs.input_ids.to(\"cuda\")\n\ngenerate_ids = model.generate(**inputs, max_length=256)\ngenerate_ids = generate_ids[:, inputs.input_ids.size(1):]\n\nresponse = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n```\n##### 批量推理\n我们也支持批量推理：\n```python\nfrom io import BytesIO\nfrom urllib.request import urlopen\nimport librosa\nfrom transformers import Qwen2AudioForConditionalGeneration, AutoProcessor\n\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\")\nmodel = Qwen2AudioForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\", device_map=\"auto\")\n\nconversation1 = [\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/glass-breaking-151256.mp3\"},\n        {\"type\": \"text\", \"text\": \"What's that sound?\"},\n    ]},\n    {\"role\": \"assistant\", \"content\": \"It is the sound of glass shattering.\"},\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/f2641_0_throatclearing.wav\"},\n        {\"type\": \"text\", \"text\": \"What can you hear?\"},\n    ]}\n]\n\nconversation2 = [\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/1272-128104-0000.flac\"},\n        {\"type\": \"text\", \"text\": \"What does the person say?\"},\n    ]},\n]\n\nconversations = [conversation1, conversation2]\n\ntext = [processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False) for conversation in conversations]\n\naudios = []\nfor conversation in conversations:\n    for message in conversation:\n        if isinstance(message[\"content\"], list):\n            for ele in message[\"content\"]:\n                if ele[\"type\"] == \"audio\":\n                    audios.append(\n                        librosa.load(\n                            BytesIO(urlopen(ele['audio_url']).read()), \n                            sr=processor.feature_extractor.sampling_rate)[0]\n                    )\n\ninputs = processor(text=text, audios=audios, return_tensors=\"pt\", padding=True)\ninputs['input_ids'] = inputs['input_ids'].to(\"cuda\")\ninputs.input_ids = inputs.input_ids.to(\"cuda\")\n\ngenerate_ids = model.generate(**inputs, max_length=256)\ngenerate_ids = generate_ids[:, inputs.input_ids.size(1):]\n\nresponse = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n```\n运行Qwen2-Audio-7B同样非常简单。\n```python\nfrom io import BytesIO\nfrom urllib.request import urlopen\nimport librosa\nfrom transformers import AutoProcessor, Qwen2AudioForConditionalGeneration\n\nmodel = Qwen2AudioForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-Audio-7B\" ,trust_remote_code=True)\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-Audio-7B\" ,trust_remote_code=True)\n\nprompt = \"<|audio_bos|><|AUDIO|><|audio_eos|>Generate the caption in English:\"\nurl = \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Audio/glass-breaking-151256.mp3\"\naudio, sr = librosa.load(BytesIO(urlopen(url).read()), sr=processor.feature_extractor.sampling_rate)\ninputs = processor(text=prompt, audios=audio, return_tensors=\"pt\")\n\ngenerated_ids = model.generate(**inputs, max_length=256)\ngenerated_ids = generated_ids[:, inputs.input_ids.size(1):]\nresponse = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n```\n#### 🤖 ModelScope\n我们强烈建议用户，特别是中国大陆地区的用户，使用 ModelScope。`snapshot_download` 可以帮助您解决下载检查点时遇到的问题。\n<br>\n## Demo\n### Web UI\n我们提供了 Web UI 的 demo 供用户使用。在开始前，确保已经安装如下代码库：\n```\npip install -r requirements_web_demo.txt\n```\n随后运行如下命令，并点击生成链接：\n```\npython demo/web_demo_audio.py\n```\n<br>\n\n## 样例展示\n更多样例将更新于[通义千问博客](https://qwenlm.github.io/blog/qwen2-audio)中的 Qwen2-Audio 博客。\n\n## 团队招聘\n\n我们是通义千问语音多模态团队，致力于以通义千问为核心，拓展音频多模态理解和生成能力，实现自由灵活的音频交互。目前团队蓬勃发展中，如有意向实习或全职加入我们，请发送简历至 `qwen_audio@list.alibaba-inc.com`.\n<br>\n\n## 使用协议\n\n请查看每个模型在其 Hugging Face 仓库中的许可证。您无需提交商业使用申请。\n<br>\n\n\n\n## 引用\n\n如果你觉得我们的论文和代码对你的研究有帮助，请考虑 :star: 和引用 :pencil: :)\n\n```BibTeX\n@article{Qwen-Audio,\n  title={Qwen-Audio: Advancing Universal Audio Understanding via Unified Large-Scale Audio-Language Models},\n  author={Chu, Yunfei and Xu, Jin and Zhou, Xiaohuan and Yang, Qian and Zhang, Shiliang and Yan, Zhijie  and Zhou, Chang and Zhou, Jingren},\n  journal={arXiv preprint arXiv:2311.07919},\n  year={2023}\n}\n```\n\n```BibTeX\n@article{Qwen2-Audio,\n  title={Qwen2-Audio Technical Report},\n  author={Chu, Yunfei and Xu, Jin and Yang, Qian and Wei, Haojie and Wei, Xipin and Guo,  Zhifang and Leng, Yichong and Lv, Yuanjun and He, Jinzheng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},\n  journal={arXiv preprint arXiv:2407.10759},\n  year={2024}\n}\n```\n## 联系我们\n\n如果你想给我们的研发团队和产品团队留言，请通过邮件（`qianwen_opensource@alibabacloud.com`）联系我们。\n<br>\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/qwen_voice/__init__.py\nimport time\nfrom typing import List, Tuple, Dict, Any, Union\nimport base64\nimport json\nimport librosa\nimport io\nfrom transformers import Qwen2AudioForConditionalGeneration, AutoProcessor\n\ndef get_meta(self):\n    return [\n        {\n            \"model_deploy_type\": \"proprietary\",\n            \"backend\": \"transformers\",\n            \"message_format\": True\n        }\n    ]\n\ndef process_input(ins: Union[str, List[Dict[str, Any]], Dict[str, Any]]):\n    if isinstance(ins, list) or isinstance(ins, dict):\n        return ins\n\n    content = []\n    try:\n        ins_json = json.loads(ins)\n    except:\n        return ins\n\n    if isinstance(ins_json, dict):\n        return ins_json\n\n    for item in ins_json:\n        if (\"audio\" in item or \"audio_url\" in item) and \"type\" not in item:\n            audio_data = item.get(\"audio\", item.get(\"audio_url\", \"\"))\n            if not audio_data.startswith(\"data:\"):\n                audio_data = \"data:audio/wav;base64,\" + audio_data\n\n            other_fields = {k: v for k, v in item.items() if k not in [\"audio\", \"text\", \"audio_url\"]}\n            content.append(\n                {\n                    \"audio_url\": {\"url\": audio_data, **other_fields},\n                    \"type\": \"audio\",\n                }\n            )\n\n        if \"text\" in item and \"type\" not in item:\n            text_data = item[\"text\"]\n            content.append({\"text\": text_data, \"type\": \"text\"})\n\n        if \"type\" in item and item[\"type\"] in [\"text\", \"audio\"]:\n            content.append(item)\n\n    if not content:\n        return ins\n\n    return content\n\ndef stream_chat(\n    self,\n    tokenizer,\n    ins: str,\n    his: List[Dict[str, str]] = [],\n    max_length: int = 4096,\n    top_p: float = 0.95,\n    temperature: float = 0.1,\n    **kwargs,\n):\n    messages = [\n        {\"role\": message[\"role\"], \"content\": process_input(message[\"content\"])}\n        for message in his\n    ] + [{\"role\": \"user\", \"content\": process_input(ins)}]\n\n    text = self.processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n    audios = []\n    for message in messages:\n        if isinstance(message[\"content\"], list):\n            for ele in message[\"content\"]:\n                if ele[\"type\"] == \"audio\":\n                    audio_data = ele[\"audio_url\"].split(\",\")[1]\n                    audio_bytes = base64.b64decode(audio_data)\n                    audio, _ = librosa.load(io.BytesIO(audio_bytes), sr=self.processor.feature_extractor.sampling_rate)\n                    audios.append(audio)\n\n    inputs = self.processor(text=text, audios=audios, return_tensors=\"pt\", padding=True)\n    inputs.input_ids = inputs.input_ids.to(\"cuda\")\n\n    start_time = time.monotonic()\n    generate_ids = self.generate(**inputs, max_length=max_length)\n    generate_ids = generate_ids[:, inputs.input_ids.size(1):]\n\n    response = self.processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n    time_cost = time.monotonic() - start_time\n\n    return [\n        (\n            response,\n            {\n                \"metadata\": {\n                    \"request_id\": \"\",\n                    \"input_tokens_count\": inputs.input_ids.size(1),\n                    \"generated_tokens_count\": generate_ids.size(1),\n                    \"time_cost\": time_cost,\n                    \"first_token_time\": 0,\n                    \"speed\": float(generate_ids.size(1)) / time_cost if time_cost > 0 else 0,\n                }\n            },\n        )\n    ]\n\ndef init_model(model_dir, infer_params: Dict[str, str] = {}, sys_conf: Dict[str, str] = {}):\n    processor = AutoProcessor.from_pretrained(model_dir)\n    model = Qwen2AudioForConditionalGeneration.from_pretrained(model_dir, device_map=\"auto\")\n    model.processor = processor\n    import types\n    model.stream_chat = types.MethodType(stream_chat, model)\n    model.get_meta = types.MethodType(get_meta, model)    \n    return (model, None)\n\n     \n\n下面是用户的需求：\n\n@/Users/allwefantasy/projects/byzer-llm/src/byzerllm/qwen_voice/__init__.py 需要避免 .to(\"cuda\") 这样的动作抛出错误\n严格遵循 SEARCH/REPLACE 格式,如果是python代码,注意空格和缩进\n"}, {"role": "assistant", "content": "```python\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/qwen_voice/__init__.py\n<<<<<<< SEARCH\ninputs.input_ids = inputs.input_ids.to(\"cuda\")\n=======\ntry:\n    inputs.input_ids = inputs.input_ids.to(\"cuda\")\nexcept Exception as e:\n    print(f\"Warning: unable to move input_ids to CUDA - {e}\")\n>>>>>>> REPLACE\n```"}], "model": "deepseek_chat", "yaml_file": "actions/039_chat_action.yml"}
{"instruction": "@/Users/allwefantasy/projects/byzer-llm/src/byzerllm/qwen_voice/__init__.py 需要避免 .to(\"cuda\") 这样的动作抛出错误\n严格遵循 SEARCH/REPLACE 格式,如果是python代码,注意空格和缩进", "conversations": [{"role": "user", "content": "\n如果你需要生成代码，对于每个需要更改的文件,你需要按 *SEARCH/REPLACE block* 的格式进行生成。\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The opening fence and code language, eg: ```python\n2. The file path alone on a line, starting with \"##File:\" and verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n*SEARCH/REPLACE* blocks will replace *all* matching occurrences.\nInclude enough lines to make the SEARCH blocks unique.\n\nInclude *ALL* the code being searched and replaced!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n下面我们来看一个例子：\n\n当前项目目录结构：\n1. 项目根目录： /tmp/projects/mathweb\n2. 项目子目录/文件列表(类似tree 命令输出)\nflask/\n    app.py\n    templates/\n        index.html\n    static/\n        style.css\n\n用户需求： Change get_factorial() to use math.factorial\n\n回答： To make this change we need to modify `/tmp/projects/mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n用户需求： Refactor hello() into its own file.\n\n回答：To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/hello.py\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/main.py\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n现在让我们开始一个新的任务:\n\n当前项目目录结构：\n1. 项目根目录： /Users/allwefantasy/projects/byzer-llm\n2. 项目子目录/文件列表(类似tree 命令输出)：\nsetup.py\ntests/\n    test_emb_rerank.py\n    test_deepseek_code.py\n    test_prompt.py\n    test_model_meta.py\n    __init__.py\n    test_transformers_deploy.py\n    test_data_analysis.py\n    test_yi.py\n    test_deploy.py\n    functions_suite.py\n    test_template.py\n    auto/\n        test_backend_llama_cpp.py\n        __init__.py\nsetup-machine/\n    convert_safetensor.py\nnotebooks/\n    test.py\n    test_sensevoice.py\nsrc/\n    byzerllm/\n        lang.py\n        store.py\n        version.py\n        log.py\n        __init__.py\n        types.py\n        easy_byzerllm_command.py\n        byzerllm_command.py\n        command_args.py\n        bark/\n            generation.py\n            __init__.py\n            model.py\n            api.py\n            model_fine.py\n            bark_voice.py\n        moss/\n            finetune_moss.py\n            __init__.py\n            moss_inference.py\n            models/\n                configuration_moss.py\n                quantization.py\n                custom_autotune.py\n                __init__.py\n                modeling_moss.py\n                tokenization_moss.py\n        llama2/\n            __init__.py\n        records/\n            __init__.py\n        visualglm/\n            __init__.py\n        auto/\n            __init__.py\n            backend_llama_cpp.py\n            backend_vllm.py\n            backend_ds.py\n        alpha_moss/\n            __init__.py\n        processor/\n            __init__.py\n        sensevoice/\n            __init__.py\n        qwen/\n            __init__.py\n        utils/\n            json_repaire.py\n            object_store_ref_util.py\n            __init__.py\n            types.py\n            tokenizer.py\n            openai_utils.py\n            ray_utils.py\n            text_generator.py\n            emb.py\n            connect_ray.py\n            testing.py\n            langutil.py\n            nontext.py\n            fulltune/\n                deepspeed_trainner.py\n                __init__.py\n                launch.py\n                trainner.py\n                base_model/\n                    __init__.py\n                    configuration_baichuan.py\n                    modeling_baichuan.py\n                pretrain/\n                    __init__.py\n                    convert_to_transformers.py\n            metrics/\n                __init__.py\n            config/\n                __init__.py\n            retrieval/\n                rest.py\n                __init__.py\n                udf.py\n            sft/\n                argument.py\n                collator.py\n                qlora.py\n                __init__.py\n                model.py\n                dataset.py\n                loss.py\n                merge_lora.py\n                trainer.py\n            client/\n                message_utils.py\n                math_utils.py\n                byzerllm_client.py\n                parallel_utils.py\n                __init__.py\n                types.py\n                code_utils.py\n                img_utils.py\n                entrypoints/\n                    __init__.py\n                    openai/\n                        protocol.py\n                        __init__.py\n                        serve.py\n                        serving_chat.py\n                        api_server.py\n                        serving_engine.py\n                        serving_completion.py\n                        tool.py\n        qwen_voice/\n            __init__.py\n        chatglm2/\n            __init__.py\n        baichuan/\n            __init__.py\n        m3e/\n            __init__.py\n        stable_diffusion/\n            config.py\n            __init__.py\n            logger.py\n            shared.py\n            model.py\n            utils.py\n            images.py\n            diffusion/\n                __init__.py\n                embeddings.py\n                utils.py\n                networks/\n                    lyco.py\n                    __init__.py\n                upscalers/\n                    __init__.py\n                    multidiffusion.py\n                    samplers.py\n                piplines/\n                    __init__.py\n                    lpw.py\n                    diffusers.py\n            lib/\n                __init__.py\n                diffusers/\n                    __init__.py\n                    scheduler.py\n            api/\n                __init__.py\n                plugin.py\n                models/\n                    diffusion.py\n                    __init__.py\n                diffusion/\n                    __init__.py\n                    pipelines/\n                        __init__.py\n                        diffusers.py\n                events/\n                    generation.py\n                    __init__.py\n        bge/\n            __init__.py\n        bge_rerank/\n            __init__.py\n        apps/\n            client.py\n            qa_strategy.py\n            __init__.py\n            builder.py\n            vector_db.py\n            utils.py\n            qa.py\n            byzer_storage/\n                memory_model_based.py\n                command.py\n                env.py\n                train_pt.py\n                train_sft.py\n                __init__.py\n                train_dpo.py\n                simple_api.py\n                generate_sft_data.py\n            byzer_sql/\n                __init__.py\n            llama_index/\n                byzerai_vectordb.py\n                byzerai.py\n                byzerai_embedding.py\n                __init__.py\n                collection_manager.py\n                byzerai_index_store.py\n                byzerai_docstore.py\n                simple_retrieval.py\n                byzerai_kvstore.py\n            agent/\n                conversable_agent.py\n                registry.py\n                groupchat.py\n                user_proxy_agent.py\n                __init__.py\n                agent.py\n                extensions/\n                    simple_retrieval_client.py\n                    planner.py\n                    python_codesandbox_agent.py\n                    preview_file_agent.py\n                    llama_index_retrieval_agent.py\n                    visualization_agent.py\n                    assistant_agent.py\n                    load_data_agent.py\n                    retrieval_agent.py\n                    output_agent.py\n                    __init__.py\n                    byzer_engine_agent.py\n                    subquestion_agent.py\n                    sql_reviewer_agent.py\n                    spark_sql_agent.py\n                    rhetorical_agent.py\n                    data_analysis.py\n                    data_analysis_pipeline_agent.py\n                    common_agent.py\n                    query_rewrite/\n                        time.py\n                        rhetorical.py\n                        __init__.py\n                        condition.py\n                        context.py\n                store/\n                    memory_store.py\n                    __init__.py\n                    stores.py\n        saas/\n            __init__.py\n            chatglm/\n                __init__.py\n            sparkdesk/\n                __init__.py\n            gemini/\n                __init__.py\n            azure_openai/\n                __init__.py\n            azure/\n                __init__.py\n            qianfan/\n                __init__.py\n            claude/\n                __init__.py\n            qianwen/\n                __init__.py\n            volcengine/\n                __init__.py\n            minimax/\n                __init__.py\n            baichuan/\n                __init__.py\n            qianwen_vl/\n                __init__.py\n            official_openai/\n                __init__.py\n            zhipu/\n                __init__.py\n            aws_bedrock/\n                __init__.py\n            openai/\n                __init__.py\n        falcon/\n            __init__.py\n        chatglm6b/\n            trainer_seq2seq.py\n            arguments.py\n            __init__.py\n            finetune.py\n            tunning/\n                __init__.py\n                infer.py\n                finetune.py\n                utils/\n                    config.py\n                    __init__.py\n                    other.py\n                    common.py\n                    seq2seq.py\n        starcode/\n            __init__.py\n        qwen_vl_chat/\n            __init__.py\n        zephyr/\n            __init__.py\n        dolly/\n            generate.py\n            __init__.py\n            consts.py\n            dolly_inference.py\n            trainer.py\n        llama/\n            __init__.py\n        whisper/\n            whisper_inference.py\n            __init__.py\n下面是一些文件路径以及每个文件对应的源码：\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/saas/official_openai/__init__.py\nimport time\nfrom typing import List, Tuple, Dict, Any, Union\nimport httpx\nfrom openai import OpenAI\nimport base64\nimport io\nimport json\nimport ray\nfrom byzerllm.utils.types import (\n    BlockVLLMStreamServer,\n    StreamOutputs,\n    SingleOutput,\n    SingleOutputMeta,\n    BlockBinaryStreamServer,\n)\nfrom byzerllm.utils.langutil import asyncfy_with_semaphore\nimport threading\nimport asyncio\nimport traceback\nimport uuid\nimport tempfile        \n\nclass CustomSaasAPI:\n\n    def __init__(self, infer_params: Dict[str, str]) -> None:\n\n        self.api_key = infer_params[\"saas.api_key\"]\n        self.model = infer_params.get(\"saas.model\", \"gpt-3.5-turbo-1106\")\n\n        other_params = {}\n\n        if \"saas.api_base\" in infer_params:\n            other_params[\"api_base\"] = infer_params[\"saas.api_base\"]\n\n        if \"saas.api_version\" in infer_params:\n            other_params[\"api_version\"] = infer_params[\"saas.api_version\"]\n\n        if \"saas.api_type\" in infer_params:\n            other_params[\"api_type\"] = infer_params[\"saas.api_type\"]\n\n        if \"saas.base_url\" in infer_params:\n            other_params[\"base_url\"] = infer_params[\"saas.base_url\"]\n\n        if \"saas.timeout\" in infer_params:\n            other_params[\"timeout\"] = float(infer_params[\"saas.timeout\"])\n\n        self.max_retries = int(infer_params.get(\"saas.max_retries\", 10))\n\n        self.meta = {\n            \"model_deploy_type\": \"saas\",\n            \"backend\": \"saas\",\n            \"support_stream\": True,\n            \"model_name\": self.model,\n        }\n\n        self.meta[\"embedding_mode\"] = \"embedding\" in self.model.lower()\n\n        self.proxies = infer_params.get(\"saas.proxies\", None)\n        self.local_address = infer_params.get(\"saas.local_address\", \"0.0.0.0\")\n\n        if not self.proxies:\n            self.client = OpenAI(**other_params, api_key=self.api_key)\n        else:\n            self.client = OpenAI(\n                **other_params,\n                api_key=self.api_key,\n                http_client=httpx.Client(\n                    proxies=self.proxies,\n                    transport=httpx.HTTPTransport(local_address=self.local_address),\n                ),\n            )\n\n        try:\n            ray.get_actor(\"BLOCK_VLLM_STREAM_SERVER\")\n        except ValueError:\n            try:\n                ray.remote(BlockVLLMStreamServer).options(\n                    name=\"BLOCK_VLLM_STREAM_SERVER\",\n                    lifetime=\"detached\",\n                    max_concurrency=1000,\n                ).remote()\n            except Exception as e:\n                pass\n        try:\n            ray.get_actor(\"BlockBinaryStreamServer\")\n        except ValueError:\n            try:\n                ray.remote(BlockBinaryStreamServer).options(\n                    name=\"BlockBinaryStreamServer\",\n                    lifetime=\"detached\",\n                    max_concurrency=1000,\n                ).remote()\n            except Exception as e:\n                pass\n\n    # saas/proprietary\n    def get_meta(self):\n        return [self.meta]\n\n    def process_input(self, ins: Union[str, List[Dict[str, Any]], Dict[str, Any]]):\n\n        if isinstance(ins, list) or isinstance(ins, dict):\n            return ins\n\n        content = []\n        try:\n            ins_json = json.loads(ins)\n        except:\n            return ins\n\n        ## speech\n        if isinstance(ins_json, dict):\n            return ins_json\n\n        content = []\n        #     [\n        #     {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n        #     {\n        #       \"type\": \"image_url\",\n        #       \"image_url\": {\n        #         \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n        #         \"detail\": \"high\"\n        #       },\n        #     },\n        #   ],\n        for item in ins_json:\n            # for format like this: {\"image\": \"xxxxxx\", \"text\": \"What’s in this image?\",\"detail\":\"high\"}\n            # or {\"image\": \"xxxxxx\"}, {\"text\": \"What’s in this image?\"}\n            if (\"image\" in item or \"image_url\" in item) and \"type\" not in item:\n                image_data = item.get(\"image\", item.get(\"image_url\", \"\"))\n                ## \"data:image/jpeg;base64,\"\n                if not image_data.startswith(\"data:\"):\n                    image_data = \"data:image/jpeg;base64,\" + image_data\n\n                ## get the other fields except image/text/image_url\n                other_fields = {\n                    k: v\n                    for k, v in item.items()\n                    if k not in [\"image\", \"text\", \"image_url\"]\n                }\n                content.append(\n                    {\n                        \"image_url\": {\"url\": image_data, **other_fields},\n                        \"type\": \"image_url\",\n                    }\n                )            \n\n            if \"text\" in item and \"type\" not in item:\n                text_data = item[\"text\"]\n                content.append({\"text\": text_data, \"type\": \"text\"})\n\n            ## for format like this: {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n            ## {\"type\": \"image_url\", \"image_url\": {\"url\":\"\",\"detail\":\"high\"}}\n            ## this is the standard format, just return it\n            if \"type\" in item and item[\"type\"] == \"text\":\n                content.append(item)\n\n            if \"type\" in item and item[\"type\"] == \"image_url\":\n                content.append(item)\n\n        if not content:\n            return ins\n\n        return content\n\n    async def async_embed_query(self, ins: str, **kwargs):\n        resp = await asyncfy_with_semaphore(\n            lambda: self.client.embeddings.create(input=[ins], model=self.model)\n        )()\n        embedding = resp.data[0].embedding\n        usage = resp.usage\n        return (\n            embedding,\n            {\n                \"metadata\": {\n                    \"input_tokens_count\": usage.prompt_tokens,\n                    \"generated_tokens_count\": 0,\n                }\n            },\n        )\n\n    def embed_query(self, ins: str, **kwargs):\n        resp = self.client.embeddings.create(input=[ins], model=self.model)\n        embedding = resp.data[0].embedding\n        usage = resp.usage\n        return (\n            embedding,\n            {\n                \"metadata\": {\n                    \"input_tokens_count\": usage.prompt_tokens,\n                    \"generated_tokens_count\": 0,\n                }\n            },\n        )\n\n    async def async_text_to_speech(\n        self, stream: bool, ins: str, voice: str, chunk_size: int = None, **kwargs\n    ):\n        if stream:\n            server = ray.get_actor(\"BlockBinaryStreamServer\")\n            request_id = [None]\n\n            def writer():\n                try:\n                    request_id[0] = str(uuid.uuid4())\n                    with self.client.with_streaming_response.audio.speech.create(\n                        model=self.model, voice=voice, input=ins, **kwargs\n                    ) as response:\n                        for chunk in response.iter_bytes(chunk_size):\n                            input_tokens_count = 0\n                            generated_tokens_count = 0\n                            ray.get(\n                                server.add_item.remote(\n                                    request_id[0],\n                                    StreamOutputs(\n                                        outputs=[\n                                            SingleOutput(\n                                                text=chunk,\n                                                metadata=SingleOutputMeta(\n                                                    input_tokens_count=input_tokens_count,\n                                                    generated_tokens_count=generated_tokens_count,\n                                                ),\n                                            )\n                                        ]\n                                    ),\n                                )\n                            )\n                except:\n                    traceback.print_exc()\n                ray.get(server.mark_done.remote(request_id[0]))\n\n            threading.Thread(target=writer, daemon=True).start()\n\n            time_count = 10 * 100\n            while request_id[0] is None and time_count > 0:\n                time.sleep(0.01)\n                time_count -= 1\n\n            if request_id[0] is None:\n                raise Exception(\"Failed to get request id\")\n\n            def write_running():\n                return ray.get(server.add_item.remote(request_id[0], \"RUNNING\"))\n\n            await asyncio.to_thread(write_running)\n            return [\n                (\n                    \"\",\n                    {\n                        \"metadata\": {\n                            \"request_id\": request_id[0],\n                            \"stream_server\": \"BlockBinaryStreamServer\",\n                        }\n                    },\n                )\n            ]\n\n        start_time = time.monotonic()\n        with io.BytesIO() as output:\n            async with asyncfy_with_semaphore(\n                lambda: self.client.with_streaming_response.audio.speech.create(\n                    model=self.model, voice=voice, input=ins, **kwargs\n                )\n            )() as response:\n                for chunk in response.iter_bytes():\n                    output.write(chunk)\n\n            base64_audio = base64.b64encode(output.getvalue()).decode()\n            time_cost = time.monotonic() - start_time\n            return [\n                (\n                    base64_audio,\n                    {\n                        \"metadata\": {\n                            \"request_id\": \"\",\n                            \"input_tokens_count\": 0,\n                            \"generated_tokens_count\": 0,\n                            \"time_cost\": time_cost,\n                            \"first_token_time\": 0,\n                            \"speed\": 0,\n                        }\n                    },\n                )\n            ]\n\n    async def async_speech_to_text(\n        self,\n        audio: str,\n        response_format: str = \"verbose_json\",\n        timestamp_granularities: List[str] = [\"word\",\"segment\"],\n    ):               \n        # Extract audio format and base64 data\n        data_prefix = \"data:audio/\"\n        base64_prefix = \";base64,\"\n        if not audio.startswith(data_prefix):\n            raise ValueError(\"Invalid audio data format\")\n        \n        format_end = audio.index(base64_prefix)\n        audio_format = audio[len(data_prefix):format_end]\n        base64_data = audio[format_end + len(base64_prefix):]\n        \n        # Decode the base64 audio data\n        audio_data = base64.b64decode(base64_data)\n\n        # Create a temporary file with the correct extension\n        with tempfile.NamedTemporaryFile(delete=False, suffix=f'.{audio_format}') as temp_audio_file:\n            temp_audio_file.write(audio_data)\n            temp_audio_file_path = temp_audio_file.name\n\n        try:\n            start_time = time.monotonic()\n                   \n            with open(temp_audio_file_path, \"rb\") as audio_file:                \n                transcription = self.client.audio.transcriptions.create(\n                    model=self.model or \"whisper-1\",\n                    file=audio_file,\n                    response_format=response_format,\n                    timestamp_granularities=timestamp_granularities,\n                )\n            time_cost = time.monotonic() - start_time\n\n            return [\n                (\n                    json.dumps(transcription.to_dict(), ensure_ascii=True),\n                    {\n                        \"metadata\": {\n                            \"request_id\": \"\",\n                            \"input_tokens_count\": 0,\n                            \"generated_tokens_count\": 0,\n                            \"time_cost\": time_cost,\n                            \"first_token_time\": 0,\n                            \"speed\": 0,\n                        }\n                    },\n                )\n            ]\n        finally:\n            # Clean up the temporary file\n            import os\n            os.unlink(temp_audio_file_path)\n\n    def image_to_text(self, ins: str, **kwargs):\n        pass\n\n    async def async_text_to_image(\n        self, stream: bool, input: str, size: str, quality: str, n: int, **kwargs\n    ):\n        if stream:\n            raise Exception(\"Stream not supported for text to image\")\n        start_time = time.monotonic()\n        response = await asyncfy_with_semaphore(\n            lambda: self.client.images.generate(\n                model=self.model,\n                prompt=input,\n                size=size,\n                quality=quality,\n                n=1,\n                response_format=\"b64_json\",\n                **kwargs,\n            )\n        )()\n        time_cost = time.monotonic() - start_time\n        base64_image = response.data[0].b64_json\n        return [\n            (\n                base64_image,\n                {\n                    \"metadata\": {\n                        \"request_id\": \"\",\n                        \"input_tokens_count\": 0,\n                        \"generated_tokens_count\": 0,\n                        \"time_cost\": time_cost,\n                        \"first_token_time\": 0,\n                        \"speed\": 0,\n                    }\n                },\n            )\n        ]\n\n    def text_to_text(self, ins: str, **kwargs):\n        pass\n\n    async def async_stream_chat(\n        self,\n        tokenizer,\n        ins: str,\n        his: List[Dict[str, Any]] = [],\n        max_length: int = 4096,\n        top_p: float = 0.7,\n        temperature: float = 0.9,\n        **kwargs,\n    ):\n\n        model = self.model\n\n        if \"model\" in kwargs:\n            model = kwargs[\"model\"]\n\n        messages = [\n            {\"role\": message[\"role\"], \"content\": self.process_input(message[\"content\"])}\n            for message in his\n        ] + [{\"role\": \"user\", \"content\": self.process_input(ins)}]\n\n        stream = kwargs.get(\"stream\", False)\n\n        ## content = [\n        ##    \"voice\": \"alloy\",\"input\": \"Hello, World!\",response_format: \"mp3\"]\n        last_message = messages[-1][\"content\"]\n\n        if isinstance(last_message, dict) and \"voice\" in last_message:\n            voice = last_message[\"voice\"]\n            response_format = last_message.get(\"response_format\", \"mp3\")\n            chunk_size = last_message.get(\"chunk_size\", None)\n            input = last_message[\"input\"]\n            return await self.async_text_to_speech(\n                stream=stream,\n                ins=input,\n                voice=voice,\n                chunk_size=chunk_size,\n                response_format=response_format,\n            )\n\n        if isinstance(last_message, dict) and \"input\" in last_message:\n            input = last_message[\"input\"]\n            size = last_message.get(\"size\", \"1024x1024\")\n            quality = last_message.get(\"quality\", \"standard\")\n            n = last_message.get(\"n\", 1)\n            return await self.async_text_to_image(\n                stream=stream, input=input, size=size, quality=quality, n=n\n            )\n\n        if isinstance(last_message, dict) and \"audio\" in last_message:            \n            audio_data = last_message.get(\"audio\", \"\")            \n            tpe = last_message.get(\"type\",\"wav\")\n            if not audio_data.startswith(\"data:\"):\n                audio_data = f\"data:audio/${tpe};base64,\" + audio_data                \n            return await self.async_speech_to_text(audio=audio_data)\n        \n        server = ray.get_actor(\"BLOCK_VLLM_STREAM_SERVER\")\n        request_id = [None]\n\n        def writer():\n            try:\n                r = \"\"\n                response = self.client.chat.completions.create(\n                    messages=messages,\n                    model=model,\n                    stream=True,\n                    max_tokens=max_length,\n                    temperature=temperature,\n                    top_p=top_p,\n                )\n                # input_tokens_count = 0\n                # generated_tokens_count = 0\n\n                request_id[0] = str(uuid.uuid4())\n\n                for chunk in response:\n                    content = chunk.choices[0].delta.content or \"\"\n                    r += content\n                    if hasattr(chunk, \"usage\") and chunk.usage:\n                        input_tokens_count = chunk.usage.prompt_tokens\n                        generated_tokens_count = chunk.usage.completion_tokens\n                    else:\n                        input_tokens_count = 0\n                        generated_tokens_count = 0\n                    ray.get(\n                        server.add_item.remote(\n                            request_id[0],\n                            StreamOutputs(\n                                outputs=[\n                                    SingleOutput(\n                                        text=r,\n                                        metadata=SingleOutputMeta(\n                                            input_tokens_count=input_tokens_count,\n                                            generated_tokens_count=generated_tokens_count,\n                                        ),\n                                    )\n                                ]\n                            ),\n                        )\n                    )\n            except:\n                traceback.print_exc()\n            ray.get(server.mark_done.remote(request_id[0]))\n\n        if stream:\n            threading.Thread(target=writer, daemon=True).start()\n\n            time_count = 60 * 100\n            while request_id[0] is None and time_count > 0:\n                time.sleep(0.01)\n                time_count -= 1\n\n            if request_id[0] is None:\n                raise Exception(\"Failed to get request id\")\n\n            def write_running():\n                return ray.get(server.add_item.remote(request_id[0], \"RUNNING\"))\n\n            await asyncio.to_thread(write_running)\n            return [\n                (\n                    \"\",\n                    {\n                        \"metadata\": {\n                            \"request_id\": request_id[0],\n                            \"stream_server\": \"BLOCK_VLLM_STREAM_SERVER\",\n                        }\n                    },\n                )\n            ]\n        else:\n            try:\n                start_time = time.monotonic()\n                response = await asyncfy_with_semaphore(\n                    lambda: self.client.chat.completions.create(\n                        messages=messages,\n                        model=model,\n                        max_tokens=max_length,\n                        temperature=temperature,\n                        top_p=top_p,\n                    )\n                )()\n\n                generated_text = response.choices[0].message.content\n                generated_tokens_count = response.usage.completion_tokens\n                input_tokens_count = response.usage.prompt_tokens\n                time_cost = time.monotonic() - start_time\n                return [\n                    (\n                        generated_text,\n                        {\n                            \"metadata\": {\n                                \"request_id\": response.id,\n                                \"input_tokens_count\": input_tokens_count,\n                                \"generated_tokens_count\": generated_tokens_count,\n                                \"time_cost\": time_cost,\n                                \"first_token_time\": 0,\n                                \"speed\": float(generated_tokens_count) / time_cost,\n                            }\n                        },\n                    )\n                ]\n            except Exception as e:\n                print(f\"Error: {e}\")\n                raise e\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/whisper/__init__.py\nimport whisper\nfrom typing import Dict, List, Tuple\nfrom byzerllm.utils.types import StopSequencesCriteria\n\nfrom typing import Dict, Any, List, Union\nfrom pyjava.storage import streaming_tar as STar\nimport json\nimport base64\nimport tempfile\nimport time\n\n\ndef get_meta(self):\n    return [\n        {\n            \"model_deploy_type\": \"proprietary\",\n            \"backend\": \"transformers\",\n            \"message_format\": True\n        }\n    ]\n\n\ndef process_input(ins: Union[str, List[Dict[str, Any]], Dict[str, Any]]):\n    # print(ins) \n    if isinstance(ins, list) or isinstance(ins, dict):\n        return ins\n\n    content = []\n    try:\n        ins_json = json.loads(ins)\n    except:\n        return ins\n\n    ## speech\n    if isinstance(ins_json, dict):\n        return ins_json\n\n    content = []\n    #     [\n    #     {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n    #     {\n    #       \"type\": \"image_url\",\n    #       \"image_url\": {\n    #         \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n    #         \"detail\": \"high\"\n    #       },\n    #     },\n    #   ],\n    for item in ins_json:\n        # for format like this: {\"image\": \"xxxxxx\", \"text\": \"What’s in this image?\",\"detail\":\"high\"}\n        # or {\"image\": \"xxxxxx\"}, {\"text\": \"What’s in this image?\"}\n        if (\"image\" in item or \"image_url\" in item) and \"type\" not in item:\n            image_data = item.get(\"image\", item.get(\"image_url\", \"\"))\n            ## \"data:image/jpeg;base64,\"\n            if not image_data.startswith(\"data:\"):\n                image_data = \"data:image/jpeg;base64,\" + image_data\n\n            ## get the other fields except image/text/image_url\n            other_fields = {\n                k: v for k, v in item.items() if k not in [\"image\", \"text\", \"image_url\"]\n            }\n            content.append(\n                {\n                    \"image_url\": {\"url\": image_data, **other_fields},\n                    \"type\": \"image_url\",\n                }\n            )\n\n        if \"text\" in item and \"type\" not in item:\n            text_data = item[\"text\"]\n            content.append({\"text\": text_data, \"type\": \"text\"})\n\n        ## for format like this: {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n        ## {\"type\": \"image_url\", \"image_url\": {\"url\":\"\",\"detail\":\"high\"}}\n        ## this is the standard format, just return it\n        if \"type\" in item and item[\"type\"] == \"text\":\n            content.append(item)\n\n        if \"type\" in item and item[\"type\"] == \"image_url\":\n            content.append(item)\n\n    if not content:\n        return ins\n\n    return content\n\n\ndef stream_chat(\n    self,\n    tokenizer,\n    ins: str,\n    his: List[Dict[str, str]] = [],\n    max_length: int = 4090,\n    top_p: float = 0.95,\n    temperature: float = 0.1,\n    **kwargs,\n):\n    messages = [\n        {\"role\": message[\"role\"], \"content\": process_input(message[\"content\"])}\n        for message in his\n    ] + [{\"role\": \"user\", \"content\": process_input(ins)}]\n\n    audio_input = messages[-1][\"content\"]\n\n    audio_data = audio_input.get(\"audio\", \"\")\n    audio = audio_input[\"audio\"]\n    response_format = audio_input.get(\"response_format\", \"json\")\n    timestamp_granularities = audio_input.get(\n        \"timestamp_granularities\", [\"word\", \"segment\"]\n    )\n    data_prefix = \"data:audio/\"\n    base64_prefix = \";base64,\"\n    if not audio.startswith(data_prefix):\n        raise ValueError(\"Invalid audio data format\")\n\n    format_end = audio.index(base64_prefix)\n    audio_format = audio[len(data_prefix) : format_end]\n    base64_data = audio[format_end + len(base64_prefix) :]\n\n    # Decode the base64 audio data\n    audio_data = base64.b64decode(base64_data)\n\n    # Create a temporary file with the correct extension\n    with tempfile.NamedTemporaryFile(\n        delete=False, suffix=f\".{audio_format}\"\n    ) as temp_audio_file:\n        temp_audio_file.write(audio_data)\n        temp_audio_file_path = temp_audio_file.name\n\n    try:\n        start_time = time.monotonic()\n        word_timestamps = \"word\" in timestamp_granularities\n        \n        result = self.transcribe(temp_audio_file_path, word_timestamps=word_timestamps)        \n            \n        time_cost = time.monotonic() - start_time\n        return [\n            (\n                json.dumps(result, ensure_ascii=False),\n                {\n                    \"metadata\": {\n                        \"request_id\": \"\",\n                        \"input_tokens_count\": 0,\n                        \"generated_tokens_count\": 0,\n                        \"time_cost\": time_cost,\n                        \"first_token_time\": 0,\n                        \"speed\": 0,\n                    }\n                },\n            )\n        ]\n    finally:\n        # Clean up the temporary file\n        import os\n\n        os.unlink(temp_audio_file_path)\n\n\ndef init_model(\n    model_dir, infer_params: Dict[str, str] = {}, sys_conf: Dict[str, str] = {}\n):\n    model = whisper.load_model(\"large\", download_root=model_dir)\n    import types\n\n    model.stream_chat = types.MethodType(stream_chat, model)\n    model.get_meta = types.MethodType(get_meta, model)\n    return (model, None)\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/notebooks/test_sensevoice.py\nimport sys\nimport os\n\n# Add the project root to the Python path\nsys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\")))\n\nfrom src.byzerllm.sensevoice import init_model, format_str_v3\nimport base64\nimport json\n\n\ndef main():\n    # Initialize the model\n    model_dir = \"/Users/allwefantasy/models/SenseVoiceSmall\"\n    model, _ = init_model(model_dir)\n\n    # Prepare test audio\n    with open(\n        \"/Users/allwefantasy/models/SenseVoiceSmall/example/zh.mp3\", \"rb\"\n    ) as audio_file:\n        audio_data = audio_file.read()\n        base64_audio = base64.b64encode(audio_data).decode(\"utf-8\")        \n        # Prepare input in the expected format\n        test_input = json.dumps(\n            {\n                            \"type\": \"audio\",\n                            \"audio\": f\"data:audio/wav;base64,{base64_audio}\",\n                        }\n        )\n\n        # Call stream_chat\n        result = model.stream_chat(tokenizer=None, ins = test_input)\n\n        # Process and print the result\n        if result and len(result) > 0:\n            output, metadata = result[0]\n            parsed_output = json.loads(output)\n            formatted_text = parsed_output.get(\"text\", \"\")\n            print(\"Transcription:\", formatted_text)\n            print(\"Metadata:\", metadata)\n        else:\n            print(\"No result returned from the model.\")\n\n\nif __name__ == \"__main__\":\n    main()\n\n\n##File: /Users/allwefantasy/projects/Qwen2-Audio/README_CN.md\n<p align=\"left\">\n        中文</a>&nbsp ｜ &nbsp<a href=\"README.md\">English</a>\n</p>\n<br><br>\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/assets/blog/qwenaudio/qwen2audio_logo.png\" width=\"400\"/>\n<p>\n\n<p align=\"center\">\nQwen2-Audio-7B <a href=\"https://modelscope.cn/models/qwen/Qwen2-Audio-7B\">🤖 </a> | <a href=\"https://huggingface.co/Qwen/Qwen2-Audio-7B\">🤗</a>&nbsp ｜ Qwen-Audio-7B-Instruct <a href=\"https://modelscope.cn/models/qwen/Qwen2-Audio-7B-Instruct\">🤖 </a>| <a href=\"https://huggingface.co/Qwen/Qwen2-Audio-7B-Instruct\">🤗</a>&nbsp ｜ Demo<a href=\"https://modelscope.cn/studios/qwen/Qwen2-Audio-Instruct-Demo\"> 🤖</a> | <a href=\"https://huggingface.co/spaces/Qwen/Qwen2-Audio-Instruct-Demo\">🤗</a>&nbsp\n<br>\n📑 <a href=\"https://arxiv.org/abs/2407.10759\">Paper</a> &nbsp&nbsp | &nbsp&nbsp 📑 <a href=\"https://qwenlm.github.io/blog/qwen2-audio\">Blog</a> &nbsp&nbsp | &nbsp&nbsp 💬 <a href=\"https://github.com/QwenLM/Qwen/blob/main/assets/wechat.png\">WeChat (微信)</a>&nbsp&nbsp | &nbsp&nbsp <a href=\"https://discord.gg/CV4E9rpNSD\">Discord</a>&nbsp&nbsp\n</p>\n\n\n\n我们介绍Qwen-Audio的最新进展：Qwen2-Audio。作为一个大规模音频语言模型，Qwen2-Audio能够接受各种音频信号输入，并根据语音指令执行音频分析或直接响应文本。我们介绍两种不同的音频交互模式：语音聊天 voice chat 和音频分析 audio analysis。\n\n* 语音聊天：用户可以自由地与 Qwen2-Audio 进行语音互动，而无需文本输入；\n* 音频分析：用户可以在互动过程中提供音频和文本指令对音频进行分析；\n\n**我们已经开源了 Qwen2-Audio 系列的两个模型：Qwen2-Audio-7B和Qwen2-Audio-7B-Instruct。**\n\n## 模型结构与训练范式\n\nQwen2-Audio 三阶段训练过程概述。\n\n<p align=\"left\">\n    <img src=\"assets/framework.png\" width=\"80%\"/>\n<p>\n\n\n\n## 新闻\n* 2024.8.9 🎉 我们在 ModelScope 和 Hugging Face 开源了`Qwen2-Audio-7B`和`Qwen2-Audio-7B-Instruct`的 checkpoint.\n* 2024.7.15 🎉 我们发布了 Qwen2-Audio 的[论文](https://arxiv.org/abs/2407.10759), 介绍了相关的模型结构，训练方法和模型表现。\n* 2023.11.30 🔥 我们发布了**Qwen-Audio**系列\n<br>\n\n## 评测\n我们在标准的13个学术数据集上评测了模型的能力如下：\n<table><thead><tr><th>Task</th><th>Description</th><th>Dataset</th><th>Split</th><th>Metric</th></tr></thead><tbody><tr><td rowspan=\"4\">ASR</td><td rowspan=\"4\">Automatic Speech Recognition</td><td>Fleurs</td><td>dev | test</td><td rowspan=\"4\">WER</td></tr><tr><td>Aishell2</td><td>test</td></tr><tr><td>Librispeech</td><td>dev | test</td></tr><tr><td>Common Voice</td><td>dev | test</td></tr><tr><td>S2TT</td><td>Speech-to-Text Translation</td><td>CoVoST2</td><td>test</td><td>BLEU </td></tr><tr><td>SER</td><td>Speech Emotion Recognition</td><td>Meld</td><td>test</td><td>ACC</td></tr><tr><td>VSC</td><td>Vocal Sound Classification</td><td>VocalSound</td><td>test</td><td>ACC</td></tr><tr><td rowspan=\"4\"><a href=\"https://github.com/OFA-Sys/AIR-Bench\">AIR-Bench</a><br></td><td>Chat-Benchmark-Speech</td><td>Fisher<br>SpokenWOZ<br>IEMOCAP<br>Common voice</td><td>dev | test</td><td>GPT-4 Eval</td></tr><tr><td>Chat-Benchmark-Sound</td><td>Clotho</td><td>dev | test</td><td>GPT-4 Eval</td></tr>\n<tr><td>Chat-Benchmark-Music</td><td>MusicCaps</td><td>dev | test</td><td>GPT-4 Eval</td></tr><tr><td>Chat-Benchmark-Mixed-Audio</td><td>Common voice<br>AudioCaps<br>MusicCaps</td><td>dev | test</td><td>GPT-4 Eval</td></tr></tbody></table>\n\n\n以下是整体表现：\n<p align=\"left\">\n    <img src=\"assets/radar_compare_qwen_audio.png\" width=\"70%\"/>\n<p>\n\n评测分数详情如下：\n<br>\n<b>（注意：我们所展示的评测结果是在原始训练框架的初始模型上的，然而在框架转换 Huggingface 后指标出现了部分波动，在这里我们展示我们的全部测评结果：首先是论文中的初始模型结果）</b>\n\n<table><thead><tr><th rowspan=\"2\">Task</th><th rowspan=\"2\">Dataset</th><th rowspan=\"2\">Model</th><th colspan=\"2\">Performance</th></tr><tr><th>Metrics</th><th>Results</th></tr></thead><tbody><tr><td rowspan=\"15\">ASR</td><td rowspan=\"7\"><b>Librispeech</b><br>dev-clean | dev-other | <br>test-clean | test-other</td><td>SpeechT5</td><td rowspan=\"7\">WER </td><td>2.1 | 5.5 | 2.4 | 5.8</td></tr><tr><td>SpeechNet</td><td>- | - | 30.7 | -</td></tr><tr><td>SLM-FT</td><td>- | - | 2.6 | 5.0</td></tr><tr><td>SALMONN</td><td>- | - | 2.1 | 4.9</td></tr><tr><td>SpeechVerse</td><td>- | - | 2.1 | 4.4</td></tr><tr><td>Qwen-Audio</td><td>1.8 | 4.0 | 2.0 | 4.2</td></tr><tr><td>Qwen2-Audio</td><td><b>1.3 | 3.4 | 1.6 | 3.6</b></td></tr><tr><td rowspan=\"2\"><b>Common Voice 15</b> <br>en | zh | yue | fr</td><td>Whisper-large-v3</td><td rowspan=\"2\">WER </td><td>9.3 | 12.8 | 10.9 | 10.8</td></tr><tr><td>Qwen2-Audio</td><td><b>8.6 | 6.9 | 5.9 | 9.6</b></td></tr>\n<tr><td rowspan=\"2\"><b>Fleurs</b> <br>zh</td><td>Whisper-large-v3</td><td rowspan=\"2\">WER </td><td>7.7</td></tr><tr><td>Qwen2-Audio</td><td><b>7.5</b></td></tr><tr><td rowspan=\"4\"><b>Aishell2</b> <br>Mic | iOS | Android</td><td>MMSpeech-base</td><td rowspan=\"4\">WER </td><td>4.5 | 3.9 | 4.0</td></tr><tr><td>Paraformer-large</td><td>- | <b>2.9</b> | -</td></tr><tr><td>Qwen-Audio</td><td>3.3 | 3.1 | 3.3</td></tr><tr><td>Qwen2-Audio</td><td><b>3.0</b> | 3.0 | <b>2.9</b></td></tr><tr><td rowspan=\"8\">S2TT</td><td rowspan=\"5\"><b>CoVoST2</b> <br>en-de | de-en | <br>en-zh | zh-en</td><td>SALMONN</td><td rowspan=\"5\">BLEU </td><td>18.6 | - | 33.1 | -</td></tr><tr><td>SpeechLLaMA</td><td>- | 27.1 | - | 12.3</td></tr><tr><td>BLSP</td><td>14.1 | - | - | -</td></tr><tr><td>Qwen-Audio</td><td>25.1 | 33.9 | 41.5 | 15.7</td></tr><tr><td>Qwen2-Audio</td><td><b>29.9 | 35.2 | 45.2 | 24.4</b></td></tr>\n<tr><td rowspan=\"3\"><b>CoVoST2</b> <br>es-en | fr-en | it-en |</td><td>SpeechLLaMA</td><td rowspan=\"3\">BLEU </td><td>27.9 | 25.2 | 25.9</td></tr><tr><td>Qwen-Audio</td><td>39.7 | <b>38.5</b> | 36.0</td></tr><tr><td>Qwen2-Audio</td><td><b>40.0 | 38.5 | 36.3</b></td></tr><tr><td rowspan=\"3\">SER</td><td rowspan=\"3\"><b>Meld</b></td><td>WavLM-large</td><td rowspan=\"3\">ACC </td><td>0.542</td></tr><tr><td>Qwen-Audio</td><td><b>0.557</b></td></tr><tr><td>Qwen2-Audio</td><td>0.553</td></tr><tr><td rowspan=\"4\">VSC</td><td rowspan=\"4\"><b>VocalSound</b></td><td>CLAP</td><td rowspan=\"4\">ACC </td><td>0.4945</td></tr><tr><td>Pengi</td><td>0.6035</td></tr><tr><td>Qwen-Audio</td><td>0.9289</td></tr><tr><td>Qwen2-Audio</td><td><b>0.9392</b></td></tr>\n<tr><td>AIR-Bench <br></td><td><b>Chat Benchmark</b><br>Speech | Sound |<br> Music | Mixed-Audio</td><td>SALMONN<br>BLSP<br>Pandagpt<br>Macaw-LLM<br>SpeechGPT<br>Next-gpt<br>Qwen-Audio<br>Gemini-1.5-pro<br>Qwen2-Audio</td><td>GPT-4 </td><td>6.16 | 6.28 | 5.95 | 6.08<br>6.17 | 5.55 | 5.08 | 5.33<br>3.58 | 5.46 | 5.06 | 4.25<br>0.97 | 1.01 | 0.91 | 1.01<br>1.57 | 0.95 | 0.95 | 4.13<br>3.86 | 4.76 | 4.18 | 4.13<br>6.47 | 6.95 | 5.52 | 6.08<br>6.97 | 5.49 | 5.06 | 5.27<br><b>7.18 | 6.99 | 6.79 | 6.77</b></td></tr></tbody></table>\n\n<b>（其次是转换 huggingface 后的）</b>\n\n<table><thead><tr><th rowspan=\"2\">Task</th><th rowspan=\"2\">Dataset</th><th rowspan=\"2\">Model</th><th colspan=\"2\">Performance</th></tr><tr><th>Metrics</th><th>Results</th></tr></thead><tbody><tr><td rowspan=\"15\">ASR</td><td rowspan=\"7\"><b>Librispeech</b><br>dev-clean | dev-other | <br>test-clean | test-other</td><td>SpeechT5</td><td rowspan=\"7\">WER </td><td>2.1 | 5.5 | 2.4 | 5.8</td></tr><tr><td>SpeechNet</td><td>- | - | 30.7 | -</td></tr><tr><td>SLM-FT</td><td>- | - | 2.6 | 5.0</td></tr><tr><td>SALMONN</td><td>- | - | 2.1 | 4.9</td></tr><tr><td>SpeechVerse</td><td>- | - | 2.1 | 4.4</td></tr><tr><td>Qwen-Audio</td><td>1.8 | 4.0 | 2.0 | 4.2</td></tr><tr><td>Qwen2-Audio</td><td><b>1.7 | 3.6 | 1.7 | 4.0</b></td></tr><tr><td rowspan=\"2\"><b>Common Voice 15</b> <br>en | zh | yue | fr</td><td>Whisper-large-v3</td><td rowspan=\"2\">WER </td><td>9.3 | 12.8 | 10.9 | 10.8</td></tr><tr><td>Qwen2-Audio</td><td><b>8.7 | 6.5 | 5.9 | 9.6</b></td></tr>\n<tr><td rowspan=\"2\"><b>Fleurs</b> <br>zh</td><td>Whisper-large-v3</td><td rowspan=\"2\">WER </td><td>7.7</td></tr><tr><td>Qwen2-Audio</td><td><b>7.0</b></td></tr><tr><td rowspan=\"4\"><b>Aishell2</b> <br>Mic | iOS | Android</td><td>MMSpeech-base</td><td rowspan=\"4\">WER </td><td>4.5 | 3.9 | 4.0</td></tr><tr><td>Paraformer-large</td><td>- | <b>2.9</b> | -</td></tr><tr><td>Qwen-Audio</td><td>3.3 | 3.1 | 3.3</td></tr><tr><td>Qwen2-Audio</td><td><b>3.2</b> | 3.1 | <b>2.9</b></td></tr><tr><td rowspan=\"8\">S2TT</td><td rowspan=\"5\"><b>CoVoST2</b> <br>en-de | de-en | <br>en-zh | zh-en</td><td>SALMONN</td><td rowspan=\"5\">BLEU </td><td>18.6 | - | 33.1 | -</td></tr><tr><td>SpeechLLaMA</td><td>- | 27.1 | - | 12.3</td></tr><tr><td>BLSP</td><td>14.1 | - | - | -</td></tr><tr><td>Qwen-Audio</td><td>25.1 | <b>33.9</b> | 41.5 | 15.7</td></tr><tr><td>Qwen2-Audio</td><td><b>29.6</b> | 33.6 | <b>45.6</b> | <b>24.0</b></td></tr>\n<tr><td rowspan=\"3\"><b>CoVoST2</b> <br>es-en | fr-en | it-en |</td><td>SpeechLLaMA</td><td rowspan=\"3\">BLEU </td><td>27.9 | 25.2 | 25.9</td></tr><tr><td>Qwen-Audio</td><td><b>39.7 | 38.5 | 36.0</b></td></tr><tr><td>Qwen2-Audio</td><td>38.7 | 37.2 | 35.2</td></tr><tr><td rowspan=\"3\">SER</td><td rowspan=\"3\"><b>Meld</b></td><td>WavLM-large</td><td rowspan=\"3\">ACC </td><td>0.542</td></tr><tr><td>Qwen-Audio</td><td><b>0.557</b></td></tr><tr><td>Qwen2-Audio</td><td>0.535</td></tr><tr><td rowspan=\"4\">VSC</td><td rowspan=\"4\"><b>VocalSound</b></td><td>CLAP</td><td rowspan=\"4\">ACC </td><td>0.4945</td></tr><tr><td>Pengi</td><td>0.6035</td></tr><tr><td>Qwen-Audio</td><td>0.9289</td></tr><tr><td>Qwen2-Audio</td><td><b>0.9395</b></td></tr>\n<tr><td>AIR-Bench <br></td><td><b>Chat Benchmark</b><br>Speech | Sound |<br> Music | Mixed-Audio</td><td>SALMONN<br>BLSP<br>Pandagpt<br>Macaw-LLM<br>SpeechGPT<br>Next-gpt<br>Qwen-Audio<br>Gemini-1.5-pro<br>Qwen2-Audio</td><td>GPT-4 </td><td>6.16 | 6.28 | 5.95 | 6.08<br>6.17 | 5.55 | 5.08 | 5.33<br>3.58 | 5.46 | 5.06 | 4.25<br>0.97 | 1.01 | 0.91 | 1.01<br>1.57 | 0.95 | 0.95 | 4.13<br>3.86 | 4.76 | 4.18 | 4.13<br>6.47 | <b>6.95</b> | 5.52 | 6.08<br>6.97 | 5.49 | 5.06 | 5.27<br><b>7.24</b> | 6.83 | <b>6.73</b> | <b>6.42</b></td></tr></tbody></table>\n\n\n我们提供了以上**所有**评测脚本以供复现我们的实验结果。请阅读 [eval_audio/EVALUATION.md](eval_audio/EVALUATION.md) 了解更多信息。\n\n## 部署要求\nThe code of Qwen2-Audio has been in the latest Hugging face transformers and we advise you to build from source with command `pip install git+https://github.com/huggingface/transformers`, or you might encounter the following error:\nQwen2-Audio的代码已经包含在最新的 Hugging Face Transformers 的主分支中，我们建议您使用命令`pip install git+https://github.com/huggingface/transformers`\n```\nKeyError: 'qwen2-audio'\n```\n## 快速使用\n我们提供简单的示例来说明如何利用 🤗 Transformers 快速使用 Qwen2-Audio-7B 和 Qwen2-Audio-7B-Instruct。\n在开始前，请确保你已经配置好环境并安装好相关的代码包。最重要的是，确保你满足上述要求，然后安装相关的依赖库。\n接下来你可以开始使用 Transformers 或者 ModelScope 来使用我们的模型。目前Qwen2-Audio-7B 及 Qwen2-Audio-7B-Instruct 模型处理30秒以内的音频表现更佳。\n#### 🤗 Hugging Face Transformers\n如希望使用 Qwen2-Audio-7B-Instruct 进行推理，我们分别演示语音聊天和音频分析的交互方式，所需要写的只是如下所示的数行代码。\n##### 语音聊天推理\n在语音聊天模式下，用户可以自由地与 Qwen2-Audio 进行语音交互，无需文字输入：\n```python\nfrom io import BytesIO\nfrom urllib.request import urlopen\nimport librosa\nfrom transformers import Qwen2AudioForConditionalGeneration, AutoProcessor\n\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\")\nmodel = Qwen2AudioForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\", device_map=\"auto\")\n\nconversation = [\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/guess_age_gender.wav\"},\n    ]},\n    {\"role\": \"assistant\", \"content\": \"Yes, the speaker is female and in her twenties.\"},\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/translate_to_chinese.wav\"},\n    ]},\n]\ntext = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\naudios = []\nfor message in conversation:\n    if isinstance(message[\"content\"], list):\n        for ele in message[\"content\"]:\n            if ele[\"type\"] == \"audio\":\n                audios.append(librosa.load(\n                    BytesIO(urlopen(ele['audio_url']).read()), \n                    sr=processor.feature_extractor.sampling_rate)[0]\n                )\n\ninputs = processor(text=text, audios=audios, return_tensors=\"pt\", padding=True)\ninputs.input_ids = inputs.input_ids.to(\"cuda\")\n\ngenerate_ids = model.generate(**inputs, max_length=256)\ngenerate_ids = generate_ids[:, inputs.input_ids.size(1):]\n\nresponse = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n```\n##### 音频分析推理\n在音频分析中，用户可以提供音频和文字问题来实现对音频的分析：\n```python\nfrom io import BytesIO\nfrom urllib.request import urlopen\nimport librosa\nfrom transformers import Qwen2AudioForConditionalGeneration, AutoProcessor\n\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\")\nmodel = Qwen2AudioForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\", device_map=\"auto\")\n\nconversation = [\n    {'role': 'system', 'content': 'You are a helpful assistant.'}, \n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/glass-breaking-151256.mp3\"},\n        {\"type\": \"text\", \"text\": \"What's that sound?\"},\n    ]},\n    {\"role\": \"assistant\", \"content\": \"It is the sound of glass shattering.\"},\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"text\", \"text\": \"What can you do when you hear that?\"},\n    ]},\n    {\"role\": \"assistant\", \"content\": \"Stay alert and cautious, and check if anyone is hurt or if there is any damage to property.\"},\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/1272-128104-0000.flac\"},\n        {\"type\": \"text\", \"text\": \"What does the person say?\"},\n    ]},\n]\ntext = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\naudios = []\nfor message in conversation:\n    if isinstance(message[\"content\"], list):\n        for ele in message[\"content\"]:\n            if ele[\"type\"] == \"audio\":\n                audios.append(\n                    librosa.load(\n                        BytesIO(urlopen(ele['audio_url']).read()), \n                        sr=processor.feature_extractor.sampling_rate)[0]\n                )\n\ninputs = processor(text=text, audios=audios, return_tensors=\"pt\", padding=True)\ninputs.input_ids = inputs.input_ids.to(\"cuda\")\n\ngenerate_ids = model.generate(**inputs, max_length=256)\ngenerate_ids = generate_ids[:, inputs.input_ids.size(1):]\n\nresponse = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n```\n##### 批量推理\n我们也支持批量推理：\n```python\nfrom io import BytesIO\nfrom urllib.request import urlopen\nimport librosa\nfrom transformers import Qwen2AudioForConditionalGeneration, AutoProcessor\n\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\")\nmodel = Qwen2AudioForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\", device_map=\"auto\")\n\nconversation1 = [\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/glass-breaking-151256.mp3\"},\n        {\"type\": \"text\", \"text\": \"What's that sound?\"},\n    ]},\n    {\"role\": \"assistant\", \"content\": \"It is the sound of glass shattering.\"},\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/f2641_0_throatclearing.wav\"},\n        {\"type\": \"text\", \"text\": \"What can you hear?\"},\n    ]}\n]\n\nconversation2 = [\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/1272-128104-0000.flac\"},\n        {\"type\": \"text\", \"text\": \"What does the person say?\"},\n    ]},\n]\n\nconversations = [conversation1, conversation2]\n\ntext = [processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False) for conversation in conversations]\n\naudios = []\nfor conversation in conversations:\n    for message in conversation:\n        if isinstance(message[\"content\"], list):\n            for ele in message[\"content\"]:\n                if ele[\"type\"] == \"audio\":\n                    audios.append(\n                        librosa.load(\n                            BytesIO(urlopen(ele['audio_url']).read()), \n                            sr=processor.feature_extractor.sampling_rate)[0]\n                    )\n\ninputs = processor(text=text, audios=audios, return_tensors=\"pt\", padding=True)\ninputs['input_ids'] = inputs['input_ids'].to(\"cuda\")\ninputs.input_ids = inputs.input_ids.to(\"cuda\")\n\ngenerate_ids = model.generate(**inputs, max_length=256)\ngenerate_ids = generate_ids[:, inputs.input_ids.size(1):]\n\nresponse = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n```\n运行Qwen2-Audio-7B同样非常简单。\n```python\nfrom io import BytesIO\nfrom urllib.request import urlopen\nimport librosa\nfrom transformers import AutoProcessor, Qwen2AudioForConditionalGeneration\n\nmodel = Qwen2AudioForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-Audio-7B\" ,trust_remote_code=True)\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-Audio-7B\" ,trust_remote_code=True)\n\nprompt = \"<|audio_bos|><|AUDIO|><|audio_eos|>Generate the caption in English:\"\nurl = \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Audio/glass-breaking-151256.mp3\"\naudio, sr = librosa.load(BytesIO(urlopen(url).read()), sr=processor.feature_extractor.sampling_rate)\ninputs = processor(text=prompt, audios=audio, return_tensors=\"pt\")\n\ngenerated_ids = model.generate(**inputs, max_length=256)\ngenerated_ids = generated_ids[:, inputs.input_ids.size(1):]\nresponse = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n```\n#### 🤖 ModelScope\n我们强烈建议用户，特别是中国大陆地区的用户，使用 ModelScope。`snapshot_download` 可以帮助您解决下载检查点时遇到的问题。\n<br>\n## Demo\n### Web UI\n我们提供了 Web UI 的 demo 供用户使用。在开始前，确保已经安装如下代码库：\n```\npip install -r requirements_web_demo.txt\n```\n随后运行如下命令，并点击生成链接：\n```\npython demo/web_demo_audio.py\n```\n<br>\n\n## 样例展示\n更多样例将更新于[通义千问博客](https://qwenlm.github.io/blog/qwen2-audio)中的 Qwen2-Audio 博客。\n\n## 团队招聘\n\n我们是通义千问语音多模态团队，致力于以通义千问为核心，拓展音频多模态理解和生成能力，实现自由灵活的音频交互。目前团队蓬勃发展中，如有意向实习或全职加入我们，请发送简历至 `qwen_audio@list.alibaba-inc.com`.\n<br>\n\n## 使用协议\n\n请查看每个模型在其 Hugging Face 仓库中的许可证。您无需提交商业使用申请。\n<br>\n\n\n\n## 引用\n\n如果你觉得我们的论文和代码对你的研究有帮助，请考虑 :star: 和引用 :pencil: :)\n\n```BibTeX\n@article{Qwen-Audio,\n  title={Qwen-Audio: Advancing Universal Audio Understanding via Unified Large-Scale Audio-Language Models},\n  author={Chu, Yunfei and Xu, Jin and Zhou, Xiaohuan and Yang, Qian and Zhang, Shiliang and Yan, Zhijie  and Zhou, Chang and Zhou, Jingren},\n  journal={arXiv preprint arXiv:2311.07919},\n  year={2023}\n}\n```\n\n```BibTeX\n@article{Qwen2-Audio,\n  title={Qwen2-Audio Technical Report},\n  author={Chu, Yunfei and Xu, Jin and Yang, Qian and Wei, Haojie and Wei, Xipin and Guo,  Zhifang and Leng, Yichong and Lv, Yuanjun and He, Jinzheng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},\n  journal={arXiv preprint arXiv:2407.10759},\n  year={2024}\n}\n```\n## 联系我们\n\n如果你想给我们的研发团队和产品团队留言，请通过邮件（`qianwen_opensource@alibabacloud.com`）联系我们。\n<br>\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/qwen_voice/__init__.py\nimport time\nfrom typing import List, Tuple, Dict, Any, Union\nimport base64\nimport json\nimport librosa\nimport io\nfrom transformers import Qwen2AudioForConditionalGeneration, AutoProcessor\n\ndef get_meta(self):\n    return [\n        {\n            \"model_deploy_type\": \"proprietary\",\n            \"backend\": \"transformers\",\n            \"message_format\": True\n        }\n    ]\n\ndef process_input(ins: Union[str, List[Dict[str, Any]], Dict[str, Any]]):\n    if isinstance(ins, list) or isinstance(ins, dict):\n        return ins\n\n    content = []\n    try:\n        ins_json = json.loads(ins)\n    except:\n        return ins\n\n    if isinstance(ins_json, dict):\n        return ins_json\n\n    for item in ins_json:\n        if (\"audio\" in item or \"audio_url\" in item) and \"type\" not in item:\n            audio_data = item.get(\"audio\", item.get(\"audio_url\", \"\"))\n            if not audio_data.startswith(\"data:\"):\n                audio_data = \"data:audio/wav;base64,\" + audio_data\n\n            other_fields = {k: v for k, v in item.items() if k not in [\"audio\", \"text\", \"audio_url\"]}\n            content.append(\n                {\n                    \"audio_url\": {\"url\": audio_data, **other_fields},\n                    \"type\": \"audio\",\n                }\n            )\n\n        if \"text\" in item and \"type\" not in item:\n            text_data = item[\"text\"]\n            content.append({\"text\": text_data, \"type\": \"text\"})\n\n        if \"type\" in item and item[\"type\"] in [\"text\", \"audio\"]:\n            content.append(item)\n\n    if not content:\n        return ins\n\n    return content\n\ndef stream_chat(\n    self,\n    tokenizer,\n    ins: str,\n    his: List[Dict[str, str]] = [],\n    max_length: int = 4096,\n    top_p: float = 0.95,\n    temperature: float = 0.1,\n    **kwargs,\n):\n    messages = [\n        {\"role\": message[\"role\"], \"content\": process_input(message[\"content\"])}\n        for message in his\n    ] + [{\"role\": \"user\", \"content\": process_input(ins)}]\n\n    text = self.processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n    audios = []\n    for message in messages:\n        if isinstance(message[\"content\"], list):\n            for ele in message[\"content\"]:\n                if ele[\"type\"] == \"audio\":\n                    audio_data = ele[\"audio_url\"].split(\",\")[1]\n                    audio_bytes = base64.b64decode(audio_data)\n                    audio, _ = librosa.load(io.BytesIO(audio_bytes), sr=self.processor.feature_extractor.sampling_rate)\n                    audios.append(audio)\n\n    inputs = self.processor(text=text, audios=audios, return_tensors=\"pt\", padding=True)\n    inputs.input_ids = inputs.input_ids.to(\"cuda\")\n\n    start_time = time.monotonic()\n    generate_ids = self.generate(**inputs, max_length=max_length)\n    generate_ids = generate_ids[:, inputs.input_ids.size(1):]\n\n    response = self.processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n    time_cost = time.monotonic() - start_time\n\n    return [\n        (\n            response,\n            {\n                \"metadata\": {\n                    \"request_id\": \"\",\n                    \"input_tokens_count\": inputs.input_ids.size(1),\n                    \"generated_tokens_count\": generate_ids.size(1),\n                    \"time_cost\": time_cost,\n                    \"first_token_time\": 0,\n                    \"speed\": float(generate_ids.size(1)) / time_cost if time_cost > 0 else 0,\n                }\n            },\n        )\n    ]\n\ndef init_model(model_dir, infer_params: Dict[str, str] = {}, sys_conf: Dict[str, str] = {}):\n    processor = AutoProcessor.from_pretrained(model_dir)\n    model = Qwen2AudioForConditionalGeneration.from_pretrained(model_dir, device_map=\"auto\")\n    model.processor = processor\n    import types\n    model.stream_chat = types.MethodType(stream_chat, model)\n    model.get_meta = types.MethodType(get_meta, model)    \n    return (model, None)\n\n     \n\n下面是用户的需求：\n\n@/Users/allwefantasy/projects/byzer-llm/src/byzerllm/qwen_voice/__init__.py 需要避免 .to(\"cuda\") 这样的动作抛出错误\n严格遵循 SEARCH/REPLACE 格式,如果是python代码,注意空格和缩进\n"}, {"role": "assistant", "content": "```python\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/qwen_voice/__init__.py\n<<<<<<< SEARCH\n    inputs.input_ids = inputs.input_ids.to(\"cuda\")\n=======\n    inputs.input_ids = inputs.input_ids.to(model.device)\n>>>>>>> REPLACE\n```"}], "model": "deepseek_chat", "yaml_file": "actions/039_chat_action.yml"}
{"instruction": "@simple_retrieval.py 修正jieba 的 ImportError\n严格遵循 SEARCH/REPLACE 格式,如果是python代码,注意空格和缩进", "conversations": [{"role": "user", "content": "\n如果你需要生成代码，对于每个需要更改的文件,你需要按 *SEARCH/REPLACE block* 的格式进行生成。\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The opening fence and code language, eg: ```python\n2. The file path alone on a line, starting with \"##File:\" and verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n*SEARCH/REPLACE* blocks will replace *all* matching occurrences.\nInclude enough lines to make the SEARCH blocks unique.\n\nInclude *ALL* the code being searched and replaced!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n下面我们来看一个例子：\n\n当前项目目录结构：\n1. 项目根目录： /tmp/projects/mathweb\n2. 项目子目录/文件列表(类似tree 命令输出)\nflask/\n    app.py\n    templates/\n        index.html\n    static/\n        style.css\n\n用户需求： Change get_factorial() to use math.factorial\n\n回答： To make this change we need to modify `/tmp/projects/mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n用户需求： Refactor hello() into its own file.\n\n回答：To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/hello.py\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/main.py\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n现在让我们开始一个新的任务:\n\n当前项目目录结构：\n1. 项目根目录： /Users/allwefantasy/projects/byzer-llm\n2. 项目子目录/文件列表(类似tree 命令输出)：\nsetup.py\ntests/\n    test_emb_rerank.py\n    test_deepseek_code.py\n    test_prompt.py\n    test_model_meta.py\n    __init__.py\n    test_transformers_deploy.py\n    test_data_analysis.py\n    test_yi.py\n    test_deploy.py\n    functions_suite.py\n    test_template.py\n    auto/\n        test_backend_llama_cpp.py\n        __init__.py\nsetup-machine/\n    convert_safetensor.py\nnotebooks/\n    test.py\n    test_sensevoice.py\nsrc/\n    byzerllm/\n        lang.py\n        store.py\n        version.py\n        log.py\n        __init__.py\n        types.py\n        easy_byzerllm_command.py\n        byzerllm_command.py\n        command_args.py\n        bark/\n            generation.py\n            __init__.py\n            model.py\n            api.py\n            model_fine.py\n            bark_voice.py\n        moss/\n            finetune_moss.py\n            __init__.py\n            moss_inference.py\n            models/\n                configuration_moss.py\n                quantization.py\n                custom_autotune.py\n                __init__.py\n                modeling_moss.py\n                tokenization_moss.py\n        llama2/\n            __init__.py\n        records/\n            __init__.py\n        visualglm/\n            __init__.py\n        auto/\n            __init__.py\n            backend_llama_cpp.py\n            backend_vllm.py\n            backend_ds.py\n        alpha_moss/\n            __init__.py\n        processor/\n            __init__.py\n        sensevoice/\n            __init__.py\n        qwen/\n            __init__.py\n        utils/\n            json_repaire.py\n            object_store_ref_util.py\n            __init__.py\n            types.py\n            tokenizer.py\n            openai_utils.py\n            ray_utils.py\n            text_generator.py\n            emb.py\n            connect_ray.py\n            testing.py\n            langutil.py\n            nontext.py\n            fulltune/\n                deepspeed_trainner.py\n                __init__.py\n                launch.py\n                trainner.py\n                base_model/\n                    __init__.py\n                    configuration_baichuan.py\n                    modeling_baichuan.py\n                pretrain/\n                    __init__.py\n                    convert_to_transformers.py\n            metrics/\n                __init__.py\n            config/\n                __init__.py\n            retrieval/\n                rest.py\n                __init__.py\n                udf.py\n            sft/\n                argument.py\n                collator.py\n                qlora.py\n                __init__.py\n                model.py\n                dataset.py\n                loss.py\n                merge_lora.py\n                trainer.py\n            client/\n                message_utils.py\n                math_utils.py\n                byzerllm_client.py\n                parallel_utils.py\n                __init__.py\n                types.py\n                code_utils.py\n                img_utils.py\n                entrypoints/\n                    __init__.py\n                    openai/\n                        protocol.py\n                        __init__.py\n                        serve.py\n                        serving_chat.py\n                        api_server.py\n                        serving_engine.py\n                        serving_completion.py\n                        tool.py\n        qwen_voice/\n            __init__.py\n        chatglm2/\n            __init__.py\n        baichuan/\n            __init__.py\n        m3e/\n            __init__.py\n        stable_diffusion/\n            config.py\n            __init__.py\n            logger.py\n            shared.py\n            model.py\n            utils.py\n            images.py\n            diffusion/\n                __init__.py\n                embeddings.py\n                utils.py\n                networks/\n                    lyco.py\n                    __init__.py\n                upscalers/\n                    __init__.py\n                    multidiffusion.py\n                    samplers.py\n                piplines/\n                    __init__.py\n                    lpw.py\n                    diffusers.py\n            lib/\n                __init__.py\n                diffusers/\n                    __init__.py\n                    scheduler.py\n            api/\n                __init__.py\n                plugin.py\n                models/\n                    diffusion.py\n                    __init__.py\n                diffusion/\n                    __init__.py\n                    pipelines/\n                        __init__.py\n                        diffusers.py\n                events/\n                    generation.py\n                    __init__.py\n        bge/\n            __init__.py\n        bge_rerank/\n            __init__.py\n        apps/\n            client.py\n            qa_strategy.py\n            __init__.py\n            builder.py\n            vector_db.py\n            utils.py\n            qa.py\n            byzer_storage/\n                memory_model_based.py\n                command.py\n                env.py\n                train_pt.py\n                train_sft.py\n                __init__.py\n                train_dpo.py\n                simple_api.py\n                generate_sft_data.py\n            byzer_sql/\n                __init__.py\n            llama_index/\n                byzerai_vectordb.py\n                byzerai.py\n                byzerai_embedding.py\n                __init__.py\n                collection_manager.py\n                byzerai_index_store.py\n                byzerai_docstore.py\n                simple_retrieval.py\n                byzerai_kvstore.py\n            agent/\n                conversable_agent.py\n                registry.py\n                groupchat.py\n                user_proxy_agent.py\n                __init__.py\n                agent.py\n                extensions/\n                    simple_retrieval_client.py\n                    planner.py\n                    python_codesandbox_agent.py\n                    preview_file_agent.py\n                    llama_index_retrieval_agent.py\n                    visualization_agent.py\n                    assistant_agent.py\n                    load_data_agent.py\n                    retrieval_agent.py\n                    output_agent.py\n                    __init__.py\n                    byzer_engine_agent.py\n                    subquestion_agent.py\n                    sql_reviewer_agent.py\n                    spark_sql_agent.py\n                    rhetorical_agent.py\n                    data_analysis.py\n                    data_analysis_pipeline_agent.py\n                    common_agent.py\n                    query_rewrite/\n                        time.py\n                        rhetorical.py\n                        __init__.py\n                        condition.py\n                        context.py\n                store/\n                    memory_store.py\n                    __init__.py\n                    stores.py\n        saas/\n            __init__.py\n            chatglm/\n                __init__.py\n            sparkdesk/\n                __init__.py\n            gemini/\n                __init__.py\n            azure_openai/\n                __init__.py\n            azure/\n                __init__.py\n            qianfan/\n                __init__.py\n            claude/\n                __init__.py\n            qianwen/\n                __init__.py\n            volcengine/\n                __init__.py\n            minimax/\n                __init__.py\n            baichuan/\n                __init__.py\n            qianwen_vl/\n                __init__.py\n            official_openai/\n                __init__.py\n            zhipu/\n                __init__.py\n            aws_bedrock/\n                __init__.py\n            openai/\n                __init__.py\n        falcon/\n            __init__.py\n        chatglm6b/\n            trainer_seq2seq.py\n            arguments.py\n            __init__.py\n            finetune.py\n            tunning/\n                __init__.py\n                infer.py\n                finetune.py\n                utils/\n                    config.py\n                    __init__.py\n                    other.py\n                    common.py\n                    seq2seq.py\n        starcode/\n            __init__.py\n        qwen_vl_chat/\n            __init__.py\n        zephyr/\n            __init__.py\n        dolly/\n            generate.py\n            __init__.py\n            consts.py\n            dolly_inference.py\n            trainer.py\n        llama/\n            __init__.py\n        whisper/\n            whisper_inference.py\n            __init__.py\n下面是一些文件路径以及每个文件对应的源码：\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/saas/official_openai/__init__.py\nimport time\nfrom typing import List, Tuple, Dict, Any, Union\nimport httpx\nfrom openai import OpenAI\nimport base64\nimport io\nimport json\nimport ray\nfrom byzerllm.utils.types import (\n    BlockVLLMStreamServer,\n    StreamOutputs,\n    SingleOutput,\n    SingleOutputMeta,\n    BlockBinaryStreamServer,\n)\nfrom byzerllm.utils.langutil import asyncfy_with_semaphore\nimport threading\nimport asyncio\nimport traceback\nimport uuid\nimport tempfile        \n\nclass CustomSaasAPI:\n\n    def __init__(self, infer_params: Dict[str, str]) -> None:\n\n        self.api_key = infer_params[\"saas.api_key\"]\n        self.model = infer_params.get(\"saas.model\", \"gpt-3.5-turbo-1106\")\n\n        other_params = {}\n\n        if \"saas.api_base\" in infer_params:\n            other_params[\"api_base\"] = infer_params[\"saas.api_base\"]\n\n        if \"saas.api_version\" in infer_params:\n            other_params[\"api_version\"] = infer_params[\"saas.api_version\"]\n\n        if \"saas.api_type\" in infer_params:\n            other_params[\"api_type\"] = infer_params[\"saas.api_type\"]\n\n        if \"saas.base_url\" in infer_params:\n            other_params[\"base_url\"] = infer_params[\"saas.base_url\"]\n\n        if \"saas.timeout\" in infer_params:\n            other_params[\"timeout\"] = float(infer_params[\"saas.timeout\"])\n\n        self.max_retries = int(infer_params.get(\"saas.max_retries\", 10))\n\n        self.meta = {\n            \"model_deploy_type\": \"saas\",\n            \"backend\": \"saas\",\n            \"support_stream\": True,\n            \"model_name\": self.model,\n        }\n\n        self.meta[\"embedding_mode\"] = \"embedding\" in self.model.lower()\n\n        self.proxies = infer_params.get(\"saas.proxies\", None)\n        self.local_address = infer_params.get(\"saas.local_address\", \"0.0.0.0\")\n\n        if not self.proxies:\n            self.client = OpenAI(**other_params, api_key=self.api_key)\n        else:\n            self.client = OpenAI(\n                **other_params,\n                api_key=self.api_key,\n                http_client=httpx.Client(\n                    proxies=self.proxies,\n                    transport=httpx.HTTPTransport(local_address=self.local_address),\n                ),\n            )\n\n        try:\n            ray.get_actor(\"BLOCK_VLLM_STREAM_SERVER\")\n        except ValueError:\n            try:\n                ray.remote(BlockVLLMStreamServer).options(\n                    name=\"BLOCK_VLLM_STREAM_SERVER\",\n                    lifetime=\"detached\",\n                    max_concurrency=1000,\n                ).remote()\n            except Exception as e:\n                pass\n        try:\n            ray.get_actor(\"BlockBinaryStreamServer\")\n        except ValueError:\n            try:\n                ray.remote(BlockBinaryStreamServer).options(\n                    name=\"BlockBinaryStreamServer\",\n                    lifetime=\"detached\",\n                    max_concurrency=1000,\n                ).remote()\n            except Exception as e:\n                pass\n\n    # saas/proprietary\n    def get_meta(self):\n        return [self.meta]\n\n    def process_input(self, ins: Union[str, List[Dict[str, Any]], Dict[str, Any]]):\n\n        if isinstance(ins, list) or isinstance(ins, dict):\n            return ins\n\n        content = []\n        try:\n            ins_json = json.loads(ins)\n        except:\n            return ins\n\n        ## speech\n        if isinstance(ins_json, dict):\n            return ins_json\n\n        content = []\n        #     [\n        #     {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n        #     {\n        #       \"type\": \"image_url\",\n        #       \"image_url\": {\n        #         \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n        #         \"detail\": \"high\"\n        #       },\n        #     },\n        #   ],\n        for item in ins_json:\n            # for format like this: {\"image\": \"xxxxxx\", \"text\": \"What’s in this image?\",\"detail\":\"high\"}\n            # or {\"image\": \"xxxxxx\"}, {\"text\": \"What’s in this image?\"}\n            if (\"image\" in item or \"image_url\" in item) and \"type\" not in item:\n                image_data = item.get(\"image\", item.get(\"image_url\", \"\"))\n                ## \"data:image/jpeg;base64,\"\n                if not image_data.startswith(\"data:\"):\n                    image_data = \"data:image/jpeg;base64,\" + image_data\n\n                ## get the other fields except image/text/image_url\n                other_fields = {\n                    k: v\n                    for k, v in item.items()\n                    if k not in [\"image\", \"text\", \"image_url\"]\n                }\n                content.append(\n                    {\n                        \"image_url\": {\"url\": image_data, **other_fields},\n                        \"type\": \"image_url\",\n                    }\n                )            \n\n            if \"text\" in item and \"type\" not in item:\n                text_data = item[\"text\"]\n                content.append({\"text\": text_data, \"type\": \"text\"})\n\n            ## for format like this: {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n            ## {\"type\": \"image_url\", \"image_url\": {\"url\":\"\",\"detail\":\"high\"}}\n            ## this is the standard format, just return it\n            if \"type\" in item and item[\"type\"] == \"text\":\n                content.append(item)\n\n            if \"type\" in item and item[\"type\"] == \"image_url\":\n                content.append(item)\n\n        if not content:\n            return ins\n\n        return content\n\n    async def async_embed_query(self, ins: str, **kwargs):\n        resp = await asyncfy_with_semaphore(\n            lambda: self.client.embeddings.create(input=[ins], model=self.model)\n        )()\n        embedding = resp.data[0].embedding\n        usage = resp.usage\n        return (\n            embedding,\n            {\n                \"metadata\": {\n                    \"input_tokens_count\": usage.prompt_tokens,\n                    \"generated_tokens_count\": 0,\n                }\n            },\n        )\n\n    def embed_query(self, ins: str, **kwargs):\n        resp = self.client.embeddings.create(input=[ins], model=self.model)\n        embedding = resp.data[0].embedding\n        usage = resp.usage\n        return (\n            embedding,\n            {\n                \"metadata\": {\n                    \"input_tokens_count\": usage.prompt_tokens,\n                    \"generated_tokens_count\": 0,\n                }\n            },\n        )\n\n    async def async_text_to_speech(\n        self, stream: bool, ins: str, voice: str, chunk_size: int = None, **kwargs\n    ):\n        if stream:\n            server = ray.get_actor(\"BlockBinaryStreamServer\")\n            request_id = [None]\n\n            def writer():\n                try:\n                    request_id[0] = str(uuid.uuid4())\n                    with self.client.with_streaming_response.audio.speech.create(\n                        model=self.model, voice=voice, input=ins, **kwargs\n                    ) as response:\n                        for chunk in response.iter_bytes(chunk_size):\n                            input_tokens_count = 0\n                            generated_tokens_count = 0\n                            ray.get(\n                                server.add_item.remote(\n                                    request_id[0],\n                                    StreamOutputs(\n                                        outputs=[\n                                            SingleOutput(\n                                                text=chunk,\n                                                metadata=SingleOutputMeta(\n                                                    input_tokens_count=input_tokens_count,\n                                                    generated_tokens_count=generated_tokens_count,\n                                                ),\n                                            )\n                                        ]\n                                    ),\n                                )\n                            )\n                except:\n                    traceback.print_exc()\n                ray.get(server.mark_done.remote(request_id[0]))\n\n            threading.Thread(target=writer, daemon=True).start()\n\n            time_count = 10 * 100\n            while request_id[0] is None and time_count > 0:\n                time.sleep(0.01)\n                time_count -= 1\n\n            if request_id[0] is None:\n                raise Exception(\"Failed to get request id\")\n\n            def write_running():\n                return ray.get(server.add_item.remote(request_id[0], \"RUNNING\"))\n\n            await asyncio.to_thread(write_running)\n            return [\n                (\n                    \"\",\n                    {\n                        \"metadata\": {\n                            \"request_id\": request_id[0],\n                            \"stream_server\": \"BlockBinaryStreamServer\",\n                        }\n                    },\n                )\n            ]\n\n        start_time = time.monotonic()\n        with io.BytesIO() as output:\n            async with asyncfy_with_semaphore(\n                lambda: self.client.with_streaming_response.audio.speech.create(\n                    model=self.model, voice=voice, input=ins, **kwargs\n                )\n            )() as response:\n                for chunk in response.iter_bytes():\n                    output.write(chunk)\n\n            base64_audio = base64.b64encode(output.getvalue()).decode()\n            time_cost = time.monotonic() - start_time\n            return [\n                (\n                    base64_audio,\n                    {\n                        \"metadata\": {\n                            \"request_id\": \"\",\n                            \"input_tokens_count\": 0,\n                            \"generated_tokens_count\": 0,\n                            \"time_cost\": time_cost,\n                            \"first_token_time\": 0,\n                            \"speed\": 0,\n                        }\n                    },\n                )\n            ]\n\n    async def async_speech_to_text(\n        self,\n        audio: str,\n        response_format: str = \"verbose_json\",\n        timestamp_granularities: List[str] = [\"word\",\"segment\"],\n    ):               \n        # Extract audio format and base64 data\n        data_prefix = \"data:audio/\"\n        base64_prefix = \";base64,\"\n        if not audio.startswith(data_prefix):\n            raise ValueError(\"Invalid audio data format\")\n        \n        format_end = audio.index(base64_prefix)\n        audio_format = audio[len(data_prefix):format_end]\n        base64_data = audio[format_end + len(base64_prefix):]\n        \n        # Decode the base64 audio data\n        audio_data = base64.b64decode(base64_data)\n\n        # Create a temporary file with the correct extension\n        with tempfile.NamedTemporaryFile(delete=False, suffix=f'.{audio_format}') as temp_audio_file:\n            temp_audio_file.write(audio_data)\n            temp_audio_file_path = temp_audio_file.name\n\n        try:\n            start_time = time.monotonic()\n                   \n            with open(temp_audio_file_path, \"rb\") as audio_file:                \n                transcription = self.client.audio.transcriptions.create(\n                    model=self.model or \"whisper-1\",\n                    file=audio_file,\n                    response_format=response_format,\n                    timestamp_granularities=timestamp_granularities,\n                )\n            time_cost = time.monotonic() - start_time\n\n            return [\n                (\n                    json.dumps(transcription.to_dict(), ensure_ascii=True),\n                    {\n                        \"metadata\": {\n                            \"request_id\": \"\",\n                            \"input_tokens_count\": 0,\n                            \"generated_tokens_count\": 0,\n                            \"time_cost\": time_cost,\n                            \"first_token_time\": 0,\n                            \"speed\": 0,\n                        }\n                    },\n                )\n            ]\n        finally:\n            # Clean up the temporary file\n            import os\n            os.unlink(temp_audio_file_path)\n\n    def image_to_text(self, ins: str, **kwargs):\n        pass\n\n    async def async_text_to_image(\n        self, stream: bool, input: str, size: str, quality: str, n: int, **kwargs\n    ):\n        if stream:\n            raise Exception(\"Stream not supported for text to image\")\n        start_time = time.monotonic()\n        response = await asyncfy_with_semaphore(\n            lambda: self.client.images.generate(\n                model=self.model,\n                prompt=input,\n                size=size,\n                quality=quality,\n                n=1,\n                response_format=\"b64_json\",\n                **kwargs,\n            )\n        )()\n        time_cost = time.monotonic() - start_time\n        base64_image = response.data[0].b64_json\n        return [\n            (\n                base64_image,\n                {\n                    \"metadata\": {\n                        \"request_id\": \"\",\n                        \"input_tokens_count\": 0,\n                        \"generated_tokens_count\": 0,\n                        \"time_cost\": time_cost,\n                        \"first_token_time\": 0,\n                        \"speed\": 0,\n                    }\n                },\n            )\n        ]\n\n    def text_to_text(self, ins: str, **kwargs):\n        pass\n\n    async def async_stream_chat(\n        self,\n        tokenizer,\n        ins: str,\n        his: List[Dict[str, Any]] = [],\n        max_length: int = 4096,\n        top_p: float = 0.7,\n        temperature: float = 0.9,\n        **kwargs,\n    ):\n\n        model = self.model\n\n        if \"model\" in kwargs:\n            model = kwargs[\"model\"]\n\n        messages = [\n            {\"role\": message[\"role\"], \"content\": self.process_input(message[\"content\"])}\n            for message in his\n        ] + [{\"role\": \"user\", \"content\": self.process_input(ins)}]\n\n        stream = kwargs.get(\"stream\", False)\n\n        ## content = [\n        ##    \"voice\": \"alloy\",\"input\": \"Hello, World!\",response_format: \"mp3\"]\n        last_message = messages[-1][\"content\"]\n\n        if isinstance(last_message, dict) and \"voice\" in last_message:\n            voice = last_message[\"voice\"]\n            response_format = last_message.get(\"response_format\", \"mp3\")\n            chunk_size = last_message.get(\"chunk_size\", None)\n            input = last_message[\"input\"]\n            return await self.async_text_to_speech(\n                stream=stream,\n                ins=input,\n                voice=voice,\n                chunk_size=chunk_size,\n                response_format=response_format,\n            )\n\n        if isinstance(last_message, dict) and \"input\" in last_message:\n            input = last_message[\"input\"]\n            size = last_message.get(\"size\", \"1024x1024\")\n            quality = last_message.get(\"quality\", \"standard\")\n            n = last_message.get(\"n\", 1)\n            return await self.async_text_to_image(\n                stream=stream, input=input, size=size, quality=quality, n=n\n            )\n\n        if isinstance(last_message, dict) and \"audio\" in last_message:            \n            audio_data = last_message.get(\"audio\", \"\")            \n            tpe = last_message.get(\"type\",\"wav\")\n            if not audio_data.startswith(\"data:\"):\n                audio_data = f\"data:audio/${tpe};base64,\" + audio_data                \n            return await self.async_speech_to_text(audio=audio_data)\n        \n        server = ray.get_actor(\"BLOCK_VLLM_STREAM_SERVER\")\n        request_id = [None]\n\n        def writer():\n            try:\n                r = \"\"\n                response = self.client.chat.completions.create(\n                    messages=messages,\n                    model=model,\n                    stream=True,\n                    max_tokens=max_length,\n                    temperature=temperature,\n                    top_p=top_p,\n                )\n                # input_tokens_count = 0\n                # generated_tokens_count = 0\n\n                request_id[0] = str(uuid.uuid4())\n\n                for chunk in response:\n                    content = chunk.choices[0].delta.content or \"\"\n                    r += content\n                    if hasattr(chunk, \"usage\") and chunk.usage:\n                        input_tokens_count = chunk.usage.prompt_tokens\n                        generated_tokens_count = chunk.usage.completion_tokens\n                    else:\n                        input_tokens_count = 0\n                        generated_tokens_count = 0\n                    ray.get(\n                        server.add_item.remote(\n                            request_id[0],\n                            StreamOutputs(\n                                outputs=[\n                                    SingleOutput(\n                                        text=r,\n                                        metadata=SingleOutputMeta(\n                                            input_tokens_count=input_tokens_count,\n                                            generated_tokens_count=generated_tokens_count,\n                                        ),\n                                    )\n                                ]\n                            ),\n                        )\n                    )\n            except:\n                traceback.print_exc()\n            ray.get(server.mark_done.remote(request_id[0]))\n\n        if stream:\n            threading.Thread(target=writer, daemon=True).start()\n\n            time_count = 60 * 100\n            while request_id[0] is None and time_count > 0:\n                time.sleep(0.01)\n                time_count -= 1\n\n            if request_id[0] is None:\n                raise Exception(\"Failed to get request id\")\n\n            def write_running():\n                return ray.get(server.add_item.remote(request_id[0], \"RUNNING\"))\n\n            await asyncio.to_thread(write_running)\n            return [\n                (\n                    \"\",\n                    {\n                        \"metadata\": {\n                            \"request_id\": request_id[0],\n                            \"stream_server\": \"BLOCK_VLLM_STREAM_SERVER\",\n                        }\n                    },\n                )\n            ]\n        else:\n            try:\n                start_time = time.monotonic()\n                response = await asyncfy_with_semaphore(\n                    lambda: self.client.chat.completions.create(\n                        messages=messages,\n                        model=model,\n                        max_tokens=max_length,\n                        temperature=temperature,\n                        top_p=top_p,\n                    )\n                )()\n\n                generated_text = response.choices[0].message.content\n                generated_tokens_count = response.usage.completion_tokens\n                input_tokens_count = response.usage.prompt_tokens\n                time_cost = time.monotonic() - start_time\n                return [\n                    (\n                        generated_text,\n                        {\n                            \"metadata\": {\n                                \"request_id\": response.id,\n                                \"input_tokens_count\": input_tokens_count,\n                                \"generated_tokens_count\": generated_tokens_count,\n                                \"time_cost\": time_cost,\n                                \"first_token_time\": 0,\n                                \"speed\": float(generated_tokens_count) / time_cost,\n                            }\n                        },\n                    )\n                ]\n            except Exception as e:\n                print(f\"Error: {e}\")\n                raise e\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/whisper/__init__.py\nimport whisper\nfrom typing import Dict, List, Tuple\nfrom byzerllm.utils.types import StopSequencesCriteria\n\nfrom typing import Dict, Any, List, Union\nfrom pyjava.storage import streaming_tar as STar\nimport json\nimport base64\nimport tempfile\nimport time\n\n\ndef get_meta(self):\n    return [\n        {\n            \"model_deploy_type\": \"proprietary\",\n            \"backend\": \"transformers\",\n            \"message_format\": True\n        }\n    ]\n\n\ndef process_input(ins: Union[str, List[Dict[str, Any]], Dict[str, Any]]):\n    # print(ins) \n    if isinstance(ins, list) or isinstance(ins, dict):\n        return ins\n\n    content = []\n    try:\n        ins_json = json.loads(ins)\n    except:\n        return ins\n\n    ## speech\n    if isinstance(ins_json, dict):\n        return ins_json\n\n    content = []\n    #     [\n    #     {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n    #     {\n    #       \"type\": \"image_url\",\n    #       \"image_url\": {\n    #         \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n    #         \"detail\": \"high\"\n    #       },\n    #     },\n    #   ],\n    for item in ins_json:\n        # for format like this: {\"image\": \"xxxxxx\", \"text\": \"What’s in this image?\",\"detail\":\"high\"}\n        # or {\"image\": \"xxxxxx\"}, {\"text\": \"What’s in this image?\"}\n        if (\"image\" in item or \"image_url\" in item) and \"type\" not in item:\n            image_data = item.get(\"image\", item.get(\"image_url\", \"\"))\n            ## \"data:image/jpeg;base64,\"\n            if not image_data.startswith(\"data:\"):\n                image_data = \"data:image/jpeg;base64,\" + image_data\n\n            ## get the other fields except image/text/image_url\n            other_fields = {\n                k: v for k, v in item.items() if k not in [\"image\", \"text\", \"image_url\"]\n            }\n            content.append(\n                {\n                    \"image_url\": {\"url\": image_data, **other_fields},\n                    \"type\": \"image_url\",\n                }\n            )\n\n        if \"text\" in item and \"type\" not in item:\n            text_data = item[\"text\"]\n            content.append({\"text\": text_data, \"type\": \"text\"})\n\n        ## for format like this: {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n        ## {\"type\": \"image_url\", \"image_url\": {\"url\":\"\",\"detail\":\"high\"}}\n        ## this is the standard format, just return it\n        if \"type\" in item and item[\"type\"] == \"text\":\n            content.append(item)\n\n        if \"type\" in item and item[\"type\"] == \"image_url\":\n            content.append(item)\n\n    if not content:\n        return ins\n\n    return content\n\n\ndef stream_chat(\n    self,\n    tokenizer,\n    ins: str,\n    his: List[Dict[str, str]] = [],\n    max_length: int = 4090,\n    top_p: float = 0.95,\n    temperature: float = 0.1,\n    **kwargs,\n):\n    messages = [\n        {\"role\": message[\"role\"], \"content\": process_input(message[\"content\"])}\n        for message in his\n    ] + [{\"role\": \"user\", \"content\": process_input(ins)}]\n\n    audio_input = messages[-1][\"content\"]\n\n    audio_data = audio_input.get(\"audio\", \"\")\n    audio = audio_input[\"audio\"]\n    response_format = audio_input.get(\"response_format\", \"json\")\n    timestamp_granularities = audio_input.get(\n        \"timestamp_granularities\", [\"word\", \"segment\"]\n    )\n    data_prefix = \"data:audio/\"\n    base64_prefix = \";base64,\"\n    if not audio.startswith(data_prefix):\n        raise ValueError(\"Invalid audio data format\")\n\n    format_end = audio.index(base64_prefix)\n    audio_format = audio[len(data_prefix) : format_end]\n    base64_data = audio[format_end + len(base64_prefix) :]\n\n    # Decode the base64 audio data\n    audio_data = base64.b64decode(base64_data)\n\n    # Create a temporary file with the correct extension\n    with tempfile.NamedTemporaryFile(\n        delete=False, suffix=f\".{audio_format}\"\n    ) as temp_audio_file:\n        temp_audio_file.write(audio_data)\n        temp_audio_file_path = temp_audio_file.name\n\n    try:\n        start_time = time.monotonic()\n        word_timestamps = \"word\" in timestamp_granularities\n        \n        result = self.transcribe(temp_audio_file_path, word_timestamps=word_timestamps)        \n            \n        time_cost = time.monotonic() - start_time\n        return [\n            (\n                json.dumps(result, ensure_ascii=False),\n                {\n                    \"metadata\": {\n                        \"request_id\": \"\",\n                        \"input_tokens_count\": 0,\n                        \"generated_tokens_count\": 0,\n                        \"time_cost\": time_cost,\n                        \"first_token_time\": 0,\n                        \"speed\": 0,\n                    }\n                },\n            )\n        ]\n    finally:\n        # Clean up the temporary file\n        import os\n\n        os.unlink(temp_audio_file_path)\n\n\ndef init_model(\n    model_dir, infer_params: Dict[str, str] = {}, sys_conf: Dict[str, str] = {}\n):\n    model = whisper.load_model(\"large\", download_root=model_dir)\n    import types\n\n    model.stream_chat = types.MethodType(stream_chat, model)\n    model.get_meta = types.MethodType(get_meta, model)\n    return (model, None)\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/notebooks/test_sensevoice.py\nimport sys\nimport os\n\n# Add the project root to the Python path\nsys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\")))\n\nfrom src.byzerllm.sensevoice import init_model, format_str_v3\nimport base64\nimport json\n\n\ndef main():\n    # Initialize the model\n    model_dir = \"/Users/allwefantasy/models/SenseVoiceSmall\"\n    model, _ = init_model(model_dir)\n\n    # Prepare test audio\n    with open(\n        \"/Users/allwefantasy/models/SenseVoiceSmall/example/zh.mp3\", \"rb\"\n    ) as audio_file:\n        audio_data = audio_file.read()\n        base64_audio = base64.b64encode(audio_data).decode(\"utf-8\")        \n        # Prepare input in the expected format\n        test_input = json.dumps(\n            {\n                            \"type\": \"audio\",\n                            \"audio\": f\"data:audio/wav;base64,{base64_audio}\",\n                        }\n        )\n\n        # Call stream_chat\n        result = model.stream_chat(tokenizer=None, ins = test_input)\n\n        # Process and print the result\n        if result and len(result) > 0:\n            output, metadata = result[0]\n            parsed_output = json.loads(output)\n            formatted_text = parsed_output.get(\"text\", \"\")\n            print(\"Transcription:\", formatted_text)\n            print(\"Metadata:\", metadata)\n        else:\n            print(\"No result returned from the model.\")\n\n\nif __name__ == \"__main__\":\n    main()\n\n\n##File: /Users/allwefantasy/projects/Qwen2-Audio/README_CN.md\n<p align=\"left\">\n        中文</a>&nbsp ｜ &nbsp<a href=\"README.md\">English</a>\n</p>\n<br><br>\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/assets/blog/qwenaudio/qwen2audio_logo.png\" width=\"400\"/>\n<p>\n\n<p align=\"center\">\nQwen2-Audio-7B <a href=\"https://modelscope.cn/models/qwen/Qwen2-Audio-7B\">🤖 </a> | <a href=\"https://huggingface.co/Qwen/Qwen2-Audio-7B\">🤗</a>&nbsp ｜ Qwen-Audio-7B-Instruct <a href=\"https://modelscope.cn/models/qwen/Qwen2-Audio-7B-Instruct\">🤖 </a>| <a href=\"https://huggingface.co/Qwen/Qwen2-Audio-7B-Instruct\">🤗</a>&nbsp ｜ Demo<a href=\"https://modelscope.cn/studios/qwen/Qwen2-Audio-Instruct-Demo\"> 🤖</a> | <a href=\"https://huggingface.co/spaces/Qwen/Qwen2-Audio-Instruct-Demo\">🤗</a>&nbsp\n<br>\n📑 <a href=\"https://arxiv.org/abs/2407.10759\">Paper</a> &nbsp&nbsp | &nbsp&nbsp 📑 <a href=\"https://qwenlm.github.io/blog/qwen2-audio\">Blog</a> &nbsp&nbsp | &nbsp&nbsp 💬 <a href=\"https://github.com/QwenLM/Qwen/blob/main/assets/wechat.png\">WeChat (微信)</a>&nbsp&nbsp | &nbsp&nbsp <a href=\"https://discord.gg/CV4E9rpNSD\">Discord</a>&nbsp&nbsp\n</p>\n\n\n\n我们介绍Qwen-Audio的最新进展：Qwen2-Audio。作为一个大规模音频语言模型，Qwen2-Audio能够接受各种音频信号输入，并根据语音指令执行音频分析或直接响应文本。我们介绍两种不同的音频交互模式：语音聊天 voice chat 和音频分析 audio analysis。\n\n* 语音聊天：用户可以自由地与 Qwen2-Audio 进行语音互动，而无需文本输入；\n* 音频分析：用户可以在互动过程中提供音频和文本指令对音频进行分析；\n\n**我们已经开源了 Qwen2-Audio 系列的两个模型：Qwen2-Audio-7B和Qwen2-Audio-7B-Instruct。**\n\n## 模型结构与训练范式\n\nQwen2-Audio 三阶段训练过程概述。\n\n<p align=\"left\">\n    <img src=\"assets/framework.png\" width=\"80%\"/>\n<p>\n\n\n\n## 新闻\n* 2024.8.9 🎉 我们在 ModelScope 和 Hugging Face 开源了`Qwen2-Audio-7B`和`Qwen2-Audio-7B-Instruct`的 checkpoint.\n* 2024.7.15 🎉 我们发布了 Qwen2-Audio 的[论文](https://arxiv.org/abs/2407.10759), 介绍了相关的模型结构，训练方法和模型表现。\n* 2023.11.30 🔥 我们发布了**Qwen-Audio**系列\n<br>\n\n## 评测\n我们在标准的13个学术数据集上评测了模型的能力如下：\n<table><thead><tr><th>Task</th><th>Description</th><th>Dataset</th><th>Split</th><th>Metric</th></tr></thead><tbody><tr><td rowspan=\"4\">ASR</td><td rowspan=\"4\">Automatic Speech Recognition</td><td>Fleurs</td><td>dev | test</td><td rowspan=\"4\">WER</td></tr><tr><td>Aishell2</td><td>test</td></tr><tr><td>Librispeech</td><td>dev | test</td></tr><tr><td>Common Voice</td><td>dev | test</td></tr><tr><td>S2TT</td><td>Speech-to-Text Translation</td><td>CoVoST2</td><td>test</td><td>BLEU </td></tr><tr><td>SER</td><td>Speech Emotion Recognition</td><td>Meld</td><td>test</td><td>ACC</td></tr><tr><td>VSC</td><td>Vocal Sound Classification</td><td>VocalSound</td><td>test</td><td>ACC</td></tr><tr><td rowspan=\"4\"><a href=\"https://github.com/OFA-Sys/AIR-Bench\">AIR-Bench</a><br></td><td>Chat-Benchmark-Speech</td><td>Fisher<br>SpokenWOZ<br>IEMOCAP<br>Common voice</td><td>dev | test</td><td>GPT-4 Eval</td></tr><tr><td>Chat-Benchmark-Sound</td><td>Clotho</td><td>dev | test</td><td>GPT-4 Eval</td></tr>\n<tr><td>Chat-Benchmark-Music</td><td>MusicCaps</td><td>dev | test</td><td>GPT-4 Eval</td></tr><tr><td>Chat-Benchmark-Mixed-Audio</td><td>Common voice<br>AudioCaps<br>MusicCaps</td><td>dev | test</td><td>GPT-4 Eval</td></tr></tbody></table>\n\n\n以下是整体表现：\n<p align=\"left\">\n    <img src=\"assets/radar_compare_qwen_audio.png\" width=\"70%\"/>\n<p>\n\n评测分数详情如下：\n<br>\n<b>（注意：我们所展示的评测结果是在原始训练框架的初始模型上的，然而在框架转换 Huggingface 后指标出现了部分波动，在这里我们展示我们的全部测评结果：首先是论文中的初始模型结果）</b>\n\n<table><thead><tr><th rowspan=\"2\">Task</th><th rowspan=\"2\">Dataset</th><th rowspan=\"2\">Model</th><th colspan=\"2\">Performance</th></tr><tr><th>Metrics</th><th>Results</th></tr></thead><tbody><tr><td rowspan=\"15\">ASR</td><td rowspan=\"7\"><b>Librispeech</b><br>dev-clean | dev-other | <br>test-clean | test-other</td><td>SpeechT5</td><td rowspan=\"7\">WER </td><td>2.1 | 5.5 | 2.4 | 5.8</td></tr><tr><td>SpeechNet</td><td>- | - | 30.7 | -</td></tr><tr><td>SLM-FT</td><td>- | - | 2.6 | 5.0</td></tr><tr><td>SALMONN</td><td>- | - | 2.1 | 4.9</td></tr><tr><td>SpeechVerse</td><td>- | - | 2.1 | 4.4</td></tr><tr><td>Qwen-Audio</td><td>1.8 | 4.0 | 2.0 | 4.2</td></tr><tr><td>Qwen2-Audio</td><td><b>1.3 | 3.4 | 1.6 | 3.6</b></td></tr><tr><td rowspan=\"2\"><b>Common Voice 15</b> <br>en | zh | yue | fr</td><td>Whisper-large-v3</td><td rowspan=\"2\">WER </td><td>9.3 | 12.8 | 10.9 | 10.8</td></tr><tr><td>Qwen2-Audio</td><td><b>8.6 | 6.9 | 5.9 | 9.6</b></td></tr>\n<tr><td rowspan=\"2\"><b>Fleurs</b> <br>zh</td><td>Whisper-large-v3</td><td rowspan=\"2\">WER </td><td>7.7</td></tr><tr><td>Qwen2-Audio</td><td><b>7.5</b></td></tr><tr><td rowspan=\"4\"><b>Aishell2</b> <br>Mic | iOS | Android</td><td>MMSpeech-base</td><td rowspan=\"4\">WER </td><td>4.5 | 3.9 | 4.0</td></tr><tr><td>Paraformer-large</td><td>- | <b>2.9</b> | -</td></tr><tr><td>Qwen-Audio</td><td>3.3 | 3.1 | 3.3</td></tr><tr><td>Qwen2-Audio</td><td><b>3.0</b> | 3.0 | <b>2.9</b></td></tr><tr><td rowspan=\"8\">S2TT</td><td rowspan=\"5\"><b>CoVoST2</b> <br>en-de | de-en | <br>en-zh | zh-en</td><td>SALMONN</td><td rowspan=\"5\">BLEU </td><td>18.6 | - | 33.1 | -</td></tr><tr><td>SpeechLLaMA</td><td>- | 27.1 | - | 12.3</td></tr><tr><td>BLSP</td><td>14.1 | - | - | -</td></tr><tr><td>Qwen-Audio</td><td>25.1 | 33.9 | 41.5 | 15.7</td></tr><tr><td>Qwen2-Audio</td><td><b>29.9 | 35.2 | 45.2 | 24.4</b></td></tr>\n<tr><td rowspan=\"3\"><b>CoVoST2</b> <br>es-en | fr-en | it-en |</td><td>SpeechLLaMA</td><td rowspan=\"3\">BLEU </td><td>27.9 | 25.2 | 25.9</td></tr><tr><td>Qwen-Audio</td><td>39.7 | <b>38.5</b> | 36.0</td></tr><tr><td>Qwen2-Audio</td><td><b>40.0 | 38.5 | 36.3</b></td></tr><tr><td rowspan=\"3\">SER</td><td rowspan=\"3\"><b>Meld</b></td><td>WavLM-large</td><td rowspan=\"3\">ACC </td><td>0.542</td></tr><tr><td>Qwen-Audio</td><td><b>0.557</b></td></tr><tr><td>Qwen2-Audio</td><td>0.553</td></tr><tr><td rowspan=\"4\">VSC</td><td rowspan=\"4\"><b>VocalSound</b></td><td>CLAP</td><td rowspan=\"4\">ACC </td><td>0.4945</td></tr><tr><td>Pengi</td><td>0.6035</td></tr><tr><td>Qwen-Audio</td><td>0.9289</td></tr><tr><td>Qwen2-Audio</td><td><b>0.9392</b></td></tr>\n<tr><td>AIR-Bench <br></td><td><b>Chat Benchmark</b><br>Speech | Sound |<br> Music | Mixed-Audio</td><td>SALMONN<br>BLSP<br>Pandagpt<br>Macaw-LLM<br>SpeechGPT<br>Next-gpt<br>Qwen-Audio<br>Gemini-1.5-pro<br>Qwen2-Audio</td><td>GPT-4 </td><td>6.16 | 6.28 | 5.95 | 6.08<br>6.17 | 5.55 | 5.08 | 5.33<br>3.58 | 5.46 | 5.06 | 4.25<br>0.97 | 1.01 | 0.91 | 1.01<br>1.57 | 0.95 | 0.95 | 4.13<br>3.86 | 4.76 | 4.18 | 4.13<br>6.47 | 6.95 | 5.52 | 6.08<br>6.97 | 5.49 | 5.06 | 5.27<br><b>7.18 | 6.99 | 6.79 | 6.77</b></td></tr></tbody></table>\n\n<b>（其次是转换 huggingface 后的）</b>\n\n<table><thead><tr><th rowspan=\"2\">Task</th><th rowspan=\"2\">Dataset</th><th rowspan=\"2\">Model</th><th colspan=\"2\">Performance</th></tr><tr><th>Metrics</th><th>Results</th></tr></thead><tbody><tr><td rowspan=\"15\">ASR</td><td rowspan=\"7\"><b>Librispeech</b><br>dev-clean | dev-other | <br>test-clean | test-other</td><td>SpeechT5</td><td rowspan=\"7\">WER </td><td>2.1 | 5.5 | 2.4 | 5.8</td></tr><tr><td>SpeechNet</td><td>- | - | 30.7 | -</td></tr><tr><td>SLM-FT</td><td>- | - | 2.6 | 5.0</td></tr><tr><td>SALMONN</td><td>- | - | 2.1 | 4.9</td></tr><tr><td>SpeechVerse</td><td>- | - | 2.1 | 4.4</td></tr><tr><td>Qwen-Audio</td><td>1.8 | 4.0 | 2.0 | 4.2</td></tr><tr><td>Qwen2-Audio</td><td><b>1.7 | 3.6 | 1.7 | 4.0</b></td></tr><tr><td rowspan=\"2\"><b>Common Voice 15</b> <br>en | zh | yue | fr</td><td>Whisper-large-v3</td><td rowspan=\"2\">WER </td><td>9.3 | 12.8 | 10.9 | 10.8</td></tr><tr><td>Qwen2-Audio</td><td><b>8.7 | 6.5 | 5.9 | 9.6</b></td></tr>\n<tr><td rowspan=\"2\"><b>Fleurs</b> <br>zh</td><td>Whisper-large-v3</td><td rowspan=\"2\">WER </td><td>7.7</td></tr><tr><td>Qwen2-Audio</td><td><b>7.0</b></td></tr><tr><td rowspan=\"4\"><b>Aishell2</b> <br>Mic | iOS | Android</td><td>MMSpeech-base</td><td rowspan=\"4\">WER </td><td>4.5 | 3.9 | 4.0</td></tr><tr><td>Paraformer-large</td><td>- | <b>2.9</b> | -</td></tr><tr><td>Qwen-Audio</td><td>3.3 | 3.1 | 3.3</td></tr><tr><td>Qwen2-Audio</td><td><b>3.2</b> | 3.1 | <b>2.9</b></td></tr><tr><td rowspan=\"8\">S2TT</td><td rowspan=\"5\"><b>CoVoST2</b> <br>en-de | de-en | <br>en-zh | zh-en</td><td>SALMONN</td><td rowspan=\"5\">BLEU </td><td>18.6 | - | 33.1 | -</td></tr><tr><td>SpeechLLaMA</td><td>- | 27.1 | - | 12.3</td></tr><tr><td>BLSP</td><td>14.1 | - | - | -</td></tr><tr><td>Qwen-Audio</td><td>25.1 | <b>33.9</b> | 41.5 | 15.7</td></tr><tr><td>Qwen2-Audio</td><td><b>29.6</b> | 33.6 | <b>45.6</b> | <b>24.0</b></td></tr>\n<tr><td rowspan=\"3\"><b>CoVoST2</b> <br>es-en | fr-en | it-en |</td><td>SpeechLLaMA</td><td rowspan=\"3\">BLEU </td><td>27.9 | 25.2 | 25.9</td></tr><tr><td>Qwen-Audio</td><td><b>39.7 | 38.5 | 36.0</b></td></tr><tr><td>Qwen2-Audio</td><td>38.7 | 37.2 | 35.2</td></tr><tr><td rowspan=\"3\">SER</td><td rowspan=\"3\"><b>Meld</b></td><td>WavLM-large</td><td rowspan=\"3\">ACC </td><td>0.542</td></tr><tr><td>Qwen-Audio</td><td><b>0.557</b></td></tr><tr><td>Qwen2-Audio</td><td>0.535</td></tr><tr><td rowspan=\"4\">VSC</td><td rowspan=\"4\"><b>VocalSound</b></td><td>CLAP</td><td rowspan=\"4\">ACC </td><td>0.4945</td></tr><tr><td>Pengi</td><td>0.6035</td></tr><tr><td>Qwen-Audio</td><td>0.9289</td></tr><tr><td>Qwen2-Audio</td><td><b>0.9395</b></td></tr>\n<tr><td>AIR-Bench <br></td><td><b>Chat Benchmark</b><br>Speech | Sound |<br> Music | Mixed-Audio</td><td>SALMONN<br>BLSP<br>Pandagpt<br>Macaw-LLM<br>SpeechGPT<br>Next-gpt<br>Qwen-Audio<br>Gemini-1.5-pro<br>Qwen2-Audio</td><td>GPT-4 </td><td>6.16 | 6.28 | 5.95 | 6.08<br>6.17 | 5.55 | 5.08 | 5.33<br>3.58 | 5.46 | 5.06 | 4.25<br>0.97 | 1.01 | 0.91 | 1.01<br>1.57 | 0.95 | 0.95 | 4.13<br>3.86 | 4.76 | 4.18 | 4.13<br>6.47 | <b>6.95</b> | 5.52 | 6.08<br>6.97 | 5.49 | 5.06 | 5.27<br><b>7.24</b> | 6.83 | <b>6.73</b> | <b>6.42</b></td></tr></tbody></table>\n\n\n我们提供了以上**所有**评测脚本以供复现我们的实验结果。请阅读 [eval_audio/EVALUATION.md](eval_audio/EVALUATION.md) 了解更多信息。\n\n## 部署要求\nThe code of Qwen2-Audio has been in the latest Hugging face transformers and we advise you to build from source with command `pip install git+https://github.com/huggingface/transformers`, or you might encounter the following error:\nQwen2-Audio的代码已经包含在最新的 Hugging Face Transformers 的主分支中，我们建议您使用命令`pip install git+https://github.com/huggingface/transformers`\n```\nKeyError: 'qwen2-audio'\n```\n## 快速使用\n我们提供简单的示例来说明如何利用 🤗 Transformers 快速使用 Qwen2-Audio-7B 和 Qwen2-Audio-7B-Instruct。\n在开始前，请确保你已经配置好环境并安装好相关的代码包。最重要的是，确保你满足上述要求，然后安装相关的依赖库。\n接下来你可以开始使用 Transformers 或者 ModelScope 来使用我们的模型。目前Qwen2-Audio-7B 及 Qwen2-Audio-7B-Instruct 模型处理30秒以内的音频表现更佳。\n#### 🤗 Hugging Face Transformers\n如希望使用 Qwen2-Audio-7B-Instruct 进行推理，我们分别演示语音聊天和音频分析的交互方式，所需要写的只是如下所示的数行代码。\n##### 语音聊天推理\n在语音聊天模式下，用户可以自由地与 Qwen2-Audio 进行语音交互，无需文字输入：\n```python\nfrom io import BytesIO\nfrom urllib.request import urlopen\nimport librosa\nfrom transformers import Qwen2AudioForConditionalGeneration, AutoProcessor\n\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\")\nmodel = Qwen2AudioForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\", device_map=\"auto\")\n\nconversation = [\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/guess_age_gender.wav\"},\n    ]},\n    {\"role\": \"assistant\", \"content\": \"Yes, the speaker is female and in her twenties.\"},\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/translate_to_chinese.wav\"},\n    ]},\n]\ntext = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\naudios = []\nfor message in conversation:\n    if isinstance(message[\"content\"], list):\n        for ele in message[\"content\"]:\n            if ele[\"type\"] == \"audio\":\n                audios.append(librosa.load(\n                    BytesIO(urlopen(ele['audio_url']).read()), \n                    sr=processor.feature_extractor.sampling_rate)[0]\n                )\n\ninputs = processor(text=text, audios=audios, return_tensors=\"pt\", padding=True)\ninputs.input_ids = inputs.input_ids.to(\"cuda\")\n\ngenerate_ids = model.generate(**inputs, max_length=256)\ngenerate_ids = generate_ids[:, inputs.input_ids.size(1):]\n\nresponse = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n```\n##### 音频分析推理\n在音频分析中，用户可以提供音频和文字问题来实现对音频的分析：\n```python\nfrom io import BytesIO\nfrom urllib.request import urlopen\nimport librosa\nfrom transformers import Qwen2AudioForConditionalGeneration, AutoProcessor\n\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\")\nmodel = Qwen2AudioForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\", device_map=\"auto\")\n\nconversation = [\n    {'role': 'system', 'content': 'You are a helpful assistant.'}, \n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/glass-breaking-151256.mp3\"},\n        {\"type\": \"text\", \"text\": \"What's that sound?\"},\n    ]},\n    {\"role\": \"assistant\", \"content\": \"It is the sound of glass shattering.\"},\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"text\", \"text\": \"What can you do when you hear that?\"},\n    ]},\n    {\"role\": \"assistant\", \"content\": \"Stay alert and cautious, and check if anyone is hurt or if there is any damage to property.\"},\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/1272-128104-0000.flac\"},\n        {\"type\": \"text\", \"text\": \"What does the person say?\"},\n    ]},\n]\ntext = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\naudios = []\nfor message in conversation:\n    if isinstance(message[\"content\"], list):\n        for ele in message[\"content\"]:\n            if ele[\"type\"] == \"audio\":\n                audios.append(\n                    librosa.load(\n                        BytesIO(urlopen(ele['audio_url']).read()), \n                        sr=processor.feature_extractor.sampling_rate)[0]\n                )\n\ninputs = processor(text=text, audios=audios, return_tensors=\"pt\", padding=True)\ninputs.input_ids = inputs.input_ids.to(\"cuda\")\n\ngenerate_ids = model.generate(**inputs, max_length=256)\ngenerate_ids = generate_ids[:, inputs.input_ids.size(1):]\n\nresponse = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n```\n##### 批量推理\n我们也支持批量推理：\n```python\nfrom io import BytesIO\nfrom urllib.request import urlopen\nimport librosa\nfrom transformers import Qwen2AudioForConditionalGeneration, AutoProcessor\n\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\")\nmodel = Qwen2AudioForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\", device_map=\"auto\")\n\nconversation1 = [\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/glass-breaking-151256.mp3\"},\n        {\"type\": \"text\", \"text\": \"What's that sound?\"},\n    ]},\n    {\"role\": \"assistant\", \"content\": \"It is the sound of glass shattering.\"},\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/f2641_0_throatclearing.wav\"},\n        {\"type\": \"text\", \"text\": \"What can you hear?\"},\n    ]}\n]\n\nconversation2 = [\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/1272-128104-0000.flac\"},\n        {\"type\": \"text\", \"text\": \"What does the person say?\"},\n    ]},\n]\n\nconversations = [conversation1, conversation2]\n\ntext = [processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False) for conversation in conversations]\n\naudios = []\nfor conversation in conversations:\n    for message in conversation:\n        if isinstance(message[\"content\"], list):\n            for ele in message[\"content\"]:\n                if ele[\"type\"] == \"audio\":\n                    audios.append(\n                        librosa.load(\n                            BytesIO(urlopen(ele['audio_url']).read()), \n                            sr=processor.feature_extractor.sampling_rate)[0]\n                    )\n\ninputs = processor(text=text, audios=audios, return_tensors=\"pt\", padding=True)\ninputs['input_ids'] = inputs['input_ids'].to(\"cuda\")\ninputs.input_ids = inputs.input_ids.to(\"cuda\")\n\ngenerate_ids = model.generate(**inputs, max_length=256)\ngenerate_ids = generate_ids[:, inputs.input_ids.size(1):]\n\nresponse = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n```\n运行Qwen2-Audio-7B同样非常简单。\n```python\nfrom io import BytesIO\nfrom urllib.request import urlopen\nimport librosa\nfrom transformers import AutoProcessor, Qwen2AudioForConditionalGeneration\n\nmodel = Qwen2AudioForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-Audio-7B\" ,trust_remote_code=True)\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-Audio-7B\" ,trust_remote_code=True)\n\nprompt = \"<|audio_bos|><|AUDIO|><|audio_eos|>Generate the caption in English:\"\nurl = \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Audio/glass-breaking-151256.mp3\"\naudio, sr = librosa.load(BytesIO(urlopen(url).read()), sr=processor.feature_extractor.sampling_rate)\ninputs = processor(text=prompt, audios=audio, return_tensors=\"pt\")\n\ngenerated_ids = model.generate(**inputs, max_length=256)\ngenerated_ids = generated_ids[:, inputs.input_ids.size(1):]\nresponse = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n```\n#### 🤖 ModelScope\n我们强烈建议用户，特别是中国大陆地区的用户，使用 ModelScope。`snapshot_download` 可以帮助您解决下载检查点时遇到的问题。\n<br>\n## Demo\n### Web UI\n我们提供了 Web UI 的 demo 供用户使用。在开始前，确保已经安装如下代码库：\n```\npip install -r requirements_web_demo.txt\n```\n随后运行如下命令，并点击生成链接：\n```\npython demo/web_demo_audio.py\n```\n<br>\n\n## 样例展示\n更多样例将更新于[通义千问博客](https://qwenlm.github.io/blog/qwen2-audio)中的 Qwen2-Audio 博客。\n\n## 团队招聘\n\n我们是通义千问语音多模态团队，致力于以通义千问为核心，拓展音频多模态理解和生成能力，实现自由灵活的音频交互。目前团队蓬勃发展中，如有意向实习或全职加入我们，请发送简历至 `qwen_audio@list.alibaba-inc.com`.\n<br>\n\n## 使用协议\n\n请查看每个模型在其 Hugging Face 仓库中的许可证。您无需提交商业使用申请。\n<br>\n\n\n\n## 引用\n\n如果你觉得我们的论文和代码对你的研究有帮助，请考虑 :star: 和引用 :pencil: :)\n\n```BibTeX\n@article{Qwen-Audio,\n  title={Qwen-Audio: Advancing Universal Audio Understanding via Unified Large-Scale Audio-Language Models},\n  author={Chu, Yunfei and Xu, Jin and Zhou, Xiaohuan and Yang, Qian and Zhang, Shiliang and Yan, Zhijie  and Zhou, Chang and Zhou, Jingren},\n  journal={arXiv preprint arXiv:2311.07919},\n  year={2023}\n}\n```\n\n```BibTeX\n@article{Qwen2-Audio,\n  title={Qwen2-Audio Technical Report},\n  author={Chu, Yunfei and Xu, Jin and Yang, Qian and Wei, Haojie and Wei, Xipin and Guo,  Zhifang and Leng, Yichong and Lv, Yuanjun and He, Jinzheng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},\n  journal={arXiv preprint arXiv:2407.10759},\n  year={2024}\n}\n```\n## 联系我们\n\n如果你想给我们的研发团队和产品团队留言，请通过邮件（`qianwen_opensource@alibabacloud.com`）联系我们。\n<br>\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/qwen_voice/__init__.py\nimport time\nfrom typing import List, Tuple, Dict, Any, Union\nimport base64\nimport json\nimport librosa\nimport io\nfrom transformers import Qwen2AudioForConditionalGeneration, AutoProcessor\n\ndef get_meta(self):\n    return [\n        {\n            \"model_deploy_type\": \"proprietary\",\n            \"backend\": \"transformers\",\n            \"message_format\": True\n        }\n    ]\n\ndef process_input(ins: Union[str, List[Dict[str, Any]], Dict[str, Any]]):\n    if isinstance(ins, list) or isinstance(ins, dict):\n        return ins\n\n    content = []\n    try:\n        ins_json = json.loads(ins)\n    except:\n        return ins\n\n    if isinstance(ins_json, dict):\n        return ins_json\n\n    for item in ins_json:\n        if (\"audio\" in item or \"audio_url\" in item) and \"type\" not in item:\n            audio_data = item.get(\"audio\", item.get(\"audio_url\", \"\"))\n            if not audio_data.startswith(\"data:\"):\n                audio_data = \"data:audio/wav;base64,\" + audio_data\n\n            other_fields = {k: v for k, v in item.items() if k not in [\"audio\", \"text\", \"audio_url\"]}\n            content.append(\n                {\n                    \"audio_url\": {\"url\": audio_data, **other_fields},\n                    \"type\": \"audio\",\n                }\n            )\n\n        if \"text\" in item and \"type\" not in item:\n            text_data = item[\"text\"]\n            content.append({\"text\": text_data, \"type\": \"text\"})\n\n        if \"type\" in item and item[\"type\"] in [\"text\", \"audio\"]:\n            content.append(item)\n\n    if not content:\n        return ins\n\n    return content\n\ndef stream_chat(\n    self,\n    tokenizer,\n    ins: str,\n    his: List[Dict[str, str]] = [],\n    max_length: int = 4096,\n    top_p: float = 0.95,\n    temperature: float = 0.1,\n    **kwargs,\n):\n    messages = [\n        {\"role\": message[\"role\"], \"content\": process_input(message[\"content\"])}\n        for message in his\n    ] + [{\"role\": \"user\", \"content\": process_input(ins)}]\n\n    text = self.processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n    audios = []\n    for message in messages:\n        if isinstance(message[\"content\"], list):\n            for ele in message[\"content\"]:\n                if ele[\"type\"] == \"audio\":\n                    audio_data = ele[\"audio_url\"].split(\",\")[1]\n                    audio_bytes = base64.b64decode(audio_data)\n                    audio, _ = librosa.load(io.BytesIO(audio_bytes), sr=self.processor.feature_extractor.sampling_rate)\n                    audios.append(audio)\n\n    inputs = self.processor(text=text, audios=audios, return_tensors=\"pt\", padding=True)\n    inputs.input_ids = inputs.input_ids.to(self.device)\n\n    start_time = time.monotonic()\n    generate_ids = self.generate(**inputs, max_length=max_length)\n    generate_ids = generate_ids[:, inputs.input_ids.size(1):]\n\n    response = self.processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n    time_cost = time.monotonic() - start_time\n\n    return [\n        (\n            response,\n            {\n                \"metadata\": {\n                    \"request_id\": \"\",\n                    \"input_tokens_count\": inputs.input_ids.size(1),\n                    \"generated_tokens_count\": generate_ids.size(1),\n                    \"time_cost\": time_cost,\n                    \"first_token_time\": 0,\n                    \"speed\": float(generate_ids.size(1)) / time_cost if time_cost > 0 else 0,\n                }\n            },\n        )\n    ]\n\ndef init_model(model_dir, infer_params: Dict[str, str] = {}, sys_conf: Dict[str, str] = {}):\n    processor = AutoProcessor.from_pretrained(model_dir)\n    model = Qwen2AudioForConditionalGeneration.from_pretrained(model_dir, device_map=\"auto\")\n    model.processor = processor\n    import types\n    model.stream_chat = types.MethodType(stream_chat, model)\n    model.get_meta = types.MethodType(get_meta, model)    \n    return (model, None)\n\n     \n\n下面是用户的需求：\n\n@simple_retrieval.py 修正jieba 的 ImportError\n严格遵循 SEARCH/REPLACE 格式,如果是python代码,注意空格和缩进\n"}, {"role": "assistant", "content": "```python\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/utils/simple_retrieval.py\n<<<<<<< SEARCH\nimport jieba\n=======\ntry:\n    import jieba\nexcept ImportError:\n    jieba = None\n>>>>>>> REPLACE\n```"}], "model": "deepseek_chat", "yaml_file": "actions/040_chat_action.yml"}
{"instruction": "@simple_retrieval.py 修正jieba 的 ImportError\n严格遵循 SEARCH/REPLACE 格式,如果是python代码,注意空格和缩进", "conversations": [{"role": "user", "content": "\n如果你需要生成代码，对于每个需要更改的文件,你需要按 *SEARCH/REPLACE block* 的格式进行生成。\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The opening fence and code language, eg: ```python\n2. The file path alone on a line, starting with \"##File:\" and verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n*SEARCH/REPLACE* blocks will replace *all* matching occurrences.\nInclude enough lines to make the SEARCH blocks unique.\n\nInclude *ALL* the code being searched and replaced!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n下面我们来看一个例子：\n\n当前项目目录结构：\n1. 项目根目录： /tmp/projects/mathweb\n2. 项目子目录/文件列表(类似tree 命令输出)\nflask/\n    app.py\n    templates/\n        index.html\n    static/\n        style.css\n\n用户需求： Change get_factorial() to use math.factorial\n\n回答： To make this change we need to modify `/tmp/projects/mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n用户需求： Refactor hello() into its own file.\n\n回答：To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/hello.py\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/main.py\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n现在让我们开始一个新的任务:\n\n当前项目目录结构：\n1. 项目根目录： /Users/allwefantasy/projects/byzer-llm\n2. 项目子目录/文件列表(类似tree 命令输出)：\nsetup.py\ntests/\n    test_emb_rerank.py\n    test_deepseek_code.py\n    test_prompt.py\n    test_model_meta.py\n    __init__.py\n    test_transformers_deploy.py\n    test_data_analysis.py\n    test_yi.py\n    test_deploy.py\n    functions_suite.py\n    test_template.py\n    auto/\n        test_backend_llama_cpp.py\n        __init__.py\nsetup-machine/\n    convert_safetensor.py\nnotebooks/\n    test.py\n    test_sensevoice.py\nsrc/\n    byzerllm/\n        lang.py\n        store.py\n        version.py\n        log.py\n        __init__.py\n        types.py\n        easy_byzerllm_command.py\n        byzerllm_command.py\n        command_args.py\n        bark/\n            generation.py\n            __init__.py\n            model.py\n            api.py\n            model_fine.py\n            bark_voice.py\n        moss/\n            finetune_moss.py\n            __init__.py\n            moss_inference.py\n            models/\n                configuration_moss.py\n                quantization.py\n                custom_autotune.py\n                __init__.py\n                modeling_moss.py\n                tokenization_moss.py\n        llama2/\n            __init__.py\n        records/\n            __init__.py\n        visualglm/\n            __init__.py\n        auto/\n            __init__.py\n            backend_llama_cpp.py\n            backend_vllm.py\n            backend_ds.py\n        alpha_moss/\n            __init__.py\n        processor/\n            __init__.py\n        sensevoice/\n            __init__.py\n        qwen/\n            __init__.py\n        utils/\n            json_repaire.py\n            object_store_ref_util.py\n            __init__.py\n            types.py\n            tokenizer.py\n            openai_utils.py\n            ray_utils.py\n            text_generator.py\n            emb.py\n            connect_ray.py\n            testing.py\n            langutil.py\n            nontext.py\n            fulltune/\n                deepspeed_trainner.py\n                __init__.py\n                launch.py\n                trainner.py\n                base_model/\n                    __init__.py\n                    configuration_baichuan.py\n                    modeling_baichuan.py\n                pretrain/\n                    __init__.py\n                    convert_to_transformers.py\n            metrics/\n                __init__.py\n            config/\n                __init__.py\n            retrieval/\n                rest.py\n                __init__.py\n                udf.py\n            sft/\n                argument.py\n                collator.py\n                qlora.py\n                __init__.py\n                model.py\n                dataset.py\n                loss.py\n                merge_lora.py\n                trainer.py\n            client/\n                message_utils.py\n                math_utils.py\n                byzerllm_client.py\n                parallel_utils.py\n                __init__.py\n                types.py\n                code_utils.py\n                img_utils.py\n                entrypoints/\n                    __init__.py\n                    openai/\n                        protocol.py\n                        __init__.py\n                        serve.py\n                        serving_chat.py\n                        api_server.py\n                        serving_engine.py\n                        serving_completion.py\n                        tool.py\n        qwen_voice/\n            __init__.py\n        chatglm2/\n            __init__.py\n        baichuan/\n            __init__.py\n        m3e/\n            __init__.py\n        stable_diffusion/\n            config.py\n            __init__.py\n            logger.py\n            shared.py\n            model.py\n            utils.py\n            images.py\n            diffusion/\n                __init__.py\n                embeddings.py\n                utils.py\n                networks/\n                    lyco.py\n                    __init__.py\n                upscalers/\n                    __init__.py\n                    multidiffusion.py\n                    samplers.py\n                piplines/\n                    __init__.py\n                    lpw.py\n                    diffusers.py\n            lib/\n                __init__.py\n                diffusers/\n                    __init__.py\n                    scheduler.py\n            api/\n                __init__.py\n                plugin.py\n                models/\n                    diffusion.py\n                    __init__.py\n                diffusion/\n                    __init__.py\n                    pipelines/\n                        __init__.py\n                        diffusers.py\n                events/\n                    generation.py\n                    __init__.py\n        bge/\n            __init__.py\n        bge_rerank/\n            __init__.py\n        apps/\n            client.py\n            qa_strategy.py\n            __init__.py\n            builder.py\n            vector_db.py\n            utils.py\n            qa.py\n            byzer_storage/\n                memory_model_based.py\n                command.py\n                env.py\n                train_pt.py\n                train_sft.py\n                __init__.py\n                train_dpo.py\n                simple_api.py\n                generate_sft_data.py\n            byzer_sql/\n                __init__.py\n            llama_index/\n                byzerai_vectordb.py\n                byzerai.py\n                byzerai_embedding.py\n                __init__.py\n                collection_manager.py\n                byzerai_index_store.py\n                byzerai_docstore.py\n                simple_retrieval.py\n                byzerai_kvstore.py\n            agent/\n                conversable_agent.py\n                registry.py\n                groupchat.py\n                user_proxy_agent.py\n                __init__.py\n                agent.py\n                extensions/\n                    simple_retrieval_client.py\n                    planner.py\n                    python_codesandbox_agent.py\n                    preview_file_agent.py\n                    llama_index_retrieval_agent.py\n                    visualization_agent.py\n                    assistant_agent.py\n                    load_data_agent.py\n                    retrieval_agent.py\n                    output_agent.py\n                    __init__.py\n                    byzer_engine_agent.py\n                    subquestion_agent.py\n                    sql_reviewer_agent.py\n                    spark_sql_agent.py\n                    rhetorical_agent.py\n                    data_analysis.py\n                    data_analysis_pipeline_agent.py\n                    common_agent.py\n                    query_rewrite/\n                        time.py\n                        rhetorical.py\n                        __init__.py\n                        condition.py\n                        context.py\n                store/\n                    memory_store.py\n                    __init__.py\n                    stores.py\n        saas/\n            __init__.py\n            chatglm/\n                __init__.py\n            sparkdesk/\n                __init__.py\n            gemini/\n                __init__.py\n            azure_openai/\n                __init__.py\n            azure/\n                __init__.py\n            qianfan/\n                __init__.py\n            claude/\n                __init__.py\n            qianwen/\n                __init__.py\n            volcengine/\n                __init__.py\n            minimax/\n                __init__.py\n            baichuan/\n                __init__.py\n            qianwen_vl/\n                __init__.py\n            official_openai/\n                __init__.py\n            zhipu/\n                __init__.py\n            aws_bedrock/\n                __init__.py\n            openai/\n                __init__.py\n        falcon/\n            __init__.py\n        chatglm6b/\n            trainer_seq2seq.py\n            arguments.py\n            __init__.py\n            finetune.py\n            tunning/\n                __init__.py\n                infer.py\n                finetune.py\n                utils/\n                    config.py\n                    __init__.py\n                    other.py\n                    common.py\n                    seq2seq.py\n        starcode/\n            __init__.py\n        qwen_vl_chat/\n            __init__.py\n        zephyr/\n            __init__.py\n        dolly/\n            generate.py\n            __init__.py\n            consts.py\n            dolly_inference.py\n            trainer.py\n        llama/\n            __init__.py\n        whisper/\n            whisper_inference.py\n            __init__.py\n下面是一些文件路径以及每个文件对应的源码：\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/saas/official_openai/__init__.py\nimport time\nfrom typing import List, Tuple, Dict, Any, Union\nimport httpx\nfrom openai import OpenAI\nimport base64\nimport io\nimport json\nimport ray\nfrom byzerllm.utils.types import (\n    BlockVLLMStreamServer,\n    StreamOutputs,\n    SingleOutput,\n    SingleOutputMeta,\n    BlockBinaryStreamServer,\n)\nfrom byzerllm.utils.langutil import asyncfy_with_semaphore\nimport threading\nimport asyncio\nimport traceback\nimport uuid\nimport tempfile        \n\nclass CustomSaasAPI:\n\n    def __init__(self, infer_params: Dict[str, str]) -> None:\n\n        self.api_key = infer_params[\"saas.api_key\"]\n        self.model = infer_params.get(\"saas.model\", \"gpt-3.5-turbo-1106\")\n\n        other_params = {}\n\n        if \"saas.api_base\" in infer_params:\n            other_params[\"api_base\"] = infer_params[\"saas.api_base\"]\n\n        if \"saas.api_version\" in infer_params:\n            other_params[\"api_version\"] = infer_params[\"saas.api_version\"]\n\n        if \"saas.api_type\" in infer_params:\n            other_params[\"api_type\"] = infer_params[\"saas.api_type\"]\n\n        if \"saas.base_url\" in infer_params:\n            other_params[\"base_url\"] = infer_params[\"saas.base_url\"]\n\n        if \"saas.timeout\" in infer_params:\n            other_params[\"timeout\"] = float(infer_params[\"saas.timeout\"])\n\n        self.max_retries = int(infer_params.get(\"saas.max_retries\", 10))\n\n        self.meta = {\n            \"model_deploy_type\": \"saas\",\n            \"backend\": \"saas\",\n            \"support_stream\": True,\n            \"model_name\": self.model,\n        }\n\n        self.meta[\"embedding_mode\"] = \"embedding\" in self.model.lower()\n\n        self.proxies = infer_params.get(\"saas.proxies\", None)\n        self.local_address = infer_params.get(\"saas.local_address\", \"0.0.0.0\")\n\n        if not self.proxies:\n            self.client = OpenAI(**other_params, api_key=self.api_key)\n        else:\n            self.client = OpenAI(\n                **other_params,\n                api_key=self.api_key,\n                http_client=httpx.Client(\n                    proxies=self.proxies,\n                    transport=httpx.HTTPTransport(local_address=self.local_address),\n                ),\n            )\n\n        try:\n            ray.get_actor(\"BLOCK_VLLM_STREAM_SERVER\")\n        except ValueError:\n            try:\n                ray.remote(BlockVLLMStreamServer).options(\n                    name=\"BLOCK_VLLM_STREAM_SERVER\",\n                    lifetime=\"detached\",\n                    max_concurrency=1000,\n                ).remote()\n            except Exception as e:\n                pass\n        try:\n            ray.get_actor(\"BlockBinaryStreamServer\")\n        except ValueError:\n            try:\n                ray.remote(BlockBinaryStreamServer).options(\n                    name=\"BlockBinaryStreamServer\",\n                    lifetime=\"detached\",\n                    max_concurrency=1000,\n                ).remote()\n            except Exception as e:\n                pass\n\n    # saas/proprietary\n    def get_meta(self):\n        return [self.meta]\n\n    def process_input(self, ins: Union[str, List[Dict[str, Any]], Dict[str, Any]]):\n\n        if isinstance(ins, list) or isinstance(ins, dict):\n            return ins\n\n        content = []\n        try:\n            ins_json = json.loads(ins)\n        except:\n            return ins\n\n        ## speech\n        if isinstance(ins_json, dict):\n            return ins_json\n\n        content = []\n        #     [\n        #     {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n        #     {\n        #       \"type\": \"image_url\",\n        #       \"image_url\": {\n        #         \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n        #         \"detail\": \"high\"\n        #       },\n        #     },\n        #   ],\n        for item in ins_json:\n            # for format like this: {\"image\": \"xxxxxx\", \"text\": \"What’s in this image?\",\"detail\":\"high\"}\n            # or {\"image\": \"xxxxxx\"}, {\"text\": \"What’s in this image?\"}\n            if (\"image\" in item or \"image_url\" in item) and \"type\" not in item:\n                image_data = item.get(\"image\", item.get(\"image_url\", \"\"))\n                ## \"data:image/jpeg;base64,\"\n                if not image_data.startswith(\"data:\"):\n                    image_data = \"data:image/jpeg;base64,\" + image_data\n\n                ## get the other fields except image/text/image_url\n                other_fields = {\n                    k: v\n                    for k, v in item.items()\n                    if k not in [\"image\", \"text\", \"image_url\"]\n                }\n                content.append(\n                    {\n                        \"image_url\": {\"url\": image_data, **other_fields},\n                        \"type\": \"image_url\",\n                    }\n                )            \n\n            if \"text\" in item and \"type\" not in item:\n                text_data = item[\"text\"]\n                content.append({\"text\": text_data, \"type\": \"text\"})\n\n            ## for format like this: {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n            ## {\"type\": \"image_url\", \"image_url\": {\"url\":\"\",\"detail\":\"high\"}}\n            ## this is the standard format, just return it\n            if \"type\" in item and item[\"type\"] == \"text\":\n                content.append(item)\n\n            if \"type\" in item and item[\"type\"] == \"image_url\":\n                content.append(item)\n\n        if not content:\n            return ins\n\n        return content\n\n    async def async_embed_query(self, ins: str, **kwargs):\n        resp = await asyncfy_with_semaphore(\n            lambda: self.client.embeddings.create(input=[ins], model=self.model)\n        )()\n        embedding = resp.data[0].embedding\n        usage = resp.usage\n        return (\n            embedding,\n            {\n                \"metadata\": {\n                    \"input_tokens_count\": usage.prompt_tokens,\n                    \"generated_tokens_count\": 0,\n                }\n            },\n        )\n\n    def embed_query(self, ins: str, **kwargs):\n        resp = self.client.embeddings.create(input=[ins], model=self.model)\n        embedding = resp.data[0].embedding\n        usage = resp.usage\n        return (\n            embedding,\n            {\n                \"metadata\": {\n                    \"input_tokens_count\": usage.prompt_tokens,\n                    \"generated_tokens_count\": 0,\n                }\n            },\n        )\n\n    async def async_text_to_speech(\n        self, stream: bool, ins: str, voice: str, chunk_size: int = None, **kwargs\n    ):\n        if stream:\n            server = ray.get_actor(\"BlockBinaryStreamServer\")\n            request_id = [None]\n\n            def writer():\n                try:\n                    request_id[0] = str(uuid.uuid4())\n                    with self.client.with_streaming_response.audio.speech.create(\n                        model=self.model, voice=voice, input=ins, **kwargs\n                    ) as response:\n                        for chunk in response.iter_bytes(chunk_size):\n                            input_tokens_count = 0\n                            generated_tokens_count = 0\n                            ray.get(\n                                server.add_item.remote(\n                                    request_id[0],\n                                    StreamOutputs(\n                                        outputs=[\n                                            SingleOutput(\n                                                text=chunk,\n                                                metadata=SingleOutputMeta(\n                                                    input_tokens_count=input_tokens_count,\n                                                    generated_tokens_count=generated_tokens_count,\n                                                ),\n                                            )\n                                        ]\n                                    ),\n                                )\n                            )\n                except:\n                    traceback.print_exc()\n                ray.get(server.mark_done.remote(request_id[0]))\n\n            threading.Thread(target=writer, daemon=True).start()\n\n            time_count = 10 * 100\n            while request_id[0] is None and time_count > 0:\n                time.sleep(0.01)\n                time_count -= 1\n\n            if request_id[0] is None:\n                raise Exception(\"Failed to get request id\")\n\n            def write_running():\n                return ray.get(server.add_item.remote(request_id[0], \"RUNNING\"))\n\n            await asyncio.to_thread(write_running)\n            return [\n                (\n                    \"\",\n                    {\n                        \"metadata\": {\n                            \"request_id\": request_id[0],\n                            \"stream_server\": \"BlockBinaryStreamServer\",\n                        }\n                    },\n                )\n            ]\n\n        start_time = time.monotonic()\n        with io.BytesIO() as output:\n            async with asyncfy_with_semaphore(\n                lambda: self.client.with_streaming_response.audio.speech.create(\n                    model=self.model, voice=voice, input=ins, **kwargs\n                )\n            )() as response:\n                for chunk in response.iter_bytes():\n                    output.write(chunk)\n\n            base64_audio = base64.b64encode(output.getvalue()).decode()\n            time_cost = time.monotonic() - start_time\n            return [\n                (\n                    base64_audio,\n                    {\n                        \"metadata\": {\n                            \"request_id\": \"\",\n                            \"input_tokens_count\": 0,\n                            \"generated_tokens_count\": 0,\n                            \"time_cost\": time_cost,\n                            \"first_token_time\": 0,\n                            \"speed\": 0,\n                        }\n                    },\n                )\n            ]\n\n    async def async_speech_to_text(\n        self,\n        audio: str,\n        response_format: str = \"verbose_json\",\n        timestamp_granularities: List[str] = [\"word\",\"segment\"],\n    ):               \n        # Extract audio format and base64 data\n        data_prefix = \"data:audio/\"\n        base64_prefix = \";base64,\"\n        if not audio.startswith(data_prefix):\n            raise ValueError(\"Invalid audio data format\")\n        \n        format_end = audio.index(base64_prefix)\n        audio_format = audio[len(data_prefix):format_end]\n        base64_data = audio[format_end + len(base64_prefix):]\n        \n        # Decode the base64 audio data\n        audio_data = base64.b64decode(base64_data)\n\n        # Create a temporary file with the correct extension\n        with tempfile.NamedTemporaryFile(delete=False, suffix=f'.{audio_format}') as temp_audio_file:\n            temp_audio_file.write(audio_data)\n            temp_audio_file_path = temp_audio_file.name\n\n        try:\n            start_time = time.monotonic()\n                   \n            with open(temp_audio_file_path, \"rb\") as audio_file:                \n                transcription = self.client.audio.transcriptions.create(\n                    model=self.model or \"whisper-1\",\n                    file=audio_file,\n                    response_format=response_format,\n                    timestamp_granularities=timestamp_granularities,\n                )\n            time_cost = time.monotonic() - start_time\n\n            return [\n                (\n                    json.dumps(transcription.to_dict(), ensure_ascii=True),\n                    {\n                        \"metadata\": {\n                            \"request_id\": \"\",\n                            \"input_tokens_count\": 0,\n                            \"generated_tokens_count\": 0,\n                            \"time_cost\": time_cost,\n                            \"first_token_time\": 0,\n                            \"speed\": 0,\n                        }\n                    },\n                )\n            ]\n        finally:\n            # Clean up the temporary file\n            import os\n            os.unlink(temp_audio_file_path)\n\n    def image_to_text(self, ins: str, **kwargs):\n        pass\n\n    async def async_text_to_image(\n        self, stream: bool, input: str, size: str, quality: str, n: int, **kwargs\n    ):\n        if stream:\n            raise Exception(\"Stream not supported for text to image\")\n        start_time = time.monotonic()\n        response = await asyncfy_with_semaphore(\n            lambda: self.client.images.generate(\n                model=self.model,\n                prompt=input,\n                size=size,\n                quality=quality,\n                n=1,\n                response_format=\"b64_json\",\n                **kwargs,\n            )\n        )()\n        time_cost = time.monotonic() - start_time\n        base64_image = response.data[0].b64_json\n        return [\n            (\n                base64_image,\n                {\n                    \"metadata\": {\n                        \"request_id\": \"\",\n                        \"input_tokens_count\": 0,\n                        \"generated_tokens_count\": 0,\n                        \"time_cost\": time_cost,\n                        \"first_token_time\": 0,\n                        \"speed\": 0,\n                    }\n                },\n            )\n        ]\n\n    def text_to_text(self, ins: str, **kwargs):\n        pass\n\n    async def async_stream_chat(\n        self,\n        tokenizer,\n        ins: str,\n        his: List[Dict[str, Any]] = [],\n        max_length: int = 4096,\n        top_p: float = 0.7,\n        temperature: float = 0.9,\n        **kwargs,\n    ):\n\n        model = self.model\n\n        if \"model\" in kwargs:\n            model = kwargs[\"model\"]\n\n        messages = [\n            {\"role\": message[\"role\"], \"content\": self.process_input(message[\"content\"])}\n            for message in his\n        ] + [{\"role\": \"user\", \"content\": self.process_input(ins)}]\n\n        stream = kwargs.get(\"stream\", False)\n\n        ## content = [\n        ##    \"voice\": \"alloy\",\"input\": \"Hello, World!\",response_format: \"mp3\"]\n        last_message = messages[-1][\"content\"]\n\n        if isinstance(last_message, dict) and \"voice\" in last_message:\n            voice = last_message[\"voice\"]\n            response_format = last_message.get(\"response_format\", \"mp3\")\n            chunk_size = last_message.get(\"chunk_size\", None)\n            input = last_message[\"input\"]\n            return await self.async_text_to_speech(\n                stream=stream,\n                ins=input,\n                voice=voice,\n                chunk_size=chunk_size,\n                response_format=response_format,\n            )\n\n        if isinstance(last_message, dict) and \"input\" in last_message:\n            input = last_message[\"input\"]\n            size = last_message.get(\"size\", \"1024x1024\")\n            quality = last_message.get(\"quality\", \"standard\")\n            n = last_message.get(\"n\", 1)\n            return await self.async_text_to_image(\n                stream=stream, input=input, size=size, quality=quality, n=n\n            )\n\n        if isinstance(last_message, dict) and \"audio\" in last_message:            \n            audio_data = last_message.get(\"audio\", \"\")            \n            tpe = last_message.get(\"type\",\"wav\")\n            if not audio_data.startswith(\"data:\"):\n                audio_data = f\"data:audio/${tpe};base64,\" + audio_data                \n            return await self.async_speech_to_text(audio=audio_data)\n        \n        server = ray.get_actor(\"BLOCK_VLLM_STREAM_SERVER\")\n        request_id = [None]\n\n        def writer():\n            try:\n                r = \"\"\n                response = self.client.chat.completions.create(\n                    messages=messages,\n                    model=model,\n                    stream=True,\n                    max_tokens=max_length,\n                    temperature=temperature,\n                    top_p=top_p,\n                )\n                # input_tokens_count = 0\n                # generated_tokens_count = 0\n\n                request_id[0] = str(uuid.uuid4())\n\n                for chunk in response:\n                    content = chunk.choices[0].delta.content or \"\"\n                    r += content\n                    if hasattr(chunk, \"usage\") and chunk.usage:\n                        input_tokens_count = chunk.usage.prompt_tokens\n                        generated_tokens_count = chunk.usage.completion_tokens\n                    else:\n                        input_tokens_count = 0\n                        generated_tokens_count = 0\n                    ray.get(\n                        server.add_item.remote(\n                            request_id[0],\n                            StreamOutputs(\n                                outputs=[\n                                    SingleOutput(\n                                        text=r,\n                                        metadata=SingleOutputMeta(\n                                            input_tokens_count=input_tokens_count,\n                                            generated_tokens_count=generated_tokens_count,\n                                        ),\n                                    )\n                                ]\n                            ),\n                        )\n                    )\n            except:\n                traceback.print_exc()\n            ray.get(server.mark_done.remote(request_id[0]))\n\n        if stream:\n            threading.Thread(target=writer, daemon=True).start()\n\n            time_count = 60 * 100\n            while request_id[0] is None and time_count > 0:\n                time.sleep(0.01)\n                time_count -= 1\n\n            if request_id[0] is None:\n                raise Exception(\"Failed to get request id\")\n\n            def write_running():\n                return ray.get(server.add_item.remote(request_id[0], \"RUNNING\"))\n\n            await asyncio.to_thread(write_running)\n            return [\n                (\n                    \"\",\n                    {\n                        \"metadata\": {\n                            \"request_id\": request_id[0],\n                            \"stream_server\": \"BLOCK_VLLM_STREAM_SERVER\",\n                        }\n                    },\n                )\n            ]\n        else:\n            try:\n                start_time = time.monotonic()\n                response = await asyncfy_with_semaphore(\n                    lambda: self.client.chat.completions.create(\n                        messages=messages,\n                        model=model,\n                        max_tokens=max_length,\n                        temperature=temperature,\n                        top_p=top_p,\n                    )\n                )()\n\n                generated_text = response.choices[0].message.content\n                generated_tokens_count = response.usage.completion_tokens\n                input_tokens_count = response.usage.prompt_tokens\n                time_cost = time.monotonic() - start_time\n                return [\n                    (\n                        generated_text,\n                        {\n                            \"metadata\": {\n                                \"request_id\": response.id,\n                                \"input_tokens_count\": input_tokens_count,\n                                \"generated_tokens_count\": generated_tokens_count,\n                                \"time_cost\": time_cost,\n                                \"first_token_time\": 0,\n                                \"speed\": float(generated_tokens_count) / time_cost,\n                            }\n                        },\n                    )\n                ]\n            except Exception as e:\n                print(f\"Error: {e}\")\n                raise e\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/whisper/__init__.py\nimport whisper\nfrom typing import Dict, List, Tuple\nfrom byzerllm.utils.types import StopSequencesCriteria\n\nfrom typing import Dict, Any, List, Union\nfrom pyjava.storage import streaming_tar as STar\nimport json\nimport base64\nimport tempfile\nimport time\n\n\ndef get_meta(self):\n    return [\n        {\n            \"model_deploy_type\": \"proprietary\",\n            \"backend\": \"transformers\",\n            \"message_format\": True\n        }\n    ]\n\n\ndef process_input(ins: Union[str, List[Dict[str, Any]], Dict[str, Any]]):\n    # print(ins) \n    if isinstance(ins, list) or isinstance(ins, dict):\n        return ins\n\n    content = []\n    try:\n        ins_json = json.loads(ins)\n    except:\n        return ins\n\n    ## speech\n    if isinstance(ins_json, dict):\n        return ins_json\n\n    content = []\n    #     [\n    #     {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n    #     {\n    #       \"type\": \"image_url\",\n    #       \"image_url\": {\n    #         \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n    #         \"detail\": \"high\"\n    #       },\n    #     },\n    #   ],\n    for item in ins_json:\n        # for format like this: {\"image\": \"xxxxxx\", \"text\": \"What’s in this image?\",\"detail\":\"high\"}\n        # or {\"image\": \"xxxxxx\"}, {\"text\": \"What’s in this image?\"}\n        if (\"image\" in item or \"image_url\" in item) and \"type\" not in item:\n            image_data = item.get(\"image\", item.get(\"image_url\", \"\"))\n            ## \"data:image/jpeg;base64,\"\n            if not image_data.startswith(\"data:\"):\n                image_data = \"data:image/jpeg;base64,\" + image_data\n\n            ## get the other fields except image/text/image_url\n            other_fields = {\n                k: v for k, v in item.items() if k not in [\"image\", \"text\", \"image_url\"]\n            }\n            content.append(\n                {\n                    \"image_url\": {\"url\": image_data, **other_fields},\n                    \"type\": \"image_url\",\n                }\n            )\n\n        if \"text\" in item and \"type\" not in item:\n            text_data = item[\"text\"]\n            content.append({\"text\": text_data, \"type\": \"text\"})\n\n        ## for format like this: {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n        ## {\"type\": \"image_url\", \"image_url\": {\"url\":\"\",\"detail\":\"high\"}}\n        ## this is the standard format, just return it\n        if \"type\" in item and item[\"type\"] == \"text\":\n            content.append(item)\n\n        if \"type\" in item and item[\"type\"] == \"image_url\":\n            content.append(item)\n\n    if not content:\n        return ins\n\n    return content\n\n\ndef stream_chat(\n    self,\n    tokenizer,\n    ins: str,\n    his: List[Dict[str, str]] = [],\n    max_length: int = 4090,\n    top_p: float = 0.95,\n    temperature: float = 0.1,\n    **kwargs,\n):\n    messages = [\n        {\"role\": message[\"role\"], \"content\": process_input(message[\"content\"])}\n        for message in his\n    ] + [{\"role\": \"user\", \"content\": process_input(ins)}]\n\n    audio_input = messages[-1][\"content\"]\n\n    audio_data = audio_input.get(\"audio\", \"\")\n    audio = audio_input[\"audio\"]\n    response_format = audio_input.get(\"response_format\", \"json\")\n    timestamp_granularities = audio_input.get(\n        \"timestamp_granularities\", [\"word\", \"segment\"]\n    )\n    data_prefix = \"data:audio/\"\n    base64_prefix = \";base64,\"\n    if not audio.startswith(data_prefix):\n        raise ValueError(\"Invalid audio data format\")\n\n    format_end = audio.index(base64_prefix)\n    audio_format = audio[len(data_prefix) : format_end]\n    base64_data = audio[format_end + len(base64_prefix) :]\n\n    # Decode the base64 audio data\n    audio_data = base64.b64decode(base64_data)\n\n    # Create a temporary file with the correct extension\n    with tempfile.NamedTemporaryFile(\n        delete=False, suffix=f\".{audio_format}\"\n    ) as temp_audio_file:\n        temp_audio_file.write(audio_data)\n        temp_audio_file_path = temp_audio_file.name\n\n    try:\n        start_time = time.monotonic()\n        word_timestamps = \"word\" in timestamp_granularities\n        \n        result = self.transcribe(temp_audio_file_path, word_timestamps=word_timestamps)        \n            \n        time_cost = time.monotonic() - start_time\n        return [\n            (\n                json.dumps(result, ensure_ascii=False),\n                {\n                    \"metadata\": {\n                        \"request_id\": \"\",\n                        \"input_tokens_count\": 0,\n                        \"generated_tokens_count\": 0,\n                        \"time_cost\": time_cost,\n                        \"first_token_time\": 0,\n                        \"speed\": 0,\n                    }\n                },\n            )\n        ]\n    finally:\n        # Clean up the temporary file\n        import os\n\n        os.unlink(temp_audio_file_path)\n\n\ndef init_model(\n    model_dir, infer_params: Dict[str, str] = {}, sys_conf: Dict[str, str] = {}\n):\n    model = whisper.load_model(\"large\", download_root=model_dir)\n    import types\n\n    model.stream_chat = types.MethodType(stream_chat, model)\n    model.get_meta = types.MethodType(get_meta, model)\n    return (model, None)\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/notebooks/test_sensevoice.py\nimport sys\nimport os\n\n# Add the project root to the Python path\nsys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\")))\n\nfrom src.byzerllm.sensevoice import init_model, format_str_v3\nimport base64\nimport json\n\n\ndef main():\n    # Initialize the model\n    model_dir = \"/Users/allwefantasy/models/SenseVoiceSmall\"\n    model, _ = init_model(model_dir)\n\n    # Prepare test audio\n    with open(\n        \"/Users/allwefantasy/models/SenseVoiceSmall/example/zh.mp3\", \"rb\"\n    ) as audio_file:\n        audio_data = audio_file.read()\n        base64_audio = base64.b64encode(audio_data).decode(\"utf-8\")        \n        # Prepare input in the expected format\n        test_input = json.dumps(\n            {\n                            \"type\": \"audio\",\n                            \"audio\": f\"data:audio/wav;base64,{base64_audio}\",\n                        }\n        )\n\n        # Call stream_chat\n        result = model.stream_chat(tokenizer=None, ins = test_input)\n\n        # Process and print the result\n        if result and len(result) > 0:\n            output, metadata = result[0]\n            parsed_output = json.loads(output)\n            formatted_text = parsed_output.get(\"text\", \"\")\n            print(\"Transcription:\", formatted_text)\n            print(\"Metadata:\", metadata)\n        else:\n            print(\"No result returned from the model.\")\n\n\nif __name__ == \"__main__\":\n    main()\n\n\n##File: /Users/allwefantasy/projects/Qwen2-Audio/README_CN.md\n<p align=\"left\">\n        中文</a>&nbsp ｜ &nbsp<a href=\"README.md\">English</a>\n</p>\n<br><br>\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/assets/blog/qwenaudio/qwen2audio_logo.png\" width=\"400\"/>\n<p>\n\n<p align=\"center\">\nQwen2-Audio-7B <a href=\"https://modelscope.cn/models/qwen/Qwen2-Audio-7B\">🤖 </a> | <a href=\"https://huggingface.co/Qwen/Qwen2-Audio-7B\">🤗</a>&nbsp ｜ Qwen-Audio-7B-Instruct <a href=\"https://modelscope.cn/models/qwen/Qwen2-Audio-7B-Instruct\">🤖 </a>| <a href=\"https://huggingface.co/Qwen/Qwen2-Audio-7B-Instruct\">🤗</a>&nbsp ｜ Demo<a href=\"https://modelscope.cn/studios/qwen/Qwen2-Audio-Instruct-Demo\"> 🤖</a> | <a href=\"https://huggingface.co/spaces/Qwen/Qwen2-Audio-Instruct-Demo\">🤗</a>&nbsp\n<br>\n📑 <a href=\"https://arxiv.org/abs/2407.10759\">Paper</a> &nbsp&nbsp | &nbsp&nbsp 📑 <a href=\"https://qwenlm.github.io/blog/qwen2-audio\">Blog</a> &nbsp&nbsp | &nbsp&nbsp 💬 <a href=\"https://github.com/QwenLM/Qwen/blob/main/assets/wechat.png\">WeChat (微信)</a>&nbsp&nbsp | &nbsp&nbsp <a href=\"https://discord.gg/CV4E9rpNSD\">Discord</a>&nbsp&nbsp\n</p>\n\n\n\n我们介绍Qwen-Audio的最新进展：Qwen2-Audio。作为一个大规模音频语言模型，Qwen2-Audio能够接受各种音频信号输入，并根据语音指令执行音频分析或直接响应文本。我们介绍两种不同的音频交互模式：语音聊天 voice chat 和音频分析 audio analysis。\n\n* 语音聊天：用户可以自由地与 Qwen2-Audio 进行语音互动，而无需文本输入；\n* 音频分析：用户可以在互动过程中提供音频和文本指令对音频进行分析；\n\n**我们已经开源了 Qwen2-Audio 系列的两个模型：Qwen2-Audio-7B和Qwen2-Audio-7B-Instruct。**\n\n## 模型结构与训练范式\n\nQwen2-Audio 三阶段训练过程概述。\n\n<p align=\"left\">\n    <img src=\"assets/framework.png\" width=\"80%\"/>\n<p>\n\n\n\n## 新闻\n* 2024.8.9 🎉 我们在 ModelScope 和 Hugging Face 开源了`Qwen2-Audio-7B`和`Qwen2-Audio-7B-Instruct`的 checkpoint.\n* 2024.7.15 🎉 我们发布了 Qwen2-Audio 的[论文](https://arxiv.org/abs/2407.10759), 介绍了相关的模型结构，训练方法和模型表现。\n* 2023.11.30 🔥 我们发布了**Qwen-Audio**系列\n<br>\n\n## 评测\n我们在标准的13个学术数据集上评测了模型的能力如下：\n<table><thead><tr><th>Task</th><th>Description</th><th>Dataset</th><th>Split</th><th>Metric</th></tr></thead><tbody><tr><td rowspan=\"4\">ASR</td><td rowspan=\"4\">Automatic Speech Recognition</td><td>Fleurs</td><td>dev | test</td><td rowspan=\"4\">WER</td></tr><tr><td>Aishell2</td><td>test</td></tr><tr><td>Librispeech</td><td>dev | test</td></tr><tr><td>Common Voice</td><td>dev | test</td></tr><tr><td>S2TT</td><td>Speech-to-Text Translation</td><td>CoVoST2</td><td>test</td><td>BLEU </td></tr><tr><td>SER</td><td>Speech Emotion Recognition</td><td>Meld</td><td>test</td><td>ACC</td></tr><tr><td>VSC</td><td>Vocal Sound Classification</td><td>VocalSound</td><td>test</td><td>ACC</td></tr><tr><td rowspan=\"4\"><a href=\"https://github.com/OFA-Sys/AIR-Bench\">AIR-Bench</a><br></td><td>Chat-Benchmark-Speech</td><td>Fisher<br>SpokenWOZ<br>IEMOCAP<br>Common voice</td><td>dev | test</td><td>GPT-4 Eval</td></tr><tr><td>Chat-Benchmark-Sound</td><td>Clotho</td><td>dev | test</td><td>GPT-4 Eval</td></tr>\n<tr><td>Chat-Benchmark-Music</td><td>MusicCaps</td><td>dev | test</td><td>GPT-4 Eval</td></tr><tr><td>Chat-Benchmark-Mixed-Audio</td><td>Common voice<br>AudioCaps<br>MusicCaps</td><td>dev | test</td><td>GPT-4 Eval</td></tr></tbody></table>\n\n\n以下是整体表现：\n<p align=\"left\">\n    <img src=\"assets/radar_compare_qwen_audio.png\" width=\"70%\"/>\n<p>\n\n评测分数详情如下：\n<br>\n<b>（注意：我们所展示的评测结果是在原始训练框架的初始模型上的，然而在框架转换 Huggingface 后指标出现了部分波动，在这里我们展示我们的全部测评结果：首先是论文中的初始模型结果）</b>\n\n<table><thead><tr><th rowspan=\"2\">Task</th><th rowspan=\"2\">Dataset</th><th rowspan=\"2\">Model</th><th colspan=\"2\">Performance</th></tr><tr><th>Metrics</th><th>Results</th></tr></thead><tbody><tr><td rowspan=\"15\">ASR</td><td rowspan=\"7\"><b>Librispeech</b><br>dev-clean | dev-other | <br>test-clean | test-other</td><td>SpeechT5</td><td rowspan=\"7\">WER </td><td>2.1 | 5.5 | 2.4 | 5.8</td></tr><tr><td>SpeechNet</td><td>- | - | 30.7 | -</td></tr><tr><td>SLM-FT</td><td>- | - | 2.6 | 5.0</td></tr><tr><td>SALMONN</td><td>- | - | 2.1 | 4.9</td></tr><tr><td>SpeechVerse</td><td>- | - | 2.1 | 4.4</td></tr><tr><td>Qwen-Audio</td><td>1.8 | 4.0 | 2.0 | 4.2</td></tr><tr><td>Qwen2-Audio</td><td><b>1.3 | 3.4 | 1.6 | 3.6</b></td></tr><tr><td rowspan=\"2\"><b>Common Voice 15</b> <br>en | zh | yue | fr</td><td>Whisper-large-v3</td><td rowspan=\"2\">WER </td><td>9.3 | 12.8 | 10.9 | 10.8</td></tr><tr><td>Qwen2-Audio</td><td><b>8.6 | 6.9 | 5.9 | 9.6</b></td></tr>\n<tr><td rowspan=\"2\"><b>Fleurs</b> <br>zh</td><td>Whisper-large-v3</td><td rowspan=\"2\">WER </td><td>7.7</td></tr><tr><td>Qwen2-Audio</td><td><b>7.5</b></td></tr><tr><td rowspan=\"4\"><b>Aishell2</b> <br>Mic | iOS | Android</td><td>MMSpeech-base</td><td rowspan=\"4\">WER </td><td>4.5 | 3.9 | 4.0</td></tr><tr><td>Paraformer-large</td><td>- | <b>2.9</b> | -</td></tr><tr><td>Qwen-Audio</td><td>3.3 | 3.1 | 3.3</td></tr><tr><td>Qwen2-Audio</td><td><b>3.0</b> | 3.0 | <b>2.9</b></td></tr><tr><td rowspan=\"8\">S2TT</td><td rowspan=\"5\"><b>CoVoST2</b> <br>en-de | de-en | <br>en-zh | zh-en</td><td>SALMONN</td><td rowspan=\"5\">BLEU </td><td>18.6 | - | 33.1 | -</td></tr><tr><td>SpeechLLaMA</td><td>- | 27.1 | - | 12.3</td></tr><tr><td>BLSP</td><td>14.1 | - | - | -</td></tr><tr><td>Qwen-Audio</td><td>25.1 | 33.9 | 41.5 | 15.7</td></tr><tr><td>Qwen2-Audio</td><td><b>29.9 | 35.2 | 45.2 | 24.4</b></td></tr>\n<tr><td rowspan=\"3\"><b>CoVoST2</b> <br>es-en | fr-en | it-en |</td><td>SpeechLLaMA</td><td rowspan=\"3\">BLEU </td><td>27.9 | 25.2 | 25.9</td></tr><tr><td>Qwen-Audio</td><td>39.7 | <b>38.5</b> | 36.0</td></tr><tr><td>Qwen2-Audio</td><td><b>40.0 | 38.5 | 36.3</b></td></tr><tr><td rowspan=\"3\">SER</td><td rowspan=\"3\"><b>Meld</b></td><td>WavLM-large</td><td rowspan=\"3\">ACC </td><td>0.542</td></tr><tr><td>Qwen-Audio</td><td><b>0.557</b></td></tr><tr><td>Qwen2-Audio</td><td>0.553</td></tr><tr><td rowspan=\"4\">VSC</td><td rowspan=\"4\"><b>VocalSound</b></td><td>CLAP</td><td rowspan=\"4\">ACC </td><td>0.4945</td></tr><tr><td>Pengi</td><td>0.6035</td></tr><tr><td>Qwen-Audio</td><td>0.9289</td></tr><tr><td>Qwen2-Audio</td><td><b>0.9392</b></td></tr>\n<tr><td>AIR-Bench <br></td><td><b>Chat Benchmark</b><br>Speech | Sound |<br> Music | Mixed-Audio</td><td>SALMONN<br>BLSP<br>Pandagpt<br>Macaw-LLM<br>SpeechGPT<br>Next-gpt<br>Qwen-Audio<br>Gemini-1.5-pro<br>Qwen2-Audio</td><td>GPT-4 </td><td>6.16 | 6.28 | 5.95 | 6.08<br>6.17 | 5.55 | 5.08 | 5.33<br>3.58 | 5.46 | 5.06 | 4.25<br>0.97 | 1.01 | 0.91 | 1.01<br>1.57 | 0.95 | 0.95 | 4.13<br>3.86 | 4.76 | 4.18 | 4.13<br>6.47 | 6.95 | 5.52 | 6.08<br>6.97 | 5.49 | 5.06 | 5.27<br><b>7.18 | 6.99 | 6.79 | 6.77</b></td></tr></tbody></table>\n\n<b>（其次是转换 huggingface 后的）</b>\n\n<table><thead><tr><th rowspan=\"2\">Task</th><th rowspan=\"2\">Dataset</th><th rowspan=\"2\">Model</th><th colspan=\"2\">Performance</th></tr><tr><th>Metrics</th><th>Results</th></tr></thead><tbody><tr><td rowspan=\"15\">ASR</td><td rowspan=\"7\"><b>Librispeech</b><br>dev-clean | dev-other | <br>test-clean | test-other</td><td>SpeechT5</td><td rowspan=\"7\">WER </td><td>2.1 | 5.5 | 2.4 | 5.8</td></tr><tr><td>SpeechNet</td><td>- | - | 30.7 | -</td></tr><tr><td>SLM-FT</td><td>- | - | 2.6 | 5.0</td></tr><tr><td>SALMONN</td><td>- | - | 2.1 | 4.9</td></tr><tr><td>SpeechVerse</td><td>- | - | 2.1 | 4.4</td></tr><tr><td>Qwen-Audio</td><td>1.8 | 4.0 | 2.0 | 4.2</td></tr><tr><td>Qwen2-Audio</td><td><b>1.7 | 3.6 | 1.7 | 4.0</b></td></tr><tr><td rowspan=\"2\"><b>Common Voice 15</b> <br>en | zh | yue | fr</td><td>Whisper-large-v3</td><td rowspan=\"2\">WER </td><td>9.3 | 12.8 | 10.9 | 10.8</td></tr><tr><td>Qwen2-Audio</td><td><b>8.7 | 6.5 | 5.9 | 9.6</b></td></tr>\n<tr><td rowspan=\"2\"><b>Fleurs</b> <br>zh</td><td>Whisper-large-v3</td><td rowspan=\"2\">WER </td><td>7.7</td></tr><tr><td>Qwen2-Audio</td><td><b>7.0</b></td></tr><tr><td rowspan=\"4\"><b>Aishell2</b> <br>Mic | iOS | Android</td><td>MMSpeech-base</td><td rowspan=\"4\">WER </td><td>4.5 | 3.9 | 4.0</td></tr><tr><td>Paraformer-large</td><td>- | <b>2.9</b> | -</td></tr><tr><td>Qwen-Audio</td><td>3.3 | 3.1 | 3.3</td></tr><tr><td>Qwen2-Audio</td><td><b>3.2</b> | 3.1 | <b>2.9</b></td></tr><tr><td rowspan=\"8\">S2TT</td><td rowspan=\"5\"><b>CoVoST2</b> <br>en-de | de-en | <br>en-zh | zh-en</td><td>SALMONN</td><td rowspan=\"5\">BLEU </td><td>18.6 | - | 33.1 | -</td></tr><tr><td>SpeechLLaMA</td><td>- | 27.1 | - | 12.3</td></tr><tr><td>BLSP</td><td>14.1 | - | - | -</td></tr><tr><td>Qwen-Audio</td><td>25.1 | <b>33.9</b> | 41.5 | 15.7</td></tr><tr><td>Qwen2-Audio</td><td><b>29.6</b> | 33.6 | <b>45.6</b> | <b>24.0</b></td></tr>\n<tr><td rowspan=\"3\"><b>CoVoST2</b> <br>es-en | fr-en | it-en |</td><td>SpeechLLaMA</td><td rowspan=\"3\">BLEU </td><td>27.9 | 25.2 | 25.9</td></tr><tr><td>Qwen-Audio</td><td><b>39.7 | 38.5 | 36.0</b></td></tr><tr><td>Qwen2-Audio</td><td>38.7 | 37.2 | 35.2</td></tr><tr><td rowspan=\"3\">SER</td><td rowspan=\"3\"><b>Meld</b></td><td>WavLM-large</td><td rowspan=\"3\">ACC </td><td>0.542</td></tr><tr><td>Qwen-Audio</td><td><b>0.557</b></td></tr><tr><td>Qwen2-Audio</td><td>0.535</td></tr><tr><td rowspan=\"4\">VSC</td><td rowspan=\"4\"><b>VocalSound</b></td><td>CLAP</td><td rowspan=\"4\">ACC </td><td>0.4945</td></tr><tr><td>Pengi</td><td>0.6035</td></tr><tr><td>Qwen-Audio</td><td>0.9289</td></tr><tr><td>Qwen2-Audio</td><td><b>0.9395</b></td></tr>\n<tr><td>AIR-Bench <br></td><td><b>Chat Benchmark</b><br>Speech | Sound |<br> Music | Mixed-Audio</td><td>SALMONN<br>BLSP<br>Pandagpt<br>Macaw-LLM<br>SpeechGPT<br>Next-gpt<br>Qwen-Audio<br>Gemini-1.5-pro<br>Qwen2-Audio</td><td>GPT-4 </td><td>6.16 | 6.28 | 5.95 | 6.08<br>6.17 | 5.55 | 5.08 | 5.33<br>3.58 | 5.46 | 5.06 | 4.25<br>0.97 | 1.01 | 0.91 | 1.01<br>1.57 | 0.95 | 0.95 | 4.13<br>3.86 | 4.76 | 4.18 | 4.13<br>6.47 | <b>6.95</b> | 5.52 | 6.08<br>6.97 | 5.49 | 5.06 | 5.27<br><b>7.24</b> | 6.83 | <b>6.73</b> | <b>6.42</b></td></tr></tbody></table>\n\n\n我们提供了以上**所有**评测脚本以供复现我们的实验结果。请阅读 [eval_audio/EVALUATION.md](eval_audio/EVALUATION.md) 了解更多信息。\n\n## 部署要求\nThe code of Qwen2-Audio has been in the latest Hugging face transformers and we advise you to build from source with command `pip install git+https://github.com/huggingface/transformers`, or you might encounter the following error:\nQwen2-Audio的代码已经包含在最新的 Hugging Face Transformers 的主分支中，我们建议您使用命令`pip install git+https://github.com/huggingface/transformers`\n```\nKeyError: 'qwen2-audio'\n```\n## 快速使用\n我们提供简单的示例来说明如何利用 🤗 Transformers 快速使用 Qwen2-Audio-7B 和 Qwen2-Audio-7B-Instruct。\n在开始前，请确保你已经配置好环境并安装好相关的代码包。最重要的是，确保你满足上述要求，然后安装相关的依赖库。\n接下来你可以开始使用 Transformers 或者 ModelScope 来使用我们的模型。目前Qwen2-Audio-7B 及 Qwen2-Audio-7B-Instruct 模型处理30秒以内的音频表现更佳。\n#### 🤗 Hugging Face Transformers\n如希望使用 Qwen2-Audio-7B-Instruct 进行推理，我们分别演示语音聊天和音频分析的交互方式，所需要写的只是如下所示的数行代码。\n##### 语音聊天推理\n在语音聊天模式下，用户可以自由地与 Qwen2-Audio 进行语音交互，无需文字输入：\n```python\nfrom io import BytesIO\nfrom urllib.request import urlopen\nimport librosa\nfrom transformers import Qwen2AudioForConditionalGeneration, AutoProcessor\n\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\")\nmodel = Qwen2AudioForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\", device_map=\"auto\")\n\nconversation = [\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/guess_age_gender.wav\"},\n    ]},\n    {\"role\": \"assistant\", \"content\": \"Yes, the speaker is female and in her twenties.\"},\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/translate_to_chinese.wav\"},\n    ]},\n]\ntext = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\naudios = []\nfor message in conversation:\n    if isinstance(message[\"content\"], list):\n        for ele in message[\"content\"]:\n            if ele[\"type\"] == \"audio\":\n                audios.append(librosa.load(\n                    BytesIO(urlopen(ele['audio_url']).read()), \n                    sr=processor.feature_extractor.sampling_rate)[0]\n                )\n\ninputs = processor(text=text, audios=audios, return_tensors=\"pt\", padding=True)\ninputs.input_ids = inputs.input_ids.to(\"cuda\")\n\ngenerate_ids = model.generate(**inputs, max_length=256)\ngenerate_ids = generate_ids[:, inputs.input_ids.size(1):]\n\nresponse = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n```\n##### 音频分析推理\n在音频分析中，用户可以提供音频和文字问题来实现对音频的分析：\n```python\nfrom io import BytesIO\nfrom urllib.request import urlopen\nimport librosa\nfrom transformers import Qwen2AudioForConditionalGeneration, AutoProcessor\n\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\")\nmodel = Qwen2AudioForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\", device_map=\"auto\")\n\nconversation = [\n    {'role': 'system', 'content': 'You are a helpful assistant.'}, \n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/glass-breaking-151256.mp3\"},\n        {\"type\": \"text\", \"text\": \"What's that sound?\"},\n    ]},\n    {\"role\": \"assistant\", \"content\": \"It is the sound of glass shattering.\"},\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"text\", \"text\": \"What can you do when you hear that?\"},\n    ]},\n    {\"role\": \"assistant\", \"content\": \"Stay alert and cautious, and check if anyone is hurt or if there is any damage to property.\"},\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/1272-128104-0000.flac\"},\n        {\"type\": \"text\", \"text\": \"What does the person say?\"},\n    ]},\n]\ntext = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\naudios = []\nfor message in conversation:\n    if isinstance(message[\"content\"], list):\n        for ele in message[\"content\"]:\n            if ele[\"type\"] == \"audio\":\n                audios.append(\n                    librosa.load(\n                        BytesIO(urlopen(ele['audio_url']).read()), \n                        sr=processor.feature_extractor.sampling_rate)[0]\n                )\n\ninputs = processor(text=text, audios=audios, return_tensors=\"pt\", padding=True)\ninputs.input_ids = inputs.input_ids.to(\"cuda\")\n\ngenerate_ids = model.generate(**inputs, max_length=256)\ngenerate_ids = generate_ids[:, inputs.input_ids.size(1):]\n\nresponse = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n```\n##### 批量推理\n我们也支持批量推理：\n```python\nfrom io import BytesIO\nfrom urllib.request import urlopen\nimport librosa\nfrom transformers import Qwen2AudioForConditionalGeneration, AutoProcessor\n\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\")\nmodel = Qwen2AudioForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\", device_map=\"auto\")\n\nconversation1 = [\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/glass-breaking-151256.mp3\"},\n        {\"type\": \"text\", \"text\": \"What's that sound?\"},\n    ]},\n    {\"role\": \"assistant\", \"content\": \"It is the sound of glass shattering.\"},\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/f2641_0_throatclearing.wav\"},\n        {\"type\": \"text\", \"text\": \"What can you hear?\"},\n    ]}\n]\n\nconversation2 = [\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/1272-128104-0000.flac\"},\n        {\"type\": \"text\", \"text\": \"What does the person say?\"},\n    ]},\n]\n\nconversations = [conversation1, conversation2]\n\ntext = [processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False) for conversation in conversations]\n\naudios = []\nfor conversation in conversations:\n    for message in conversation:\n        if isinstance(message[\"content\"], list):\n            for ele in message[\"content\"]:\n                if ele[\"type\"] == \"audio\":\n                    audios.append(\n                        librosa.load(\n                            BytesIO(urlopen(ele['audio_url']).read()), \n                            sr=processor.feature_extractor.sampling_rate)[0]\n                    )\n\ninputs = processor(text=text, audios=audios, return_tensors=\"pt\", padding=True)\ninputs['input_ids'] = inputs['input_ids'].to(\"cuda\")\ninputs.input_ids = inputs.input_ids.to(\"cuda\")\n\ngenerate_ids = model.generate(**inputs, max_length=256)\ngenerate_ids = generate_ids[:, inputs.input_ids.size(1):]\n\nresponse = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n```\n运行Qwen2-Audio-7B同样非常简单。\n```python\nfrom io import BytesIO\nfrom urllib.request import urlopen\nimport librosa\nfrom transformers import AutoProcessor, Qwen2AudioForConditionalGeneration\n\nmodel = Qwen2AudioForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-Audio-7B\" ,trust_remote_code=True)\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-Audio-7B\" ,trust_remote_code=True)\n\nprompt = \"<|audio_bos|><|AUDIO|><|audio_eos|>Generate the caption in English:\"\nurl = \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Audio/glass-breaking-151256.mp3\"\naudio, sr = librosa.load(BytesIO(urlopen(url).read()), sr=processor.feature_extractor.sampling_rate)\ninputs = processor(text=prompt, audios=audio, return_tensors=\"pt\")\n\ngenerated_ids = model.generate(**inputs, max_length=256)\ngenerated_ids = generated_ids[:, inputs.input_ids.size(1):]\nresponse = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n```\n#### 🤖 ModelScope\n我们强烈建议用户，特别是中国大陆地区的用户，使用 ModelScope。`snapshot_download` 可以帮助您解决下载检查点时遇到的问题。\n<br>\n## Demo\n### Web UI\n我们提供了 Web UI 的 demo 供用户使用。在开始前，确保已经安装如下代码库：\n```\npip install -r requirements_web_demo.txt\n```\n随后运行如下命令，并点击生成链接：\n```\npython demo/web_demo_audio.py\n```\n<br>\n\n## 样例展示\n更多样例将更新于[通义千问博客](https://qwenlm.github.io/blog/qwen2-audio)中的 Qwen2-Audio 博客。\n\n## 团队招聘\n\n我们是通义千问语音多模态团队，致力于以通义千问为核心，拓展音频多模态理解和生成能力，实现自由灵活的音频交互。目前团队蓬勃发展中，如有意向实习或全职加入我们，请发送简历至 `qwen_audio@list.alibaba-inc.com`.\n<br>\n\n## 使用协议\n\n请查看每个模型在其 Hugging Face 仓库中的许可证。您无需提交商业使用申请。\n<br>\n\n\n\n## 引用\n\n如果你觉得我们的论文和代码对你的研究有帮助，请考虑 :star: 和引用 :pencil: :)\n\n```BibTeX\n@article{Qwen-Audio,\n  title={Qwen-Audio: Advancing Universal Audio Understanding via Unified Large-Scale Audio-Language Models},\n  author={Chu, Yunfei and Xu, Jin and Zhou, Xiaohuan and Yang, Qian and Zhang, Shiliang and Yan, Zhijie  and Zhou, Chang and Zhou, Jingren},\n  journal={arXiv preprint arXiv:2311.07919},\n  year={2023}\n}\n```\n\n```BibTeX\n@article{Qwen2-Audio,\n  title={Qwen2-Audio Technical Report},\n  author={Chu, Yunfei and Xu, Jin and Yang, Qian and Wei, Haojie and Wei, Xipin and Guo,  Zhifang and Leng, Yichong and Lv, Yuanjun and He, Jinzheng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},\n  journal={arXiv preprint arXiv:2407.10759},\n  year={2024}\n}\n```\n## 联系我们\n\n如果你想给我们的研发团队和产品团队留言，请通过邮件（`qianwen_opensource@alibabacloud.com`）联系我们。\n<br>\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/qwen_voice/__init__.py\nimport time\nfrom typing import List, Tuple, Dict, Any, Union\nimport base64\nimport json\nimport librosa\nimport io\nfrom transformers import Qwen2AudioForConditionalGeneration, AutoProcessor\n\ndef get_meta(self):\n    return [\n        {\n            \"model_deploy_type\": \"proprietary\",\n            \"backend\": \"transformers\",\n            \"message_format\": True\n        }\n    ]\n\ndef process_input(ins: Union[str, List[Dict[str, Any]], Dict[str, Any]]):\n    if isinstance(ins, list) or isinstance(ins, dict):\n        return ins\n\n    content = []\n    try:\n        ins_json = json.loads(ins)\n    except:\n        return ins\n\n    if isinstance(ins_json, dict):\n        return ins_json\n\n    for item in ins_json:\n        if (\"audio\" in item or \"audio_url\" in item) and \"type\" not in item:\n            audio_data = item.get(\"audio\", item.get(\"audio_url\", \"\"))\n            if not audio_data.startswith(\"data:\"):\n                audio_data = \"data:audio/wav;base64,\" + audio_data\n\n            other_fields = {k: v for k, v in item.items() if k not in [\"audio\", \"text\", \"audio_url\"]}\n            content.append(\n                {\n                    \"audio_url\": {\"url\": audio_data, **other_fields},\n                    \"type\": \"audio\",\n                }\n            )\n\n        if \"text\" in item and \"type\" not in item:\n            text_data = item[\"text\"]\n            content.append({\"text\": text_data, \"type\": \"text\"})\n\n        if \"type\" in item and item[\"type\"] in [\"text\", \"audio\"]:\n            content.append(item)\n\n    if not content:\n        return ins\n\n    return content\n\ndef stream_chat(\n    self,\n    tokenizer,\n    ins: str,\n    his: List[Dict[str, str]] = [],\n    max_length: int = 4096,\n    top_p: float = 0.95,\n    temperature: float = 0.1,\n    **kwargs,\n):\n    messages = [\n        {\"role\": message[\"role\"], \"content\": process_input(message[\"content\"])}\n        for message in his\n    ] + [{\"role\": \"user\", \"content\": process_input(ins)}]\n\n    text = self.processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n    audios = []\n    for message in messages:\n        if isinstance(message[\"content\"], list):\n            for ele in message[\"content\"]:\n                if ele[\"type\"] == \"audio\":\n                    audio_data = ele[\"audio_url\"].split(\",\")[1]\n                    audio_bytes = base64.b64decode(audio_data)\n                    audio, _ = librosa.load(io.BytesIO(audio_bytes), sr=self.processor.feature_extractor.sampling_rate)\n                    audios.append(audio)\n\n    inputs = self.processor(text=text, audios=audios, return_tensors=\"pt\", padding=True)\n    inputs.input_ids = inputs.input_ids.to(self.device)\n\n    start_time = time.monotonic()\n    generate_ids = self.generate(**inputs, max_length=max_length)\n    generate_ids = generate_ids[:, inputs.input_ids.size(1):]\n\n    response = self.processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n    time_cost = time.monotonic() - start_time\n\n    return [\n        (\n            response,\n            {\n                \"metadata\": {\n                    \"request_id\": \"\",\n                    \"input_tokens_count\": inputs.input_ids.size(1),\n                    \"generated_tokens_count\": generate_ids.size(1),\n                    \"time_cost\": time_cost,\n                    \"first_token_time\": 0,\n                    \"speed\": float(generate_ids.size(1)) / time_cost if time_cost > 0 else 0,\n                }\n            },\n        )\n    ]\n\ndef init_model(model_dir, infer_params: Dict[str, str] = {}, sys_conf: Dict[str, str] = {}):\n    processor = AutoProcessor.from_pretrained(model_dir)\n    model = Qwen2AudioForConditionalGeneration.from_pretrained(model_dir, device_map=\"auto\")\n    model.processor = processor\n    import types\n    model.stream_chat = types.MethodType(stream_chat, model)\n    model.get_meta = types.MethodType(get_meta, model)    \n    return (model, None)\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/apps/llama_index/simple_retrieval.py\n\nfrom byzerllm.utils.retrieval import ByzerRetrieval\nfrom typing import Any, Callable, Dict, List, Optional, Tuple, Type, Union\nfrom byzerllm.utils.client import ByzerLLM\nfrom byzerllm.utils import generate_str_md5\nfrom byzerllm.utils.retrieval import ByzerRetrieval\nimport time\nfrom byzerllm.utils.client import LLMHistoryItem,LLMRequest\nfrom byzerllm.utils.retrieval import TableSettings,SearchQuery\nimport uuid\nimport json\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\ntry:\n    from termcolor import colored\nexcept ImportError:\n\n    def colored(x, *args, **kwargs):\n        return x\n\nimport jieba  \n\nclass SimpleRetrieval:\n    def __init__(self,llm:ByzerLLM, retrieval: ByzerRetrieval,\n                 chunk_collection: Optional[str] = \"default\",\n                 retrieval_cluster:str=\"byzerai_store\",\n                 retrieval_db:str=\"byzerai_store\",\n                 max_output_length=10000):\n        self.chunk_collection = chunk_collection\n        self.retrieval_cluster = retrieval_cluster\n        self.retrieval_db = retrieval_db\n        self.max_output_length = max_output_length\n        self.llm = llm\n        self.retrieval = retrieval                \n        if self.llm.default_emb_model_name is None:\n            raise Exception(f'''\nemb model does not exist. Try to use `llm.setup_default_emb_model_name` to set the default emb model name.\n''')        \n\n        # create the retrieval database/table if not exists\n        if self.retrieval and not self.retrieval.check_table_exists(self.retrieval_cluster,self.retrieval_db,\"text_content\"):\n           self.retrieval.create_table(self.retrieval_cluster,tableSettings=TableSettings(\n                database=self.retrieval_db,\n                table=\"text_content\",schema='''st(\nfield(_id,string),\nfield(doc_id,string),\nfield(file_path,string),\nfield(owner,string),\nfield(content,string,analyze),\nfield(raw_content,string,no_index),\nfield(metadata,string,analyze),\nfield(json_data,string,no_index),\nfield(collection,string),\nfield(created_time,long,sort)\n)''',\n                location=f\"/tmp/{self.retrieval_cluster}\",num_shards=1 \n           ))\n\n           self.retrieval.create_table(self.retrieval_cluster,tableSettings=TableSettings(\n                database=self.retrieval_db,\n                table=\"text_content_chunk\",schema='''st(\nfield(_id,string),\nfield(doc_id,string),\nfield(file_path,string),\nfield(owner,string),\nfield(chunk,string,analyze),\nfield(raw_chunk,string,no_index),\nfield(chunk_vector,array(float)),\nfield(metadata,string,analyze),\nfield(json_data,string,no_index),\nfield(chunk_collection,string),\nfield(created_time,long,sort)\n)''',\n                location=f\"/tmp/{self.retrieval_cluster}\",num_shards=1                \n           )) \n            \n\n    def save_doc(self,data:List[Dict[str,Any]],owner:Optional[str]=None,):\n\n        if not self.retrieval:\n            raise Exception(\"retrieval is not setup\")\n  \n        owner = owner or \"default\"\n    \n        result = []\n        for item in data:\n            \n            doc_id = item[\"doc_id\"]\n            json_data_obj = json.loads(item[\"json_data\"])\n            collection = item[\"collection\"]\n            _id = f\"{collection}/{doc_id}\"\n                    \n            file_path = json_data_obj.get(\"metadata\",{}).get(\"file_path\",\"\")                \n            result.append({\"_id\":_id,\n            \"doc_id\":doc_id,     \n            \"file_path\":file_path,          \n            \"json_data\":item[\"json_data\"],\n            \"metadata\":self.search_tokenize(item[\"json_data\"]), \n            \"collection\":collection,            \n            \"content\":self.search_tokenize(item[\"content\"]),\n            \"raw_content\":item[\"content\"],\n            \"owner\":owner,          \n            \"created_time\":int(time.time()*1000),\n            })        \n                    \n        self.retrieval.build_from_dicts(self.retrieval_cluster,self.retrieval_db,\"text_content\",result)            \n\n    \n    def _owner_filter(self,owner:str):\n        return {\"field\":\"owner\",\"value\":owner}\n            \n    def search_content_chunks(self,\n                              owner:str,\n                              query_str:Optional[str]=None,\n                              query_embedding:Optional[List[float]]=None,  \n                              doc_ids:Optional[List[str]]=None,                            \n                              limit:int=4,        \n                              return_json:bool=True):   \n        keyword = None\n        fields = []\n        vector = []\n        vectorField = None\n        filters = {\"and\":[{\"field\":\"chunk_collection\",\"value\":self.chunk_collection}]}\n    \n        if query_str is not None:\n            keyword = self.search_tokenize(query_str)\n            fields = [\"chunk\"]\n\n        if query_embedding is not None:\n            vector = query_embedding\n            vectorField = \"chunk_vector\"    \n        \n        \n        if doc_ids:\n            filters[\"and\"].append({\"or\":[{\"field\":\"doc_id\",\"value\":x} for x in doc_ids]})\n        \n        query = SearchQuery(self.retrieval_db,\"text_content_chunk\",\n                                    filters=filters,\n                                    keyword=keyword,fields=fields,\n                                    vector=vector,vectorField=vectorField,\n                                    limit=limit)\n        \n        docs = self.retrieval.search(self.retrieval_cluster,\n                        [query])\n    \n        if return_json:\n            context = json.dumps([{\"content\":x[\"raw_chunk\"]} for x in docs],ensure_ascii=False,indent=4)    \n            return context \n        else:\n            return docs\n        \n\n    def search_content_by_filename(self,filename:str,collection:str): \n        filters = [{\"field\":\"chunk_collection\",\"value\":collection}]\n        docs = self.retrieval.search(self.retrieval_cluster,\n                        [SearchQuery(self.retrieval_db,\"text_content_chunk\",\n                                    filters={\"and\":filters},\n                                    keyword=self.search_tokenize(filename),fields=[\"metadata\"],\n                                    vector=[],vectorField=None,\n                                    limit=10000)])\n        return docs \n\n    def search_content(self,q:str,owner:str,url:str,auth_tag:str=None,limit:int=4,return_json:bool=True): \n        filters = [self._owner_filter(owner)]\n        \n        if auth_tag:\n            filters.append({\"field\":\"auth_tag\",\"value\":self.search_tokenize(auth_tag)})\n        \n        if url:\n            filters.append({\"field\":\"url\",\"value\":url})    \n\n        if q:\n            keyword = self.search_tokenize(q)\n            vector = self.emb(q)\n            vectorField = \"content_vector\"\n            fields = [\"content\"]\n        else:\n            keyword = None\n            vector = []\n            vectorField = None\n            fields = []\n\n        docs = self.retrieval.search(self.retrieval_cluster,\n                            [SearchQuery(self.retrieval_db,\"text_content\",\n                                         filters={\"and\":filters},\n                                        keyword=keyword,fields=fields,\n                                        vector=vector,vectorField=vectorField,\n                                        limit=limit)])\n\n        if return_json:\n            context = json.dumps([{\"content\":x[\"raw_content\"]} for x in docs],ensure_ascii=False,indent=4)    \n            return context \n        else:\n            return docs \n\n    def get_chunk_by_id(self,chunk_id:str):\n        filters = {\"and\":[{\"field\":\"_id\",\"value\":chunk_id}]}        \n        docs = self.retrieval.filter(self.retrieval_cluster,\n                            [SearchQuery(self.retrieval_db,\"text_content_chunk\",\n                                         filters=filters,\n                                        keyword=None,fields=[],\n                                        vector=[],vectorField=None,\n                                        limit=1)])\n        return docs\n    \n    def get_chunks_by_docid(self,doc_id:str):\n        docs = self.retrieval.filter(self.retrieval_cluster,\n                            [SearchQuery(self.retrieval_db,\"text_content_chunk\",\n                                         filters={\"and\":[{\"field\":\"doc_id\",\"value\":doc_id}]},\n                                        keyword=None,fields=[],\n                                        vector=[],vectorField=None,\n                                        limit=100000)])\n        return docs\n    \n    def delete_chunk_by_ids(self,chunk_ids:List[str]):        \n        self.retrieval.delete_by_ids(self.retrieval_cluster,self.retrieval_db,\"text_content_chunk\",chunk_ids)\n        self.retrieval.commit(self.retrieval_cluster,self.retrieval_db)    \n    \n    def save_chunks(self,chunks:List[Dict[str,Any]]):        \n        text_content_chunks = []\n        for chunk in chunks:\n            chunk_id = chunk[\"chunk_id\"]\n            ref_doc_id = chunk[\"ref_doc_id\"]\n            owner = chunk.get(\"owner\",\"default\")\n            chunk_content = chunk[\"chunk_content\"]\n            chunk_embedding = chunk[\"chunk_embedding\"]\n            metadata = chunk.get(\"metadata\",{})\n            _id = f\"{self.chunk_collection}/{chunk_id}\"\n            file_path = metadata.get(\"file_path\",\"\")            \n            \n            text_content_chunks.append({\"_id\":_id,\n                \"doc_id\":ref_doc_id or \"\",\n                \"file_path\": file_path,\n                \"owner\":owner or \"default\",                \n                \"chunk\":self.search_tokenize(chunk_content),\n                \"raw_chunk\":chunk_content,\n                \"chunk_vector\":chunk_embedding,  \n                \"chunk_collection\":self.chunk_collection,              \n                \"metadata\":self.search_tokenize(json.dumps(metadata,ensure_ascii=False)),\n                \"json_data\":json.dumps(metadata,ensure_ascii=False),\n                \"created_time\":int(time.time()*1000),\n                })                   \n            \n        self.retrieval.build_from_dicts(self.retrieval_cluster,\n                                        self.retrieval_db,\n                                        \"text_content_chunk\",text_content_chunks)           \n\n            \n        \n    def get_doc(self,doc_id:str,collection:str):\n        filters = {\"and\":[{\"field\":\"_id\",\"value\":f'{collection}/{doc_id}'}]}        \n        docs = self.retrieval.filter(self.retrieval_cluster,\n                            [SearchQuery(self.retrieval_db,\"text_content\",\n                                         filters=filters,\n                                        keyword=None,fields=[],\n                                        vector=[],vectorField=None,\n                                        limit=1)])\n        return docs[0] if docs else None\n    \n    @DeprecationWarning\n    def delete_doc(self,doc_ids:List[str],collection:str):\n        ids = []\n        for doc_id in doc_ids:\n            ids.append(f'{collection}/{doc_id}')\n        self.retrieval.delete_by_ids(self.retrieval_cluster,self.retrieval_db,\"text_content\",[ids])\n\n    def truncate_table(self):\n        self.retrieval.truncate(self.retrieval_cluster,self.retrieval_db,\"text_content\")\n        self.retrieval.truncate(self.retrieval_cluster,self.retrieval_db,\"text_content_chunk\")\n        self.commit_doc()\n        self.commit_chunk()\n\n    def delete_doc_and_chunks_by_filename(self,collection:str,file_name:str):\n        doc = self.get_doc(f\"{collection}/ref_doc_info\",file_name)\n                \n        if doc:\n            node_ids = json.loads(doc[\"metadata\"])[\"node_ids\"]\n            for node_id in node_ids:\n                id = f\"{collection}/data/{node_id}\"\n                self.retrieval.delete_by_ids(self.retrieval_cluster,self.retrieval_db,\"text_content\",[id])\n\n                id = f\"{collection}/metadata/{node_id}\"\n                self.retrieval.delete_by_ids(self.retrieval_cluster,self.retrieval_db,\"text_content\",[id])\n\n                ## default/index/ can not be deleted\n            \n            ##  cleanup the chunk collection  \n            self.retrieval.delete_by_filter(self.retrieval_cluster,self.retrieval_db,\"text_content_chunk\",\n                                            {\"file_path\":file_name})                      \n\n\n    def delete_from_doc_collection(self,collection:str):\n        for suffix in [\"index\",\"data\",\"ref_doc_info\",\"metadata\"]:\n            self.retrieval.delete_by_filter(self.retrieval_cluster,self.retrieval_db,\"text_content\",\n                                            {\"collection\":f\"{collection}/{suffix}\"})\n        \n\n    def delete_from_chunk_collection(self,collection:str):\n        self.retrieval.delete_by_filter(self.retrieval_cluster,self.retrieval_db,\"text_content_chunk\",\n                                        {\"chunk_collection\":collection})    \n\n    def commit_doc(self):\n        self.retrieval.commit(self.retrieval_cluster,self.retrieval_db,\"text_content\")    \n\n    def commit_chunk(self):\n        self.retrieval.commit(self.retrieval_cluster,self.retrieval_db,\"text_content_chunk\")    \n                                  \n\n    def emb(self,s:str):        \n        return self.llm.emb(self.llm.default_emb_model_name,LLMRequest(instruction=s))[0].output\n\n\n    def split_text_into_chunks(self,s:str):\n        # self.llm.apply_sql_func(\n        #     '''select llm_split(value,array(\",\",\"。\",\"\\n\"),1600) as value ''',[{\"value\":content}],\n        #     url=self.byzer_engine_url\n        #     )[\"value\"]\n        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1600, chunk_overlap=200)\n        split_docs = text_splitter.split_documents([Document(page_content=s)])         \n        return [s.page_content for s in split_docs] \n\n    \n    def search_tokenize(self,s:str):        \n        seg_list = jieba.cut(s, cut_all=False)\n        # return self.llm.apply_sql_func(\"select mkString(' ',parse(value)) as value\",[\n        # {\"value\":s}],url=self.byzer_engine_url)[\"value\"]\n        return \" \".join(seg_list) \n        \n    \n               \n    \n\n\n\n     \n\n下面是用户的需求：\n\n@simple_retrieval.py 修正jieba 的 ImportError\n严格遵循 SEARCH/REPLACE 格式,如果是python代码,注意空格和缩进\n"}, {"role": "assistant", "content": "```python\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/apps/llama_index/simple_retrieval.py\n<<<<<<< SEARCH\nimport jieba  \n=======\ntry:\n    import jieba\nexcept ImportError:\n    jieba = None\n>>>>>>> REPLACE\n```"}], "model": "deepseek_chat", "yaml_file": "actions/040_chat_action.yml"}
{"instruction": "@simple_retrieval.py 修正jieba 的 ImportError, 同时打印warning\n严格遵循 SEARCH/REPLACE 格式,如果是python代码,注意空格和缩进", "conversations": [{"role": "user", "content": "\n如果你需要生成代码，对于每个需要更改的文件,你需要按 *SEARCH/REPLACE block* 的格式进行生成。\n\n# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The opening fence and code language, eg: ```python\n2. The file path alone on a line, starting with \"##File:\" and verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: ```\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\n\n*SEARCH/REPLACE* blocks will replace *all* matching occurrences.\nInclude enough lines to make the SEARCH blocks unique.\n\nInclude *ALL* the code being searched and replaced!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\n下面我们来看一个例子：\n\n当前项目目录结构：\n1. 项目根目录： /tmp/projects/mathweb\n2. 项目子目录/文件列表(类似tree 命令输出)\nflask/\n    app.py\n    templates/\n        index.html\n    static/\n        style.css\n\n用户需求： Change get_factorial() to use math.factorial\n\n回答： To make this change we need to modify `/tmp/projects/mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/flask/app.py\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n```\n\n用户需求： Refactor hello() into its own file.\n\n回答：To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n```python\n##File: /tmp/projects/mathweb/hello.py\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n```\n\n```python\n##File: /tmp/projects/mathweb/main.py\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n```\n\n现在让我们开始一个新的任务:\n\n当前项目目录结构：\n1. 项目根目录： /Users/allwefantasy/projects/byzer-llm\n2. 项目子目录/文件列表(类似tree 命令输出)：\nsetup.py\ntests/\n    test_emb_rerank.py\n    test_deepseek_code.py\n    test_prompt.py\n    test_model_meta.py\n    __init__.py\n    test_transformers_deploy.py\n    test_data_analysis.py\n    test_yi.py\n    test_deploy.py\n    functions_suite.py\n    test_template.py\n    auto/\n        test_backend_llama_cpp.py\n        __init__.py\nsetup-machine/\n    convert_safetensor.py\nnotebooks/\n    test.py\n    test_sensevoice.py\nsrc/\n    byzerllm/\n        lang.py\n        store.py\n        version.py\n        log.py\n        __init__.py\n        types.py\n        easy_byzerllm_command.py\n        byzerllm_command.py\n        command_args.py\n        bark/\n            generation.py\n            __init__.py\n            model.py\n            api.py\n            model_fine.py\n            bark_voice.py\n        moss/\n            finetune_moss.py\n            __init__.py\n            moss_inference.py\n            models/\n                configuration_moss.py\n                quantization.py\n                custom_autotune.py\n                __init__.py\n                modeling_moss.py\n                tokenization_moss.py\n        llama2/\n            __init__.py\n        records/\n            __init__.py\n        visualglm/\n            __init__.py\n        auto/\n            __init__.py\n            backend_llama_cpp.py\n            backend_vllm.py\n            backend_ds.py\n        alpha_moss/\n            __init__.py\n        processor/\n            __init__.py\n        sensevoice/\n            __init__.py\n        qwen/\n            __init__.py\n        utils/\n            json_repaire.py\n            object_store_ref_util.py\n            __init__.py\n            types.py\n            tokenizer.py\n            openai_utils.py\n            ray_utils.py\n            text_generator.py\n            emb.py\n            connect_ray.py\n            testing.py\n            langutil.py\n            nontext.py\n            fulltune/\n                deepspeed_trainner.py\n                __init__.py\n                launch.py\n                trainner.py\n                base_model/\n                    __init__.py\n                    configuration_baichuan.py\n                    modeling_baichuan.py\n                pretrain/\n                    __init__.py\n                    convert_to_transformers.py\n            metrics/\n                __init__.py\n            config/\n                __init__.py\n            retrieval/\n                rest.py\n                __init__.py\n                udf.py\n            sft/\n                argument.py\n                collator.py\n                qlora.py\n                __init__.py\n                model.py\n                dataset.py\n                loss.py\n                merge_lora.py\n                trainer.py\n            client/\n                message_utils.py\n                math_utils.py\n                byzerllm_client.py\n                parallel_utils.py\n                __init__.py\n                types.py\n                code_utils.py\n                img_utils.py\n                entrypoints/\n                    __init__.py\n                    openai/\n                        protocol.py\n                        __init__.py\n                        serve.py\n                        serving_chat.py\n                        api_server.py\n                        serving_engine.py\n                        serving_completion.py\n                        tool.py\n        qwen_voice/\n            __init__.py\n        chatglm2/\n            __init__.py\n        baichuan/\n            __init__.py\n        m3e/\n            __init__.py\n        stable_diffusion/\n            config.py\n            __init__.py\n            logger.py\n            shared.py\n            model.py\n            utils.py\n            images.py\n            diffusion/\n                __init__.py\n                embeddings.py\n                utils.py\n                networks/\n                    lyco.py\n                    __init__.py\n                upscalers/\n                    __init__.py\n                    multidiffusion.py\n                    samplers.py\n                piplines/\n                    __init__.py\n                    lpw.py\n                    diffusers.py\n            lib/\n                __init__.py\n                diffusers/\n                    __init__.py\n                    scheduler.py\n            api/\n                __init__.py\n                plugin.py\n                models/\n                    diffusion.py\n                    __init__.py\n                diffusion/\n                    __init__.py\n                    pipelines/\n                        __init__.py\n                        diffusers.py\n                events/\n                    generation.py\n                    __init__.py\n        bge/\n            __init__.py\n        bge_rerank/\n            __init__.py\n        apps/\n            client.py\n            qa_strategy.py\n            __init__.py\n            builder.py\n            vector_db.py\n            utils.py\n            qa.py\n            byzer_storage/\n                memory_model_based.py\n                command.py\n                env.py\n                train_pt.py\n                train_sft.py\n                __init__.py\n                train_dpo.py\n                simple_api.py\n                generate_sft_data.py\n            byzer_sql/\n                __init__.py\n            llama_index/\n                byzerai_vectordb.py\n                byzerai.py\n                byzerai_embedding.py\n                __init__.py\n                collection_manager.py\n                byzerai_index_store.py\n                byzerai_docstore.py\n                simple_retrieval.py\n                byzerai_kvstore.py\n            agent/\n                conversable_agent.py\n                registry.py\n                groupchat.py\n                user_proxy_agent.py\n                __init__.py\n                agent.py\n                extensions/\n                    simple_retrieval_client.py\n                    planner.py\n                    python_codesandbox_agent.py\n                    preview_file_agent.py\n                    llama_index_retrieval_agent.py\n                    visualization_agent.py\n                    assistant_agent.py\n                    load_data_agent.py\n                    retrieval_agent.py\n                    output_agent.py\n                    __init__.py\n                    byzer_engine_agent.py\n                    subquestion_agent.py\n                    sql_reviewer_agent.py\n                    spark_sql_agent.py\n                    rhetorical_agent.py\n                    data_analysis.py\n                    data_analysis_pipeline_agent.py\n                    common_agent.py\n                    query_rewrite/\n                        time.py\n                        rhetorical.py\n                        __init__.py\n                        condition.py\n                        context.py\n                store/\n                    memory_store.py\n                    __init__.py\n                    stores.py\n        saas/\n            __init__.py\n            chatglm/\n                __init__.py\n            sparkdesk/\n                __init__.py\n            gemini/\n                __init__.py\n            azure_openai/\n                __init__.py\n            azure/\n                __init__.py\n            qianfan/\n                __init__.py\n            claude/\n                __init__.py\n            qianwen/\n                __init__.py\n            volcengine/\n                __init__.py\n            minimax/\n                __init__.py\n            baichuan/\n                __init__.py\n            qianwen_vl/\n                __init__.py\n            official_openai/\n                __init__.py\n            zhipu/\n                __init__.py\n            aws_bedrock/\n                __init__.py\n            openai/\n                __init__.py\n        falcon/\n            __init__.py\n        chatglm6b/\n            trainer_seq2seq.py\n            arguments.py\n            __init__.py\n            finetune.py\n            tunning/\n                __init__.py\n                infer.py\n                finetune.py\n                utils/\n                    config.py\n                    __init__.py\n                    other.py\n                    common.py\n                    seq2seq.py\n        starcode/\n            __init__.py\n        qwen_vl_chat/\n            __init__.py\n        zephyr/\n            __init__.py\n        dolly/\n            generate.py\n            __init__.py\n            consts.py\n            dolly_inference.py\n            trainer.py\n        llama/\n            __init__.py\n        whisper/\n            whisper_inference.py\n            __init__.py\n下面是一些文件路径以及每个文件对应的源码：\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/saas/official_openai/__init__.py\nimport time\nfrom typing import List, Tuple, Dict, Any, Union\nimport httpx\nfrom openai import OpenAI\nimport base64\nimport io\nimport json\nimport ray\nfrom byzerllm.utils.types import (\n    BlockVLLMStreamServer,\n    StreamOutputs,\n    SingleOutput,\n    SingleOutputMeta,\n    BlockBinaryStreamServer,\n)\nfrom byzerllm.utils.langutil import asyncfy_with_semaphore\nimport threading\nimport asyncio\nimport traceback\nimport uuid\nimport tempfile        \n\nclass CustomSaasAPI:\n\n    def __init__(self, infer_params: Dict[str, str]) -> None:\n\n        self.api_key = infer_params[\"saas.api_key\"]\n        self.model = infer_params.get(\"saas.model\", \"gpt-3.5-turbo-1106\")\n\n        other_params = {}\n\n        if \"saas.api_base\" in infer_params:\n            other_params[\"api_base\"] = infer_params[\"saas.api_base\"]\n\n        if \"saas.api_version\" in infer_params:\n            other_params[\"api_version\"] = infer_params[\"saas.api_version\"]\n\n        if \"saas.api_type\" in infer_params:\n            other_params[\"api_type\"] = infer_params[\"saas.api_type\"]\n\n        if \"saas.base_url\" in infer_params:\n            other_params[\"base_url\"] = infer_params[\"saas.base_url\"]\n\n        if \"saas.timeout\" in infer_params:\n            other_params[\"timeout\"] = float(infer_params[\"saas.timeout\"])\n\n        self.max_retries = int(infer_params.get(\"saas.max_retries\", 10))\n\n        self.meta = {\n            \"model_deploy_type\": \"saas\",\n            \"backend\": \"saas\",\n            \"support_stream\": True,\n            \"model_name\": self.model,\n        }\n\n        self.meta[\"embedding_mode\"] = \"embedding\" in self.model.lower()\n\n        self.proxies = infer_params.get(\"saas.proxies\", None)\n        self.local_address = infer_params.get(\"saas.local_address\", \"0.0.0.0\")\n\n        if not self.proxies:\n            self.client = OpenAI(**other_params, api_key=self.api_key)\n        else:\n            self.client = OpenAI(\n                **other_params,\n                api_key=self.api_key,\n                http_client=httpx.Client(\n                    proxies=self.proxies,\n                    transport=httpx.HTTPTransport(local_address=self.local_address),\n                ),\n            )\n\n        try:\n            ray.get_actor(\"BLOCK_VLLM_STREAM_SERVER\")\n        except ValueError:\n            try:\n                ray.remote(BlockVLLMStreamServer).options(\n                    name=\"BLOCK_VLLM_STREAM_SERVER\",\n                    lifetime=\"detached\",\n                    max_concurrency=1000,\n                ).remote()\n            except Exception as e:\n                pass\n        try:\n            ray.get_actor(\"BlockBinaryStreamServer\")\n        except ValueError:\n            try:\n                ray.remote(BlockBinaryStreamServer).options(\n                    name=\"BlockBinaryStreamServer\",\n                    lifetime=\"detached\",\n                    max_concurrency=1000,\n                ).remote()\n            except Exception as e:\n                pass\n\n    # saas/proprietary\n    def get_meta(self):\n        return [self.meta]\n\n    def process_input(self, ins: Union[str, List[Dict[str, Any]], Dict[str, Any]]):\n\n        if isinstance(ins, list) or isinstance(ins, dict):\n            return ins\n\n        content = []\n        try:\n            ins_json = json.loads(ins)\n        except:\n            return ins\n\n        ## speech\n        if isinstance(ins_json, dict):\n            return ins_json\n\n        content = []\n        #     [\n        #     {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n        #     {\n        #       \"type\": \"image_url\",\n        #       \"image_url\": {\n        #         \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n        #         \"detail\": \"high\"\n        #       },\n        #     },\n        #   ],\n        for item in ins_json:\n            # for format like this: {\"image\": \"xxxxxx\", \"text\": \"What’s in this image?\",\"detail\":\"high\"}\n            # or {\"image\": \"xxxxxx\"}, {\"text\": \"What’s in this image?\"}\n            if (\"image\" in item or \"image_url\" in item) and \"type\" not in item:\n                image_data = item.get(\"image\", item.get(\"image_url\", \"\"))\n                ## \"data:image/jpeg;base64,\"\n                if not image_data.startswith(\"data:\"):\n                    image_data = \"data:image/jpeg;base64,\" + image_data\n\n                ## get the other fields except image/text/image_url\n                other_fields = {\n                    k: v\n                    for k, v in item.items()\n                    if k not in [\"image\", \"text\", \"image_url\"]\n                }\n                content.append(\n                    {\n                        \"image_url\": {\"url\": image_data, **other_fields},\n                        \"type\": \"image_url\",\n                    }\n                )            \n\n            if \"text\" in item and \"type\" not in item:\n                text_data = item[\"text\"]\n                content.append({\"text\": text_data, \"type\": \"text\"})\n\n            ## for format like this: {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n            ## {\"type\": \"image_url\", \"image_url\": {\"url\":\"\",\"detail\":\"high\"}}\n            ## this is the standard format, just return it\n            if \"type\" in item and item[\"type\"] == \"text\":\n                content.append(item)\n\n            if \"type\" in item and item[\"type\"] == \"image_url\":\n                content.append(item)\n\n        if not content:\n            return ins\n\n        return content\n\n    async def async_embed_query(self, ins: str, **kwargs):\n        resp = await asyncfy_with_semaphore(\n            lambda: self.client.embeddings.create(input=[ins], model=self.model)\n        )()\n        embedding = resp.data[0].embedding\n        usage = resp.usage\n        return (\n            embedding,\n            {\n                \"metadata\": {\n                    \"input_tokens_count\": usage.prompt_tokens,\n                    \"generated_tokens_count\": 0,\n                }\n            },\n        )\n\n    def embed_query(self, ins: str, **kwargs):\n        resp = self.client.embeddings.create(input=[ins], model=self.model)\n        embedding = resp.data[0].embedding\n        usage = resp.usage\n        return (\n            embedding,\n            {\n                \"metadata\": {\n                    \"input_tokens_count\": usage.prompt_tokens,\n                    \"generated_tokens_count\": 0,\n                }\n            },\n        )\n\n    async def async_text_to_speech(\n        self, stream: bool, ins: str, voice: str, chunk_size: int = None, **kwargs\n    ):\n        if stream:\n            server = ray.get_actor(\"BlockBinaryStreamServer\")\n            request_id = [None]\n\n            def writer():\n                try:\n                    request_id[0] = str(uuid.uuid4())\n                    with self.client.with_streaming_response.audio.speech.create(\n                        model=self.model, voice=voice, input=ins, **kwargs\n                    ) as response:\n                        for chunk in response.iter_bytes(chunk_size):\n                            input_tokens_count = 0\n                            generated_tokens_count = 0\n                            ray.get(\n                                server.add_item.remote(\n                                    request_id[0],\n                                    StreamOutputs(\n                                        outputs=[\n                                            SingleOutput(\n                                                text=chunk,\n                                                metadata=SingleOutputMeta(\n                                                    input_tokens_count=input_tokens_count,\n                                                    generated_tokens_count=generated_tokens_count,\n                                                ),\n                                            )\n                                        ]\n                                    ),\n                                )\n                            )\n                except:\n                    traceback.print_exc()\n                ray.get(server.mark_done.remote(request_id[0]))\n\n            threading.Thread(target=writer, daemon=True).start()\n\n            time_count = 10 * 100\n            while request_id[0] is None and time_count > 0:\n                time.sleep(0.01)\n                time_count -= 1\n\n            if request_id[0] is None:\n                raise Exception(\"Failed to get request id\")\n\n            def write_running():\n                return ray.get(server.add_item.remote(request_id[0], \"RUNNING\"))\n\n            await asyncio.to_thread(write_running)\n            return [\n                (\n                    \"\",\n                    {\n                        \"metadata\": {\n                            \"request_id\": request_id[0],\n                            \"stream_server\": \"BlockBinaryStreamServer\",\n                        }\n                    },\n                )\n            ]\n\n        start_time = time.monotonic()\n        with io.BytesIO() as output:\n            async with asyncfy_with_semaphore(\n                lambda: self.client.with_streaming_response.audio.speech.create(\n                    model=self.model, voice=voice, input=ins, **kwargs\n                )\n            )() as response:\n                for chunk in response.iter_bytes():\n                    output.write(chunk)\n\n            base64_audio = base64.b64encode(output.getvalue()).decode()\n            time_cost = time.monotonic() - start_time\n            return [\n                (\n                    base64_audio,\n                    {\n                        \"metadata\": {\n                            \"request_id\": \"\",\n                            \"input_tokens_count\": 0,\n                            \"generated_tokens_count\": 0,\n                            \"time_cost\": time_cost,\n                            \"first_token_time\": 0,\n                            \"speed\": 0,\n                        }\n                    },\n                )\n            ]\n\n    async def async_speech_to_text(\n        self,\n        audio: str,\n        response_format: str = \"verbose_json\",\n        timestamp_granularities: List[str] = [\"word\",\"segment\"],\n    ):               \n        # Extract audio format and base64 data\n        data_prefix = \"data:audio/\"\n        base64_prefix = \";base64,\"\n        if not audio.startswith(data_prefix):\n            raise ValueError(\"Invalid audio data format\")\n        \n        format_end = audio.index(base64_prefix)\n        audio_format = audio[len(data_prefix):format_end]\n        base64_data = audio[format_end + len(base64_prefix):]\n        \n        # Decode the base64 audio data\n        audio_data = base64.b64decode(base64_data)\n\n        # Create a temporary file with the correct extension\n        with tempfile.NamedTemporaryFile(delete=False, suffix=f'.{audio_format}') as temp_audio_file:\n            temp_audio_file.write(audio_data)\n            temp_audio_file_path = temp_audio_file.name\n\n        try:\n            start_time = time.monotonic()\n                   \n            with open(temp_audio_file_path, \"rb\") as audio_file:                \n                transcription = self.client.audio.transcriptions.create(\n                    model=self.model or \"whisper-1\",\n                    file=audio_file,\n                    response_format=response_format,\n                    timestamp_granularities=timestamp_granularities,\n                )\n            time_cost = time.monotonic() - start_time\n\n            return [\n                (\n                    json.dumps(transcription.to_dict(), ensure_ascii=True),\n                    {\n                        \"metadata\": {\n                            \"request_id\": \"\",\n                            \"input_tokens_count\": 0,\n                            \"generated_tokens_count\": 0,\n                            \"time_cost\": time_cost,\n                            \"first_token_time\": 0,\n                            \"speed\": 0,\n                        }\n                    },\n                )\n            ]\n        finally:\n            # Clean up the temporary file\n            import os\n            os.unlink(temp_audio_file_path)\n\n    def image_to_text(self, ins: str, **kwargs):\n        pass\n\n    async def async_text_to_image(\n        self, stream: bool, input: str, size: str, quality: str, n: int, **kwargs\n    ):\n        if stream:\n            raise Exception(\"Stream not supported for text to image\")\n        start_time = time.monotonic()\n        response = await asyncfy_with_semaphore(\n            lambda: self.client.images.generate(\n                model=self.model,\n                prompt=input,\n                size=size,\n                quality=quality,\n                n=1,\n                response_format=\"b64_json\",\n                **kwargs,\n            )\n        )()\n        time_cost = time.monotonic() - start_time\n        base64_image = response.data[0].b64_json\n        return [\n            (\n                base64_image,\n                {\n                    \"metadata\": {\n                        \"request_id\": \"\",\n                        \"input_tokens_count\": 0,\n                        \"generated_tokens_count\": 0,\n                        \"time_cost\": time_cost,\n                        \"first_token_time\": 0,\n                        \"speed\": 0,\n                    }\n                },\n            )\n        ]\n\n    def text_to_text(self, ins: str, **kwargs):\n        pass\n\n    async def async_stream_chat(\n        self,\n        tokenizer,\n        ins: str,\n        his: List[Dict[str, Any]] = [],\n        max_length: int = 4096,\n        top_p: float = 0.7,\n        temperature: float = 0.9,\n        **kwargs,\n    ):\n\n        model = self.model\n\n        if \"model\" in kwargs:\n            model = kwargs[\"model\"]\n\n        messages = [\n            {\"role\": message[\"role\"], \"content\": self.process_input(message[\"content\"])}\n            for message in his\n        ] + [{\"role\": \"user\", \"content\": self.process_input(ins)}]\n\n        stream = kwargs.get(\"stream\", False)\n\n        ## content = [\n        ##    \"voice\": \"alloy\",\"input\": \"Hello, World!\",response_format: \"mp3\"]\n        last_message = messages[-1][\"content\"]\n\n        if isinstance(last_message, dict) and \"voice\" in last_message:\n            voice = last_message[\"voice\"]\n            response_format = last_message.get(\"response_format\", \"mp3\")\n            chunk_size = last_message.get(\"chunk_size\", None)\n            input = last_message[\"input\"]\n            return await self.async_text_to_speech(\n                stream=stream,\n                ins=input,\n                voice=voice,\n                chunk_size=chunk_size,\n                response_format=response_format,\n            )\n\n        if isinstance(last_message, dict) and \"input\" in last_message:\n            input = last_message[\"input\"]\n            size = last_message.get(\"size\", \"1024x1024\")\n            quality = last_message.get(\"quality\", \"standard\")\n            n = last_message.get(\"n\", 1)\n            return await self.async_text_to_image(\n                stream=stream, input=input, size=size, quality=quality, n=n\n            )\n\n        if isinstance(last_message, dict) and \"audio\" in last_message:            \n            audio_data = last_message.get(\"audio\", \"\")            \n            tpe = last_message.get(\"type\",\"wav\")\n            if not audio_data.startswith(\"data:\"):\n                audio_data = f\"data:audio/${tpe};base64,\" + audio_data                \n            return await self.async_speech_to_text(audio=audio_data)\n        \n        server = ray.get_actor(\"BLOCK_VLLM_STREAM_SERVER\")\n        request_id = [None]\n\n        def writer():\n            try:\n                r = \"\"\n                response = self.client.chat.completions.create(\n                    messages=messages,\n                    model=model,\n                    stream=True,\n                    max_tokens=max_length,\n                    temperature=temperature,\n                    top_p=top_p,\n                )\n                # input_tokens_count = 0\n                # generated_tokens_count = 0\n\n                request_id[0] = str(uuid.uuid4())\n\n                for chunk in response:\n                    content = chunk.choices[0].delta.content or \"\"\n                    r += content\n                    if hasattr(chunk, \"usage\") and chunk.usage:\n                        input_tokens_count = chunk.usage.prompt_tokens\n                        generated_tokens_count = chunk.usage.completion_tokens\n                    else:\n                        input_tokens_count = 0\n                        generated_tokens_count = 0\n                    ray.get(\n                        server.add_item.remote(\n                            request_id[0],\n                            StreamOutputs(\n                                outputs=[\n                                    SingleOutput(\n                                        text=r,\n                                        metadata=SingleOutputMeta(\n                                            input_tokens_count=input_tokens_count,\n                                            generated_tokens_count=generated_tokens_count,\n                                        ),\n                                    )\n                                ]\n                            ),\n                        )\n                    )\n            except:\n                traceback.print_exc()\n            ray.get(server.mark_done.remote(request_id[0]))\n\n        if stream:\n            threading.Thread(target=writer, daemon=True).start()\n\n            time_count = 60 * 100\n            while request_id[0] is None and time_count > 0:\n                time.sleep(0.01)\n                time_count -= 1\n\n            if request_id[0] is None:\n                raise Exception(\"Failed to get request id\")\n\n            def write_running():\n                return ray.get(server.add_item.remote(request_id[0], \"RUNNING\"))\n\n            await asyncio.to_thread(write_running)\n            return [\n                (\n                    \"\",\n                    {\n                        \"metadata\": {\n                            \"request_id\": request_id[0],\n                            \"stream_server\": \"BLOCK_VLLM_STREAM_SERVER\",\n                        }\n                    },\n                )\n            ]\n        else:\n            try:\n                start_time = time.monotonic()\n                response = await asyncfy_with_semaphore(\n                    lambda: self.client.chat.completions.create(\n                        messages=messages,\n                        model=model,\n                        max_tokens=max_length,\n                        temperature=temperature,\n                        top_p=top_p,\n                    )\n                )()\n\n                generated_text = response.choices[0].message.content\n                generated_tokens_count = response.usage.completion_tokens\n                input_tokens_count = response.usage.prompt_tokens\n                time_cost = time.monotonic() - start_time\n                return [\n                    (\n                        generated_text,\n                        {\n                            \"metadata\": {\n                                \"request_id\": response.id,\n                                \"input_tokens_count\": input_tokens_count,\n                                \"generated_tokens_count\": generated_tokens_count,\n                                \"time_cost\": time_cost,\n                                \"first_token_time\": 0,\n                                \"speed\": float(generated_tokens_count) / time_cost,\n                            }\n                        },\n                    )\n                ]\n            except Exception as e:\n                print(f\"Error: {e}\")\n                raise e\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/whisper/__init__.py\nimport whisper\nfrom typing import Dict, List, Tuple\nfrom byzerllm.utils.types import StopSequencesCriteria\n\nfrom typing import Dict, Any, List, Union\nfrom pyjava.storage import streaming_tar as STar\nimport json\nimport base64\nimport tempfile\nimport time\n\n\ndef get_meta(self):\n    return [\n        {\n            \"model_deploy_type\": \"proprietary\",\n            \"backend\": \"transformers\",\n            \"message_format\": True\n        }\n    ]\n\n\ndef process_input(ins: Union[str, List[Dict[str, Any]], Dict[str, Any]]):\n    # print(ins) \n    if isinstance(ins, list) or isinstance(ins, dict):\n        return ins\n\n    content = []\n    try:\n        ins_json = json.loads(ins)\n    except:\n        return ins\n\n    ## speech\n    if isinstance(ins_json, dict):\n        return ins_json\n\n    content = []\n    #     [\n    #     {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n    #     {\n    #       \"type\": \"image_url\",\n    #       \"image_url\": {\n    #         \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n    #         \"detail\": \"high\"\n    #       },\n    #     },\n    #   ],\n    for item in ins_json:\n        # for format like this: {\"image\": \"xxxxxx\", \"text\": \"What’s in this image?\",\"detail\":\"high\"}\n        # or {\"image\": \"xxxxxx\"}, {\"text\": \"What’s in this image?\"}\n        if (\"image\" in item or \"image_url\" in item) and \"type\" not in item:\n            image_data = item.get(\"image\", item.get(\"image_url\", \"\"))\n            ## \"data:image/jpeg;base64,\"\n            if not image_data.startswith(\"data:\"):\n                image_data = \"data:image/jpeg;base64,\" + image_data\n\n            ## get the other fields except image/text/image_url\n            other_fields = {\n                k: v for k, v in item.items() if k not in [\"image\", \"text\", \"image_url\"]\n            }\n            content.append(\n                {\n                    \"image_url\": {\"url\": image_data, **other_fields},\n                    \"type\": \"image_url\",\n                }\n            )\n\n        if \"text\" in item and \"type\" not in item:\n            text_data = item[\"text\"]\n            content.append({\"text\": text_data, \"type\": \"text\"})\n\n        ## for format like this: {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n        ## {\"type\": \"image_url\", \"image_url\": {\"url\":\"\",\"detail\":\"high\"}}\n        ## this is the standard format, just return it\n        if \"type\" in item and item[\"type\"] == \"text\":\n            content.append(item)\n\n        if \"type\" in item and item[\"type\"] == \"image_url\":\n            content.append(item)\n\n    if not content:\n        return ins\n\n    return content\n\n\ndef stream_chat(\n    self,\n    tokenizer,\n    ins: str,\n    his: List[Dict[str, str]] = [],\n    max_length: int = 4090,\n    top_p: float = 0.95,\n    temperature: float = 0.1,\n    **kwargs,\n):\n    messages = [\n        {\"role\": message[\"role\"], \"content\": process_input(message[\"content\"])}\n        for message in his\n    ] + [{\"role\": \"user\", \"content\": process_input(ins)}]\n\n    audio_input = messages[-1][\"content\"]\n\n    audio_data = audio_input.get(\"audio\", \"\")\n    audio = audio_input[\"audio\"]\n    response_format = audio_input.get(\"response_format\", \"json\")\n    timestamp_granularities = audio_input.get(\n        \"timestamp_granularities\", [\"word\", \"segment\"]\n    )\n    data_prefix = \"data:audio/\"\n    base64_prefix = \";base64,\"\n    if not audio.startswith(data_prefix):\n        raise ValueError(\"Invalid audio data format\")\n\n    format_end = audio.index(base64_prefix)\n    audio_format = audio[len(data_prefix) : format_end]\n    base64_data = audio[format_end + len(base64_prefix) :]\n\n    # Decode the base64 audio data\n    audio_data = base64.b64decode(base64_data)\n\n    # Create a temporary file with the correct extension\n    with tempfile.NamedTemporaryFile(\n        delete=False, suffix=f\".{audio_format}\"\n    ) as temp_audio_file:\n        temp_audio_file.write(audio_data)\n        temp_audio_file_path = temp_audio_file.name\n\n    try:\n        start_time = time.monotonic()\n        word_timestamps = \"word\" in timestamp_granularities\n        \n        result = self.transcribe(temp_audio_file_path, word_timestamps=word_timestamps)        \n            \n        time_cost = time.monotonic() - start_time\n        return [\n            (\n                json.dumps(result, ensure_ascii=False),\n                {\n                    \"metadata\": {\n                        \"request_id\": \"\",\n                        \"input_tokens_count\": 0,\n                        \"generated_tokens_count\": 0,\n                        \"time_cost\": time_cost,\n                        \"first_token_time\": 0,\n                        \"speed\": 0,\n                    }\n                },\n            )\n        ]\n    finally:\n        # Clean up the temporary file\n        import os\n\n        os.unlink(temp_audio_file_path)\n\n\ndef init_model(\n    model_dir, infer_params: Dict[str, str] = {}, sys_conf: Dict[str, str] = {}\n):\n    model = whisper.load_model(\"large\", download_root=model_dir)\n    import types\n\n    model.stream_chat = types.MethodType(stream_chat, model)\n    model.get_meta = types.MethodType(get_meta, model)\n    return (model, None)\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/notebooks/test_sensevoice.py\nimport sys\nimport os\n\n# Add the project root to the Python path\nsys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\")))\n\nfrom src.byzerllm.sensevoice import init_model, format_str_v3\nimport base64\nimport json\n\n\ndef main():\n    # Initialize the model\n    model_dir = \"/Users/allwefantasy/models/SenseVoiceSmall\"\n    model, _ = init_model(model_dir)\n\n    # Prepare test audio\n    with open(\n        \"/Users/allwefantasy/models/SenseVoiceSmall/example/zh.mp3\", \"rb\"\n    ) as audio_file:\n        audio_data = audio_file.read()\n        base64_audio = base64.b64encode(audio_data).decode(\"utf-8\")        \n        # Prepare input in the expected format\n        test_input = json.dumps(\n            {\n                            \"type\": \"audio\",\n                            \"audio\": f\"data:audio/wav;base64,{base64_audio}\",\n                        }\n        )\n\n        # Call stream_chat\n        result = model.stream_chat(tokenizer=None, ins = test_input)\n\n        # Process and print the result\n        if result and len(result) > 0:\n            output, metadata = result[0]\n            parsed_output = json.loads(output)\n            formatted_text = parsed_output.get(\"text\", \"\")\n            print(\"Transcription:\", formatted_text)\n            print(\"Metadata:\", metadata)\n        else:\n            print(\"No result returned from the model.\")\n\n\nif __name__ == \"__main__\":\n    main()\n\n\n##File: /Users/allwefantasy/projects/Qwen2-Audio/README_CN.md\n<p align=\"left\">\n        中文</a>&nbsp ｜ &nbsp<a href=\"README.md\">English</a>\n</p>\n<br><br>\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/assets/blog/qwenaudio/qwen2audio_logo.png\" width=\"400\"/>\n<p>\n\n<p align=\"center\">\nQwen2-Audio-7B <a href=\"https://modelscope.cn/models/qwen/Qwen2-Audio-7B\">🤖 </a> | <a href=\"https://huggingface.co/Qwen/Qwen2-Audio-7B\">🤗</a>&nbsp ｜ Qwen-Audio-7B-Instruct <a href=\"https://modelscope.cn/models/qwen/Qwen2-Audio-7B-Instruct\">🤖 </a>| <a href=\"https://huggingface.co/Qwen/Qwen2-Audio-7B-Instruct\">🤗</a>&nbsp ｜ Demo<a href=\"https://modelscope.cn/studios/qwen/Qwen2-Audio-Instruct-Demo\"> 🤖</a> | <a href=\"https://huggingface.co/spaces/Qwen/Qwen2-Audio-Instruct-Demo\">🤗</a>&nbsp\n<br>\n📑 <a href=\"https://arxiv.org/abs/2407.10759\">Paper</a> &nbsp&nbsp | &nbsp&nbsp 📑 <a href=\"https://qwenlm.github.io/blog/qwen2-audio\">Blog</a> &nbsp&nbsp | &nbsp&nbsp 💬 <a href=\"https://github.com/QwenLM/Qwen/blob/main/assets/wechat.png\">WeChat (微信)</a>&nbsp&nbsp | &nbsp&nbsp <a href=\"https://discord.gg/CV4E9rpNSD\">Discord</a>&nbsp&nbsp\n</p>\n\n\n\n我们介绍Qwen-Audio的最新进展：Qwen2-Audio。作为一个大规模音频语言模型，Qwen2-Audio能够接受各种音频信号输入，并根据语音指令执行音频分析或直接响应文本。我们介绍两种不同的音频交互模式：语音聊天 voice chat 和音频分析 audio analysis。\n\n* 语音聊天：用户可以自由地与 Qwen2-Audio 进行语音互动，而无需文本输入；\n* 音频分析：用户可以在互动过程中提供音频和文本指令对音频进行分析；\n\n**我们已经开源了 Qwen2-Audio 系列的两个模型：Qwen2-Audio-7B和Qwen2-Audio-7B-Instruct。**\n\n## 模型结构与训练范式\n\nQwen2-Audio 三阶段训练过程概述。\n\n<p align=\"left\">\n    <img src=\"assets/framework.png\" width=\"80%\"/>\n<p>\n\n\n\n## 新闻\n* 2024.8.9 🎉 我们在 ModelScope 和 Hugging Face 开源了`Qwen2-Audio-7B`和`Qwen2-Audio-7B-Instruct`的 checkpoint.\n* 2024.7.15 🎉 我们发布了 Qwen2-Audio 的[论文](https://arxiv.org/abs/2407.10759), 介绍了相关的模型结构，训练方法和模型表现。\n* 2023.11.30 🔥 我们发布了**Qwen-Audio**系列\n<br>\n\n## 评测\n我们在标准的13个学术数据集上评测了模型的能力如下：\n<table><thead><tr><th>Task</th><th>Description</th><th>Dataset</th><th>Split</th><th>Metric</th></tr></thead><tbody><tr><td rowspan=\"4\">ASR</td><td rowspan=\"4\">Automatic Speech Recognition</td><td>Fleurs</td><td>dev | test</td><td rowspan=\"4\">WER</td></tr><tr><td>Aishell2</td><td>test</td></tr><tr><td>Librispeech</td><td>dev | test</td></tr><tr><td>Common Voice</td><td>dev | test</td></tr><tr><td>S2TT</td><td>Speech-to-Text Translation</td><td>CoVoST2</td><td>test</td><td>BLEU </td></tr><tr><td>SER</td><td>Speech Emotion Recognition</td><td>Meld</td><td>test</td><td>ACC</td></tr><tr><td>VSC</td><td>Vocal Sound Classification</td><td>VocalSound</td><td>test</td><td>ACC</td></tr><tr><td rowspan=\"4\"><a href=\"https://github.com/OFA-Sys/AIR-Bench\">AIR-Bench</a><br></td><td>Chat-Benchmark-Speech</td><td>Fisher<br>SpokenWOZ<br>IEMOCAP<br>Common voice</td><td>dev | test</td><td>GPT-4 Eval</td></tr><tr><td>Chat-Benchmark-Sound</td><td>Clotho</td><td>dev | test</td><td>GPT-4 Eval</td></tr>\n<tr><td>Chat-Benchmark-Music</td><td>MusicCaps</td><td>dev | test</td><td>GPT-4 Eval</td></tr><tr><td>Chat-Benchmark-Mixed-Audio</td><td>Common voice<br>AudioCaps<br>MusicCaps</td><td>dev | test</td><td>GPT-4 Eval</td></tr></tbody></table>\n\n\n以下是整体表现：\n<p align=\"left\">\n    <img src=\"assets/radar_compare_qwen_audio.png\" width=\"70%\"/>\n<p>\n\n评测分数详情如下：\n<br>\n<b>（注意：我们所展示的评测结果是在原始训练框架的初始模型上的，然而在框架转换 Huggingface 后指标出现了部分波动，在这里我们展示我们的全部测评结果：首先是论文中的初始模型结果）</b>\n\n<table><thead><tr><th rowspan=\"2\">Task</th><th rowspan=\"2\">Dataset</th><th rowspan=\"2\">Model</th><th colspan=\"2\">Performance</th></tr><tr><th>Metrics</th><th>Results</th></tr></thead><tbody><tr><td rowspan=\"15\">ASR</td><td rowspan=\"7\"><b>Librispeech</b><br>dev-clean | dev-other | <br>test-clean | test-other</td><td>SpeechT5</td><td rowspan=\"7\">WER </td><td>2.1 | 5.5 | 2.4 | 5.8</td></tr><tr><td>SpeechNet</td><td>- | - | 30.7 | -</td></tr><tr><td>SLM-FT</td><td>- | - | 2.6 | 5.0</td></tr><tr><td>SALMONN</td><td>- | - | 2.1 | 4.9</td></tr><tr><td>SpeechVerse</td><td>- | - | 2.1 | 4.4</td></tr><tr><td>Qwen-Audio</td><td>1.8 | 4.0 | 2.0 | 4.2</td></tr><tr><td>Qwen2-Audio</td><td><b>1.3 | 3.4 | 1.6 | 3.6</b></td></tr><tr><td rowspan=\"2\"><b>Common Voice 15</b> <br>en | zh | yue | fr</td><td>Whisper-large-v3</td><td rowspan=\"2\">WER </td><td>9.3 | 12.8 | 10.9 | 10.8</td></tr><tr><td>Qwen2-Audio</td><td><b>8.6 | 6.9 | 5.9 | 9.6</b></td></tr>\n<tr><td rowspan=\"2\"><b>Fleurs</b> <br>zh</td><td>Whisper-large-v3</td><td rowspan=\"2\">WER </td><td>7.7</td></tr><tr><td>Qwen2-Audio</td><td><b>7.5</b></td></tr><tr><td rowspan=\"4\"><b>Aishell2</b> <br>Mic | iOS | Android</td><td>MMSpeech-base</td><td rowspan=\"4\">WER </td><td>4.5 | 3.9 | 4.0</td></tr><tr><td>Paraformer-large</td><td>- | <b>2.9</b> | -</td></tr><tr><td>Qwen-Audio</td><td>3.3 | 3.1 | 3.3</td></tr><tr><td>Qwen2-Audio</td><td><b>3.0</b> | 3.0 | <b>2.9</b></td></tr><tr><td rowspan=\"8\">S2TT</td><td rowspan=\"5\"><b>CoVoST2</b> <br>en-de | de-en | <br>en-zh | zh-en</td><td>SALMONN</td><td rowspan=\"5\">BLEU </td><td>18.6 | - | 33.1 | -</td></tr><tr><td>SpeechLLaMA</td><td>- | 27.1 | - | 12.3</td></tr><tr><td>BLSP</td><td>14.1 | - | - | -</td></tr><tr><td>Qwen-Audio</td><td>25.1 | 33.9 | 41.5 | 15.7</td></tr><tr><td>Qwen2-Audio</td><td><b>29.9 | 35.2 | 45.2 | 24.4</b></td></tr>\n<tr><td rowspan=\"3\"><b>CoVoST2</b> <br>es-en | fr-en | it-en |</td><td>SpeechLLaMA</td><td rowspan=\"3\">BLEU </td><td>27.9 | 25.2 | 25.9</td></tr><tr><td>Qwen-Audio</td><td>39.7 | <b>38.5</b> | 36.0</td></tr><tr><td>Qwen2-Audio</td><td><b>40.0 | 38.5 | 36.3</b></td></tr><tr><td rowspan=\"3\">SER</td><td rowspan=\"3\"><b>Meld</b></td><td>WavLM-large</td><td rowspan=\"3\">ACC </td><td>0.542</td></tr><tr><td>Qwen-Audio</td><td><b>0.557</b></td></tr><tr><td>Qwen2-Audio</td><td>0.553</td></tr><tr><td rowspan=\"4\">VSC</td><td rowspan=\"4\"><b>VocalSound</b></td><td>CLAP</td><td rowspan=\"4\">ACC </td><td>0.4945</td></tr><tr><td>Pengi</td><td>0.6035</td></tr><tr><td>Qwen-Audio</td><td>0.9289</td></tr><tr><td>Qwen2-Audio</td><td><b>0.9392</b></td></tr>\n<tr><td>AIR-Bench <br></td><td><b>Chat Benchmark</b><br>Speech | Sound |<br> Music | Mixed-Audio</td><td>SALMONN<br>BLSP<br>Pandagpt<br>Macaw-LLM<br>SpeechGPT<br>Next-gpt<br>Qwen-Audio<br>Gemini-1.5-pro<br>Qwen2-Audio</td><td>GPT-4 </td><td>6.16 | 6.28 | 5.95 | 6.08<br>6.17 | 5.55 | 5.08 | 5.33<br>3.58 | 5.46 | 5.06 | 4.25<br>0.97 | 1.01 | 0.91 | 1.01<br>1.57 | 0.95 | 0.95 | 4.13<br>3.86 | 4.76 | 4.18 | 4.13<br>6.47 | 6.95 | 5.52 | 6.08<br>6.97 | 5.49 | 5.06 | 5.27<br><b>7.18 | 6.99 | 6.79 | 6.77</b></td></tr></tbody></table>\n\n<b>（其次是转换 huggingface 后的）</b>\n\n<table><thead><tr><th rowspan=\"2\">Task</th><th rowspan=\"2\">Dataset</th><th rowspan=\"2\">Model</th><th colspan=\"2\">Performance</th></tr><tr><th>Metrics</th><th>Results</th></tr></thead><tbody><tr><td rowspan=\"15\">ASR</td><td rowspan=\"7\"><b>Librispeech</b><br>dev-clean | dev-other | <br>test-clean | test-other</td><td>SpeechT5</td><td rowspan=\"7\">WER </td><td>2.1 | 5.5 | 2.4 | 5.8</td></tr><tr><td>SpeechNet</td><td>- | - | 30.7 | -</td></tr><tr><td>SLM-FT</td><td>- | - | 2.6 | 5.0</td></tr><tr><td>SALMONN</td><td>- | - | 2.1 | 4.9</td></tr><tr><td>SpeechVerse</td><td>- | - | 2.1 | 4.4</td></tr><tr><td>Qwen-Audio</td><td>1.8 | 4.0 | 2.0 | 4.2</td></tr><tr><td>Qwen2-Audio</td><td><b>1.7 | 3.6 | 1.7 | 4.0</b></td></tr><tr><td rowspan=\"2\"><b>Common Voice 15</b> <br>en | zh | yue | fr</td><td>Whisper-large-v3</td><td rowspan=\"2\">WER </td><td>9.3 | 12.8 | 10.9 | 10.8</td></tr><tr><td>Qwen2-Audio</td><td><b>8.7 | 6.5 | 5.9 | 9.6</b></td></tr>\n<tr><td rowspan=\"2\"><b>Fleurs</b> <br>zh</td><td>Whisper-large-v3</td><td rowspan=\"2\">WER </td><td>7.7</td></tr><tr><td>Qwen2-Audio</td><td><b>7.0</b></td></tr><tr><td rowspan=\"4\"><b>Aishell2</b> <br>Mic | iOS | Android</td><td>MMSpeech-base</td><td rowspan=\"4\">WER </td><td>4.5 | 3.9 | 4.0</td></tr><tr><td>Paraformer-large</td><td>- | <b>2.9</b> | -</td></tr><tr><td>Qwen-Audio</td><td>3.3 | 3.1 | 3.3</td></tr><tr><td>Qwen2-Audio</td><td><b>3.2</b> | 3.1 | <b>2.9</b></td></tr><tr><td rowspan=\"8\">S2TT</td><td rowspan=\"5\"><b>CoVoST2</b> <br>en-de | de-en | <br>en-zh | zh-en</td><td>SALMONN</td><td rowspan=\"5\">BLEU </td><td>18.6 | - | 33.1 | -</td></tr><tr><td>SpeechLLaMA</td><td>- | 27.1 | - | 12.3</td></tr><tr><td>BLSP</td><td>14.1 | - | - | -</td></tr><tr><td>Qwen-Audio</td><td>25.1 | <b>33.9</b> | 41.5 | 15.7</td></tr><tr><td>Qwen2-Audio</td><td><b>29.6</b> | 33.6 | <b>45.6</b> | <b>24.0</b></td></tr>\n<tr><td rowspan=\"3\"><b>CoVoST2</b> <br>es-en | fr-en | it-en |</td><td>SpeechLLaMA</td><td rowspan=\"3\">BLEU </td><td>27.9 | 25.2 | 25.9</td></tr><tr><td>Qwen-Audio</td><td><b>39.7 | 38.5 | 36.0</b></td></tr><tr><td>Qwen2-Audio</td><td>38.7 | 37.2 | 35.2</td></tr><tr><td rowspan=\"3\">SER</td><td rowspan=\"3\"><b>Meld</b></td><td>WavLM-large</td><td rowspan=\"3\">ACC </td><td>0.542</td></tr><tr><td>Qwen-Audio</td><td><b>0.557</b></td></tr><tr><td>Qwen2-Audio</td><td>0.535</td></tr><tr><td rowspan=\"4\">VSC</td><td rowspan=\"4\"><b>VocalSound</b></td><td>CLAP</td><td rowspan=\"4\">ACC </td><td>0.4945</td></tr><tr><td>Pengi</td><td>0.6035</td></tr><tr><td>Qwen-Audio</td><td>0.9289</td></tr><tr><td>Qwen2-Audio</td><td><b>0.9395</b></td></tr>\n<tr><td>AIR-Bench <br></td><td><b>Chat Benchmark</b><br>Speech | Sound |<br> Music | Mixed-Audio</td><td>SALMONN<br>BLSP<br>Pandagpt<br>Macaw-LLM<br>SpeechGPT<br>Next-gpt<br>Qwen-Audio<br>Gemini-1.5-pro<br>Qwen2-Audio</td><td>GPT-4 </td><td>6.16 | 6.28 | 5.95 | 6.08<br>6.17 | 5.55 | 5.08 | 5.33<br>3.58 | 5.46 | 5.06 | 4.25<br>0.97 | 1.01 | 0.91 | 1.01<br>1.57 | 0.95 | 0.95 | 4.13<br>3.86 | 4.76 | 4.18 | 4.13<br>6.47 | <b>6.95</b> | 5.52 | 6.08<br>6.97 | 5.49 | 5.06 | 5.27<br><b>7.24</b> | 6.83 | <b>6.73</b> | <b>6.42</b></td></tr></tbody></table>\n\n\n我们提供了以上**所有**评测脚本以供复现我们的实验结果。请阅读 [eval_audio/EVALUATION.md](eval_audio/EVALUATION.md) 了解更多信息。\n\n## 部署要求\nThe code of Qwen2-Audio has been in the latest Hugging face transformers and we advise you to build from source with command `pip install git+https://github.com/huggingface/transformers`, or you might encounter the following error:\nQwen2-Audio的代码已经包含在最新的 Hugging Face Transformers 的主分支中，我们建议您使用命令`pip install git+https://github.com/huggingface/transformers`\n```\nKeyError: 'qwen2-audio'\n```\n## 快速使用\n我们提供简单的示例来说明如何利用 🤗 Transformers 快速使用 Qwen2-Audio-7B 和 Qwen2-Audio-7B-Instruct。\n在开始前，请确保你已经配置好环境并安装好相关的代码包。最重要的是，确保你满足上述要求，然后安装相关的依赖库。\n接下来你可以开始使用 Transformers 或者 ModelScope 来使用我们的模型。目前Qwen2-Audio-7B 及 Qwen2-Audio-7B-Instruct 模型处理30秒以内的音频表现更佳。\n#### 🤗 Hugging Face Transformers\n如希望使用 Qwen2-Audio-7B-Instruct 进行推理，我们分别演示语音聊天和音频分析的交互方式，所需要写的只是如下所示的数行代码。\n##### 语音聊天推理\n在语音聊天模式下，用户可以自由地与 Qwen2-Audio 进行语音交互，无需文字输入：\n```python\nfrom io import BytesIO\nfrom urllib.request import urlopen\nimport librosa\nfrom transformers import Qwen2AudioForConditionalGeneration, AutoProcessor\n\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\")\nmodel = Qwen2AudioForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\", device_map=\"auto\")\n\nconversation = [\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/guess_age_gender.wav\"},\n    ]},\n    {\"role\": \"assistant\", \"content\": \"Yes, the speaker is female and in her twenties.\"},\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/translate_to_chinese.wav\"},\n    ]},\n]\ntext = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\naudios = []\nfor message in conversation:\n    if isinstance(message[\"content\"], list):\n        for ele in message[\"content\"]:\n            if ele[\"type\"] == \"audio\":\n                audios.append(librosa.load(\n                    BytesIO(urlopen(ele['audio_url']).read()), \n                    sr=processor.feature_extractor.sampling_rate)[0]\n                )\n\ninputs = processor(text=text, audios=audios, return_tensors=\"pt\", padding=True)\ninputs.input_ids = inputs.input_ids.to(\"cuda\")\n\ngenerate_ids = model.generate(**inputs, max_length=256)\ngenerate_ids = generate_ids[:, inputs.input_ids.size(1):]\n\nresponse = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n```\n##### 音频分析推理\n在音频分析中，用户可以提供音频和文字问题来实现对音频的分析：\n```python\nfrom io import BytesIO\nfrom urllib.request import urlopen\nimport librosa\nfrom transformers import Qwen2AudioForConditionalGeneration, AutoProcessor\n\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\")\nmodel = Qwen2AudioForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\", device_map=\"auto\")\n\nconversation = [\n    {'role': 'system', 'content': 'You are a helpful assistant.'}, \n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/glass-breaking-151256.mp3\"},\n        {\"type\": \"text\", \"text\": \"What's that sound?\"},\n    ]},\n    {\"role\": \"assistant\", \"content\": \"It is the sound of glass shattering.\"},\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"text\", \"text\": \"What can you do when you hear that?\"},\n    ]},\n    {\"role\": \"assistant\", \"content\": \"Stay alert and cautious, and check if anyone is hurt or if there is any damage to property.\"},\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/1272-128104-0000.flac\"},\n        {\"type\": \"text\", \"text\": \"What does the person say?\"},\n    ]},\n]\ntext = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\naudios = []\nfor message in conversation:\n    if isinstance(message[\"content\"], list):\n        for ele in message[\"content\"]:\n            if ele[\"type\"] == \"audio\":\n                audios.append(\n                    librosa.load(\n                        BytesIO(urlopen(ele['audio_url']).read()), \n                        sr=processor.feature_extractor.sampling_rate)[0]\n                )\n\ninputs = processor(text=text, audios=audios, return_tensors=\"pt\", padding=True)\ninputs.input_ids = inputs.input_ids.to(\"cuda\")\n\ngenerate_ids = model.generate(**inputs, max_length=256)\ngenerate_ids = generate_ids[:, inputs.input_ids.size(1):]\n\nresponse = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n```\n##### 批量推理\n我们也支持批量推理：\n```python\nfrom io import BytesIO\nfrom urllib.request import urlopen\nimport librosa\nfrom transformers import Qwen2AudioForConditionalGeneration, AutoProcessor\n\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\")\nmodel = Qwen2AudioForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\", device_map=\"auto\")\n\nconversation1 = [\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/glass-breaking-151256.mp3\"},\n        {\"type\": \"text\", \"text\": \"What's that sound?\"},\n    ]},\n    {\"role\": \"assistant\", \"content\": \"It is the sound of glass shattering.\"},\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/f2641_0_throatclearing.wav\"},\n        {\"type\": \"text\", \"text\": \"What can you hear?\"},\n    ]}\n]\n\nconversation2 = [\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/1272-128104-0000.flac\"},\n        {\"type\": \"text\", \"text\": \"What does the person say?\"},\n    ]},\n]\n\nconversations = [conversation1, conversation2]\n\ntext = [processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False) for conversation in conversations]\n\naudios = []\nfor conversation in conversations:\n    for message in conversation:\n        if isinstance(message[\"content\"], list):\n            for ele in message[\"content\"]:\n                if ele[\"type\"] == \"audio\":\n                    audios.append(\n                        librosa.load(\n                            BytesIO(urlopen(ele['audio_url']).read()), \n                            sr=processor.feature_extractor.sampling_rate)[0]\n                    )\n\ninputs = processor(text=text, audios=audios, return_tensors=\"pt\", padding=True)\ninputs['input_ids'] = inputs['input_ids'].to(\"cuda\")\ninputs.input_ids = inputs.input_ids.to(\"cuda\")\n\ngenerate_ids = model.generate(**inputs, max_length=256)\ngenerate_ids = generate_ids[:, inputs.input_ids.size(1):]\n\nresponse = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n```\n运行Qwen2-Audio-7B同样非常简单。\n```python\nfrom io import BytesIO\nfrom urllib.request import urlopen\nimport librosa\nfrom transformers import AutoProcessor, Qwen2AudioForConditionalGeneration\n\nmodel = Qwen2AudioForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-Audio-7B\" ,trust_remote_code=True)\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-Audio-7B\" ,trust_remote_code=True)\n\nprompt = \"<|audio_bos|><|AUDIO|><|audio_eos|>Generate the caption in English:\"\nurl = \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Audio/glass-breaking-151256.mp3\"\naudio, sr = librosa.load(BytesIO(urlopen(url).read()), sr=processor.feature_extractor.sampling_rate)\ninputs = processor(text=prompt, audios=audio, return_tensors=\"pt\")\n\ngenerated_ids = model.generate(**inputs, max_length=256)\ngenerated_ids = generated_ids[:, inputs.input_ids.size(1):]\nresponse = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n```\n#### 🤖 ModelScope\n我们强烈建议用户，特别是中国大陆地区的用户，使用 ModelScope。`snapshot_download` 可以帮助您解决下载检查点时遇到的问题。\n<br>\n## Demo\n### Web UI\n我们提供了 Web UI 的 demo 供用户使用。在开始前，确保已经安装如下代码库：\n```\npip install -r requirements_web_demo.txt\n```\n随后运行如下命令，并点击生成链接：\n```\npython demo/web_demo_audio.py\n```\n<br>\n\n## 样例展示\n更多样例将更新于[通义千问博客](https://qwenlm.github.io/blog/qwen2-audio)中的 Qwen2-Audio 博客。\n\n## 团队招聘\n\n我们是通义千问语音多模态团队，致力于以通义千问为核心，拓展音频多模态理解和生成能力，实现自由灵活的音频交互。目前团队蓬勃发展中，如有意向实习或全职加入我们，请发送简历至 `qwen_audio@list.alibaba-inc.com`.\n<br>\n\n## 使用协议\n\n请查看每个模型在其 Hugging Face 仓库中的许可证。您无需提交商业使用申请。\n<br>\n\n\n\n## 引用\n\n如果你觉得我们的论文和代码对你的研究有帮助，请考虑 :star: 和引用 :pencil: :)\n\n```BibTeX\n@article{Qwen-Audio,\n  title={Qwen-Audio: Advancing Universal Audio Understanding via Unified Large-Scale Audio-Language Models},\n  author={Chu, Yunfei and Xu, Jin and Zhou, Xiaohuan and Yang, Qian and Zhang, Shiliang and Yan, Zhijie  and Zhou, Chang and Zhou, Jingren},\n  journal={arXiv preprint arXiv:2311.07919},\n  year={2023}\n}\n```\n\n```BibTeX\n@article{Qwen2-Audio,\n  title={Qwen2-Audio Technical Report},\n  author={Chu, Yunfei and Xu, Jin and Yang, Qian and Wei, Haojie and Wei, Xipin and Guo,  Zhifang and Leng, Yichong and Lv, Yuanjun and He, Jinzheng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},\n  journal={arXiv preprint arXiv:2407.10759},\n  year={2024}\n}\n```\n## 联系我们\n\n如果你想给我们的研发团队和产品团队留言，请通过邮件（`qianwen_opensource@alibabacloud.com`）联系我们。\n<br>\n\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/qwen_voice/__init__.py\nimport time\nfrom typing import List, Tuple, Dict, Any, Union\nimport base64\nimport json\nimport librosa\nimport io\nfrom transformers import Qwen2AudioForConditionalGeneration, AutoProcessor\n\ndef get_meta(self):\n    return [\n        {\n            \"model_deploy_type\": \"proprietary\",\n            \"backend\": \"transformers\",\n            \"message_format\": True\n        }\n    ]\n\ndef process_input(ins: Union[str, List[Dict[str, Any]], Dict[str, Any]]):\n    if isinstance(ins, list) or isinstance(ins, dict):\n        return ins\n\n    content = []\n    try:\n        ins_json = json.loads(ins)\n    except:\n        return ins\n\n    if isinstance(ins_json, dict):\n        return ins_json\n\n    for item in ins_json:\n        if (\"audio\" in item or \"audio_url\" in item) and \"type\" not in item:\n            audio_data = item.get(\"audio\", item.get(\"audio_url\", \"\"))\n            if not audio_data.startswith(\"data:\"):\n                audio_data = \"data:audio/wav;base64,\" + audio_data\n\n            other_fields = {k: v for k, v in item.items() if k not in [\"audio\", \"text\", \"audio_url\"]}\n            content.append(\n                {\n                    \"audio_url\": {\"url\": audio_data, **other_fields},\n                    \"type\": \"audio\",\n                }\n            )\n\n        if \"text\" in item and \"type\" not in item:\n            text_data = item[\"text\"]\n            content.append({\"text\": text_data, \"type\": \"text\"})\n\n        if \"type\" in item and item[\"type\"] in [\"text\", \"audio\"]:\n            content.append(item)\n\n    if not content:\n        return ins\n\n    return content\n\ndef stream_chat(\n    self,\n    tokenizer,\n    ins: str,\n    his: List[Dict[str, str]] = [],\n    max_length: int = 4096,\n    top_p: float = 0.95,\n    temperature: float = 0.1,\n    **kwargs,\n):\n    messages = [\n        {\"role\": message[\"role\"], \"content\": process_input(message[\"content\"])}\n        for message in his\n    ] + [{\"role\": \"user\", \"content\": process_input(ins)}]\n\n    text = self.processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n    audios = []\n    for message in messages:\n        if isinstance(message[\"content\"], list):\n            for ele in message[\"content\"]:\n                if ele[\"type\"] == \"audio\":\n                    audio_data = ele[\"audio_url\"].split(\",\")[1]\n                    audio_bytes = base64.b64decode(audio_data)\n                    audio, _ = librosa.load(io.BytesIO(audio_bytes), sr=self.processor.feature_extractor.sampling_rate)\n                    audios.append(audio)\n\n    inputs = self.processor(text=text, audios=audios, return_tensors=\"pt\", padding=True)\n    inputs.input_ids = inputs.input_ids.to(self.device)\n\n    start_time = time.monotonic()\n    generate_ids = self.generate(**inputs, max_length=max_length)\n    generate_ids = generate_ids[:, inputs.input_ids.size(1):]\n\n    response = self.processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n    time_cost = time.monotonic() - start_time\n\n    return [\n        (\n            response,\n            {\n                \"metadata\": {\n                    \"request_id\": \"\",\n                    \"input_tokens_count\": inputs.input_ids.size(1),\n                    \"generated_tokens_count\": generate_ids.size(1),\n                    \"time_cost\": time_cost,\n                    \"first_token_time\": 0,\n                    \"speed\": float(generate_ids.size(1)) / time_cost if time_cost > 0 else 0,\n                }\n            },\n        )\n    ]\n\ndef init_model(model_dir, infer_params: Dict[str, str] = {}, sys_conf: Dict[str, str] = {}):\n    processor = AutoProcessor.from_pretrained(model_dir)\n    model = Qwen2AudioForConditionalGeneration.from_pretrained(model_dir, device_map=\"auto\")\n    model.processor = processor\n    import types\n    model.stream_chat = types.MethodType(stream_chat, model)\n    model.get_meta = types.MethodType(get_meta, model)    \n    return (model, None)\n\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/apps/llama_index/simple_retrieval.py\nfrom byzerllm.utils.retrieval import ByzerRetrieval\nfrom typing import Any, Callable, Dict, List, Optional, Tuple, Type, Union\nfrom byzerllm.utils.client import ByzerLLM\nfrom byzerllm.utils import generate_str_md5\nfrom byzerllm.utils.retrieval import ByzerRetrieval\nimport time\nfrom byzerllm.utils.client import LLMHistoryItem, LLMRequest\nfrom byzerllm.utils.retrieval import TableSettings, SearchQuery\nimport uuid\nimport json\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\ntry:\n    from termcolor import colored\nexcept ImportError:\n\n    def colored(x, *args, **kwargs):\n        return x\n\n\ntry:\n    import jieba\nexcept ImportError:\n    jieba = None\n\n\nclass SimpleRetrieval:\n    def __init__(\n        self,\n        llm: ByzerLLM,\n        retrieval: ByzerRetrieval,\n        chunk_collection: Optional[str] = \"default\",\n        retrieval_cluster: str = \"byzerai_store\",\n        retrieval_db: str = \"byzerai_store\",\n        max_output_length=10000,\n    ):\n        self.chunk_collection = chunk_collection\n        self.retrieval_cluster = retrieval_cluster\n        self.retrieval_db = retrieval_db\n        self.max_output_length = max_output_length\n        self.llm = llm\n        self.retrieval = retrieval\n        if self.llm.default_emb_model_name is None:\n            raise Exception(\n                f\"\"\"\nemb model does not exist. Try to use `llm.setup_default_emb_model_name` to set the default emb model name.\n\"\"\"\n            )\n\n        # create the retrieval database/table if not exists\n        if self.retrieval and not self.retrieval.check_table_exists(\n            self.retrieval_cluster, self.retrieval_db, \"text_content\"\n        ):\n            self.retrieval.create_table(\n                self.retrieval_cluster,\n                tableSettings=TableSettings(\n                    database=self.retrieval_db,\n                    table=\"text_content\",\n                    schema=\"\"\"st(\nfield(_id,string),\nfield(doc_id,string),\nfield(file_path,string),\nfield(owner,string),\nfield(content,string,analyze),\nfield(raw_content,string,no_index),\nfield(metadata,string,analyze),\nfield(json_data,string,no_index),\nfield(collection,string),\nfield(created_time,long,sort)\n)\"\"\",\n                    location=f\"/tmp/{self.retrieval_cluster}\",\n                    num_shards=1,\n                ),\n            )\n\n            self.retrieval.create_table(\n                self.retrieval_cluster,\n                tableSettings=TableSettings(\n                    database=self.retrieval_db,\n                    table=\"text_content_chunk\",\n                    schema=\"\"\"st(\nfield(_id,string),\nfield(doc_id,string),\nfield(file_path,string),\nfield(owner,string),\nfield(chunk,string,analyze),\nfield(raw_chunk,string,no_index),\nfield(chunk_vector,array(float)),\nfield(metadata,string,analyze),\nfield(json_data,string,no_index),\nfield(chunk_collection,string),\nfield(created_time,long,sort)\n)\"\"\",\n                    location=f\"/tmp/{self.retrieval_cluster}\",\n                    num_shards=1,\n                ),\n            )\n\n    def save_doc(\n        self,\n        data: List[Dict[str, Any]],\n        owner: Optional[str] = None,\n    ):\n\n        if not self.retrieval:\n            raise Exception(\"retrieval is not setup\")\n\n        owner = owner or \"default\"\n\n        result = []\n        for item in data:\n\n            doc_id = item[\"doc_id\"]\n            json_data_obj = json.loads(item[\"json_data\"])\n            collection = item[\"collection\"]\n            _id = f\"{collection}/{doc_id}\"\n\n            file_path = json_data_obj.get(\"metadata\", {}).get(\"file_path\", \"\")\n            result.append(\n                {\n                    \"_id\": _id,\n                    \"doc_id\": doc_id,\n                    \"file_path\": file_path,\n                    \"json_data\": item[\"json_data\"],\n                    \"metadata\": self.search_tokenize(item[\"json_data\"]),\n                    \"collection\": collection,\n                    \"content\": self.search_tokenize(item[\"content\"]),\n                    \"raw_content\": item[\"content\"],\n                    \"owner\": owner,\n                    \"created_time\": int(time.time() * 1000),\n                }\n            )\n\n        self.retrieval.build_from_dicts(\n            self.retrieval_cluster, self.retrieval_db, \"text_content\", result\n        )\n\n    def _owner_filter(self, owner: str):\n        return {\"field\": \"owner\", \"value\": owner}\n\n    def search_content_chunks(\n        self,\n        owner: str,\n        query_str: Optional[str] = None,\n        query_embedding: Optional[List[float]] = None,\n        doc_ids: Optional[List[str]] = None,\n        limit: int = 4,\n        return_json: bool = True,\n    ):\n        keyword = None\n        fields = []\n        vector = []\n        vectorField = None\n        filters = {\n            \"and\": [{\"field\": \"chunk_collection\", \"value\": self.chunk_collection}]\n        }\n\n        if query_str is not None:\n            keyword = self.search_tokenize(query_str)\n            fields = [\"chunk\"]\n\n        if query_embedding is not None:\n            vector = query_embedding\n            vectorField = \"chunk_vector\"\n\n        if doc_ids:\n            filters[\"and\"].append(\n                {\"or\": [{\"field\": \"doc_id\", \"value\": x} for x in doc_ids]}\n            )\n\n        query = SearchQuery(\n            self.retrieval_db,\n            \"text_content_chunk\",\n            filters=filters,\n            keyword=keyword,\n            fields=fields,\n            vector=vector,\n            vectorField=vectorField,\n            limit=limit,\n        )\n\n        docs = self.retrieval.search(self.retrieval_cluster, [query])\n\n        if return_json:\n            context = json.dumps(\n                [{\"content\": x[\"raw_chunk\"]} for x in docs],\n                ensure_ascii=False,\n                indent=4,\n            )\n            return context\n        else:\n            return docs\n\n    def search_content_by_filename(self, filename: str, collection: str):\n        filters = [{\"field\": \"chunk_collection\", \"value\": collection}]\n        docs = self.retrieval.search(\n            self.retrieval_cluster,\n            [\n                SearchQuery(\n                    self.retrieval_db,\n                    \"text_content_chunk\",\n                    filters={\"and\": filters},\n                    keyword=self.search_tokenize(filename),\n                    fields=[\"metadata\"],\n                    vector=[],\n                    vectorField=None,\n                    limit=10000,\n                )\n            ],\n        )\n        return docs\n\n    def search_content(\n        self,\n        q: str,\n        owner: str,\n        url: str,\n        auth_tag: str = None,\n        limit: int = 4,\n        return_json: bool = True,\n    ):\n        filters = [self._owner_filter(owner)]\n\n        if auth_tag:\n            filters.append(\n                {\"field\": \"auth_tag\", \"value\": self.search_tokenize(auth_tag)}\n            )\n\n        if url:\n            filters.append({\"field\": \"url\", \"value\": url})\n\n        if q:\n            keyword = self.search_tokenize(q)\n            vector = self.emb(q)\n            vectorField = \"content_vector\"\n            fields = [\"content\"]\n        else:\n            keyword = None\n            vector = []\n            vectorField = None\n            fields = []\n\n        docs = self.retrieval.search(\n            self.retrieval_cluster,\n            [\n                SearchQuery(\n                    self.retrieval_db,\n                    \"text_content\",\n                    filters={\"and\": filters},\n                    keyword=keyword,\n                    fields=fields,\n                    vector=vector,\n                    vectorField=vectorField,\n                    limit=limit,\n                )\n            ],\n        )\n\n        if return_json:\n            context = json.dumps(\n                [{\"content\": x[\"raw_content\"]} for x in docs],\n                ensure_ascii=False,\n                indent=4,\n            )\n            return context\n        else:\n            return docs\n\n    def get_chunk_by_id(self, chunk_id: str):\n        filters = {\"and\": [{\"field\": \"_id\", \"value\": chunk_id}]}\n        docs = self.retrieval.filter(\n            self.retrieval_cluster,\n            [\n                SearchQuery(\n                    self.retrieval_db,\n                    \"text_content_chunk\",\n                    filters=filters,\n                    keyword=None,\n                    fields=[],\n                    vector=[],\n                    vectorField=None,\n                    limit=1,\n                )\n            ],\n        )\n        return docs\n\n    def get_chunks_by_docid(self, doc_id: str):\n        docs = self.retrieval.filter(\n            self.retrieval_cluster,\n            [\n                SearchQuery(\n                    self.retrieval_db,\n                    \"text_content_chunk\",\n                    filters={\"and\": [{\"field\": \"doc_id\", \"value\": doc_id}]},\n                    keyword=None,\n                    fields=[],\n                    vector=[],\n                    vectorField=None,\n                    limit=100000,\n                )\n            ],\n        )\n        return docs\n\n    def delete_chunk_by_ids(self, chunk_ids: List[str]):\n        self.retrieval.delete_by_ids(\n            self.retrieval_cluster, self.retrieval_db, \"text_content_chunk\", chunk_ids\n        )\n        self.retrieval.commit(self.retrieval_cluster, self.retrieval_db)\n\n    def save_chunks(self, chunks: List[Dict[str, Any]]):\n        text_content_chunks = []\n        for chunk in chunks:\n            chunk_id = chunk[\"chunk_id\"]\n            ref_doc_id = chunk[\"ref_doc_id\"]\n            owner = chunk.get(\"owner\", \"default\")\n            chunk_content = chunk[\"chunk_content\"]\n            chunk_embedding = chunk[\"chunk_embedding\"]\n            metadata = chunk.get(\"metadata\", {})\n            _id = f\"{self.chunk_collection}/{chunk_id}\"\n            file_path = metadata.get(\"file_path\", \"\")\n\n            text_content_chunks.append(\n                {\n                    \"_id\": _id,\n                    \"doc_id\": ref_doc_id or \"\",\n                    \"file_path\": file_path,\n                    \"owner\": owner or \"default\",\n                    \"chunk\": self.search_tokenize(chunk_content),\n                    \"raw_chunk\": chunk_content,\n                    \"chunk_vector\": chunk_embedding,\n                    \"chunk_collection\": self.chunk_collection,\n                    \"metadata\": self.search_tokenize(\n                        json.dumps(metadata, ensure_ascii=False)\n                    ),\n                    \"json_data\": json.dumps(metadata, ensure_ascii=False),\n                    \"created_time\": int(time.time() * 1000),\n                }\n            )\n\n        self.retrieval.build_from_dicts(\n            self.retrieval_cluster,\n            self.retrieval_db,\n            \"text_content_chunk\",\n            text_content_chunks,\n        )\n\n    def get_doc(self, doc_id: str, collection: str):\n        filters = {\"and\": [{\"field\": \"_id\", \"value\": f\"{collection}/{doc_id}\"}]}\n        docs = self.retrieval.filter(\n            self.retrieval_cluster,\n            [\n                SearchQuery(\n                    self.retrieval_db,\n                    \"text_content\",\n                    filters=filters,\n                    keyword=None,\n                    fields=[],\n                    vector=[],\n                    vectorField=None,\n                    limit=1,\n                )\n            ],\n        )\n        return docs[0] if docs else None\n\n    @DeprecationWarning\n    def delete_doc(self, doc_ids: List[str], collection: str):\n        ids = []\n        for doc_id in doc_ids:\n            ids.append(f\"{collection}/{doc_id}\")\n        self.retrieval.delete_by_ids(\n            self.retrieval_cluster, self.retrieval_db, \"text_content\", [ids]\n        )\n\n    def truncate_table(self):\n        self.retrieval.truncate(\n            self.retrieval_cluster, self.retrieval_db, \"text_content\"\n        )\n        self.retrieval.truncate(\n            self.retrieval_cluster, self.retrieval_db, \"text_content_chunk\"\n        )\n        self.commit_doc()\n        self.commit_chunk()\n\n    def delete_doc_and_chunks_by_filename(self, collection: str, file_name: str):\n        doc = self.get_doc(f\"{collection}/ref_doc_info\", file_name)\n\n        if doc:\n            node_ids = json.loads(doc[\"metadata\"])[\"node_ids\"]\n            for node_id in node_ids:\n                id = f\"{collection}/data/{node_id}\"\n                self.retrieval.delete_by_ids(\n                    self.retrieval_cluster, self.retrieval_db, \"text_content\", [id]\n                )\n\n                id = f\"{collection}/metadata/{node_id}\"\n                self.retrieval.delete_by_ids(\n                    self.retrieval_cluster, self.retrieval_db, \"text_content\", [id]\n                )\n\n                ## default/index/ can not be deleted\n\n            ##  cleanup the chunk collection\n            self.retrieval.delete_by_filter(\n                self.retrieval_cluster,\n                self.retrieval_db,\n                \"text_content_chunk\",\n                {\"file_path\": file_name},\n            )\n\n    def delete_from_doc_collection(self, collection: str):\n        for suffix in [\"index\", \"data\", \"ref_doc_info\", \"metadata\"]:\n            self.retrieval.delete_by_filter(\n                self.retrieval_cluster,\n                self.retrieval_db,\n                \"text_content\",\n                {\"collection\": f\"{collection}/{suffix}\"},\n            )\n\n    def delete_from_chunk_collection(self, collection: str):\n        self.retrieval.delete_by_filter(\n            self.retrieval_cluster,\n            self.retrieval_db,\n            \"text_content_chunk\",\n            {\"chunk_collection\": collection},\n        )\n\n    def commit_doc(self):\n        self.retrieval.commit(self.retrieval_cluster, self.retrieval_db, \"text_content\")\n\n    def commit_chunk(self):\n        self.retrieval.commit(\n            self.retrieval_cluster, self.retrieval_db, \"text_content_chunk\"\n        )\n\n    def emb(self, s: str):\n        return self.llm.emb(self.llm.default_emb_model_name, LLMRequest(instruction=s))[\n            0\n        ].output\n\n    def split_text_into_chunks(self, s: str):\n        # self.llm.apply_sql_func(\n        #     '''select llm_split(value,array(\",\",\"。\",\"\\n\"),1600) as value ''',[{\"value\":content}],\n        #     url=self.byzer_engine_url\n        #     )[\"value\"]\n        text_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=1600, chunk_overlap=200\n        )\n        split_docs = text_splitter.split_documents([Document(page_content=s)])\n        return [s.page_content for s in split_docs]\n\n    def search_tokenize(self, s: str):\n        seg_list = jieba.cut(s, cut_all=False)\n        # return self.llm.apply_sql_func(\"select mkString(' ',parse(value)) as value\",[\n        # {\"value\":s}],url=self.byzer_engine_url)[\"value\"]\n        return \" \".join(seg_list)\n\n\n     \n\n下面是用户的需求：\n\n@simple_retrieval.py 修正jieba 的 ImportError, 同时打印warning\n严格遵循 SEARCH/REPLACE 格式,如果是python代码,注意空格和缩进\n"}, {"role": "assistant", "content": "```python\n##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/apps/llama_index/simple_retrieval.py\n<<<<<<< SEARCH\ntry:\n    import jieba\nexcept ImportError:\n    jieba = None\n=======\ntry:\n    import jieba\nexcept ImportError:\n    print(\"Warning: jieba module is not installed. Some functionalities may be limited.\")\n    jieba = None\n>>>>>>> REPLACE\n```"}], "model": "deepseek_chat", "yaml_file": "actions/041_chat_action.yml"}
